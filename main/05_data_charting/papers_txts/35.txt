arXiv:2311.09608v1 [physics.geo-ph] 16 Nov 2023

Deep Neural Helmholtz Operators for 3D Elastic Wave
Propagation and Inversion
1

Caifeng Zou1 , Kamyar Azizzadenesheli2 , Zachary E. Ross1 , and Robert W. Clayton1
Seismological Laboratory, California Institute of Technology, Pasadena, CA 91125, USA
2
Nvidia Corporation, Santa Clara, CA 95051, USA

Abstract
Numerical simulations of seismic wave propagation in heterogeneous 3D media are central to
investigating subsurface structures and understanding earthquake processes, yet are computationally expensive for large problems. This is particularly problematic for full waveform inversion,
which typically involves numerous runs of the forward process. In machine learning there has been
considerable recent work in the area of operator learning, with a new class of models called neural
operators allowing for data-driven solutions to partial differential equations. Recent works in
seismology have shown that when neural operators are adequately trained, they can significantly
shorten the compute time for wave propagation. However, the memory required for the 3D
time domain equations may be prohibitive. In this study, we show that these limitations can be
overcome by solving the wave equations in the frequency domain, also known as the Helmholtz
equations, since the solutions for a set of frequencies can be determined in parallel. The 3D
Helmholtz neural operator is 40 times more memory-efficient than an equivalent time-domain
version. We employ a U-shaped neural operator for 2D and 3D elastic wave modeling, achieving
two orders of magnitude acceleration compared to a baseline spectral element method. The
neural operator accurately generalizes to variable velocity structures and can be evaluated on
denser input meshes than used in the training simulations. We also show that when solving
for wavefields strictly on the surface, the accuracy can be significantly improved via a graph
neural operator layer. In leveraging automatic differentiation, the proposed method can serve
as an alternative to the adjoint-state approach for 3D full-waveform inversion, reducing the
computation time by a factor of 350.

Keywords: 3D elastic wave propagation; Helmholtz equations; Machine learning; Neural operators;
Full-waveform inversion; Automatic differentiation.

1

Introduction

Numerical simulations of seismic wave propagation serve as the foundation for a wide array of
seismological investigations, including subsurface imaging, ground motion simulation for seismic
hazard assessment, and deriving earthquake source properties. However, computational cost and
memory requirements become major concerns for large 3D problems, as well as inverse problems,
which require numerous evaluations of the forward model. In addition, the availability of denser
seismic networks spanning wider regions offers an unprecedented wealth of data to be used. Therefore,

1

accelerating seismic wave modeling has become a pressing need for modern seismology to align with
the rapid advancements in big data and computer capabilities.
Traditional methods for solving partial differential equations (PDEs) are based on brute-force
numerical schemes that discretize a physical domain and solve a governing equation on a grid.
This discretization introduces a trade-off between the speed and accuracy: coarser grids provide
faster results but with reduced accuracy, while finer grids offer higher accuracy at the cost of slower
computations. In seismology, the finite difference method (FDM) and finite element method (FEM)
are generally the most common approaches for numerical simulation of wave propagation (Kelly
et al., 1976; Olsen, 2000; Fichtner et al., 2009; De Basabe & Sen, 2009). As a version of FEM, the
spectral element method (SEM) developed by Komatitsch et al. (2002) combines the higher-order
accuracy from spectral methods with the flexibility of FEMs. For any of the above-mentioned
solvers, a certain number of elements or grid points per seismic wavelength are required to achieve a
specific level of accuracy. The most demanding calculations are at high frequencies and the slowest
parts require the densest grids. The computational expense remains a bottleneck for conventional
numerical methods, especially in 3D.
The recent emergence of machine learning approaches has the potential to overcome the limitations
of conventional PDE solvers in computing speed and cost, especially for large inverse problems
(Raissi et al., 2019; Li et al., 2020a,b; Khoo et al., 2021; Lu et al., 2021). To solve PDEs with
neural networks, one can simply discretize the input (e.g., the elastic properties of the continuum)
and output (e.g., the displacement wavefield) function spaces into finite-dimensional grids, which
naturally fits into a standard neural network framework such as the convolutional neural network
(Guo et al., 2016; Zhu & Zabaras, 2018; Winovich et al., 2019; Moseley et al., 2020b). However,
this approach is limited to a specific discretization and cannot provide solutions for new, abitrary
meshes within the same physical domain. The so-called physics-informed neural networks (PINNs)
(Raissi et al., 2019) allow for the querying of solutions at new points by directly parameterizing the
solution function as a neural network. PINNs penalize the residual of the governing equation in the
loss function, which can be trained on their own and do not necessarily need data from a separate
numerical solver.
In the last few years, there have been a number of studies applying PINNs to seismological problems.
They have been used successfully to solve Eikonal equations and derive earthquake hypocenters
(Smith et al., 2020, 2022). PINNs have been used to solve the 2D and 3D scattered forms of the
frequency-domain acoustic wave equations (Song & Alkhalifah, 2021; Alkhalifah et al., 2021; Song
et al., 2021; Huang & Alkhalifah, 2022a,b). Other applications lie in acoustic wave propagation
(Moseley et al., 2020a; Song et al., 2022), inversion (Rasht-Behesht et al., 2022; Zhang et al., 2023b),
and wavefield-modeling-based source imaging (Huang & Alkhalifah, 2023). Compared to acoustic
wave equations, the use of PINNs for the more computationally expensive solutions to elastic wave
equations is sparsely documented (Ren et al., 2022; Song et al., 2023).
However, in most realistic cases these models only learn a specific instance of the PDE and generally
require training a new network for every new instance of the PDE parameters (e.g., elastic media and
source properties). To address the shortcomings of the above-mentioned methods, Li et al. (2020b)
proposed the discretization-invariant neural operator that learns an entire family of PDEs, removing
the need for retraining with varied PDE coefficients. The universal approximation theorem (Hornik
et al., 1989) serves as the theoretical foundation for the neural operators composed of linear operators
2

and nonlinear activations to approximate any given nonlinear continuous operator (Kovachki et al.,
2023).
Neural operators have seen increasing usage in seismology in recent years. The appeal of neural
operators is that one can train a single deep learning model and apply it to arbitrary PDE parameter
functions. Neural operators have been explored for 2D acoustic (Yang et al., 2021; Li et al., 2022),
elastic (Yang et al., 2023; Zhang et al., 2023a), and viscoelastic wave modeling (Wei & Fu, 2022),
high-frequency wavefield extrapolation (Song & Wang, 2022), and earthquake locating (Sun et al.,
2022). The only documented application by far in 3D seismic wave propagation was conducted by
Lehmann et al. (2023), who simulated the ground motions by mapping the depth of the geological
model to the time of the velocity wavefields.
Most of the applications have thus-far been limited to 2D, as the amount of GPU memory required
for 3D wave propagation in the time domain can quickly end up in the range of hundreds of
GB. This is because the time domain problem requires a 4D neural operator, and so the memory
requirements grow extremely quickly. One way to address this issue is to solve the wave equations in
the frequency domain, which are also referred to as the Helmholtz equations. In this way, the 4D
machine learning model can be simplified to a 3D model. Moreover, individual frequencies can be
handled independently, benefitting further from parallelized computing. The time-domain solutions
can be obtained by taking the inverse Fourier transform of the frequency-domain solutions.
In this study, our contributions are as follows. We propose a neural operator for 3D elastic wave
propagation and inversion. We train a general solution operator, parameterized as a U-shaped
deep neural operator, to solve the Helmholtz equations for variable velocity structures, source
locations, mesh discretizations, and multiple frequencies. We provide options to model either
complete wavefields or ground motions only, with the latter being more accurately predicted via a
graph neural operator (GNO) layer (Li et al., 2020b). The size of the Helmholtz neural operator
is only one-fortieth of that of an equivalent 3D time domain version. The 3D Helmholtz neural
operator is roughly 100 times faster than the baseline numerical solvers for forward propagation
and 350 times faster for inverse problems. The trained neural operator combined with automatic
differentiation facilitates rapid full-waveform inversion (FWI) with accuracy measured by relative L2
misfit of 0.03.

2

Methods

2.1

Elastic wave equations

The time-domain elastic wave equation for inhomogeneous, isotropic media is given by
h
i
∂2u
ρ 2 =∇λ (∇ · u) + ∇µ · ∇u + (∇u)T +
∂t
(λ + 2µ) ∇ (∇ · u) − µ∇ × ∇ × u + f,

(1)

where ρ is the density, u is the displacement vector, λ and µ are the Lamé parameters, and f is
the body force (or source term). Equation 1 can be expressed in terms of P-wave velocity VP and
S-wave velocity VS through
s
r
λ + 2µ
µ
VP =
, VS =
.
(2)
ρ
ρ
3

The time-dependent wave equation is an initial value problem, which can be solved by explicit or
implicit time-stepping schemes. The Fourier transform of the time-domain equations are referred to
as the Helmholtz equations:
 


∂
∂Ux ∂Uy
∂Ux
∂Uz
2
ω ρUx +
λ
+ 2µ
+
+
∂x
∂x
∂y
∂z
∂x
 

∂Ux ∂Uy
∂
µ
+
+
∂y
∂y
∂x
 

∂
∂Ux ∂Uz
+
µ
= −Fx ,
+
∂z
∂z
∂x
 

∂
∂Ux ∂Uy
2
ω ρUy +
µ
+
∂x
∂y
∂x
 


∂Uy
∂Ux ∂Uy
∂Uz
∂
λ
+ 2µ
+
+
(3)
+
∂y
∂x
∂y
∂z
∂y
 

∂Uy
∂
∂Uz
+
µ
+
= −Fy ,
∂z
∂z
∂y
 

∂Ux ∂Uz
∂
µ
+
ω 2 ρUz +
∂x
∂z
∂x
 

∂Uy
∂
∂Uz
+
µ
+
∂y
∂z
∂y
 


∂
∂Ux ∂Uy
∂Uz
∂Uz
+
λ
+
+
+ 2µ
= −Fz ,
∂z
∂x
∂y
∂z
∂z
where ω is the angular frequency, U = (Ux , Uy , Uz ) is the displacement vector in the frequency
domain, and F = (Fx , Fy , Fz ) is the frequency-domain body force. The wave propagation in the
frequency domain becomes a boundary value problem, which can be expressed in a more compact,
discretized formulation (Pratt, 1990):
L (ω) [U (ω)] = −F (ω) ,

(4)

where L (ω) is an N × N matrix and N is the number of discretization points. In this linear
system, the frequency-domain wavefield U (ω) is related to the source term F (ω) via a sparse matrix
L (ω). The solution to Equation 4 can be obtained using either direct or iterative solvers. Direct
solvers, such as lower-upper (LU) decomposition or Cholesky factorization, suffer from intensive
memory requirements and prohibitive computational time for large linear systems. Iterative solvers
heavily rely on a well-designed preconditioner to prevent divergence or slow convergence (Huang &
Greenhalgh, 2021).

2.2

Background on Neural Operators

Neural operators (Li et al., 2020b) are a class of models composed of linear integral operators and
nonlinear activations. Such models are universal approximators of arbitrary nonlinear continuous
operators (Kovachki et al., 2023). Let La be a linear differential operator determined by parameter
a and consider the following PDE:
(La u) (x) = f (x) , x ∈ D,
4

(5)

where u is the PDE solution, f is a fixed function such as the source term in a wave equation, and
D ⊂ Rd is a bounded, open set. The Green’s function G is defined as the unique solution to
La G (x, ·) = δx ,

(6)

where δx is the Dirac delta function centered at x. Note that G depends on a, so the solution to
Equation 5 is given by
Z
u (x) =

Ga (x, y) f (y) dy.

(7)

D

In seismology, Equation 7 remains a linear system when the velocity structure, a(x) is fixed, and
describes the mapping from the input source function f (x) to the output wavefield solution u (x).
This study considers a more complicated problem, however, in which we aim to obtain a solution
operator that maps from a(x) to u(x). This operator becomes nonlinear and is not known in closed
form; here we instead aim to approximate it with a learnable parametric model, i.e. a neural operator.
These neural operators incorporate a nonlinear point-wise activation function following each linear
integral operator. A neural operator containing L layers is formulated with an iterative architecture
in the following manner:
v0 (x) = (P a) (x) ,


Z
κl (x, y) vl (y) dy ,
vl+1 (x) = σ Wl vl (x) +

(8)

D

u (x) = (Q vL ) (x) , l = 0, .., L − 1,
where a (x) is the input function(s) (e.g., velocity and source functions), u (x) is the output function(s)
(e.g., displacement wavefields), vl is the hidden representation of the lth layer and is input to the
next layer, P is a point-wise operator lifting the input to a higher dimensional representation, Q is a
point-wise operator projecting the last hidden representation to the output dimensionality, Wl is
a point-wise operator included to learn nonperiodic behavior on the boundaries of the domain, κl
is a parametric kernel function, and σ is a point-wise nonlinear activation. In order to speed up
computations, Li et al. (2020a) proposed the Fourier neural operator (FNO) which replaces the
kernel integral operator with a convolution operator defined in the Fourier space:
Z
κl (x, y) vl (y) dy = F −1 (F (κl ) · F (vl )) ,
(9)
D

where F and F −1 denote the Fourier transform and inverse Fourier transform, respectively.
Since the development of FNO, other neural operator models have been developed to improve upon
it further. Inspired by the U-net (Ronneberger et al., 2015), Rahman et al. (2022) designed a
U-shaped neural operator (U-NO) that allows for much deeper models by progressive contraction and
expansion of the physical domain and co-domain. These deeper models achieve better performance
and memory usage, and are the choice used in this study. Specifically, we use a 6-layer U-NO taking
the FNO as the inner integral operator, as illustrated in Fig. 1.
Note that in this study the U-NO learns the solution in the frequency domain, mainly to break
the memory bottleneck. For a 3D problem, the solution to the time-domain wave equation has a
form of u(x, y, z, t). This requires a neural operator with four dimensions, which will be significantly
5

memory demanding. However, the solution to the frequency-domain wave equation (Helmholtz
equation) can be divided by frequencies. Each individual frequency has its independent solution in
the form of Uω (x, y, z), so that a 3D neural operator is qualified. For the 3D problems in this study,
an equivalent time-domain neural operator can take up 215 GB only for model parameters. But
with the proposed Helmholtz neural operator, the model size can be reduced by a factor of 40. In
addition to saving model memory, this formulation enables parallelization at the frequency level to
accelerate model training and reduce data memory.

2.3

Automatic Differentiation

Automatic differentiation (AD) exploits the fact that all numerical computations can be seen as a
sequence of elementary operations with known derivatives, and by applying the chain rule to the
derivatives of the constituent operations we can obtain the derivative of the overall composition
(Griewank & Walther, 2008). AD falls between numerical differentiation and symbolic differentiation.
It differentiates common functions and expressions using a symbolic differentiation approach and
populates the obtained derivatives with numerical values, saving the intermediate results. These
saved results are then combined to obtain the final desired value. AD is compatible not only with
closed-form expressions but also with control flow structures such as loops, branching, recursion,
and procedure calls, because they will all be translated into numeric evaluation traces which form
the basis of AD (Baydin et al., 2018).
There are two modes of automatic differentiation, the forward mode and the reverse mode. In the
forward mode, derivatives of all dependent variables with respect to a certain independent variable
are computed simultaneously with the forward propagation of the computational graph. Hence,
the forward accumulation is more efficient in scenarios where the number of independent variables
is significantly smaller than the number of dependent variables. However, in machine learning
practice, the primary focus revolves around computing the gradients of a scalar-valued objective
function with respect to a large number of parameters. This establishes the reverse-mode automatic
differentiation as the cornerstone of the backpropagation algorithm. As opposed to the forward mode,
the reverse-mode AD is a two-phase process. Firstly, a forward pass of the code is implemented
to populate the intermediate variables and record the dependencies in the computational graph.
Secondly, gradients are computed by propagating the derivatives back from the objective function to
the parameters of interest using the chain rule. These gradients are later employed by a gradient
descent method to iteratively update the parameters of interest, aiming to minimize the objective
function.

3

Data

In this study, we train neural operators with supervised learning, i.e. a set of previously computed
numerical simulations are used to learn from. We use a SEM software package named SALVUS
(Afanasiev et al., 2019) to generate 31000 simulations for both 2D and 3D data sets, hereafter
referred to as the ground truth. Within each simulation data set, 27000 simulations are used for
neural operator training, 3000 simulations are used to validate the model hyperparameters, and
1000 simulations are used to test the generalizability of the trained models. To further demonstrate
the generalization ability, a 3D overthrust velocity model depicting complex thrusting overlying an
6

extensional and rift sequence (Aminzadeh et al., 1994) is used for U-NO-based waveform modeling
and full-waveform inversion.
For the SEM solver, the computational domain is 5 km × 5 km (× 5 km). We take one element per
wavelength at the maximum frequency and a polynomial degree of 4, as recommended in the SALVUS
documentation. This combination should provide sufficient grid points to sample the domain. The
time step for numerical simulation is set to 0.002 s to satisfy the Courant–Friedrichs–Lewy (CFL)
(Courant et al., 1967) condition. The displacement wavefield is excited with a Ricker wavelet with a
central frequency of 3 Hz. The spatial component of this source is a delta function, which in practice
is approximated by a narrow Gaussian for differentiability. The Ricker source is configured as an
isotropic explosive source randomly placed within the computational domain. The S-wave velocity
(VS ) models are generated from a background of 3 km/s perturbed by von Kármán-type random
fields (Von Karman, 1948) with a Hurst exponent of 0.5, correlation length of 8 grid cells, and a
fractional magnitude of the fluctuation set to 10%. Several authors have shown that the von Kármán
correlation function can represent the Earth’s inhomogeneities(Chemingui, 2001; Mai & Beroza,
2002; Nakata & Beroza, 2015). The P- to S-wave velocity ratio (VP /VS ) models are generated from
a background of 1.732 perturbed by Gaussian random fields with a correlation length of 32 grid cells
and standard deviation of 2%. The P-wave velocity (VP ) models are obtained by multiplying VS
by VP /VS and the density is set to a constant 2.7 g/cm³. The free-surface condition is set for the
top boundary, while absorbing boundary conditions (Clayton & Engquist, 1977) are configured for
the other five sides in 3D or three edges in 2D. Displacement wavefields with a total duration of 2.5
s are simulated in both 2D and 3D. Each 2D simulation takes about 1.6 s using one GPU with a
memory usage of 0.3 GB and each 3D simulation takes about 30 s using one GPU with a memory
usage of 0.8 GB.
The source-time functions and solutions to the elastic wave equations from SALVUS are Fourier
transformed for use with U-NO. As our model computes a solution for a single frequency component,
the number of training samples available for learning is the number of simulations in time multiplied
by the number of frequencies of interest. Alternatively, if a Helmholtz solver is available, it would be
possible to directly generate solutions for a desired frequency and avoid this extra computational
overhead. The input to U-NO consists of only spatial functions, comprising VP , VS , the complexvalued source term indicating the source location, and the frequency desired given as a spatiallyconstant function. The output of U-NO is either a 2D or 3D complex-valued displacement wavefield
that corresponds to the given frequency input. By evaluating the forward model for multiple
frequencies, we can return to the time domain by taking the inverse Fourier transform. In this
formulation, frequencies of interest can be tackled by U-NO in parallel, making the problem highly
efficient for use with GPUs. Both the input and output data are standardized using the statistics of
training data for basic pre-processing.
Note that neural operators are not required to adhere to the spatial and temporal discretization
schemes of the numerical solver. For computational efficiency, the time step for U-NO is set to 0.05
s, with the Nyquist-Shannon sampling theorem taken into account. The spatial discretization adopts
a regular mesh of 64 × 64 (× 64) covering the same physical domain as the SEM solver. To improve
computational efficiency further, frequencies with little energy in the power density spectrum of the
wavefield are eliminated. The significant frequencies range from 1.6 Hz to 6 Hz with an interval of
0.4 Hz determined by the 2.5-s duration, which are able to reconstruct the time-domain waveform.

7

Thus by evaluating the solution in the Fourier domain, we can gain performance advantages by not
computing solutions for unnecessary frequencies.

4

Results

4.1

2D elastic wave modeling with U-NO

The 2D U-NO has 65,652,068 trainable parameters taking up 0.24 GB. The model architecture,
depicted in the Methods section, is determined through trial and error based on the validation
performance. The other hyperparameters are also optimized with the validation set. To strike a
balance between accuracy and robustness, we adopt a loss function comprising the relative L1 norm
with a weight of 0.9 plus the relative L2 norm with a weight of 0.1. A batch size of 32 is used for
neural operator training. We employ an Adam (adaptive moment estimation) (Kingma & Ba, 2014)
optimizer with a learning rate of 0.001 and a learning rate scheduler that decays the learning rate
by half every 30 epochs. The machine learning model is trained for 100 epochs until the validation
set converges well, as shown in Fig. 2. Each epoch takes around 15 minutes using eight NVIDIA
RTX A6000 GPUs, each with an average memory usage of 1.3 GB. Note that the model can be
trained with just a single GPU, but here we want to take advantage of PyTorch’s parallel computing
capability, especially for the many frequencies that can be handled independently. Once the U-NO is
trained, one run of the 2D elastic wave modeling with new velocity parameters and source locations
for all the frequencies of interest only takes 0.03 s using one GPU with a memory usage of 0.7 GB,
achieving a 53-fold acceleration in comparison to the SEM solver.
Fig. 2 shows that the U-NO is able to fit the training data well and has an impressive predictive
performance on the validation set after hyperparameter optimization. However, the generalization
ability of the trained U-NO should be evaluated on a separate test set excluded from either model
parameter training or hyperparameter optimization. The test set consists of 1000 simulations in the
time domain, which after the Fourier transform becomes 12000 input-output pairs in the frequency
domain. The trained U-NO achieves a relative test loss of 0.116 in the frequency domain. Because
the relative loss tends to be dominated by small amplitudes, frequencies with less energy can be
overweighted. For a fairer evaluation, the predicted results are also assessed in the time domain
through cross-correlation with the ground truth simulated by the SEM. The correlation coefficient is
more robust to small errors in Fourier phase. Fig. 3 shows the distribution of correlation coefficients
between the 2D U-NO predictions and ground truth in the time domain for the test set. The 2D
predictive performance is excellent with a mean correlation coefficient of 0.994, and all of the 1000
simulations have correlation coefficients over 0.98.
We can also evaluate the performance of U-NO at a frequency level. Fig. 4 shows 2D Helmholtz
solutions with U-NO for displacement fields of 2 Hz, 4 Hz, and 6 Hz for an instance from the test
set. The relative losses for these cases are 0.060, 0.077, and 0.126, respectively. We only show the
real parts of the wavefields here for brevity, and the imaginary parts are given in supplementary
materials (Fig. S1). The U-NO predictive accuracy depends on the given frequency and intuitively,
higher frequencies are harder to learn. This will be discussed in detail in the following section. For
the time-domain evaluation, this example demonstrates the excellent predictive capability of neural
operators for seismic wave modeling with a correlation coefficient of 0.997 (Fig. 5). The success
in 2D serves as a stepping-stone towards the exciting application of neural operators for 3D elastic
8

wave modeling.

4.2

3D elastic wave modeling with U-NO

Through hyperparameter optimization, the 3D model has 1,453,400,214 trainable parameters taking
up 5.4 GB. Each epoch of model training takes about 3.7 hours using eight NVIDIA RTX A6000
GPUs, using an average of 16 GB memory during the training process after including the data and
other overhead. The 3D loss curves during training are given in Fig. 6, which demonstrate similar
training behavior to the 2D model. It is necessary to mention that the learning rate scheduler has
an appreciable influence on the training performance and can be considered a critical component.
Although the training stage is time-consuming, it is only done once. With U-NO, one 3D forward
simulation for all the frequencies of interest (explained in the Data section) takes 0.3 s with one
GPU, while using 20 GB of GPU memory. This is about 100 times faster than the classic SEM
and allows for more computationally efficient 3D FWI. The relatively heavy GPU memory usage
arises from the fact that we evaluate the Helmholtz solutions for the full set of frequencies desired in
parallel, which contrasts with the time stepping scheme of the SEM that only processes one time
step at a time. One can conserve memory by solving for the Helmholtz solutions sequentially, but
this comes at the expense of increased processing time.
In addition to the random velocity fields, we incorporate an overthrust model commonly used in
exploration studies (Aminzadeh et al., 1994) into the generalization test for the 3D U-NO. 1000
random subpanels are extracted from the 3D overthrust model and a perturbation range of 30% is
imposed on an average VS of 3 km/s. The VP models are calculated from the VS models multiplied
by 1.732. The density is set to 2.7 g/cm³ and the source is randomly placed in the computational
domain for both test sets. The overall performance of the 3D U-NO on the test set of random
velocity fields is 0.176 measured by the relative loss in the frequency domain, and 0.238 on the test
set of overthrust models. Fig. 7 shows the 3D predicted results for the real parts of 2-, 4-, and 6-Hz
wavefields for a subpanel from the overthrust model. The imaginary parts are given in supplementary
materials (Fig. S2). Fig. 8 illustrates the reconstructed wavefields for several snapshots in time, with
the corresponding ground truth. It is hard to distinguish between the U-NO-predicted waveforms
and SEM-simulated waveforms by eye. Importantly, the neural operator learns the diffraction effects
at the sharp discontinuities. Figs S3, S4 and S5 in supplementary materials show the predicted
results for an instance of random velocity fields in the frequency and time domains, respectively.
For the 1000 simulations in each test data set, the 3D U-NO achieves a mean correlation coefficient
of 0.986 for random velocity fields and 0.971 for random subpanels from the overthrust model, as
shown in Fig. 9. The higher standard deviation observed in the overthrust case could be in part
attributed to the model’s complex heterogeneities. In general, the evaluation metrics demonstrate
the promising generalization power of U-NO for real-world applications.

4.3

Full-waveform inversion with automatic differentiation

Full-waveform inversion (FWI) is a prime beneficiary of the accelerated seismic wave simulations,
as it uses the complete information of recorded waveforms to infer subsurface physical parameters
sampled by seismic waves. The adjoint-state method is conventionally used for FWI, which requires
derivation of the gradient of the objective function with respect to each parameter of interest
(Plessix, 2006). A counterpart to the adjoint-state method has been discovered in the deep learning
9

community, known as automatic differentiation (AD). It has been shown that these two methods are
mathematically equivalent (LeCun et al., 1988; Zhu et al., 2021). The advantage of AD over the
adjoint-state method is that it automatically computes gradients for any desired parameter based
on the computational graph using the chain rule, integrating a wide range of inverse problems in
a unified framework. Neural operators are inherently designed to be compatible with automatic
differentiation. In performing FWI, we freeze the U-NO model parameters and instead treat the
velocity parameters as the objects of training. The objective function is the same as used for training
the U-NO model, except we add a regularization term that encourages model smoothness using the
Laplacian operator. We take the overthrust velocity models used in Fig. 8 for the FWI experiment.
The “observations” are wavefields simulated with SALVUS for 30 events evenly distributed on the
surface of a sphere with a radius of 1.5 km (Fig. 10). Receivers are placed at each grid point of the
64×64×64 mesh. The VP and VS models are simultaneously updated, starting from homogeneous
initial values of 5 km/s and 3 km/s, respectively. After conducting 10 epochs of training using an
Adam optimizer with a learning rate of 0.03 and a batch size of 16, we achieve VP and VS models
with relative L2 misfit of 0.03, as depicted in Fig. 10.
This FWI experiment validates the effectiveness of automatic differentiation as a substitute for the
adjoint-state method, and further showcases the accuracy of 3D elastic wave modeling with U-NO.
We also conduct a similar experiment with receivers only on the surface (Fig. 11). In plotting the
results, we mask out the regions with poor ray coverage as they are difficult to infer. The regions
with sufficient ray density still obtain satisfactory inversion accuracy, with relative L2 misfit of 0.03.
For one event and one tomographic iteration, the U-NO-based FWI takes 0.4 s (including the forward
propagation) using one GPU, while the SEM with the adjoint method takes 140 s under the same
settings. This translates to a speed-up of 350 times.

4.4

Generalization to denser input/output meshes

While classic neural networks are mesh-dependent, neural operators are discretization-invariant in
physical space and can be evaluated on denser input and output meshes after training has concluded
(Li et al., 2020a). One of the most valuable aspects about this is that the training simulations do not
need to be performed at the target resolution, which may be too computationally expensive. Here,
we test this critical property of neural operators by applying the U-NO trained on a 64×64×64 mesh
to a 128×128×64 mesh with finer spacing in the horizontal plane. Both the input velocity models
and the output displacement wavefields are evaluated on denser grids. We conduct 100 experiments
using random fields to get an average correlation coefficient of 0.971. Fig. 12 shows one of the
generalization experiments, with the ground motion plotted as an example. It can be seen that the
U-NO can effectively generalize to denser discretization without additional training. For the 3D
super-resolution test, the neural operator has a further speed-up of 144 times over the SEM solver in
forward propagation.

4.5

Predicting wavefields at only the free surface

Thus far, we have employed the machine learning model to simulate wavefields over the full spatial
domain. However, in most real-world cases we will only have access to waveform data at/near the
surface. In the framework of supervised learning, it is technically feasible to directly map from
elastic properties to surface ground motions directly at (irregular) receiver points. Under many
10

circumstances, this could be more memory- and computationally-efficient. To achieve this, the model
architecture needs to be slightly modified for generalizability. We add a GNO (Li et al., 2020b)
layer to the top of the U-NO architecture to query the ground motions at the desired points in
3D space. GNO layers are evalulated using message passing graph neural networks (Gilmer et al.,
2017). They incorporate positional information into edge features, allowing for the querying of
solutions at arbitrary points on arbitrary discretizations, not limited to a planar free surface. Fig.
13(a) gives the model architecture for predicting wavefields on the free surface, where the GNO
employs a kernel function κl parameterized as a three-layer neural network. κl takes the positional
information and the output of the U-NO as input. For the GNO, we let messages pass across the
whole computational domain to the free surface through a linear integral. The remaining part of
the model adheres to the U-NO architecture in Fig. 1. We compare the predictive accuracy for
3-component surface wavefields using the plain U-NO and the GNO-embedded model. For the plain
U-NO, the ground motions are output over the whole domain but evaluated strictly at the surface.
We calculate the correlation coefficients for 1000 random velocity models unseen in training, as
shown in Fig. 13(b). Incorporating the GNO systematically improves surface wavefield predictions
while also being more data-efficient. We re-emphasize that a GNO layer can query arbitrary points
on arbitrary discretizations, not just the free surface. This is crucial for future applications having
an irregular geometry of seismic stations at variable elevation.

5

Discussion

Excluding the dataset preparation, the most computationally expensive part of the proposed method
is the model training process. Optimizing the model architecture and hyperparameters is quite time
consuming and tedious. Rather than documenting every detail of this trial-and-error process, we
summarize the key findings here:
1. We find that skip connections play an important role in the predictive accuracy of U-NO. They
also contribute to alleviating vanishing gradient issues.
2. Expanding the size of the co-domain can improve the U-NO performance, but at the cost of
higher memory usage.
3. The number of Fourier modes in each layer is a sensitive hyperparameter and should be tuned
carefully. In seismic wave propagation, the elastic media exhibit complex heterogeneities that
amplify high-frequency components in the resulting wavefields. Therefore, we retain all the
Fourier modes in respective dimensions.
4. As with many deep learning studies, the batch size has an appreciable effect on the performance.
5. Learning rate scheduling is very effective, which reduces the learning rate as training progresses.
6. Lastly, in the specific setting described in this paper, neural operators are very data hungry. We
use 27000 simulations (which make ∼300000 frequency-domain instances) to train each U-NO.
The belief is that the increased data volume will enhance accuracy, but this also translates
into additional time taken up by numerical simulations and U-NO training.
The frequencies in this study are discrete values determined by the fast Fourier transform, as we
use a time-domain solver to generate data. If a Helmholtz solver is available, the frequencies can
11

be randomly picked from the real number line R for training and not restricted to a discrete set.
Within a predefined frequency band [fmin , fmax ], a well-trained neural operator can be queried at
arbitrary frequencies for Helmholtz solutions, because it learns mappings between function spaces.
This adapts well to the band-limited characteristics of real seismic data. An additional advantage of
the proposed method over a classical time-domain solver emerges when extending the simulation
duration. The time consumed by a conventional solver following the time-stepping scheme increases
linearly with the simulation duration, theoretically. However, because extending duration translates
to querying intermediate frequencies, the increased run time for the Helmholtz neural operator can
be alleviated by parallelization.
In nearly all numerical methods, higher frequencies correspond to shorter wavelengths, necessitating
finer gridding. Therefore, for any given same mesh, lower accuracy is expected for higher frequencies.
This holds true for the machine learning method as well. We compute the relative loss with respect
to each frequency of interest for the 2D test set, as shown in Fig. 14. It is evident that the loss
monotonously goes up with increasing frequencies. Similar behavior is observed in the training set
and 3D cases. While this arises naturally in part from the physics, it can also be partially attributed
to the tendency of neural networks to generate smooth output to enhance generalizability (Neal
et al., 2018). To reconcile the biased learning of neural operators towards low frequencies, future
work can focus on exploring sampling methods, improved positional encoding, and weighted training
(Li et al., 2021a; Zhao et al., 2021), among other potential approaches.
This study serves as a toolbox for applying neural operators to real data applications. In a realistic
scenario, the physical domain could be larger, which places greater demands on computing efficiency.
One way to reduce memory requirements and accelerate model training is to employ mixed precision
training for neural operators (White et al., 2023). For now, our machine learning model still depends
on a separate numerical solver, but a physics-informed version of neural operators (Li et al., 2021b)
has the potential to free the machine learning model from any external solver. In this study, we have
given particular emphasis to the discretization invariance of neural operators and demonstrated it
with an example of denser grids, but neural operators can also deal with irregular grids. Through a
GNO layer, local predictions at arbitrary points (e.g., where there are seismic stations) can be queried
and enhanced. For now, we have still kept the source as isotropic, but for earthquake scenarios,
future work will involve parameterizing the source as different types of moment tensors.
Ultimately, we aim to provide a few models pretrained to high accuracy with very large datasets that
can be shared as community resources. These models would be applicable to any regions on Earth
as long as the study area fits within the constraints of the physical domain for which the model was
trained. The training cost would therefore be a one time upfront expense, as future downstream
users would not need to repeat this process.

6

Conclusions

We show that elastic wave modeling in 2D and 3D can be accelerated by two orders of magnitude
with U-shaped Neural Operators. The prohibitive expense of memory-intensive models required
in 3D is alleviated by modeling the seismic wave propagation in the frequency domain, due to the
fact that multiple frequencies can be handled separately by parallel computing. We demonstrate
the generalizability of the once-trained neural operator to variable velocity structures with random
12

source locations, which is based on the theory that random fields can approximate most physical
functions with arbitrary accuracy. The overall generalization accuracy is above 0.97 evaluated by
cross-correlation coefficients, taking simulations with a spectral element method as ground truth.
This sheds some light on the potential of the proposed method in real-world applications. We also
show that neural operators can generalize to denser discretization without additional training. An
additional GNO layer greatly improves the waveform predictions on the free surface. Moreover,
we incorporate the trained model with automatic differentiation to facilitate rapid full-waveform
inversion for velocity structures. The 3D inversion process can be accelerated by a factor of 350 in
comparison to the SEM with the adjoint-state method, with accuracy measured by relative L2 misfit
of 0.03. While being mathematically equivalent to the adjoint method, automatic differentiation can
integrate a wide range of inverse problems in a unified framework without manual derivation.

Acknowledgements
ZER thanks the David and Lucile Packard Foundation for supporting this study through a Packard
Fellowship.

References
Afanasiev, M., Boehm, C., van Driel, M., Krischer, L., Rietmann, M., May, D. A., Knepley, M. G.,
& Fichtner, A., 2019. Modular and flexible spectral-element waveform modelling in two and three
dimensions, Geophysical Journal International , 216(3), 1675–1692.
Alkhalifah, T., Song, C., bin Waheed, U., & Hao, Q., 2021. Wavefield solutions from machine learned
functions constrained by the helmholtz equation, Artificial Intelligence in Geosciences, 2, 11–19.
Aminzadeh, F., Burkhard, N., Nicoletis, L., Rocca, F., & Wyatt, K., 1994. Seg/eaeg 3-d modeling
project: 2nd update, The Leading Edge, 13(9), 949–952.
Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M., 2018. Automatic differentiation
in machine learning: a survey, Journal of Marchine Learning Research, 18, 1–43.
Chemingui, N., 2001. Modeling 3-d anisotropic fractal media, Stanford Exploration Project, Report,
80, 1–586.
Clayton, R. & Engquist, B., 1977. Absorbing boundary conditions for acoustic and elastic wave
equations, Bulletin of the seismological society of America, 67(6), 1529–1540.
Courant, R., Friedrichs, K., & Lewy, H., 1967. On the partial difference equations of mathematical
physics, IBM journal of Research and Development, 11(2), 215–234.
De Basabe, J. D. & Sen, M. K., 2009. New developments in the finite-element method for seismic
modeling, The Leading Edge, 28(5), 562–567.
Fichtner, A., Igel, H., Bunge, H.-P., & Kennett, B. L., 2009. Simulation and inversion of seismic
wave propagation on continental scales based on a spectral-element method, Journal of Numerical
Analysis, Industrial and Applied Mathematics, 4(1-2), 11–22.
13

Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., & Dahl, G. E., 2017. Neural message passing
for quantum chemistry, in International conference on machine learning, pp. 1263–1272, PMLR.
Griewank, A. & Walther, A., 2008. Evaluating derivatives: principles and techniques of algorithmic
differentiation, SIAM.
Guo, X., Li, W., & Iorio, F., 2016. Convolutional neural networks for steady flow approximation, in
Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data
mining, pp. 481–490.
Hornik, K., Stinchcombe, M., & White, H., 1989. Multilayer feedforward networks are universal
approximators, Neural networks, 2(5), 359–366.
Huang, X. & Alkhalifah, T., 2022a. Pinnup: Robust neural network wavefield solutions using
frequency upscaling and neuron splitting, Journal of Geophysical Research: Solid Earth, 127(6),
e2021JB023703.
Huang, X. & Alkhalifah, T., 2022b. Single reference frequency loss for multifrequency wavefield
representation using physics-informed neural networks, IEEE Geoscience and Remote Sensing
Letters, 19, 1–5.
Huang, X. & Alkhalifah, T., 2023. Microseismic source imaging using physics-informed neural
networks with hard constraints, arXiv preprint arXiv:2304.04315 .
Huang, X. & Greenhalgh, S., 2021. A finite-difference iterative solver of the helmholtz equation
for frequency-domain seismic wave modeling and full-waveform inversion, Geophysics, 86(2),
T107–T116.
Kelly, K. R., Ward, R. W., Treitel, S., & Alford, R. M., 1976. Synthetic seismograms: A finitedifference approach, Geophysics, 41(1), 2–27.
Khoo, Y., Lu, J., & Ying, L., 2021. Solving parametric pde problems with artificial neural networks,
European Journal of Applied Mathematics, 32(3), 421–435.
Kingma, D. P. & Ba, J., 2014. Adam: A method for stochastic optimization, arXiv preprint
arXiv:1412.6980 .
Komatitsch, D., Ritsema, J., & Tromp, J., 2002. The spectral-element method, beowulf computing,
and global seismology, Science, 298(5599), 1737–1742.
Kovachki, N. B., Li, Z., Liu, B., Azizzadenesheli, K., Bhattacharya, K., Stuart, A. M., & Anandkumar,
A., 2023. Neural operator: Learning maps between function spaces with applications to pdes., J.
Mach. Learn. Res., 24(89), 1–97.
LeCun, Y., Touresky, D., Hinton, G., & Sejnowski, T., 1988. A theoretical framework for backpropagation, in Proceedings of the 1988 connectionist models summer school , vol. 1, pp. 21–28,
San Mateo, CA, USA.
Lehmann, F., Gatti, F., Bertin, M., & Clouteau, D., 2023. Fourier neural operator surrogate model
to predict 3d seismic waves propagation, arXiv preprint arXiv:2304.10242 .
14

Li, B., Wang, H., Feng, S., Yang, X., & Lin, Y., 2022. Solving seismic wave equations on variable
velocity models with fourier neural operator, arXiv preprint arXiv:2209.12340 .
Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., & Anandkumar,
A., 2020a. Fourier neural operator for parametric partial differential equations, arXiv preprint
arXiv:2010.08895 .
Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., & Anandkumar, A.,
2020b. Neural operator: Graph kernel network for partial differential equations, arXiv preprint
arXiv:2003.03485 .
Li, Z., Liu-Schiaffini, M., Kovachki, N., Liu, B., Azizzadenesheli, K., Bhattacharya, K., Stuart, A.,
& Anandkumar, A., 2021a. Learning dissipative dynamics in chaotic systems, arXiv preprint
arXiv:2106.06898 .
Li, Z., Zheng, H., Kovachki, N., Jin, D., Chen, H., Liu, B., Azizzadenesheli, K., & Anandkumar, A.,
2021b. Physics-informed neural operator for learning partial differential equations, arXiv preprint
arXiv:2111.03794 .
Lu, L., Jin, P., Pang, G., Zhang, Z., & Karniadakis, G. E., 2021. Learning nonlinear operators via
deeponet based on the universal approximation theorem of operators, Nature machine intelligence,
3(3), 218–229.
Mai, P. M. & Beroza, G. C., 2002. A spatial random field model to characterize complexity in
earthquake slip, Journal of Geophysical Research: Solid Earth, 107(B11), ESE–10.
Moseley, B., Markham, A., & Nissen-Meyer, T., 2020a. Solving the wave equation with physicsinformed deep learning, arXiv preprint arXiv:2006.11894 .
Moseley, B., Nissen-Meyer, T., & Markham, A., 2020b. Deep learning for fast simulation of seismic
waves in complex media, Solid Earth, 11(4), 1527–1549.
Nakata, N. & Beroza, G. C., 2015. Stochastic characterization of mesoscale seismic velocity
heterogeneity in long beach, california, Geophysical Supplements to the Monthly Notices of the
Royal Astronomical Society, 203(3), 2049–2054.
Neal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna, M., Lacoste-Julien, S., & Mitliagkas, I., 2018.
A modern take on the bias-variance tradeoff in neural networks, arXiv preprint arXiv:1810.08591 .
Olsen, K., 2000. Site amplification in the los angeles basin from three-dimensional modeling of
ground motion, Bulletin of the Seismological Society of America, 90(6B), S77–S94.
Plessix, R.-E., 2006. A review of the adjoint-state method for computing the gradient of a functional
with geophysical applications, Geophysical Journal International , 167(2), 495–503.
Pratt, R. G., 1990. Frequency-domain elastic wave modeling by finite differences: A tool for crosshole
seismic imaging, Geophysics, 55(5), 626–632.
Rahman, M. A., Ross, Z. E., & Azizzadenesheli, K., 2022. U-no: U-shaped neural operators, arXiv
preprint arXiv:2204.11127 .
15

Raissi, M., Perdikaris, P., & Karniadakis, G. E., 2019. Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial differential
equations, Journal of Computational physics, 378, 686–707.
Rasht-Behesht, M., Huber, C., Shukla, K., & Karniadakis, G. E., 2022. Physics-informed neural
networks (pinns) for wave propagation and full waveform inversions, Journal of Geophysical
Research: Solid Earth, 127(5), e2021JB023120.
Ren, P., Rao, C., Chen, S., Wang, J.-X., Sun, H., & Liu, Y., 2022. Seismicnet: Physics-informed neural
networks for seismic wave modeling in semi-infinite domain, arXiv preprint arXiv:2210.14044 .
Ronneberger, O., Fischer, P., & Brox, T., 2015. U-net: Convolutional networks for biomedical image
segmentation, in Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015:
18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pp.
234–241, Springer.
Smith, J. D., Azizzadenesheli, K., & Ross, Z. E., 2020. Eikonet: Solving the eikonal equation
with deep neural networks, IEEE Transactions on Geoscience and Remote Sensing, 59(12),
10685–10696.
Smith, J. D., Ross, Z. E., Azizzadenesheli, K., & Muir, J. B., 2022. Hyposvi: Hypocentre inversion with stein variational inference and physics informed neural networks, Geophysical Journal
International , 228(1), 698–710.
Song, C. & Alkhalifah, T. A., 2021. Wavefield reconstruction inversion via physics-informed neural
networks, IEEE Transactions on Geoscience and Remote Sensing, 60, 1–12.
Song, C. & Wang, Y., 2022. High-frequency wavefield extrapolation using the fourier neural operator,
Journal of Geophysics and Engineering, 19(2), 269–282.
Song, C., Alkhalifah, T., & Waheed, U. B., 2021. Solving the frequency-domain acoustic vti wave
equation using physics-informed neural networks, Geophysical Journal International , 225(2),
846–859.
Song, C., Alkhalifah, T., & Waheed, U. B., 2022. A versatile framework to solve the helmholtz
equation using physics-informed neural networks, Geophysical Journal International , 228(3),
1750–1762.
Song, C., Liu, Y., Zhao, P., Zhao, T., Zou, J., & Liu, C., 2023. Simulating multicomponent elastic
seismic wavefield using deep learning, IEEE Geoscience and Remote Sensing Letters, 20, 1–5.
Sun, H., Yang, Y., Azizzadenesheli, K., Clayton, R. W., & Ross, Z. E., 2022. Accelerating time-reversal
imaging with neural operators for real-time earthquake locations, arXiv preprint arXiv:2210.06636 .
Von Karman, T., 1948. Progress in the statistical theory of turbulence, Proceedings of the National
Academy of Sciences, 34(11), 530–539.
Wei, W. & Fu, L.-Y., 2022. Small-data-driven fast seismic simulations for complex media using
physics-informed fourier neural operators, Geophysics, 87(6), T435–T446.
16

White, C., Tu, R., Kossaifi, J., Pekhimenko, G., Azizzadenesheli, K., & Anandkumar, A., 2023.
Speeding up fourier neural operators via mixed precision, arXiv preprint arXiv:2307.15034 .
Winovich, N., Ramani, K., & Lin, G., 2019. Convpde-uq: Convolutional neural networks with
quantified uncertainty for heterogeneous elliptic partial differential equations on varied domains,
Journal of Computational Physics, 394, 263–279.
Yang, Y., Gao, A. F., Castellanos, J. C., Ross, Z. E., Azizzadenesheli, K., & Clayton, R. W., 2021.
Seismic wave propagation and inversion with neural operators, The Seismic Record , 1(3), 126–134.
Yang, Y., Gao, A. F., Azizzadenesheli, K., Clayton, R. W., & Ross, Z. E., 2023. Rapid seismic
waveform modeling and inversion with neural operators, IEEE Transactions on Geoscience and
Remote Sensing, 61, 1–12.
Zhang, T., Trad, D., & Innanen, K., 2023a. Learning to solve the elastic wave equation with fourier
neural operators, Geophysics, 88(3), T101–T119.
Zhang, Y., Zhu, X., & Gao, J., 2023b. Seismic inversion based on acoustic wave equations using
physics-informed neural network, IEEE transactions on geoscience and remote sensing, 61, 1–11.
Zhao, L., Zou, C., Chen, Y., Shen, W., Wang, Y., Chen, H., & Geng, J., 2021. Fluid and lithofacies
prediction based on integration of well-log data and seismic inversion: A machine-learning approach,
Geophysics, 86(4), M151–M165.
Zhu, W., Xu, K., Darve, E., & Beroza, G. C., 2021. A general approach to seismic inversion with
automatic differentiation, Computers & Geosciences, 151, 104751.
Zhu, Y. & Zabaras, N., 2018. Bayesian deep convolutional encoder–decoder networks for surrogate
modeling and uncertainty quantification, Journal of Computational Physics, 366, 415–447.

17

Figure 1: U-NO architecture. The input a comprises P- and S- wave velocity (VP and VS ), the source
location, and the frequency as a constant function. The output u comprises the frequency-domain
displacement wavefields. P denotes a point-wise lifting operator, Q denotes a point-wise projection
operator, and G denotes an FNO as the inner integral operator. Smaller blue circles denote functionspace concatenations. Inside the dotted box is the composition of each FNO layer, where v is the
layer input, F is the Fourier transform, F −1 is the inverse Fourier transform, R and W are linear
operators, and σ is the nonlinear activation function.

18

Figure 2: Loss curves during the 2D U-NO training for 100 epochs. The loss function is defined
as the relative L1 norm with a weight of 0.9 plus the relative L2 norm with a weight of 0.1. The
batch size is 32. An Adam optimizer is employed with a learning rate of 0.001 and a learning rate
scheduler that decays the learning rate by half every 30 epochs.

19

Figure 3: Distribution of correlation coefficients between the 2D U-NO predictions and ground truth
in the time domain for the test set. The red and black dashed lines indicate the mean and standard
deviation of the histograms.

20

Figure 4: 2D elastic wave modeling with U-NO evaluated in the frequency domain for an instance
from the test set. The first row displays the VP and VP /VS models with the source location marked
with a white star. The second, third, and fourth rows display the predicted results for the real parts
of displacement fields of 2 Hz, 4 Hz, and 6 Hz, respectively. The relative loss of the U-NO prediction
is 0.060 for 2 Hz, 0.077 for 4 Hz, and 0.126 for 6 Hz.

21

Figure 5: 2D elastic wave modeling with U-NO evaluated in the time domain for an instance from
the test set. The first row displays the VP and VP /VS models with the source location marked with
a white star. The second, third, and fourth rows display the predicted results for displacement fields
at 0.7 s, 1 s, and 1.8 s, respectively. The cross-correlation coefficient between the U-NO and SEM
simulations is 0.997.

22

Figure 6: Loss curves during the 3D U-NO training for 100 epochs. The loss function is defined
as the relative L1 norm with a weight of 0.9 plus the relative L2 norm with a weight of 0.1. The
batch size is 32. An Adam optimizer is employed with a learning rate of 0.001 and a learning rate
scheduler that decays the learning rate by half every 30 epochs.

23

Figure 7: 3D elastic wave modeling with U-NO evaluated in the frequency domain for an instance of
random subpanels from the overthrust model. The first row displays the VP and VP /VS models with
the source location marked with a red star. The second, third, and fourth rows display the predicted
results for the real parts of displacement fields of 2 Hz, 4 Hz, and 6 Hz, respectively. The relative
loss of the U-NO prediction is 0.110 for 2 Hz, 0.154 for 4 Hz, and 0.241 for 6 Hz.

Figure 8: 3D elastic wave modeling with U-NO evaluated in the time domain for an instance of
random subpanels from the overthrust model. The first row displays the VP and VP /VS models with
the source location marked with a red star. The second, third, and fourth rows display the predicted
results for displacement fields at 0.7 s, 1 s, and 1.5 s, respectively. The cross-correlation coefficient
between the U-NO and SEM simulations is 0.991.
24

(a)

(b)

Figure 9: Distribution of correlation coefficients between the 3D U-NO predictions and ground truth
in the time domain for (a) random velocity fields from the test set and (b) random subpanels from
the 3D overthrust model. The red and black dashed lines indicate the mean and standard deviation
of the histograms.

Figure 10: U-NO based full-waveform inversion for a random subpanel from the overthrust model
with automatic differentiation. 30 sources are evenly distributed on the surface of a sphere with a
radius of 1.5 km (red stars) and receivers are configured at every grid point of the 64×64×64 mesh.
The true VS and VP models are plotted in the first row. The inverted velocity models with Laplacian
smoothness regularization are plotted in the second row. The relative L2 norm of the misfit between
the true and inverted VS and VP is 0.03.

25

Figure 11: U-NO based full-waveform inversion for a random subpanel from the overthrust model
with automatic differentiation. 30 sources are evenly distributed on the surface of a sphere with
a radius of 1.5 km (red stars) and 64×64 receivers are configured on the surface (blue area). The
true VS and VP models are plotted in the first row. The inverted velocity models with Laplacian
smoothness regularization are plotted in the second row, where parts without ray path coverage are
masked by gray shadows. The relative L2 misfit for the ray-covered parts is 0.03.

26

Figure 12: 3D elastic wave modeling on a 128×128×64 mesh with the U-NO trained on a 64×64×64
mesh. The first row displays the VP and VP /VS models from random fields, with the source location
marked with a red star. The second, third, and fourth rows display the true and predicted ground
motions at 1.1 s, 1.3 s, and 1.5 s, respectively. The cross-correlation coefficient between the U-NO
and SEM simulations for full wavefields of this example is 0.977.

27

(a)

(b)

Figure 13: (a) Model architecture for predicting wavefields at the free surface comprising the U-NO
in Fig. 1 and a GNO that queries the ground motions.(b) Distribution of correlation coefficients
between true and predicted 3D ground motions for 1000 random velocity models using the U-NO
with (red) and without (cyan) a GNO layer.

Figure 14: Relative loss associated with frequencies for the 2D test set. The training set and 3D
cases exhibit similar behavior.

28

Supplementary Materials

Figure S1: 2D elastic wave modeling with U-NO evaluated in the frequency domain for an instance
from the test set. The first row displays the VP and VP /VS models with the source location marked
with a white star. The second, third, and fourth rows display the predicted results for the imaginary
parts of displacement fields of 2 Hz, 4 Hz, and 6 Hz, respectively. The relative loss of the U-NO
prediction is 0.060 for 2 Hz, 0.077 for 4 Hz, and 0.126 for 6 Hz.

S1

Figure S2: 3D elastic wave modeling with U-NO evaluated in the frequency domain for an instance
of random subpanels from the overthrust model. The first row displays the VP and VP /VS models
with the source location marked with a red star. The second, third, and fourth rows display the
predicted results for the imaginary parts of displacement fields of 2 Hz, 4 Hz, and 6 Hz, respectively.
The relative loss of the U-NO prediction is 0.110 for 2 Hz, 0.154 for 4 Hz, and 0.241 for 6 Hz.

Figure S3: 3D elastic wave modeling with U-NO evaluated in the frequency domain for an instance
of random velocity fields. The first row displays the VP and VP /VS models with the source location
marked with a red star. The second, third, and fourth rows display the predicted results for the real
parts of displacement fields of 2 Hz, 4 Hz, and 6 Hz, respectively. The relative loss of the U-NO
prediction is 0.096 for 2 Hz, 0.149 for 4 Hz, and 0.238 for 6 Hz.
S2

Figure S4: 3D elastic wave modeling with U-NO evaluated in the frequency domain for an instance
of random velocity fields. The first row displays the VP and VP /VS models with the source location
marked with a red star. The second, third, and fourth rows display the predicted results for the
imaginary parts of displacement fields of 2 Hz, 4 Hz, and 6 Hz, respectively. The relative loss of the
U-NO prediction is 0.096 for 2 Hz, 0.149 for 4 Hz, and 0.238 for 6 Hz.

Figure S5: 3D elastic wave modeling with U-NO evaluated in the time domain for an instance of
random velocity fields. The first row displays the VP and VP /VS models with the source location
marked with a red star. The second, third, and fourth rows display the predicted results for
displacement fields at 0.7 s, 1 s, and 1.5 s, respectively. The cross-correlation coefficient between the
U-NO and SEM simulations is 0.989.
S3

