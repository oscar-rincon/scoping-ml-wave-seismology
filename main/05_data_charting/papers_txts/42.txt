IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

5900717

Seismic Velocity Inversion Based on Physically
Constrained Neural Networks
Yan Zhang , Decong Meng, Yifan Zhou, Liwei Song , and Hongli Dong

Abstract— The propagation velocity of seismic waves is a
crucial parameter in seismic exploration, encompassing the entire
process of seismic data acquisition, processing, and interpretation. Traditional model-driven full-waveform inversion (FWI)
methods, which rely on an initial velocity, suffer from low
computational efficiency. Conversely, data-driven deep-learning
(DL) approaches heavily rely on extensive training data and
lack interpretability due to overreliance on training data for
generalization. To address these challenges, we present a seismic velocity inversion network model that incorporates prior
knowledge and constraints based on physical laws. The proposed
approach involves constructing a data-driven inversion network
with dual encoders and single decoder structure, enabling the
learning of nonlinear mappings from seismic data to velocity
models. By incorporating prior well-logging data and attention
mechanisms, the inversion process is improved. In addition,
a seismic forward modeling network based on recurrent neural
networks (RNNs) is developed to solve the acoustic wave equation.
Leveraging the advantages of parallel computing, the forward
modeling process achieves fast calculations. The automatic differentiation algorithm in DL facilitates gradient calculations,
specifically back propagation of the residuals to incorporate the
physical constraints. Ultimately, the proposed seismic velocity
inversion network combines the two network structures while
incorporating the constraint of wave field extrapolation law.
Numerical experiments demonstrate that this network exhibits
advantages in terms of result accuracy and model generalization.
Index Terms— Attention mechanism, physical constraints,
prior logging, recurrent neural network (RNN), velocity inversion.

I. I NTRODUCTION

I

N THE realm of seismic exploration, accurate determination of seismic wave velocity is a critical factor that
influences the imaging quality, the hypocenter localization
accuracy, and the effectiveness of geological interpretation.
It represents a challenging and significant subject within
the field of oil and gas resource exploration. Full-waveform
inversion (FWI) stands as one of the established model-driven
Manuscript received 16 July 2023; revised 25 October 2023; accepted
28 November 2023. Date of publication 5 December 2023; date of current version 13 December 2023. This work was supported by the Natural
Science Foundation of Heilongjiang Province under Grant LH2023D009.
(Corresponding author: Yan Zhang.)
Yan Zhang, Decong Meng, and Yifan Zhou are with the School of Computer
and Information Technology, Northeast Petroleum University, Daqing 163318,
China (e-mail: zhangyan1999@nepu.edu.cn; mdc741768803@163.com;
zyfiivv@outlook.com).
Liwei Song is with the School of Physics and Electronic Engineering, Northeast Petroleum University, Daqing 163318, China (e-mail:
zhidao90@163.com).
Hongli Dong is with the Institute of Artificial Intelligence Energy
Research, Northeast Petroleum University, Daqing 163318, China (e-mail:
shiningdhl@vip.126.com).
Digital Object Identifier 10.1109/TGRS.2023.3339783

methods for wave velocity inversion, and it is widely regarded
as an advanced technique in seismic exploration. By harnessing the principles of the wave equation, FWI effectively
exploits the kinematics (travel time) and dynamics (amplitude
and phase) characteristics of seismic waves. Consequently,
the inversion results are better suited for complex geological
conditions. Notably, FWI offers numerous advantages, including high inversion accuracy and excellent imaging efficacy
for complex structures. However, FWI faces the challenge
of solving a strongly nonlinear problem of mapping seismic
wave velocities from observed seismic data. As a result, the
inversion outcomes heavily rely on the large-scale wave velocity information incorporated within the initial wave velocity
model. In cases where the accuracy of the initial model is
insufficient, it can lead to the inversion result being trapped in
local minimum [1], [2], [3]. To address this issue, researchers
have utilized tomographic inversion methods to provide initial
velocity models for FWI [4]. However, these methods often
rely heavily on human experience and struggle to meet the
accuracy requirements of FWI. To overcome these challenges,
Xu et al. [5] based on a nonlinear iterative relaxation approach
where short and long wavelength components of the velocity
model are updated alternatively to improve the resolution from
FWI. Alkhalifah and Wu [6] described it in detail and combined it with image-based waveform inversion. Chen et al. [7]
proposed the envelope-based sparse-constrained deconvolution
(E-SCD) inversion method, which reduces the complexity of
the travel-time inversion and improves the reconstruction accuracy of the reflection sequences. Chen et al. [8] proposed a salt
structure elastic FWI based on the multiscale signed envelope,
effectively overcoming the problems that restrict EFWI, such
as the lack of low-frequency seismic data and multiparameter
coupling. Li and Alkhalifah [9] introduced a method that
combines extended waveform inversion with the matching
filter between the predicted and observed data. This approach
mitigates cycle skipping. Dokter et al. [10] developed a 2-D
FWI approach for the simultaneous determination of S-wave
velocity and density models from SH- and Love-wave data.
Guitton [11] proposed a blocky regularization scheme for FWI,
effectively constraining the ill-posed inverse problems and not
smoothing the model. While these methods have effectively
mitigated the dependence of FWI on the initial velocity
model, these approaches still encounter challenges related to
computational cost and difficulty. Furthermore, FWI involves
multiple calculations of seismic wave field forward modeling
during gradient calculations and model iteration updates, with
the computational requirements scaling with the number of
sources. Consequently, FWI becomes highly computationally

1558-0644 © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

5900717

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

intensive and time-consuming [12], [13], [14]. In response
to the aforementioned limitations of traditional methods,
data-driven deep-learning (DL) approaches have rapidly
emerged.
DL methods, including multilayer perception, convolutional neural networks (CNN), and recurrent neural networks
(RNNs), have gained significant attention and adoption in
various fields. In the domain of seismic exploration, DL has
demonstrated remarkable achievements in tasks, such as
seismic denoising [15], [16], [17], [18], [19], interpolation
reconstruction [20], seismic migration [21], earthquake prediction [22], initial velocity model building [23], and fault
detection [24], [25], [26]. Both theoretical research and practical applications have demonstrated that DL excels in fitting
nonlinear mapping relationships and has a strong potential for
solving geophysical inverse problems.
In the context of seismic velocity inversion, ArayaPolo et al. [27] proposed a 2-D velocity modeling method
based on deep neural networks. This approach utilizes manually extracted feature information as input and generates
a velocity model as output. Through neural networks, the
nonlinear mapping between feature information and the
velocity model is approximated. Kazei et al. [28] and
Ovcharenko et al. [29] have shown early applications of
direct inversion on real data. Yang and Ma [30] introduced a
purely data-driven DL velocity inversion method using U-Net
networks. By combining supervised training with extensive
datasets, the trained network model effectively maps seismic data to the velocity model. Li et al. [31] constructed
SeisIvNet, a velocity inversion network based on deep neural networks. This method incorporates all information from
seismic channel data and global seismic profile features by
employing fully connected layers, leading to more accurate
velocity model inversion results. Cao et al. [32] constructed
BiInNet, a lightweight architecture for real-time velocity
inversion networks based on BiSeNet. The advantages of
BiInNet lie in smaller volumes, lower computational complexity, and faster inference speed. Yang et al. [33] proposed a
DL-based algorithm to build high-resolution velocity models
using low-resolution velocity models, migration images, and
well-log velocities as inputs.
Moreover, traditional FWI suffers from limitations due to
local optimization algorithms that rely on gradient guidance
to solve nonlinear inversion problems. These algorithms tend
to converge to local minima near the initial value, which can
result in inaccurate or meaningless inversion outcomes when
the initial model is inadequate. This drawback underscores
the importance of prior knowledge in velocity inversion.
To address this issue, Zhang et al. [34] developed a neural network that combines a prior initial velocity model
for seismic velocity inversion, achieving promising results.
Zhang and Gao [35] developed a deep-learning full-waveform
inversion (DLFWI) approach using seismic migration images,
which outperforms the conventional data-driven deep-learning
full-waveform inversion (DD-DLFWI) approach in terms of
reconstruction accuracy, antinoise, and generalization ability.
Feng et al. [36] proposed InversionNet, a multiscale velocity
inversion network based on the fully convolutional network

architecture. Their approach involved utilizing a style transfer
network to convert natural scenery images into underground
velocity models, laying the foundation for constructing an
extensive dataset required for DL. Subsequently, the U-Net
network was employed to obtain the initial velocity model, and
finally, the high-wavenumber components of the underground
velocity model were reconstructed by incorporating the initial
velocity model. These advancements in DL methodologies
have opened up new possibilities for improving seismic velocity inversion and addressing the challenges associated with
traditional methods.
The seismic velocity inversion method based on DL has
shown advantages such as reduced dependency on initial
models compared with traditional FWI methods, and higher
prediction efficiency. However, challenges remain in terms of
requiring a large number of training samples and ensuring
the generalization ability of deep neural networks in the
field of seismic exploration velocity inversion. As a result,
research is expanding to explore velocity inversion methods that combine model-based and data-driven approaches.
These methods aim to achieve automation, efficiency, and
relatively improved network generalization and interpretability. Richardson [37] demonstrated the equivalence between
gradients calculated using traditional adjoint state methods
and DL automatic differentiation tools by constructing an
RNN network for FWI of the acoustic wave equation. Sun
and Alkhalifah [38] used RNN to enhance the convergence
of FWI. And, Sun et al. [39] conducted comparative tests
on an RNN inversion network based on the acoustic wave
equation and flexibly used multiple optimization algorithms
within the DL framework, resulting in improved calculation efficiency. Ren et al. [40] proposed SWINet, a seismic
waveform inversion network, based on wave-equation-based
forward modeling network cells. Wang et al. [41] implemented
an FWI network using RNN for isotropic and anisotropic
elastic wave media. Song and Alkhalifah [42] developed the
physics informed neural network (PINN)-based WRI method,
and it is able to invert for a reasonable velocity with very
limited iterations and frequencies. Rasht-Behesht et al. [43]
proposed PINN based on the acoustic wave equation. The
meshless nature of solving partial differential wave equations
using neural networks enables the PINN network to handle
different types of boundary absorption conditions, resulting in
more accurate forward modeling results and addressing the
FWI problem. Malovichko et al. [44] developed a general
approach to integrating petrophysical models in 3-D seismic
FWI based on the Gramian constraints. The model-based and
data-driven method, when combined with the advantages of
FWI theory within the framework of DL algorithms, not only
potentially may improve inversion accuracy and generalization
but also may reduce the reliance of data-driven methods on
a large number of training samples, which is one of the
research highlights in the field of seismic velocity inversion
in conjunction with DL methods.
To address the nonuniqueness of the inversion problem and
accelerate model convergence, we propose a model-driven and
data-driven seismic velocity inversion network based on physical constraints and prior knowledge from well-logging data.

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

ZHANG et al.: SEISMIC VELOCITY INVERSION BASED ON PHYSICALLY CONSTRAINED NEURAL NETWORKS

This network incorporates wave field continuation constraints
to ensure consistency between the synthetic and observed
seismic data, improving the accuracy of the inversion results.
By integrating prior knowledge derived from well-log data,
which provides valuable information about subsurface rock
properties and seismic velocities, the solution space is further
constrained, enabling faster convergence to more accurate
velocity models. This combined approach offers a comprehensive and effective solution for high-precision seismic velocity
inversion in seismic exploration.
The major contributions include the following aspects.
1) We introduce the prior knowledge of well logs, observation systems, and seismic source location to reduce the
multiplicity in the inversion problem.
2) We combined attention mechanism and fully connected
layers to design a U-Net network with two encoders and
one decoder, serving as a data-driven velocity inversion
network and demonstrating its effectiveness.
3) We combined the FWI theory with an RNN network
to simulate the seismic wave propagation process and
achieved an effective integration of physical laws and
neural networks.
4) Integrating data-driven neural networks with
physics-driven RNN forward modeling networks
enables a data-model dual-driven approach for velocity
inversion.
II. T HEORY
In this section, we will start by reviewing the FWI, as well
as the idea of using CNNs for solving inverse problems. Then,
our RNN-based forward modeling will be discussed.
A. FWI
FWI, initially introduced by Tarantola [45], is a method for
seismic velocity inversion that operates in the time domain.
It iteratively updates the model by minimizing the discrepancy between the numerically simulated seismic data and the
observed data. The inversion process can be divided into two
main steps: wave field simulation and gradient calculation.
In this study, we utilize the 2-D constant density acoustic wave
equation for forward modeling, which can be expressed as
follows:
∂ 2 u(x, z, t)
1
− ∇ 2 u(x, z, t) = s(x, z, t)
v 2 (x, z)
∂t 2

(1)

where ∇ 2 is the Laplace operator, v(x, z) is the wave velocity,
and u(x, z, t) and s(x, z, t) are the wave field and source
at time t, respectively. By employing the second-order finite
difference scheme in the time domain, the wave field value
at the next time step can be calculated. This simplifies the
expression for the forward modeling simulation as follows:
ut+1t = 1t 2 v 2 (∇ 2 ut + st ) + 2ut − ut−1t

(2)

where 1t is the sampling time step, ut+1t is the wave field
value of the next time step, and st is the source function
value at time t. The prediction of seismic data is obtained by
recording the wave field values at each time point using the

5900717

detector mapping operator. As a result, the forward modeling
simulation process can be simplified as follows:
d pred = f (v)

(3)

where d pred is the predicting seismic data, f is the acoustic forward modeling operator, and v is the velocity model
parameter.
Seismic velocity inversion involves solving the inverse
problem of (3) in the forward modeling process. FWI can
be viewed as an optimization problem that aims to minimize
the mismatch between the predicted seismic data generated by
forward numerical simulation and the observed seismic data.
By iteratively updating the model, the objective is to minimize
the error between the predicted and observed seismic data in
order to invert the parameters of the underground medium
velocity model. The objective function can be expressed as
follows:
1
(4)
J (v) = ∥ f (v) − d obs ∥2
2
where d obs represents the actual observed seismic data. FWI
utilizes the adjoint state method [46] to calculate the gradient
during the inversion process and iteratively updates the model
parameters through local optimization algorithms.
B. Principle of Convolutional Neural Network for Solving
Inversion Problems
Compared with multilayer perceptron, CNNs have a strong
advantage in processing data samples with spatial characteristics, such as images and 2-D seismic data, due to their strong
spatial feature extraction ability and spatial weight-sharing
characteristics [47]. In the purely data-driven velocity inversion problem, if we assume that the convolutional neural
network is denoted by Net, then the inversion process can
be formulated as follows:
y = Net(x, θ )

(5)

where x is the observed seismic data, y is the 2-D velocity
model predicted by the network, and θ is the parameter of the
network. The convolutional neural network approximates the
target by fitting extensive training data. The average loss on
the training dataset D is regarded as an empirical risk, and
Remp is computed as follows:
N

Remp (Net) =

s
1 X
L(Net(x i , θ ), ỹi )
Ns i=1

(6)

where ỹ is the 2-D velocity model label, L is the loss
function, and Ns is the number of samples. The objective
of the convolutional neural network algorithm is to minimize
the empirical risk, and in the context of velocity inversion,
it aims to learn the nonlinear mapping relationship between
the seismic data and the velocity model. Therefore, under the
guidance of empirical risk minimization, the objective function
J of the neural network used for seismic velocity inversion can
be defined as follows:
X
J (x, θ, ỹ) = arg min
L(Net(x i , θ ), ỹi )
(7)
θ

i

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

5900717

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

Fig. 1.
RNN network structural diagram: after unfolding in timeline,
assuming that the current time step is t, xt represents the input to the current
network, h t represents the value of the hidden layer, and yt represents the
output at the current time step. U, G, and W are the trainable network weights.

where L is the loss function. Gradient descent is used as
an iterative optimization method to approximate the optimal
solution of a convolutional neural network. The objective
is to minimize the empirical risk by updating the network
parameters in the direction of steepest descent of the loss
function. Through the computation of gradients with respect to
the network parameters, we update the network in small steps
to gradually minimize the loss and improve the network’s performance. The iterative process continues until a satisfactory
convergence is achieved or a predefined stopping criterion is
met. The network parameter θt+1 is expressed as follows:
θ t+1 = θ t + µ(∇θ i=1,2,...,t J (x, θ i , ỹ))

(8)

where µ represents the update direction and step, and ∇θ i=1,2,...,t
represents the gradient relative to parameter θ i .
C. Forward Modeling Principle of RNN
RNN is a suitable and effective model for processing
sequential data in the field of seismic exploration. Unlike
CNN, RNN considers the sequential relationship between
input data, making it well-suited for temporal data processing.
In the context of seismic forward simulation, where the
wave field values at each time depend on the values at previous
times, RNN [48] can capture this temporal dependency.
RNN has a unique structure, as shown in Fig. 1, with a
cyclic architecture that allows it to analyze and process sequential data effectively. This cyclic structure gives RNN a natural
advantage in handling sequences such as text, voice, waveform
data, and well-logging data. Another characteristic of RNN is
parameter sharing, where the parameters of each layer (U, G,
and W ) are shared across time steps. This parameter sharing
enables RNN to perform the same operation at each step
but with different inputs and hidden states. Consequently, the
number of parameters and network complexity to be learned
in RNN is reduced, enhancing its flexibility for sequences of
varying lengths.
We can describe the calculation process of the hidden layer
by assuming that at time t, the input of the network is xt , the
hidden state (hidden layer neuron activity value) h t is not only
related to the input xt at the current time but also to the saved
hidden state h t−1 at the previous time
z t = W ht−1 + Uxt + b
ht = σ (z t )
yt = G(ht ).

(9)
(10)
(11)

From a physical perspective, the process of seismic wave
propagation in the underground medium and its reception by
geophones involves a complex mapping relationship.
The observed seismic data represents the response of the
underground medium to the excitation caused by the artificial
source. However, it is important to note that this mapping
relationship is not necessarily reversible. In other words,
given the same source and geophone conditions, the observed
seismic data may correspond to scattered wave information
from different locations within the underground medium.
This one-to-many relationship between the observed data and
the underlying subsurface properties is a fundamental cause
of inversion instability, adding to the challenge of learning
accurate mappings using neural networks.
To overcome this challenge, it is necessary to consider both
the model space (underground medium) and the data space
(observed data) in the inversion process. The propagation of
seismic waves can only be fully described by combining these
two spaces. Therefore, the inversion problem cannot solely
rely on approximating the data space, but must also incorporate
constraints from the model space to obtain more unique and
stable inversion results. The forward numerical simulation
process based on the wave equation exhibits similarities to the
forward propagation process of RNN. By introducing model
parameters, source functions, and geophone functions, and
solving the wave equation using RNN, it is possible to obtain
predicted seismic data through forward numerical simulation
calculations of the seismic wave field. By combining (2)
and (9), it can be concluded that

 
 

 
ut+1t
2 + 1t 2 v 2 ∇ 2 −1
ut
2 2 st
=
·
+ 1t v
.
ut
1
0
ut−1t
0
(12)
Formula (12) represents the fundamental calculation step
within the RNN forward modeling network. This step involves
solving the wave equation to obtain and transmit the wave field
value at time t + 1t. Subsequently, the geophone mapping
operator P is utilized to derive the output seismic data
represented as follows:
d t+1t = P(ut+1t ).

(13)

The output at each time step is combined to synthesize the
predicted seismic data, completing the forward numerical simulation. The RNN forward modeling utilizes the mean squared
error (MSE) loss function to quantify the discrepancy between
the predicted and observed seismic data. The optimization
result is obtained by minimizing the MSE loss function
1
[(d obs − d pred )]2 .
(14)
n
In addition to forward numerical simulation, RNN can
also compute gradients through backpropagation to facilitate
the inversion process. RNN leverages automatic differentiation algorithms [49] to calculate gradients and updates the
model parameters using optimization algorithms within the
DL framework. The gradients obtained through automatic
differentiation are equivalent to those obtained through the
conventional adjoint state method [50].
L r (d obs , d pred ) =

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

ZHANG et al.: SEISMIC VELOCITY INVERSION BASED ON PHYSICALLY CONSTRAINED NEURAL NETWORKS

5900717

Compared with traditional methods, employing RNN for
forward modeling and inversion operations eliminates the need
for manually deriving gradient calculation expressions and
enables flexible utilization of various optimization algorithms
within the DL framework for experimental research. This
approach enhances computational efficiency and facilitates the
integration of other DL algorithms.
III. C ONSTRUCTION OF V ELOCITY I NVERSION N ETWORK
BASED ON P RIOR P HYSICAL C ONSTRAINTS
In this section, we first focus on the construction of a
data-driven velocity inversion subnet. We then delve into
the construction intricacies of an RNN-based forward subnet,
laying the groundwork for robust predictive modeling within
our framework. Finally, we delineate the construction of
the velocity inversion network, strategically integrating prior
physical constraints to enhance the accuracy and reliability of
our inversion methodology.
A. Construction of a Data-Driven Velocity Inversion Subnet
1) Prior Knowledge Processing and Application: Deep
neural networks have the ability to learn distribution characteristics and mapping relationships between seismic data
and the corresponding velocity models using large amounts of
data, even without accurate initial velocity models. However,
incorporating prior knowledge obtained during seismic exploration can help constrain the solution space, reduce ambiguity,
and enhance algorithm convergence. In particular, the source
location and observation system information serve as valuable
prior knowledge that captures the specific location-related
details of seismic data and the underground medium. In addition, well-logging data represent an essential source of prior
knowledge in seismic exploration, offering rich geological
information with high vertical resolution and strong continuity.
It provides valuable insights into the local characteristics of
the underground medium. Therefore, we introduce the source,
observation system information, and well-logging data as
reliable prior knowledge for the network.
To incorporate this prior knowledge, we utilize one-hot
vector encoding to represent the source and geophone location
information in the network. Since the location information of
the source and geophone is closely associated with seismic
data and velocity models, we manually define two lines of
encoding vectors. One line represents the source location
information, while the other line represents the geophone
location information. These lines are concatenated with the
seismic trace corresponding to a specific source and receiver.
In addition, a 2-D matrix, matching the size of the velocity
model, is employed to encode the source and geophone locations using one-hot encoding. Fig. 2 illustrates this process.
Furthermore, well-log data serves as reliable prior knowledge in seismic exploration and is often employed as a
constraint in inversion methods. In this context, we utilize
well-log data from two shafts to obtain velocity values corresponding to the 30th and 70th columns of the actual seismic
velocity model. By applying the nearest-neighbor interpolation
algorithm and Gaussian blur operation, we generate a rough

Fig. 2.

Source function and observation system prior.

initial velocity model that serves as the initial velocity prior
knowledge. The resulting model is depicted in Fig. 3, showcasing the incorporation of well-log data into the inversion
process.
2) Attention Gate Mechanism: To network performance
enhance the feature extraction of well-log areas and improve,
an attention gate (AG) [51] can be incorporated into the
encoding and decoding structure of the network. This attention
mechanism allows for focused feature extraction in areas of
interest by learning the weights of attention regions through
neural network gradient propagation. The specific structure of
the network with the AG is depicted in Fig. 4.
Given the presence of distinct location information and wave
velocity information within the well-log data, the attention
mechanism can effectively enhance the extraction of features
related to the well-log data. Moreover, the characteristics of
different areas are integrated into Feature Map C as auxiliary
and complementary information. The adjusted feature map
encompasses crucial details from various spatial positions,
highlighting the salient features of specific well-logging areas
and improving the model’s sensitivity and prediction accuracy.
3) Overall Design of Velocity Inversion Subnet: Based on
the experience gained from traditional FWI, the recovery of
large-scale information in the velocity model heavily relies
on the initial velocity model, whereas small-scale structural
details primarily rely on the reflected wave information contained in seismic data. In the context of DL, neural networks
tend to learn low frequencies before gradually capturing higher
frequencies as they fit the data [52]. This frequency bias phenomenon holds significance when designing velocity inversion
networks.
To address this, we propose two encoder structures for
the network. The first encoder network utilizes prior logs
to provide a large-scale wave velocity model. On the other
hand, the second encoder network leverages the waveform
information present in the seismic data to capture small-scale
feature information. By combining the outputs of these two
encoders, we aim to enhance the learning performance of the
network, allowing it to effectively capture both the large-scale
and small-scale characteristics of the velocity model.
Furthermore, seismic data represent the time-series information, capturing the propagation and variation of seismic waves
within the underground medium over time. In contrast, the
velocity model provides information about the spatial location
and structure of the underground medium. However, the correspondence between seismic data and the velocity model in the
element space is often weak, posing challenges for network

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

5900717

Fig. 3.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

Velocity model is shown on the left. The well-logging data are shown in the middle. (Right) Initial speed after well-logging data processing.

Fig. 4. AG mechanism combines both the information from the previous
context represented as x, and the subsequent context represented as g. x and g
are transformed into matrices A and B of the same size via a 1 × 1 convolution
and added together. This sum is then passed through the rectified linear unit
(ReLU) activation function, producing matrix C. The attention coefficients α
are computed via convolution and sigmoid operations applied to matrix C.
Finally, the output y is obtained by multiplying x with α.

training and learning. To address this issue, we leverage the
benefits of fully connected layers, which can effectively utilize
the full waveform information present in seismic data despite
the weak spatial correspondence. However, when dealing with
large input and output data, fully connected layers require
a substantial number of parameters, extensive computational
resources, and high memory consumption. To mitigate these
challenges, we adopt an encoding network to compress the
features of seismic data into a one-dimensional vector format
[bs, c, 1, 1], where bs represents the batch size and c represents
the number of feature channels. This compressed feature
vector is then passed through a fully connected layer for
feature mapping. By employing this approach, we can leverage
the high-level semantic features extracted from the seismic
data to map them to the structural features of the model space,
thus alleviating the resource consumption issues associated
with the large amount of data in the fully connected layer.
Based on the aforementioned analysis, the structure of our
velocity inversion network, which incorporates prior knowledge, is illustrated in Fig. 5. The network comprises two
encoders and one decoder, working in tandem to facilitate the
inversion process and enhance learning performance.

velocity inversion subnet to incorporate physical law constraints via the velocity model.
Referring to (2), when the velocity model is known, the
forward modeling simulation process can be simplified as a
calculation involving the Laplacian values of the wave field.
The calculation of the Laplacian values can be achieved
through convolution operations within the neural network.
The convolution kernel used in the convolution operation is
typically determined by the finite difference format and the
dimensions of the velocity model. For a 2-D wave velocity
model using a spatial second-order finite difference format,
the convolutional kernel can be represented as a matrix


0
1
0
1 −4 1.
0
1
0

B. Construction of RNN-Based Forward Subnet

Like traditional finite difference calculation methods, RNN
forward modeling networks utilize convolution operations to
compute the Laplace operator of the wave field. This approach
offers the advantage of parallel computing, resulting in higher
computational efficiency, especially when dealing with multiple velocity models and multiple forward modeling for various
sources. Furthermore, the use of GPUs can further accelerate
the computations.
In the context of traditional seismic forward modeling,
absorption boundary conditions are employed to attenuate
the reflections of the wave field at artificial boundaries.
In our RNN forward modeling network, we introduce perfectly
matched layer (PML) absorption boundary conditions. The
acoustic wave equation with PML conditions can be expressed
as follows:
∂2u
∂u
+ 2m
+ m 2 u = v2∇ 2 u
(15)
2
∂t
∂t
where m is the cosine-type attenuation factor matrix. By using
the second-order finite difference method of time




2 − m2 1t 2 + 1t 2 v 2 ∇ 2
1 − m1t
ut+1t =
ut −
ut−1t .
1 + m1t
1 + m1t
(16)

Traditional FWI has demonstrated its effectiveness in practical data applications by utilizing the wave field continuation
law as a driving force for velocity inversion. The physical
law-driven approach has yielded promising results. In our
proposed framework, we establish a connection between the
forward modeling subnet, implemented using RNN, and the

The RNN architecture is utilized to process the time series
data by storing hidden states. In the context of seismic velocity
modeling, each moment’s wave field calculation is treated as
a layer in the RNN, referred to as a forward network unit.
The velocity model is considered a trainable parameter of the
network, and the internal operations of the forward modeling

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

ZHANG et al.: SEISMIC VELOCITY INVERSION BASED ON PHYSICALLY CONSTRAINED NEURAL NETWORKS

Fig. 5.

5900717

Data-driven velocity inversion network structural diagram.

Fig. 6. RNN forward modeling network process. Sn represents the source value at time step n, which is used to compute the wavefield Pn at time step n
through network units. The wavefield value dn at time step n is then obtained by applying the detector operator to Pn and projected as a row of seismic data
output.

network unit involve convolution operations on the Laplacian
values of the wave field and simple matrix operations on
the wave field. The results of the forward modeling network
unit calculations are then mapped to a line of seismic data
output using the detector operator P. The overall structure
of the RNN forward modeling network is depicted in Fig. 6.
Within each layer of the network, the wave fields from the
previous two moments are used as inputs, along with the
source function values at the current moment. By performing
the necessary calculations, new wave field values and corresponding predicted seismic data are obtained. This calculation
process aligns with (16).
C. Construction of the Velocity Inversion Network Based on
Prior Physical Constraints
The seismic velocity inversion network, which combines the
data-driven velocity inversion subnet and the RNN forward
modeling network, is designed to integrate physical laws

and prior knowledge constraints. The overall structure of
the network is illustrated in Fig. 7. The predicted velocity
model generated by the inversion network serves as a trainable
parameter input for the RNN forward network, establishing a
connection between the two networks. During training, the
L2 loss is computed using the simulated seismic data and
the predicted seismic data obtained from the RNN forward
modeling network. The gradient of this loss function with
respect to velocity model parameters is obtained using automatic differentiation algorithm. By jointly training the two
networks, the gradient of the velocity model in the RNN can be
backpropagated to the inversion subnetwork, providing physical constraints for the inversion subnetwork. Upon completion
of the final training, the predicted output from the inversion
subnet is used as the final result of the overall network.
The loss function of the inversion subnet combines the MSE
function and the structural similarity function (SSIM) to jointly
measure the difference between the output results and the real
velocity model. The specific form of the loss function is as

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

5900717

Fig. 7.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

Overall network structural diagram.

follows:
2µ yv µ ỹv + c1 2σ yv ỹv + c2


µ2yv + µ2ỹ + c1 σ y2v + σ ỹ2 + c2



SSIM yv , ỹv = 

v



(17)

v


1
(18)
L u ( yv , ỹv ) = [( yv − ỹv )2 1 − SSIM( yv , ỹv ) ]
n
where µ yv and µ ỹv represent the average values of image
pixels within windows yv and ỹv , respectively, σ y2v and σ ỹ2
v
represent their variance, respectively, σ yv ỹv represents their
covariance, c1 and c2 represent the calculated stability constant, and n represents the number of batch samples. SSIM is
a metric commonly used to assess the similarity between two
images, with a specific focus on edge and texture similarity.
In the context of velocity model inversion, the fine-scale
stratum interfaces are often the key areas of interest, which
aligns with SSIM’s emphasis on texture. Therefore, SSIM can
be employed to measure the structural difference between the
inversion results and the real velocity model. Typically, SSIM
values range between 0 and 1, where a value of 1 indicates
complete similarity between the two images. To align with
the optimization objective of minimizing the neural network’s
loss function, we utilize negative values in the loss function
formulation. This ensures that the optimization goal is met.
Finally, the loss function of the overall network is
L total = L u ( yv , ỹv ) + λ L r (d obs , d pred ).

(19)

IV. E XPERIMENT
In this section, we will first share information on the data
organization, followed by how we trained the network. We also
share our accuracy evaluation metrics. We follow that by
showing the prediction results and analyzing the performance
of the network. We finally compare various algorithms.
A. Data Organization
To address the scarcity of real velocity models in the
context of velocity inversion, we employ a manual generation
approach to expand the dataset Dv . This involves creating
2-D velocity models with different structures, including simple
layered models, fault models, and salt dome models. In the
generation of simple layered velocity models, we superpose

sine, cosine, and various linear functions to create layers
ranging from 4 to 7. The velocities of these layers are within
the range of 1500–4500 m/s. The size of these velocity models
is 1000 × 640 m, with a grid spacing of 10 m in both
the horizontal and vertical directions. Fig. 8(a) illustrates
an example of such a layered velocity model. In addition,
we generate fault models by introducing random parameters
such as fault length, position, and inclination angle within the
range of 30◦ –150◦ . Fig. 8(b) demonstrates an example of a
fault model. To further diversify the dataset, we incorporate
salt dome objects. These objects involve the addition of a
randomly shaped salt dome with a size of 20 × 20 grid
points and a velocity of 5 km/s to the previously generated velocity models. Fig. 8(c) provides a visualization of
a salt dome model. By generating these artificial velocity
models with different structures, we can augment the dataset
and provide additional samples for the velocity inversion
training.
Once a sufficient number of velocity models have been
obtained, the finite difference forward modeling method is
used to generate corresponding simulated seismic data. First,
for the setup of the source and observation system, we adopt
surface shooting and surface observation setup. Geophones
are placed every 10 m horizontally on the surface (every
grid point on the surface), with a sampling time interval of
0.001 s. The total acquisition time is 1.024 s, implying that
there are 1024 sampling point records received. The seismic
source wavelet Ricker adopts a 20-Hz main frequency, with a
total of ten shots and a horizontal interval of 100 m between
the seismic sources that are shot point-by-point on the surface.
The final partial simulated seismic data results are shown in
Fig. 9.
To include the effects of noise and missing channels in real
seismic data, we introduce random Gaussian noise and random
channel dropout to the seismic data in the dataset Dv . A 15%
of the seismic data are added with random Gaussian noise,
with a maximum noise intensity σ of 0.15. Furthermore, a
15% of the seismic data undergo random channel dropout,
with a maximum random missing ratio of 35%. The resulting
observed seismic data, processed with noise and missing
channels, is depicted in Fig. 10. This augmentation of the
dataset enhances the robustness of the network model.

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

ZHANG et al.: SEISMIC VELOCITY INVERSION BASED ON PHYSICALLY CONSTRAINED NEURAL NETWORKS

Fig. 8.

5900717

Display of velocity model dataset. (a) Simple layered model. (b) Including a fault. (c) Salt dome object.
TABLE I
T EST R ESULTS OF A BLATION E XPERIMENTS

Fig. 9.

Simulated seismic data generated by forward modeling.

B. Training the Network
We share here the training details that resulted in the
predictions we will see in section IV-D. We first individually
trained the data-driven inversion network using supervised
learning methods. We also designed ablation experiments to
verify the effectiveness of the network. We used an Adam
optimizer with an initial learning rate of 0.001. When the loss
value did not decrease 5 consecutive times on the validation
set, we reduced the learning rate by a factor of 0.1. The batch
size was set to 10, and the total number of training iterations
was 200.
In the section on overall network performance below,
we consider that the high memory consumption of RNN networks and the difficulty of retraining concatenated networks.

Fig. 10. Seismic data processing. (a) Noisy seismic data with σ = 0.15.
(b) Randomly missing 30% of seismic data.

This study adopts the idea of transfer learning. We utilize a
pretrained velocity inversion subnetwork and combine it with
an RNN forward modeling network for retraining with a small
amount of data. The overall training process is as follows:
first, the inversion subnetwork is trained on the Dv dataset for
70 iterations. Then, the pretrained model is combined with the
RNN forward modeling network and trained for an additional
130 iterations. Regarding the RNN forward modeling network,
we used the Adam optimizer and conducted tests with learning
rates set to 1, 10, 50, 100, and 150, respectively. Ultimately,
the learning rate of a 100 yielded good training results for the
network.

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

5900717

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

Fig. 11. Comparison of test samples. The first line is the label. The second line is the result of Model III. The third line is the result of Model II. The fourth
line is the result of Model I.

C. Evaluation Index
MSE, mean absolute error (MAE), and SSIM metrics were
used to measure the recovery accuracy
b

L MSE ( ỹ, y) =

1X
( ỹi − yi )2
n i=1

L MAE ( ỹ, y) =

1X
| ỹi − yi |
n i=1

(20)

b

(21)

where n represents the number of batch samples, ỹ is the
network prediction output, and y is the real label data.
D. Data-Driven Inversion Network Results
1) Ablation Experiment: To assess the impact of prior
knowledge and AG on the inversion results, we conducted
an ablation experiment by comparing Model II (without prior
knowledge inversion network) and Model III (without AG)
with Model I, which incorporates prior knowledge into the
inversion subnet. This experiment aims to examine the influence of different mechanisms on network performance and
effectiveness.

Table I displays the quantitative results of the three network
models on the test set. Model II (without prior knowledge of
inversion network) and Model III (without AG) exhibit inferior
performance compared with the proposed network models in
terms of MSE, MAE, and SSIM indicators.
This indicates that the incorporation of the AG and prior
knowledge in Model I (prior inversion subnet) has enhanced
the inversion accuracy of the network. Fig. 11 presents the
test results of the three models on the simple layered velocity
model, fault velocity model, and salt dome velocity model.
Observing the graphs, we can see that all three network models
perform well in restoring the simple layered model. However,
Model III (without AG) exhibits relatively poor restoration at
the edges, resulting in fuzziness. In the fault model, there are
variations in the restoration quality among the three models,
but Model III (without AG) lacks distinct fault features and
appears relatively blurry. In contrast, Model II (without prior
knowledge inversion network) and the proposed network in
this study exhibit more prominent fault features at the fault
location, and the network recovery effect in this study is better
than Model II (without prior knowledge inversion network).
Regarding the salt dome model, Model III (without AG)
exhibits the worst restoration effect, particularly with a blurry

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

ZHANG et al.: SEISMIC VELOCITY INVERSION BASED ON PHYSICALLY CONSTRAINED NEURAL NETWORKS

Fig. 12. Output results of the training set model corresponding to the first
35 training times. (a) Ground truth. (b) Output for the fifth round of training.
(c) Output for the 15th round of training. (d) Output for the 25th round of
training. (e) Output for the 35th round of training.

bottom interface. Model II (without prior knowledge inversion
network) performs better in restoring the background layered
model but struggles with blurry boundaries and shape discrepancies when predicting the salt dome. Based on these findings,
we conclude that the attention mechanism improves the network’s inversion performance in regions with pronounced
velocity changes. It enables the inversion subnet to restore
structural features, such as faults and salt domes, which exhibit
significant velocity variations. However, Model II, without
prior knowledge, primarily learns background layered features
and performs relatively poorly in high-frequency feature areas
with substantial velocity changes.
To gain a visual understanding of the network learning
process and the impact of prior knowledge and the attention
mechanism, we have selected a model from the training set
to showcase the prediction results in the initial 35 rounds of
network training, as illustrated in Fig. 12. The graph reveals
several noteworthy observations. Initially, the network begins
by learning large-scale, low-frequency features, and gradually transitioning toward higher-frequency feature learning.
As training progresses, the network starts recovering information about the layered structure of the underground media.
Notably, the network demonstrates a preference for learning
relevant feature information near the two vertical wells, which
are based on the initial velocity model and well logging
data. The layered structure becomes increasingly apparent
around the location where the prior well-logging information is
present. The network successfully restores the velocity values
in this region, displaying significant distinctions from the
surrounding velocity information. This observation serves as
evidence of the effectiveness of the attention mechanism and

5900717

the integration of well-logging prior information in the velocity
inversion subnet. Overall, Fig. 12 shows a clear demonstration
of the network’s learning trajectory, highlighting its capability
to progressively recover the layered structure of the velocity
model based on the initial velocity model and well-logging
data. The attention mechanism and prior well-logging information play vital roles in enhancing the network’s effectiveness
in velocity inversion.
2) Seismic Data Testing With Different Noise Intensities
and Sampling Rates: To assess the model’s robustness in
handling real-world scenarios, we conducted tests using noisy
seismic data and data with missing seismic channels. The noise
intensities were set at 0.1, 0.3, and 0.5, while the missing
seismic data had missing proportions of 15%, 25%, and 35%.
The results are presented in Figs. 13 and 14. From the
findings, we can see that Model I network maintains excellent
performance when the noise intensity is 0.1, which falls within
the range of noise intensity in the training data. Even with a
noise intensity of 0.3, the model still performs well, although
there are slight velocity variations in the finer details. However,
as the noise intensity increases, the inversion results of the
network become noticeably affected, particularly in regions
with significant structural changes. This indicates that noise
has a significant impact on the network’s inversion results.
In experiments involving missing traces, the overall prediction
results exhibit minimal changes when 15% of seismic channels
are missing, unlike the results affected by noise interference.
As the proportion of missing channels gradually increases,
the overall prediction results display only minor alterations.
This phenomenon can be attributed to the convolutional neural
network’s feature extraction process, which leverages spatial proximity to extract data features. The early layers of
the network, particularly in the downsampling stage, employ
larger-sized convolutional kernels with a broad receptive field.
This enables better learning of the spatial features of seismic
data and offers robustness against the absence of seismic
channels. Furthermore, due to the weak spatial correspondence between seismic data and velocity models, disturbances
in spatial structures have a relatively small impact on the
inversion results compared to noise interference. The layered
characteristics of velocity models, on the other hand, are
sensitive to the time information of reflected waves in seismic
data. As a result, the network exhibits greater robustness
in handling missing seismic channels in spatial positions,
resulting in relatively minor changes in the inversion results.
Overall, the experimental results demonstrate the model’s
performance under varying intensities of noise and missing
seismic channel data. The network displays resilience to the
absence of seismic channels in spatial positions, while noise
interference has a more significant impact on the inversion
results.
E. Overall Network Results
The training indicators of the network on the training set
are presented in Fig. 15. From the changes in MSE, it is
observed that the network quickly converges within the first
20 iterations, after which the numerical changes become more
gradual but continue to trend downward. The SSIM also

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

5900717

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

Fig. 13. Network performance for different noise intensities. The first column is the label. The second column shows the network output without noise
interference. The third column shows the network output when the noise intensity is 0.1. The fourth column shows the network output when the noise intensity
is 0.3. The fifth column shows the network output when the noise intensity is 0.5.

Fig. 14. Network performance under varying degrees of missing channels. The first column is the label. The second column shows the network output
without missing channel interference. The third column shows the network output when 15% of data is missing. The fourth column shows the network output
when 25% of data is missing. The fifth column shows the network output when 35% of data is missing.

exhibits rapid changes in the early stages of training, followed
by a more gradual but upward trend. Overall, the network
demonstrates excellent convergence. A comparison of various
indicators on the test set between model A (physical law
constraint network) and model B (unconstrained network) is
shown in Table II. It can be observed that model A has
improved in MSE, MAE, and SSIM results compared to

model B. This indicates that model A, with the incorporation
of physical constraints, exhibits comparable performance.
In the robustness testing of the network against noise and
missing sampling, Figs. 16 and 17 present the results of
models A and B on the test set. We can see that model A
maintains excellent performance even when the missing ratio
exceeds 20%. At the maximum missing ratio, the network

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

ZHANG et al.: SEISMIC VELOCITY INVERSION BASED ON PHYSICALLY CONSTRAINED NEURAL NETWORKS

5900717

TABLE II
E FFECTIVENESS T ESTING OF J OINT P HYSICAL L AW C ONSTRAINTS D RIVE

Fig. 15.

Training indicator curve.

Fig. 16. Comparison of test results for different degrees of missing channels. (a) Comparison of different missing faults, where the first line is a network
without physical constraints, and the second line is a network with physical constraints. (b) Comparison of different missing salt domes, where the first line
is a network without physical constraints, and the second line is a network with physical constraints.

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

5900717

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

Fig. 17. Comparison of test results for different noise intensities. (a) Comparison of faults with different noise intensities, where the first line is a network
without physical constraints, and the second line is a network with physical constraints. (b) Comparison of salt domes with different noise intensities, where
the first line is a network without physical constraints, and the second line is a network with physical constraints.
TABLE III
C OMPARISON W ITH S IMILAR A LGORITHMS

with physical constraints still outperforms the network without
physical constraints. In the testing of different noise data,
model A also demonstrates robustness against noise interference in seismic data. Overall, the combination of the pretrained
prior inversion subnet and the RNN forward modeling network
through transfer learning achieves favorable results. Model
A, with physical constraints, exhibits better performance than
model B in terms of MSE, MAE, and SSIM while demonstrating improved robustness against noise and missing sampling.
F. Algorithm Comparison
In order to demonstrate the effectiveness of the algorithm
proposed in this article, a comparison is made between

the velocity inversion network based on physical law constraints (proposed method), the U-Net inversion method,
and the InversionNet inversion network. The performance
of these three methods on the test set is illustrated in
Fig. 18. From the figure, it is evident that all three networks
achieve satisfactory reconstructions on the simple layered
model and the salt dome model. However, on the fault
model, the InversionNet network exhibits fuzziness of the
deep layer and fault location, with an unclear fault structure.
The U-Net network also produces unsatisfactory results in
capturing fault details. In contrast, the network model proposed in this article effectively reconstructs the fault velocity
model.

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

ZHANG et al.: SEISMIC VELOCITY INVERSION BASED ON PHYSICALLY CONSTRAINED NEURAL NETWORKS

5900717

Fig. 18. Comparison results of similar algorithms. The first line is labeled data. The second line presents the algorithm results of this study. The third line
presents the U-Net algorithm results. The fourth line presents the inversion algorithm results.

of the method proposed in this article, showcasing better
performance compared to other methods in terms of numerical
evaluation and visual comparison.
V. C ONCLUSION

Fig. 19. Comparison of velocity values in the 70th column of the fault model
using similar algorithms.

The specific indicator values are provided in Table III,
which reveals that the algorithm proposed in this article
outperforms similar methods in terms of MSE, MAE, and
SSIM. Furthermore, Fig. 19 presents a comparison of the
velocity values in the 70th column of the fault velocity
model for the three network models. It demonstrates that the
algorithm proposed in this article exhibits the best fit with the
labels. The results collectively demonstrate the effectiveness

We propose a seismic velocity inversion network that
leverages physical law constraints to achieve high-precision
velocity inversion. The key points of our approach are summarized as follows.
1) Utilizing Prior Knowledge in an Attention Mechanism:
To expedite model convergence, we incorporate prior
knowledge from well logging, observation systems, and
source locations. We combine this knowledge with an
U-Net network that includes an attention mechanism,
enhancing the utilization of prior information. Considering the limited spatial correspondence between seismic
data and velocity models, as well as the rich but weak
energy of reflected waves in seismic data, we design
a data-driven velocity inversion network. This network
employs deep convolution layers to extract features and
combines them with fully connected layers for spatial
mapping, effectively utilizing available features.
2) Integration of FWI and RNN: By combining FWI theory
with the capabilities of RNN in processing time series

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

5900717

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024

data, we develop a novel framework. We utilize the
wave equation to drive wave field continuation, and
RNN is employed to calculate the wave field Laplacian
value using convolutional methods. We leverage the
hidden state layer of the RNN to store wave field
information from previous time steps, thereby improving computational efficiency. In addition, we employ
automatic differentiation and the Adam optimization
algorithm to automatically obtain gradients and update
the model parameters. This approach replaces traditional
FWI methods and effectively combines physical laws
with neural networks.
3) Data and Model Dual-Driven Network: We implement
a velocity inversion network that combines data-driven
neural networks with a physically informed RNN forward modeling network. First, we employ transfer
learning to pretrain the data-driven velocity inversion
network. Subsequently, we integrate this pretrained network with RNN forward modeling networks for further
training. Through this approach, we achieve highprecision velocity inversion even with limited samples.
By leveraging these methodologies, our proposed network
achieves accurate velocity inversion results with improved
efficiency. It provides the effective combination of data-driven
approaches and physical laws.
R EFERENCES
[1] S. Lian, S. Yuan, G. Wang, T. Liu, Y. Liu, and S. Wang, “Enhancing lowwavenumber components of full-waveform inversion using an improved
wavefield decomposition method in the time-space domain,” J. Appl.
Geophys., vol. 157, pp. 10–22, Oct. 2018.
[2] X. Guo, Y. Shi, W. Wang, and H. Liu, “A robust source-independent
misfit function for time domain waveform inversion based on normalized convolved wavefield,” J. Appl. Geophys., vol. 166, pp. 129–146,
Jul. 2019.
[3] R. Biswas et al., “Two-step velocity inversion using trans-dimensional
tomography and elastic FWI,” in Proc. SEG Tech. Program Expanded
Abstr., Sep. 2020, pp. 3628–3633.
[4] T. Alkhalifah and Y. Choi, “From tomography to FWI with a single
objective function,” Geophysics, vol. 79, no. 2, pp. 55–61, Feb. 2014.
[5] S. Xu, D. Wang, F. Chen, G. Lambare, and Y. Zhang, “Inversion on
reflected seismic wave,” in Proc. 82nd Annu. Int. Meeting, SEG, Expand
Abstr., Sep. 2012, pp. 1–7.
[6] T. Alkhalifah and Z. Wu, “The natural combination of full and imagebased waveform inversion,” Geophys. Prospecting, vol. 64, no. 1,
pp. 19–30, Jan. 2016.
[7] G. Chen, W. Yang, Y. Liu, J. Luo, and H. Jing, “Envelope-based sparseconstrained deconvolution for velocity model building,” IEEE Trans.
Geosci. Remote Sens., vol. 60, 2022, Art. no. 4501413.
[8] G. Chen, W. Yang, Y. Liu, H. Wang, and X. Huang, “Salt structure elastic
full waveform inversion based on the multiscale signed envelope,” IEEE
Trans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 4508912.
[9] Y. Li and T. Alkhalifah, “Extended full waveform inversion with
matching filter,” Geophys. Prospecting, vol. 69, no. 7, pp. 1441–1454,
Jun. 2021.
[10] E. Dokter, D. Köhn, D. Wilken, D. De Nil, and W. Rabbel, “Full waveform inversion of SH- and love-wave data in near-surface prospecting,”
Geophys. Prospecting, vol. 65, no. 1, pp. 216–236, Dec. 2017.
[11] A. Guitton, “Blocky regularization schemes for full-waveform inversion,” Geophys. Prospecting, vol. 60, no. 5, pp. 870–884, Sep. 2012.
[12] Z. Meng and J. A. Scales, “2-D tomography in multi-resolution analysis
model space,” in Proc. SEG Tech. Program Expanded Abstr., Jan. 1996,
pp. 1126–1129.
[13] X. Li, A. Y. Aravkin, T. van Leeuwen, and F. J. Herrmann, “Fast randomized full-waveform inversion with compressive sensing,” Geophysics,
vol. 77, no. 3, pp. 13–17, May 2012.

[14] X. Chai, G. Tang, R. Peng, and S. Liu, “The linearized Bregman
method for frugal full-waveform inversion with compressive sensing and sparsity-promoting,” Pure Appl. Geophys., vol. 175, no. 3,
pp. 1085–1101, Mar. 2018.
[15] W. Cao, Y. Shi, W. Wang, X. Guo, F. Tian, and Y. Zhao, “Self-supervised
multitask 3-D partial convolutional neural network for random noise
attenuation and reconstruction in 3-D seismic data,” IEEE Trans. Geosci.
Remote Sens., vol. 60, 2022, Art. no. 5924619.
[16] C. Qiu, B. Wu, N. Liu, X. Zhu, and H. Ren, “Deep learning prior model
for unsupervised seismic data random noise attenuation,” IEEE Geosci.
Remote Sens. Lett., vol. 19, pp. 1–5, 2022.
[17] O. M. Saad and Y. Chen, “Deep denoising autoencoder for seismic
random noise attenuation,” Geophysics, vol. 85, no. 4, pp. 367–376,
Jul. 2020.
[18] Y. Zhang, X. Li, B. Wang, J. Li, H. Wang, and H. Dong, “Robust seismic
data denoising based on deep learning,” Oil. Geophys. Prospecting,
vol. 57, no. 1, pp. 12–25, 2022.
[19] C. Birnie, M. Ravasi, S. Li, and T. Alkhalifah, “The potential of selfsupervised networks for random noise suppression in seismic data,” Artif.
Intell. Geosci., vol. 2, pp. 47–59, Dec. 2021.
[20] Y. Zhang, J. Li, B. Wang, X. Li, and H. Dong, “Seismic data regularization based on deep learning combining wavelet domain,” Oil. Geophys.
Prospecting, vol. 57, no. 4, pp. 777–788, 2022.
[21] H. Kaur, N. Pham, and S. Fomel, “Improving the resolution of migrated
images by approximating the inverse Hessian using deep learning,”
Geophysics, vol. 85, no. 4, pp. 173–183, Jul. 2020.
[22] Md. H. A. Banna et al., “Application of artificial intelligence in predicting earthquakes: State-of-the-art and future challenges,” IEEE Access,
vol. 8, pp. 192880–192923, 2020.
[23] C. Wei et al., “Seismic velocity inversion based on CNN-LSTM fusion
deep neural network,” Appl. Geophys., vol. 18, no. 4, pp. 499–514,
Dec. 2021.
[24] X. Hu et al., “Image recognition-based identification of multifractal features of faults,” Frontiers Earth Sci., vol. 10, May 2022, Art. no. 909166.
[25] A. Cunha, A. Pochet, H. Lopes, and M. Gattass, “Seismic fault detection
in real data using transfer learning from a convolutional neural network
pre-trained with synthetic seismic data,” Comput. Geosci., vol. 135,
Feb. 2020, Art. no. 104344.
[26] D. Yang, Y. Cai, G. Hu, X. Yao, and W. Zou, “Seismic fault detection
based on 3D UNet++ model,” in Proc. SEG Tech. Program Expanded
Abstr., Sep. 2020, pp. 1631–1635.
[27] M. Araya-Polo, J. Jennings, A. Adler, and T. Dahlke, “Deep-learning
tomography,” Lead. Edge, vol. 37, no. 1, pp. 58–66, 2018.
[28] V. Kazei, O. Ovcharenko, P. Plotnitskii, D. Peter, X. Zhang, and T.
Alkhalifah, “Mapping seismic data cubes to vertical velocity profiles by
deep learning,” Geophysics, vol. 86, no. 5, pp. 711–721, Aug. 2021.
[29] O. Ovcharenko, V. Kazei, T. A. Alkhalifah, and D. B. Peter, “Multi-task
learning for low-frequency extrapolation and elastic model building from
seismic data,” IEEE Trans. Geosci. Remote Sens., vol. 60, 2022.
[30] F. Yang and J. Ma, “Deep-learning inversion: A next-generation seismic velocity model building method,” Geophysics, vol. 84, no. 4,
pp. 583–599, Jul. 2019.
[31] S. Li et al., “Deep-learning inversion of seismic data,” IEEE Trans.
Geosci. Remote Sens., vol. 58, no. 3, pp. 2135–2149, Mar. 2020.
[32] W. Cao, Y. Shi, X. Guo, F. Tian, X. Ke, and C. Li, “BiInNet: Bilateral
inversion network for real-time velocity analysis,” IEEE Trans. Geosci.
Remote Sens., vol. 60, 2022, Art. no. 5905617.
[33] S. Yang, T. Alkhalifah, Y. Ren, B. Liu, Y. Li, and P. Jiang, “Well-log
information-assisted high-resolution waveform inversion based on deep
learning,” IEEE Geosci. Remote Sens. Lett., vol. 20, pp. 1–5, 2023.
[34] J. Zhang, J. Li, X. Chen, Y. Li, G. Huang, and Y. Chen, “Robust
deep learning seismic inversion with a priori initial model constraint,”
Geophys. J. Int., vol. 225, no. 3, pp. 2001–2019, Jan. 2021.
[35] W. Zhang and J. Gao, “Deep-learning full-waveform inversion using
seismic migration images,” IEEE Trans. Geosci. Remote Sens., vol. 60,
2022, Art. no. 3062688.
[36] S. Feng, Y. Lin, and B. Wohlberg, “Multiscale data-driven seismic fullwaveform inversion with field data study,” IEEE Trans. Geosci. Remote
Sens., vol. 60, 2022, Art. no. 4506114.
[37] A. Richardson, “Seismic full-waveform inversion using deep learning
tools and techniques,” 2018, arXiv:1801.07232.
[38] B. Sun and T. Alkhalifah, “ML-descent: An optimization algorithm for
FWI using machine learning,” Geophysics, vol. 85, no. 6, pp. 477–492,
Oct. 2020.

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

ZHANG et al.: SEISMIC VELOCITY INVERSION BASED ON PHYSICALLY CONSTRAINED NEURAL NETWORKS

[39] J. Sun, Z. Niu, K. A. Innanen, J. Li, and D. O. Trad, “A theoryguided deep-learning formulation and optimization of seismic waveform
inversion,” Geophysics, vol. 85, no. 2, pp. 87–99, Mar. 2020.
[40] Y. Ren, X. Xu, S. Yang, L. Nie, and Y. Chen, “A physics-based neuralnetwork way to perform seismic full waveform inversion,” IEEE Access,
vol. 8, pp. 112266–112277, 2020.
[41] W. Wang, G. A. McMechan, and J. Ma, “Elastic isotropic and anisotropic
full-waveform inversions using automatic differentiation for gradient
calculations in a framework of recurrent neural networks,” Geophysics,
vol. 86, no. 6, pp. 795–810, Nov. 2021.
[42] C. Song and T. Alkhalifah, “Wavefield reconstruction inversion via
physics-informed neural networks,” IEEE Trans. Geosci. Remote Sens.,
vol. 60, 2022, Art. no. 5908012.
[43] M. Rasht-Behesht, C. Huber, K. Shukla, and G. E. Karniadakis,
“Physics-informed neural networks (PINNs) for wave propagation and
full waveform inversions,” J. Geophys. Res., Solid Earth, vol. 127, no. 5,
pp. 1–21, May 2022.
[44] M. Malovichko, N. Khokhlov, N. Yavich, and M. S. Zhdanov, “Incorporating known petrophysical model in the seismic full-waveform inversion
using the Gramian constraint,” Geophys. Prospecting, vol. 68, no. 4,
pp. 1361–1378, May 2020.
[45] A. Tarantola, “Inversion of seismic reflection data in the acoustic
approximation,” Geophysics, vol. 49, no. 8, pp. 1259–1266, Aug. 1984.
[46] R.-E. Plessix, “A review of the adjoint-state method for computing the
gradient of a functional with geophysical applications,” Geophys. J. Int.,
vol. 167, no. 2, pp. 495–503, Nov. 2006.
[47] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proc. IEEE, vol. 86, no. 11,
pp. 2278–2324, Nov. 1998.
[48] J. L. Elman, “Finding structure in time,” Cognit. Sci., vol. 14, no. 2,
pp. 179–211, Mar. 1990.
[49] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind,
“Automatic differentiation in machine learning: A survey,” J. Mach.
Learn. Res., vol. 18, no. 1, pp. 5595–5637, Jan. 2017.
[50] M. Sambridge, P. Rickwood, N. Rawlinson, and S. Sommacal, “Automatic differentiation in geophysical inverse problems,” Geophys. J. Int.,
vol. 170, no. 1, pp. 1–8, Jul. 2007.
[51] O. Oktay et al., “Attention U-Net: Learning where to look for the
pancreas,” 2018, arXiv:1804.03999.
[52] N. Rahaman et al., “On the spectral bias of neural networks,” 2018,
arXiv:1806.08734.

5900717

Yan Zhang was born Dalian, Liaoning, China,
in 1980. He received the B.S. degree in computer
science and technology, the M.S. degree in computer
application technology, and the Ph.D. degree in
oil and gas engineering from Northeast Petroleum
University, Daqing, China, in 2003, 2010, and 2018,
respectively.
He is currently an Associate Professor with the
School of Computer and Information Technology,
Northeast Petroleum University. His research interests include artificial intelligence and seismic data
processing.

Decong Meng, photograph and biography not available at the time of
publication.

Yifan Zhou, photograph and biography not available at the time of
publication.

Liwei Song, photograph and biography not available at the time of publication.

Hongli Dong, photograph and biography not available at the time of
publication.

Authorized licensed use limited to: Universidad EAFIT. Downloaded on August 24,2024 at 16:06:03 UTC from IEEE Xplore. Restrictions apply.

