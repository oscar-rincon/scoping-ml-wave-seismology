Improving Seismic Wave Simulation
and Inversion Using Deep Learning
Lei Huang(B) , Edward Clee, and Nishath Ranasinghe
Department of Computer Science, Prairie View A&M University,
Prairie View, TX 77446, USA
{lhuang,niranasinghe}@pvamu.edu, T Clee@acm.org

Abstract. Accurate simulation of wave motion for the modeling and
inversion of seismic wave propagation is a classical high-performance
computing (HPC) application using the ﬁnite diﬀerence, the ﬁnite element methods and spectral element methods to solve the wave equations
numerically. The paper presents a new method to improve the performance of the seismic wave simulation and inversion by integrating the
deep learning software platform and deep learning models with the HPC
application. The paper has three contributions: 1) Instead of using traditional HPC software, the authors implement the numerical solutions for
the wave equation employing recently developed tensor processing capabilities widely used in the deep learning software platform of PyTorch. By
using PyTorch, the classical HPC application is reformulated as a deep
learning recurrent neural network (RNN) framework; 2) The authors customize the automatic diﬀerentiation of PyTorch to integrate the adjoint
state method for an eﬃcient gradient calculation; 3) The authors build a
deep learning model to reduce the physical model dimensions to improve
the accuracy and performance of seismic inversion. The authors use the
automatic diﬀerentiation functionality and a variety of optimizers provided by PyTorch to enhance the performance of the classical HPC application. Additionally, methods developed in the paper can be extended
into other physics-based scientiﬁc computing applications such as computational ﬂuid dynamics, medical imaging, nondestructive testing, as
well as the propagation of electromagnetic waves in the earth.
Keywords: Machine learning · Inverse problem · Wave propagation

1

Introduction

Physical simulation and inversion are classical scientiﬁc computing applications
to discover the physical phenomenon and reveal the underlying properties. The
simulation solves the partial diﬀerential equations (PDE) that governs the physical phenomenon using numerical approximation methods, while the inversion
applies the gradient-based optimizations to ﬁnd the underlying properties by
minimizing the observed data and the simulated results. The entire process takes
c Springer Nature Switzerland AG 2020

J. Nichols et al. (Eds.): SMC 2020, CCIS 1315, pp. 3–19, 2020.
https://doi.org/10.1007/978-3-030-63393-6_1

4

L. Huang et al.

signiﬁcant computing resources to achieve the satisﬁed accuracy. However, the
inverse problem is naturally challenging since it is ill-posed and nonlinear for
most cases.
Recent advances in high-performance tensor processing hardware and software are providing new opportunities for accelerated linear algebra calculations
as used in machine learning, especially for deep learning neural networks, that
contributes signiﬁcantly to the success of data science. Such calculations are
also at the heart of many simulations of physical systems such as wave propagation. The use of tensor processing in neural networks, with its need for backpropagation through multi-layered networks, has led to capabilities for automatic
diﬀerentiation [1] for gradient calculations in deep learning software.
Motivations: The motivations of the work have twofold. The ﬁrst one is to
understand the new deep learning software package such as PyTorch and TensorFlow, and their capacity of solving a scientiﬁc computational problem. Especially, we are interested in how to model the traditional partial diﬀerential equations (PDEs) used in the scientiﬁc computational problem with a deep learning
model. The other is to study how to integrate the machine learning models that
are data-driven into the scientiﬁc computational model that are physics-driven.
The diﬀerentiable programming has the potential to smoothly integrate them
together with a global optimization. The authors believe the study will lead to
more interesting research ﬁndings in the topic of Scientiﬁc Machine Learning
(SciML) and to ﬁnd an eﬃcient way to combine the power of these two diﬀerent
methods to facilitate scientiﬁc discovery.
In this paper, we study how to use the tensor-based machine learning software to formulate the physical simulation and to compute the gradients for
optimizations to solve the inverse problem. We use the seismic wave propagation simulation and the Full Wave Inversion (FWI) as the physical case study.
We have adapted the techniques of others in this area of wave propagation [2,3]
to demonstrate how direct ﬁnite diﬀerence integration can be implemented via a
deep learning software platform, allowing the gradients calculated by automatic
diﬀerentiation to be used for the FWI of seismic reﬂection survey data as an
augmentation to the well-known PySIT [4] seismic research platform.
We summarize the paper’s contributions in the following:
i) We formulate the PDE solver in the seismic forward model using the Recurrent Neural Network (RNN) implemented with the deep learning software
package PyTorch, which allows us to take advantages of the tensor processing software and its accelerator implementation.
ii) We apply the automatic diﬀerentiation implemented in PyTorch to solve the
seismic inverse problem to uncover the earth’s interior physical properties.
iii) We improve the automatic diﬀerentiation eﬃciency by creating a hybrid
back propagation method with the adjoint-state method to calculate the
gradients.
iv) We implement an AutoEncoder network to reduce the dimensions of the
inverted parameters to argument the convergence process and get more
accurate results for the ill-posed problem.

Seismic Simulation, Inversion and Deep Learning

2

Wave Equations and RNN

2.1

Wave Equations

5

The wave motion is governed by physical rules that can be expressed in the
following partial diﬀerential equation (PDE) (1) and the boundary conditions
(2) and (3). We use the 1D scalar wave equation for simplicity purpose in this
paper:
1

∂ 2 u(x, t) ∂ 2 u(x, t)
−
= f (x, t)
∂t2
∂x2
1 ∂u(0, t) ∂u(0, t)
−
=0
c(0) ∂t
∂x
1 ∂u(1, t) ∂u(1, t)
−
=0
c(1) ∂t
∂x

c2 (x)

(1)
(2)
(3)

where c(x) is the spatial velocity distribution, u(x, t) is the wave ﬁeld distribution
in space and time, and f (x, t) is the energy source distribution in space and time.
The Eq. (1) can be solved numerically using a ﬁnite diﬀerence approximation:
u(x − Δx, t) − 2u(x, t), +u(x + Δx, t)
Δx2
1 u(x, t − Δt) − 2u(x, t) + u(x, t + Δt)
+ 2
.
c
Δt2

f (x, t) = −

(4)

After factoring, the Eq. (4) can be expressed as
u(x, t + Δt) = f (x, t)c2 Δt2 + (2u(x, t) − u(x, t − Δt))
+c2

Δt2
(u(x − Δx, t) − 2u(x, t) + u(x + Δx, t))
Δx2

(5)

which shows that the next wave ﬁeld in time u(x, t+Δt) can be calculated based
on the current and prior wave ﬁelds, as well as spatial neighbors in the current
wave ﬁeld. The wave motion simulation follows the time sequence to produce
the next state based on the prior ones, which is similar to the Recurrent Neural
Network (RNN) in deep learning to model a time sequence function.
2.2

Recurrent Neural Network

Recurrent Neural Network (RNN) is used to model the pattern in a sequence of
data, mostly in time sequence. In recent years, RNN and its variants have been
applied successfully to problems such as speech recognition, machine translation,
and text-to-speech rendering. It has an internal cell that repeatedly processes an
input, carries a hidden state, and produces an output at each step. The RNN cell

6

L. Huang et al.

can be designed to be simple or complex to model a problem with a forgettable
memory mechanism (Long Short-Term Memory (LSTM) [5]) or/and a gating
mechanism (Gated Recurrent Unit (GRU) [6]).

Fig. 1. A Simple RNN Model (a) with feedback loop, and (b) with loop unfolded

Figure 1(a) shows a typical RNN structure that repeatedly takes an input,
updates its hidden state, and produces an output at every step. The RNN model
can be unfolded as shown in Fig. 1(b) that learns the recurrence relationship from
a sequence of data. The hidden state hi remembers the prior state of the process
and is updated at each step. The hidden state enables RNN to learn the temporal
relationships among the inputs since most of the time sequence data do contain
temporal patterns. LSTM allows RNN to forget long-term relationships built up
in the hidden state and emphasizes the short-term relationships, which can be
useful for many cases.
A simple RNN can be expressed in the Eq. (6):
ht = σh (Wh xt + Wh ht−1 + bh )
yh = σy (Wy ht + by )

(6)

where xt is the input, ht is the hidden state, W is the weights, b is the bias, and
σ is the activation function.
Looking back to the Eq. (5), there are two hidden states u(x, t) and u(x, t −
Δt) if we can restructure the ﬁnite diﬀerence method using an RNN. There is
also a spatial stencil relationship of neighboring velocity distribution. We deﬁne
a new function F with input of f (x, t), two hidden states u(x, t) and u(x, t − 1),
and the constant velocity distribution c:

Seismic Simulation, Inversion and Deep Learning

7

F (f (x, t), u(x, t), u(x, t − 1), c)
= f (x, t)c2 Δt2 + (2u(x, t) − u(x, t − 1))
Δt2
+ c2
(u(x − 1, t) − 2u(x, t) + u(x + 1, t)).
Δx2

(7)

Then, the Eq. (5) can be restructured as an RNN format:
ht+1 = σ(F (f (t), h(t), h(t − 1), c))
yt+1 = P (ht+1 )

(8)

where P is the projection function to get the sample of a trace from a receiver.
The Eq. (8) is then a non-learnable, deterministic physical solution represented
as the deep learning RNN model. Figure 2 shows the RNN model we designed
that solves the wave equation with four inputs f (x, t), h(t), h(t − 1), and c, the
velocity distribution which is constant in the equation. The output yt is the trace
sample of a receiver at each time step.

Fig. 2. A RNN model for wave equation

2.3

PyTorch RNN Implementation

The wave equation RNN model we designed in Fig. 2 enables us to utilize the
deep learning software platform to solve the wave equations. The beneﬁts of using
a deep learning model to represent an HPC application include: (1) we will be
able to leverage the HPC implementation of the deep learning model exploiting
the advantages of GPUs/multicores and vectorization for better performance;
(2) have an automatic gradients calculation using the built-in automatic diﬀerentiation package in deep learning; (3) utilize the variety of built-in optimizers
to apply the gradients to ﬁnd the global/local optimums; (4) use the data- and

8

L. Huang et al.

model- parallelism framework implemented in deep learning package to run the
application on a HPC cluster.
The following shows a code snippet of our RNN-similar implementation of wave equation using PyTorch. There are two classes derived from
torch.nn.Module for RNN cell and RNN driver respectively. We called them
Wave PGNNcell and Wave Propagator in our code. The Wave PGNNcell implemented a cell function in RNN that computes the waveﬁeld at a time step. The
Wave Propagator iterates over all time steps and takes the Ricker source waveform sample as the input at each time step. The hidden state (self.H) contains
the next and current waveﬁelds, which are fed into the cell for the next iteration.
The trace is collected by projecting the current waveﬁeld based on the receiver
location. The program returns the simulated waveﬁeld and sampled trace at the
end.
c l a s s Wave PGNNcell ( t o r c h . nn . Module ) :
def f o r w a r d ( s e l f , H, s r c ) :
uC , uP = [ H [ 0 ] , H [ 1 ] ]
...
return [ uN , uC ]
c l a s s Wave Propagator ( t o r c h . nn . Module ) :
s e l f . c e l l = Wave PGNNcell (C, c o n f i g )
def f o r w a r d ( s e l f ) :
us = [ ]
# l i s t of output wavefields
traces = [ ]
rcv = s e l f . rcvrs
f o r i t in r a n g e ( s e l f . nt ) :
s e l f . H = s e l f . c e l l . f o r w a r d ( s e l f . H, s e l f . ws [ i t ] )
us . append ( s e l f .H [ 0 ] . d e t a c h ( ) . numpy ( ) )
# E x t r a c t w a v e f i e l d sample a t each r e c e i v e r
samps = r c v . sample ( s e l f . H [ 0 ] . c l o n e ( ) )
t r a c e s . append ( samps )
t r c = t o r c h . s t a c k ( t r a c e s , dim=1)
return us , t r c

2.4

Seismic Wave Simulation

For seismic wave simulation, we use our RNN model to simulate the acoustic
wave propagation for the scalar wave equation. We create a “true” synthetic
model and an initial model, which can be a smoothed version of the true model
or some other separately chosen function. We use the Ricker wavelet as a waveform for one or more energy sources (shots) and create an array of receivers for
collecting traces. We assume the constant density in these models.
As we stated earlier, one beneﬁt of using deep learning software is to take
advantage of its multiple CPUs and GPUs implementation. We only need to specify which devices the code will operate on and deﬁne tensors to these devices. All
remaining device-speciﬁc implementation and optimizations are done internally
by PyTorch. We do not need to use CUDA or OpenACC to port the code to
these devices.
Another beneﬁt is to use the data-parallelism implemented in PyTorch. We
can parallelize the code by the number of the sources/shots to run the code on
multiple GPUs and distributed clusters.

Seismic Simulation, Inversion and Deep Learning

9

In our implementation, we use PyTorch1 1.5 to build the RNN model.
PyTorch is an open source machine learning framework developed by Facebook by merging Torch and Caﬀe2, which supports a variety of hardware platforms including multiple CPUs, GPUs, distributed systems, and mobile devices.
Besides the machine learning and deep learning functions, one unique feature
of PyTorch is that it contains a just-in-time compiler to optimize the code if it
complies with TorchScript, which is a subset of Python. It has a built-in automatic diﬀerentiation package for calculating derivatives, as well as a distributed
training module to train a model on a HPC cluster. PyTorch has both Python
and C++ frontends.
Figure 3 shows a 1D seismic Velocity Inversion case applying our physicsruled RNN implementation. The Fig. 3(a) shows a true synthetic velocity model
and an initial model; Fig. 3(b) shows the inverted model comparing with the
true model (up) and a slightly smoothed ﬁnal inverted model (down); Fig. 3(c)
shows the comparison of the true traces and the inverted traces; and Fig. 3(d)
shows the waveﬁeld on how the seismic wave propagates with respect to space
and time.

(a) a True Model and an Initial Model

(b) The Inverted Model Comparison

(c) Trace Comparison

(d) Waveﬁeld

Fig. 3. Applying RNN for 1D seismic velocity inversion
1

https://pytorch.org/.

10

L. Huang et al.

The 1D inversion experiment ﬁnds a close-to the true model solution after 100
iterations. We use Adam optimizer [7] with L2 regularization. We are currently
working on 2D cases by revising PySIT package. We continue performing more
testing cases to evaluate the performance with both data and model parallelism
provided by PyTorch on a CPU cluster and multiple GPUs.

3

Diﬀerentiable Programming

3.1

Automatic Diﬀerentiation and Adjoint-State Method

The automatic diﬀerentiation (AD) is also called algorithmic diﬀerentiation that
calculates the derivatives of any arbitrary diﬀerentiable program. Unlike using
the numerical diﬀerentiation of the adjoint state method that is an approximation to calculate the derivatives, the automatic diﬀerentiation returns the
exact answer of the derivatives, though subject to the intrinsic rounding error.
Machine learning software such as TensorFlow and Pytorch all have the built-in
implementation of AD as the core functionality of backpropagation to optimize
machine learning models. Accurate gradients are critical to the gradient-based
optimizations used in both scientiﬁc computing and machine learning.
In order to calculate the derivatives of any diﬀerentiable programs, AD needs
to store all operations on the execution path along with the intermediate results.
It then propagates derivatives backward from the ﬁnal output for every single
operation connected with the chain rule. For large scale application, AD faces
the challenge of meeting the demands of fast-growing storage in proportion to
the executed operations. Furthermore, the individual derivative function for each
operation also slows down the computation with intrinsic sequential execution.
More work needs to be done if AD can be directly applied to a real scientiﬁc
application.
Computationally expensive scientiﬁc applications typically use the adjoint
state method to calculate the gradient of a function with much better computation eﬃciency, although it is a numerical approximation. In FWI, the adjoint
state method calculates the derivative of a forward function J(m) that depends
on u(m). The forward function J can be deﬁned using h, as following [8]:
J(m) = h(u(m), m)

(9)

where m is the model parameter, which belongs to the model parameter space
M and u belongs to the state variable space, U. The state variables, u follow
the state equations outlined with the mapping function, F, which is also known
as the forward problem or forward equation [8]:
F (u(m), m) = 0.

(10)

The mapping function F is mapping from U * M to U and is satisﬁed by the
state variable u. If the condition F(u, m) = 0 is satisﬁed, the state variable u
becomes a physical realization. Then, the adjoint state equation can be given as
following, where λ is the adjoint state variable and ũ is any element of U [8]:

Seismic Simulation, Inversion and Deep Learning

[

δF (u, m) ∗
δh(u, m)
] λ=
.
δ ũ
δ ũ

11

(11)

This adjoint-state gradient calculation involves computing the reverse-time
propagated residual waveﬁeld, combining with the saved forward-propagated
waveﬁeld snapshots at speciﬁed time intervals to provide adjustments to the
medium properties (the gradient) at each spatial mesh point. In summary, the
forward propagation computes data observations representing the response of
the model, and the residual between the model response and actual observed
data is backward propagated and combined with the forward model response to
compute adjustments to the current model estimate.
Intervening in the calculation of the gradient in this manner allows for management of the required computational resources by saving the forward waveﬁelds only as often as numerically required, explicitly managing data resources
through staging to disk or check-pointing as needed, implementing shot-level
parallelism, and other specially tailored techniques.
3.2

Extended Automatic Diﬀerentiation

A diﬃculty with the auto-diﬀerentiation (AD) procedure is that memory requirements for the back-propagation graph can become excessive, as noted by
Richardson [2]. Applying chain-rule diﬀerentiation on elemental network nodes
over thousands of RNN time steps for a large mesh of physical parameter values is a reasonably-sized task for 1D problems, but the graph quickly becomes
intractable for 2D and 3D models. This issue renders impractical the use of pure
AD for such model inversion problems.
In order to solve the problem, we extended the AD backward process using
PyTorch AD workﬂow to integrate the adjoint-state method for the more eﬃcient
gradient calculation. In PyTorch, we can customize the AD workﬂow by providing a backward function to calculate the gradients of any function. We need to
pass the required parameters of the forward function, the model parameters and
loss function to allow the backward function to pick up these parameters for the
adjoint-state calculation.
Control over this auto-diﬀerentiation process is available through use of a
PyTorch extension to the Autograd feature pictured conceptually in Fig. 4,
wherein the RNN layer of the network can be replaced by a forward propagation loop and corresponding adjoint back-propagation loop for an equivalent
gradient calculation provided by the user. This alternative gradient calculation
can take advantage of well-known techniques in seismic inversion processing,
enabling existing performance enhancements to be applied using the extended
PyTorch capability for specially designed back-propagation.
In the present case, the physical medium properties to be optimized are provided to the “forward” wave propagation problem implemented using the publicly available PySIT seismic inversion toolkit [4], creating a simulated seismic
response. The corresponding “backward” propagation consists in using the residual waveﬁeld represented by the diﬀerence between the simulated data and the

12

L. Huang et al.

Fig. 4. Adjoint gradient: Automatic diﬀerentiation vs. Adjoint gradient calculation.
Diﬀerentiation respect to model parameters are replaced by gradients from adjoint
state in the backward automatic diﬀerentiation.

observed seismic trace data from the corresponding actual ﬁeld data recording
(or recordings from a “true” model in our synthetic studies), and implementing the “adjoint-state” solution to provide the required gradient of the model
parameters. Other implementations of wave propagation solutions may also be
used in this framework, such as spectral-element methods [9] for 2D, 3D and
spherical 3D wave propagation.
The beneﬁcial end result is that traditional adjoint-state solution methods
are incorporated into the AD workﬂow, so that seismic inversion calculations can
be integrated within the broader deep learning process with eﬃcient calculation.

4

Seismic Inversion

4.1

Seismic Inversion

Seismic Inversion [10] is the method to reconstruct the earth subsurface image
by inverting seismic data observed via the multiple distributed sensors on the
surface. It is typically implemented using the adjoint state method [8] to calculate the gradients. As described in Sect. 2 and Sect. 3, by reconstructing the
forward problem using deep learning software, the seismic inversion problem can
be solved by the automatic diﬀerentiation package, a variety of optimizers provided by PyTorch, and a customized loss function. The automatic diﬀerentiation
package in PyTorch implements the methodology of automatic diﬀerentiation
by recording all the forward operations in sequence and performing backward
derivative computation based on the chain rule.
Figure 5 shows the workﬂow of seismic inversion. The initial model M 0 is a
guess of the true model M that needs to be inverted. In these early experiments
using several shots of a synthetic seismic reﬂection survey over a small 2D Earth
model, we used for convenience an initial model guess that is a smoothed version

Seismic Simulation, Inversion and Deep Learning

13

Fig. 5. The full waveform inversion workﬂow

of the true model. The seismic traces are either observed via distributed sensors
on top of the earth surface in the real-world application or are simulated using
the seismic wave forward function in this paper. The residual is obtained by
∂u
is calculated
comparing the synthetic data and observed data. The gradient ∂M
based on the residual with respect to the initial model. The gradients are used
by a gradient-based optimizer to update the initial model to get a step close
to the real model. The entire process ends when the initial model and the true
model are converged or exceeded the speciﬁed number of iterations.
4.2

AutoEncoder for Dimensionality Reduction

The seismic inversion process needs to uncover the physical properties at every
point represented in the geological space, which quickly leads to a large number
of model parameters to optimize in the traditional FWI process. The nature of
the nonlinear and ill-posed inverse problem often falls into the local minimum
traps. It is a sound solution to apply the dimensionality-reduction technique to
reduce the optimization parameters to improve the optimization accuracy by
engaging with machine learning models.
Since we have customized the automatic diﬀerentiation workﬂow by integrating the adjoint state method for the FWI gradients (described in Sect. 3), it is
now feasible to integrate the machine learning models into the FWI workﬂow
and keep the program diﬀerentiable. Since the AutoEncoder A(x) is diﬀerentiable
and the forward model F (x) is diﬀerentiable, the composition of the F (A(x))
is diﬀerentiable. We choose the AutoEncoder as the dimensionality-reduction
method and apply it before the forward model as shown in Fig. 6.
The AutoEncoder contains 743,938 parameters as shown in Fig. 7a and b. The
AutoEncoder is an unsupervised learning model that compresses the information
representation of the input data to a sparse latent variable with less dimensions
at the middle of the encoded layer. It then reconstructs the data from the encoded
latent variable to the original or enhanced data. The compression process is
called encoder and the reconstruction is called decoder. The encoder learns how
to compress the input data and describes it with the latent variable, while the
decoder learns how to reconstruct the data from the latent variable.

14

L. Huang et al.

Fig. 6. The full waveform inversion workﬂow

8

1

4
I/

4

Encoded
256

256

2

128
64

I

I

64

128

I/

I/

2

I/

I/

I/

8

10

8

1

1

Input

Output

(a) The AutoEncoder Network Structure
(b) The AutoEncoder Model Parameters

Fig. 7. Traditional seismic velocity inversion

We start the AutoEncoder training by generating a large number of random
seismic velocity models. In this work, we are using some simple and ﬂat velocity
layers representing the velocities of diﬀerent earth interiors including water and
rocks. Speciﬁcally, these models contain one or more low velocity layers in the
middle or bottom of these layers that is challenging for the low velocity inversion.
All of these models have the ﬁxed dimensions of 60 × 80. As indicated in Fig. 7a,
the AutoEncoder has two components: a encoder and a decoder. The encoder
compresses the input model with dimension of 60 × 80 to an encoded latent
variable with dimension of 8 × 10, which is 1/60 of the original dimension. The
latent variable is then decompressed by the decoder to restore to its original
dimension.
The loss function we used to train the AutoEncoder is the mean-square-error
(MSE) loss and the optimizer is Adam with learning rate of 0.001. The batch
size used is 128. The loss values during the training process is shown in Fig. 8.

Seismic Simulation, Inversion and Deep Learning

15

Fig. 8. The autoEncoder training loss

Figure 6 shows the AutoEncoder enhanced FWI process, where the AutoEncoder is inserted before the forward function simulation starts. Note that the
encoder is only applied to the ﬁrst iteration to get the encoded latent variable.
For the rest of optimization iterations, the decoder is applied to decompress the
encoded latent variable to get a new velocity model with the original dimension.
During the gradient-based optimization process, the gradients are calculated
with respected to the encoded latent variable, instead of the original model,
which reduced the dimensionality of the optimization search space to 1/60. We
use the MSE loss and Adam optimizer during the process.
4.3

Results

PyTorch has a list of optimizers including Adam [7], RMSprop [11], stochastic
gradient descent (SGD), Adadelta [12], Adagrad [13], LBFGS, and their variants. The learning rate, scheduler and regularizations can be speciﬁed to ﬁt
diﬀerent optimization problems. There are also multiple regression and classiﬁcation loss functions implemented in PyTorch. All of these packages provide a
rich environment to solve inverse problems.
In our implementation, we have demonstrated how to invoke the extended
automatic gradient calculation for the velocity model. We choose the Adam
optimizer and the MSE loss function to compare the misﬁt of the simulated
traces and observed traces after each iteration of the forward model. The partial
derivative (the gradient) of the loss function with respect to the initial model
and the encoded latent variable is calculated by the automatic diﬀerentiation
process, which is applied by the optimizer to minimize the misﬁt. These iterations
gradually ﬁnd an approximation of the true velocity distribution.
Figure 9 and Fig. 10 show the diﬀerences of the traditional FWI and the
AutoEncoder enhanced FWI results. Fig. 9(a) shows the initial model, the true
model, and the inverted model; the loss graph Fig. 9(b) shows the loss values
(at diﬀerent scales) after each optimization iteration, and Fig. 9(c) shows the
diﬀerence between the inverted model and the initial model (top), as well as
the diﬀerence between the inverted model and the true model. It appears that

16

L. Huang et al.

(a) The Initial, True and Inverted Model Comparison

(b) Loss Function Value

(c) Diﬀerences

Fig. 9. Traditional seismic velocity inversion

the traditional FWI does not optimize well in the low velocity layer case after
40 iterations ended with a high loss value, which falls into a local trap. The
AutoEncoder-enhanced FWI discovers the low velocity layer very well and continues to optimize the misﬁt for all 100 iterations. The diﬀerence graphs also conﬁrm that the AutoEncoder case identiﬁes all layers well showing less structured
misﬁts. Noticeably, there are also less artifacts introduced in the AutoEncoder
enhanced FWI compared with the traditional FWI.
As described in Sect. 3, the automatic diﬀerentiation provided by the PyTorch
software does not provide suﬃcient eﬃciency to solve the FWI 2D problem. The
gradients calculated for the whole program takes too long and too much space
to store them. We use the hybrid method describe in Sect. 3.2 to overcome the
problem by incorporating the adjoint state method. As the result, the gradient
calculation using the hybrid approach achieves both accuracy and eﬃciency,

Seismic Simulation, Inversion and Deep Learning

17

(a) The Initial, True and Inverted Model Comparison

(b) Loss Function Value

(c) Diﬀerences

Fig. 10. The AutoEncoder enhanced seismic velocity inversion

which is feasible to be used for a large scale scientiﬁc computation problem
integrating with machine learning models.

5

Discussion

There are a few of points that worth noting for the work. The ﬁrst is that the
automatic diﬀerentiation is key for diﬀerentiable programming, which can bridge
the physics-based scientiﬁc computing with the machine learning (ML)/artiﬁcial
intelligence (AI) technologies. ML/AI methods do not have physics principles
built in that may create an infeasible solution given the fact that most of the scientiﬁc inverse problems may be ill-posed. In our prior work [14], the convergence
of ML with a scientiﬁc application without diﬀerentiable programming may not
ﬁnd a generalized solution since optimizations of the two diﬀerent methods are
disconnected.

18

L. Huang et al.

The second point we would like to make is that the automatic diﬀerentiation
needs additional improvements to make it feasible to other applications. In our
method, we integrate the adjoint-state method to make it feasible to solve a large
case, however the solution is an approximation. If the automatic diﬀerentiation
method can be more memory-eﬃcient and parallelizable, it can be much more
useful to compute the exact gradients for the large complex problems.
The last point is the deep learning model AutoEncoder requires a revisit
to reduce the loss during decoding. Although it reduces the dimension by compressing the input data into a sparse latent variable, the reconstruction is not
lossless. There are some errors introduced during the reconstruction process that
may hinder the optimization process. There is a trade-oﬀ to take into the consideration when designing the convergence of ML/AI with scientiﬁc computing.
The good news is that there are many options to integrate them waiting for us
to explore.

6

Conclusion and Future Work

We have successfully demonstrated two case studies of restructuring the wave
equation using ﬁnite diﬀerence method in a deep learning RNN model framework
and an AutoEncoder enhanced FWI process. The beneﬁts of the work include
fully utilizing the high-performance tensor processing and optimization capabilities implemented in the deep learning package PyTorch, as well as the deep
integration of machine learning models with the inverse problem. By integrating
an HPC application with a deep learning framework with diﬀerential programming, we can explore a large number of combinations of machine learning models
with physical numerical solutions to achieve better accuracy and eﬃciency.
Acknowledgment. This research work is supported by the US National Science Foundation (NSF) awards ##1649788, #1832034 and by the Oﬃce of the Assistant Secretary of Defense for Research and Engineering (OASD(R&E)) under agreement number
FA8750-15-2-0119. The U.S. Government is authorized to reproduce and distribute
reprints for Governmental purposes notwithstanding any copyright notation thereon.
The views and conclusions contained herein are those of the authors and should not
be interpreted as necessarily representing the oﬃcial policies or endorsements, either
expressed or implied, of the US NSF, or the Oﬃce of the Assistant Secretary of Defense
for Research and Engineering (OASD(R&E)) or the U.S. Government. The authors
would also like to thank the XSEDE for providing the computing resources.

References
1. Baydin, A.G., Pearlmutter, B.A., Radul, A.A., Siskind, J.M.: Automatic diﬀerentiation in machine learning: a survey. J. Mach. Learn. Res. 18(1), 5595–5637
(2017)
2. Richardson, A.: Seismic full-waveform inversion using deep learning tools and techniques (2018). https://arxiv.org/pdf/1801.07232v2.pdf

Seismic Simulation, Inversion and Deep Learning

19

3. Hughes, T.W., Williamson, I.A.D., Minkov, M., Fan, S.: Wave physics as an analog
recurrent neural network (2019). https://arxiv.org/pdf/1904.12831v1.pdf
4. Hewett, R.J., Demanet, L., The PySIT Team: PySIT: Python seismic imaging
toolbox (January 2020). https://doi.org/10.5281/zenodo.3603367
5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997). https://doi.org/10.1162/neco.1997.9.8.1735
6. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recurrent neural networks on sequence modeling (2014)
7. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization (2014)
8. Plessix, R.-E.: A review of the adjoint-state method for computing the gradient
of a functional with geophysical applications. Geophys. J. Int. 167(2), 495–503
(2006). https://doi.org/10.1111/j.1365-246X.2006.02978.x
9. Tromp, J., Komatitsch, D., Liu, Q.: Spectral-element and adjoint methods in seismology. Commun. Comput. Phys. 3(1), 1–32 (2008)
10. Schuster, G.: Seismic Inversion. Society of Exploration Geophysicists (2017).
https://library.seg.org/doi/abs/10.1190/1.9781560803423
11. Ruder, S.: An overview of gradient descent optimization algorithms (2016)
12. Zeiler, M.D.: ADADELTA: an adaptive learning rate method (2012)
13. Duchi, J., Hazan, E., Singer, Y.: Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res. 12, 2121–2159 (2011)
14. Huang, L., Polanco, M., Clee, T.E.: Initial experiments on improving seismic data
inversion with deep learning. In: 2018 New York Scientiﬁc Data Summit (NYSDS),
August 2018, pp. 1–3 (2018)

