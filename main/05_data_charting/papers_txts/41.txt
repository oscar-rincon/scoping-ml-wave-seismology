CHAPTER 7

Applying scientific machine learning
to improve seismic wave simulation
and inversion
Lei Huang, Edward Clee, Nishath Ranasinghe

Department of Computer Science, Prairie View A&M University, Prairie View, TX, United States

Abstract
Accurate simulation of wave motion for the modeling and inversion of seismic wave propagation is a
classical high-performance computing (HPC) application using the finite difference, finite element, and
spectral element methods to solve the wave equation numerically. The article presents a new method
to improve the performance of seismic wave simulation and inversion by integrating the deep learning
software platform and deep learning models with the HPC application. The paper has three contributions: (1) instead of using traditional HPC software, the authors implement the numerical solutions for
the wave equation employing recently developed tensor processing capabilities widely used in the
deep learning software platform of PyTorch. By using PyTorch, the classical HPC application is reformulated as a deep learning recurrent neural network framework; (2) the authors customize the automatic
differentiation of PyTorch to integrate the adjoint state method for an efficient gradient calculation;
(3) the authors build a deep learning model to reduce the physical model dimensions to improve the
accuracy and performance of seismic inversion. The automatic differentiation functionality and a variety
of optimizers provided by PyTorch are used to enhance the performance of the classical HPC application. Additionally, methods developed in the paper can be extended into other physics-based scientific
computing applications, such as computational fluid dynamics, medical imaging, nondestructive testing, as well as the propagation of electromagnetic waves in the earth.

Keywords
Inverse problem; Scientific machine learning; Seismic imaging; Seismic wave propagation

7.1 Introduction
This chapter explores the use of Scientific Machine Learning (SciML)1 concepts in
addressing the problem of estimating properties of the Earth associated with the propagation of seismic waves. SciML has recently emerged as a new method to solve the scientific computing problems using machine learning models and vice versa. The method
leverages the success of traditional scientific computational models and the advances in
data-driven machine learning models to augment the efficiency and accuracy of scientific simulation and inversion. Moreover, it facilitates the scientific discovery by modeling both well-known scientific rules and the unknown patterns based on observed data.
Advances in Subsurface Data Analytics
DOI: https://doi.org/10.1016/B978-0-12-822295-9.00011-X

© 2022 Elsevier Inc.
All rights reserved.

167

168

Advances in subsurface data analytics

In this chapter, SciML is studied and applied to simulate the wave propagation and
to enhance the quality of the seismic inversions. In seismic reflection surveys, sound
waves from controlled sources are recorded by a large number of surface- or boreholedeployed sensors (receivers) which capture signals both directly transmitted as well as
reflected from structures at depth, for the purpose of deducing structural variations in
the velocity and density properties of the subsurface rock formations.

7.1.1 Seismic imaging
Advanced seismic imaging is the process of constructing a model (or “image”) of the
Earth properties through an “inversion” process2, starting with an initial guess, simulating
the wave propagation for the field survey and computing a misfit with respect to the
field-recorded data, and iterating through a gradient-based optimization process until an
acceptable Earth model is achieved. The particular method of Full-Waveform Inversion
(FWI) is typically implemented using the adjoint state method3 to calculate the gradients.
Fig. 7.1 shows the workflow of seismic inversion. The initial model M0 is a guess of
the true model M that is the unknown “target” of the inversion. These early experiments, comprising several shots of a synthetic seismic reflection survey over a small
2-D Earth model, used for convenience an initial model guess that is a smoothed version of the true model. The seismic traces are either observed via real-world survey
recordings or are simulated using the seismic wave forward function in this paper. The
residual is obtained by comparing the synthetic data and observed data. The gradient
∂u
is calculated based on the residual with respect to the initial model. The gradients
∂M
are used by a gradient-based optimizer to update the initial model to get a step closer

Fig. 7.1 The full waveform inversion workflow.

Applying scientific machine learning to improve seismic wave simulation and inversion

to the real model. The entire process ends when the initial model and the true model
are adequately converged or the specified limit of iterations is exceeded. Enhancements
to this process using machine learning are treated in a later section, after discussion of
additional required concepts.

7.1.2 Computational issues for wave propagation
Physical simulation and inversion are classical scientific computing applications to
examine the physical phenomenon and reveal the underlying medium properties. The
simulation solves the partial differential equations (PDEs) that govern the physical
phenomenon using numerical approximation methods, while the inversion applies the
gradient-based optimizations to find the underlying properties by minimizing the misfit
between the observed data and the simulated results. The entire process takes significant
computing resources to achieve the required accuracy. However, the inverse problem
is naturally challenging since it is ill-posed and nonlinear for most cases4. In particular,
the optimization process in the non-convex system frequently attempts to converge to
a local minimum instead of the desired global minimum, due to the effect of “cycle
skipping” in which the alignment of seismic events is improperly discerned because of
insufficient sampling in time or space. Practical approaches to mitigate these problems
are discussed by Virieux and Operto5, and still need to be applied even when using
machine-learning alternative methods.
Another challenge in implementing the seismic inversion process is the need to correctly compute the adjoint-state solution whereby the gradient for updating the physical parameters in the model space must be calculated by backward propagation of the
residual data field at each observation point for each source. This calculation is at least
as challenging as the original forward propagation through the model, and represents a
differential process with stringent accuracy requirements.

7.1.3 Opportunity for data analytics
A potential application of data analytics technology in the analysis of seismic reflection imaging data is to utilize the characteristic patterns exhibited in seismic reflection
recordings to learn the full-waveform responses of various Earth structure patterns for
different survey geometries by means of a large number of model experiments6, and
train a very deep neural network to recognize such responses in actual field-recorded
surveys and discern likely structures that are generating such responses. Inference of
structure from such a data-driven approach could then be faster than traditional inversion operations that require many forward and back-propagated simulations in the
optimization process. Such a model-driven process can be made efficient by the using
a recurrent neural network (RNN) implementation of the wave propagation forward
model simulations, which can be transparently implemented on specialized hardware
through the newer tensor processing systems, such as TensorFlow and PyTorch.

169

170

Advances in subsurface data analytics

Ultimately, the objective is to integrate the machine learning models that are datadriven into the scientific computational models that are physics-driven. The differentiable programming method has the potential to smoothly integrate the two models
together with a global optimization. It is expected that this study will lead to more
interesting research findings in the topic of SciML and to discovering ways to combine
the power of these two different methods to facilitate scientific discovery.

7.1.4 Scientific machine learning
A recent push to incorporate domain specific knowledge with machine learn ing has
shown great promise and it is emerging as a sub-field under the name of SciML. In a
recent workshop organized by the US Department of Energy to identify basic research
needs for SciML, a group of leading researchers in the field identified domain-aware,
interpretable and robust machine learning as fundamental to the development of
SciML7. They further highlighted the growing enthusiasm on SciML among the wider
scientific community is due to the broad availability of HPC resources; developments
in computationally effective data analysis algorithms as well as the availability of large
amounts of high-quality discrete and continuous data collected from scientific instruments7. Furthermore, SciML could replace meshing associated with classical numerical
discretization techniques, such as finite element method, finite difference method with
neural networks that could approximate the solutions of physical problems described
as PDEs with the use of automatic differentiation8, and also break free from the curse
of dimensionality9. The main idea behind obtaining approximate solutions to PDEs in
SciML is to constrain the neural network to minimize the residual of the PDE. The
SciML research currently includes the physics-informed neural networks (PINNs)10,11,
the universal differential equation (UDE) method12, the Hamiltonian neural networks
(HNNs)13,14, and the neural ordinary differential equation (NODE)15 to learn the nonlinear dynamics in physics.
The SciML method with integration of data and physics modeling is demonstrated
in two ways in this chapter:
1. By expressing wave propagation in the form of a RNN, allowing the gradients
calculated by automatic differentiation to be used for the FWI of seismic reflection
survey data, and
2. Incorporating existing forward wave propagation and adjoint-state inversion
solutions through extensions to the deep learning auto-differentiation tools, thus
allowing seismic inversion to be integrated with a deep learning model to create an
end-to-end differentiable programming environments.
The contributions of this work are summarized in the following:
1. The PDE solver in the seismic forward model is reformulated using RNN and
implemented with the deep learning software package, PyTorch, which allows taking advantage of the tensor processing software and its accelerator implementation.

Applying scientific machine learning to improve seismic wave simulation and inversion

2. The automatic differentiation feature implemented in PyTorch enables calculating a
model gradient to solve the seismic inverse problem to uncover the earth’s interior
physical properties.
3. Efficiency of the automatic differentiation feature is improved by extending it to create
a hybrid back propagation using the adjoint-state method to calculate the gradients.
4. The optimization is incorporated into an autoencoder network to reduce the
dimensions of the inverted parameters to augment the convergence process and get
more accurate results for the ill-posed problem.

7.2 Related work
7.2.1 Seismic wave simulation and inversion
In this study, the application of a tensor-based machine learning system for solving physical simulation and gradient-based optimization problems is illustrated using seismic wave
propagation simulation and full waveform inversion (FWI) as the physical case study.
Both Richardson16 and Zhu and Xu17 have mathematically demonstrated the equivalence of wave-propagation inversion by the adjoint-state method with a RNN and
its back-propagation gradient computed through automatic differentiation. Richardson
uses this equivalence to incorporate an existing implementation of forward modeling
and adjoint-state inversion as an extension to the automatic differentiation feature of
tensor processing, allowing interventions for explicitly managing memory and computational demands. Zhu and Xu, taking an opposite approach, have formulated the
wave propagation problem as an RNN for which the adjoint-state inverse need not
be explicitly coded, but instead is implemented automatically using the automatic differentiation capability built into the tensor processing system. This allows both forward
and back propagation to be transparently managed on CPU and GPU processors, with
memory and computational demands satisfied simply by allocating more resources in
parallel as needed.

7.2.2 Surrogate models using machine learning
Longer run times and slower convergence inhibit the rapid integration of scientific
models in real time applications and critical decision making processes. Additionally,
the “curse of dimensionality” is encountered when the number of samples required to
cover the parameter space increases with the number of parameters in the model. The
issue of longer run times demands that models be simplified, ignoring physical process
and reduction in the numerical accuracy of the models. On the other hand, surrogate
models facilitate the execution of complex physical models while preserving numerical accuracy. Surrogate models are also referred to as meta models18, model emulators19,
reduced order models20,21, physics based proxy models22, multifidelity models23, and
response surfaces24.

171

172

Advances in subsurface data analytics

7.2.3 Dimensionality reduction
Large realistic seismic inversions in geologically complex settings often utilize global
optimization algorithms, such as grid search25, simulated annealing26, very fast simulated
annealing27, Monte Carlo optimization28, particle swarm optimization29 due to the presence of many local minima in the misfit function between the model and the data.
However, global optimization modeling suffers from slow convergence rates as the dimensionality of the model space is increased. In machine learning, autoencoders are widely utilized to reduce dimensionality of the model space. An autoencoder neural network30 is an
unsupervised learning algorithm that implements backpropagation in an effort to generate
output values that are equal to the input values. It consists of an encoder and decoder; the
encoder operator reduces the dimensionality of the model space and the decoder operator tries to regenerate the original input data from the low-dimensional representation.
Moseley et al.31 used WaveNet network architecture32 and a conditional autoencoder
to simulate the earth response to the propagation of acoustic waves in horizontally
layered mediums and more general faulted 2D mediums, respectively. The encoder
network in the autoencoder is composed of 10 convolutional layers, which reduces
the spatial dimension of the input velocity model to a latent vector of 1*1 with
1,024 hidden channels. The decoder network utilizes 14 convolutional layers to expand
the latent space back into the original dimen sion. All the hidden layers in the autoencoder network utilizes ReLU activation functions33 except the final output layer which
utilizes an identity activation function. For the both networks, they observed the resulting seismic inversion are 20–500 times faster than finite difference modeling. But the
authors noted, extending the deep neural network based seismic inversion into more
complex, elastic and 3-D models will be challenging.
Gao et al.34 used an autoencoder neural network to efficiently conduct a 1-D seismic
impedance inversion implementing trace-by-trace inversion with regularization using
a fully connected convolutional neural network. They used differential evolution35, a
stochastic, population-based global optimization algorithm, which solves optimization
problems based on a population of individuals in which each is a candidate solution.The
authors initially trained the autoencoder network to learn to encode the large dimension problem into a reduced order dimension problem and then decode it back to the
initial large dimension problem. They solve the large dimension problem by searching
the best acceptable solution in the reduced order space and then map the selected solution back to the higher order space to obtain the most acceptable model for the initial
problem. They further observed a good initial model supplemented with well-log data
is necessary to obtain an accurate inversion. The authors further showed their method
converges faster and produces more robust results than common inversion methods for
both synthetic and field data.
Another study by Chen and Schuster36 used skeletonized seismic data, which consists
of the low-rank latent-space variables produce by an adequately trained autoencoder

Applying scientific machine learning to improve seismic wave simulation and inversion

network, to obtain a subsurface velocity model by inverting the wave equation. The
authors coined the name Newtonian machine learning (NML) for their method as it
inverts for the model parameters by utilizing both forward and backward operations of
the Newtonian wave propagation while utilizing dimension reduction capabilities of
machine learning. They used seismic traces as the input to the autoencoder network,
which calculates the perturbation of skeletonized data with regard to velocity perturbations (Fréchet derivative). Then the gradient is calculated by migrating the shifted
observed traces weighted by the the skeletonized data residual. Finally, the model which
best determines the observed latent-space parameters are chosen as the final velocity
model. The authors also noted the their method could reduce the effects of cycle skipping which plagues FWI and it does not require manual picking of important features as
the skeletal data are automatically generated by the autoencoder network. They further
noted that the velocity model produced from their model has low resolution and it can
be used as a starting model for FWI.
A recent FWI study by Sun and Alkhalifah37 used meta-learning to train a neural
network to learn the optimization algorithm instead of using a predesigned optimization algorithm. They used gradient of the misfit function as the input of a RNN and
the history information of the gradient was used as the hidden states in the RNN37.
The authors formulated the loss function for the training as a weighted summation of
the L2-norm of the data residuals. Furthermore, they accelerated the training process
of the neural network by minimizing randomly generated quadratic functions by locally
approximating the optimization as a a linear convex problem. Additionally, they used
variational autoencoder38 methods to project and represent the model in latent space
to achieve more accurate and robust velocity model. They obtained faster convergence
rates compared to conventional FWI for both Marmousi39 and the Overthrust40 models.
Mosser et al.41 used deep generative models in the context of inverting the acoustic
wave equation to obtain subsurface models. They used Wasserstein generative adversarial networks (GAN)42–44 to create subsurface geological structures and their respective petrophysical properties as a priori model. Then they combined the models with
the acoustic wave equation and performed Bayesian inversion using an approximate
Metropolis-adjusted Langevin algorithm (MALA)45 drawing samples from a posterior
seismic observations. The authors used the adjoint method to calculate the gradients
with respect to the model parameters governing the forward problem and they exploited the differential nature of the deep neural network to calculate the gradient mismatch.
Swischuk et al. demonstrated projection-based model reduction.46

7.2.4 Differentiable programming
Differentiable programming is a programming paradigm that the whole numerical
computation in the program is differentiable via automatic differentiation (AD)47.
Differentiability is the core to machine learning and inverse problems since the program

173

174

Advances in subsurface data analytics

can be optimized via the gradient-based optimization algorithms. AD is also known as
algorithmic differentiation has been widely used in machine learning software to calculate the derivatives of a sequence of operations. AD stores the sequence of operations
and applies the chain-rule to calculate the derivative either in the forward or reverse
model. The back-propagation process, which is the backbone of machine learning,
relies on AD to calculate the accurate gradients for training a model. The development
of domain-aware SciML by combining capabilities of AD with the domain knowledge
will help to understand the full potential of SciML. AD is natural for scientists to apply
since it is automatically implemented, but it comes with a hefty price of efficiency48.The
computation of AD requires to store all operations, and it needs to follow the chain-rule
with a sequential execution fashion. In contrast, many scientific computing applications
use the adjoint-state method to calculate derivatives based upon numerical solutions
to the associated partial differential equation systems. The numerical solution methods
offer opportunity for critical time and memory savings that improve the efficiency
while allowing preservation of sufficient accuracy.
AD is the key component for a programming model that can potentially bridge
scientific computing and machine learning49. Julia50 is such a programming model
built with AD as its core component to provide a high-level and high-performance
programming model for numerical computing with differentiable programming. XU
et al.51 developed the AD seismic package based on Julia differential programming and
mathematically proved that the adjoint-state method is equivalent to the reverse-mode
AD, which can be used very effectively for seismic inverse problems. The authors also
learned hidden dynamics using intelligent AD52 that demonstrates a promising usage of
applying the differentiable programming on bridging scientific computing and machine
learning domains.

7.3 Wave equations and RNN
7.3.1 Wave equations
The wave motion is governed by physical rules that can be expressed in the following
partial differential equation (PDE) (1) along with the boundary conditions (2)(3). The
1D scalar wave equation is used for simplicity purpose in this paper:
1 ∂ u ( x, t ) ∂ u ( x, t )
−
= f ( x, t )
2
c ( x ) ∂t 2
∂x 2

(1.1)

1 ∂u ( 0, t ) ∂u ( 0, t )
−
=0
c ( 0 ) ∂t
∂x

(1.2)

1 ∂u (1, t ) ∂u (1, t )
−
=0
c (1) ∂t
∂x

(1.3)

2

2

Applying scientific machine learning to improve seismic wave simulation and inversion

where, c(x) is the spatial velocity distribution, u(x, t) is the wave field distribution in
space and time, and f (x, t) is the energy source distribution in space and time.
The Eq. (1.1) can be solved numerically using a finite difference approximation:
u( x − ∆x, t ) − 2u( x, t ), +u( x + ∆x, t )
∆x 2
1 u( x, t − ∆t ) − 2u( x, t ) + u( x, t + ∆t )
+ 2
c
∆t 2
After factoring, the Eq (1.4) can be expressed as:
f ( x, t ) = −

(1.4)

u ( x, t + ∆t ) = f ( x, t ) c 2 ∆t 2 + ( 2u ( x, t ) − u( x, t − ∆t ))
+ c2

∆t 2
(u( x − ∆x, t ) − 2u( x, t ) + u( x + ∆x, t ))
∆x 2

(1.5)

which shows that the next wave field in time u(x, t + ∆t) can be calculated based on
the current and prior wave fields, as well as spatial neighbors in the current wave field.
The wave motion simulation follows the time sequence to produce the next state based
on the prior ones, which is similar to RNN in deep learning to model a time sequence
function.

7.3.2 Recurrent neural network
RNN is used to model the pattern in a sequence of data, mostly in time sequence. In
recent years, RNN and its variants have been applied successfully to problems such as
speech recognition, machine translation, and text-to-speech rendering. It has an internal
cell that repeatedly processes an input, carries a hidden state, and produces an output at
each step. The RNN cell can be designed to be simple or complex to model a problem
with a forgettable memory mechanism (long short-term memory [LSTM]53) or/and a
gating mechanism (gated recurrent unit [GRU]54).
Fig. 7.2A shows a typical RNN structure that repeatedly takes an input, updates its
hidden state, and produces an output at every step. The RNN model can be unfolded
as shown in Fig. 7.2B that learns the recurrence relationship from a sequence of data.
The hidden state hi remembers the prior state of the process and is updated at each step.
The hidden state enables RNN to learn the temporal relationships among the inputs
since most of the time sequence data do contain temporal patterns. LSTM allows RNN
to forget long-term relationships built up in the hidden state and emphasizes the shortterm relationships, which can be useful for many cases.
A simple RNN can be expressed in the Eq. (1.6):
ht = σ h (Wh xt + Whht −1 + bh )
yh = σ y (Wy ht + by )

(1.6)

175

176

Advances in subsurface data analytics

Fig. 7.2 A simple RNN model (A) with feedback loop and (B) with loop unfolded.

where xt is the input, ht is the hidden state, W represents the weights, b is the bias, and
σ is the activation function.
The discretized wave Eq. (1.5) can be restructured as an RNN with two hidden
states u(x, t) and u(x, t − ∆t). There is also a spatial stencil relationship of neighboring
velocity distribution. The internal operation of the RNN can be defined using a new
function F with input of f (x, t), two hidden states u(x, t) and u(x, t − 1), and the constant
velocity distribution c:
F ( f ( x, t ) , u ( x, t ) , u ( x, t − 1) , c )

= f ( x, t ) c 2 ∆t 2 + ( 2u ( x, t ) − u ( x, t − 1) )
+ c2

(1.7)

∆t
(u ( x − 1, t ) − 2u ( x, t ) + u ( x + 1, t ) )
∆x 2
2

Then, the Eq. (1.5) can be restructured as an RNN format:

(

ht +1 = σ F ( f (t ) , h (t ) , h (t − 1) , c )
yt +1 = P ( ht +1 )

)

(1.8)

where, P is the projection function to get the sample of a trace from a receiver.
The Eq. (1.8) is then a nonlearnable, deterministic physical solution represented as the
deep learning RNN model. Fig. 7.3 shows the resulting RNN model that solves the wave
equation with four inputs f (x, t), h(t), h(t − 1), and c, the velocity distribution, which is
constant in the equation. The output yt is the trace sample of a receiver at each time step.

7.3.3 PyTorch RNN implementation
The wave equation RNN model presented in Fig. 7.3 enables use of the deep learning
software platform to solve the wave equations. The benefits of using a deep learning

Applying scientific machine learning to improve seismic wave simulation and inversion

Fig. 7.3 A RNN model for wave equation.

model to represent an HPC application include the ability to: (1) leverage the HPC
implementation of the deep learning model exploiting the advantages of GPUs/multicores and vectorization for better performance, (2) have an automatic gradients calculation using the built-in automatic differentiation package in deep learning, (3) utilize the
variety of built-in optimizers to apply the gradients to find the global/local optimums,
and (4) use the data- and model-parallelism framework implemented in deep learning
package to run the application on a HPC cluster.
The implementation in this work uses PyTorch1 v1.5 to build the RNN model.
PyTorch is an open source machine learning framework developed by Facebook by
merging Torch and Caffe2, which supports a variety of hardware platforms including
multiple CPUs, GPUs, distributed systems, and mobile devices. Besides the machine
learning and deep learning functions, one unique feature of PyTorch is that it contains
a just-in-time compiler to optimize the code if it complies with TorchScript, which is
a subset of Python. It has a built-in automatic differentiation package for calculating
derivatives, as well as a distributed training module to train a model on a HPC cluster.
PyTorch has both Python and C++ frontends.
The following Python code snippet illustrates an RNN-similar implementation of
wave equation using PyTorch. The two classes derived from torch.nn. Module for RNN
cell and RNN driver respectively are called Wave PGNNcell and Wave Propagator. The
wave PGNNcell implemented a cell function in RNN that computes the wavefield at
a time step. The Wave Propagator iterates over all time steps and takes the seismic source
waveform sample as the input at each time step.The hidden state (self.H) contains the previous and current wave fields, which are fed into the cell for the next iteration.The trace is
collected by projecting the current wavefield based on the receiver location.The program
returns the simulated wavefield and sampled trace at the end.
1https://pytorch.org/

177

178

Advances in subsurface data analytics

7.3.4 Seismic wave simulation
For a simple demonstration of seismic wave simulation, the RNN model is used to
simulate the acoustic wave propagation for the scalar wave equation. A “true” synthetic
model is used to generate a target “recorded” data set, and an initial model can be
constructed as a smoothed version of the true model or some other separately chosen
function. A time-symmetric Ricker wavelet is used as a waveform for one or more
energy sources (shots), and the wavefield is sampled as traces at an array of receivers.
Rock density is assumed constant in these models.
As stated earlier, one benefit of using deep learning technology is to take advantage
of its multiple CPUs and GPUs implementation. It is required only to specify which
devices the code will operate on and define tensors to these devices. All remaining
device-specific implementation and optimizations are done internally by PyTorch; porting of application code using CUDA or OpenACC is not required. Another benefit is
to use the data-parallelism implemented in PyTorch. We can parallelize the code by the
number of the sources/shots to run the code on multiple GPUs and distributed clusters.
Fig. 7.4 shows a 1D seismic velocity Inversion case applying the physics-ruled
RNN implementation. The Fig. 7.4A shows a true synthetic velocity model and an
initial model; 4B shows the inverted model comparing with the true model (up) and a
slightly smoothed final inverted model (down); 4C shows the comparison of the true
traces and the inverted traces; and 4D shows the wave field illustrating how the seismic

Applying scientific machine learning to improve seismic wave simulation and inversion

Fig. 7.4 Applying RNN for 1D seismic velocity inversion.

wave propagates with respect to space and time. The 1D inversion experiment finds a
solution close to the true model after 100 iterations, using the Adam optimizer55 with
L2 regularization.
Ongoing efforts include working on 2D cases by revising PySIT package, and performing more test cases to evaluate the performance with both data and model parallelism provided by PyTorch on a CPU cluster and multiple GPUs.

7.4 Differentiable programming
7.4.1 Automatic differentiation and adjoint-state method
The method of automatic differentiation (AD), also called algorithmic differentiation, calculates the derivatives for any arbitrary differentiable program. Unlike using
the numerical differentiation of the adjoint state method that is an approximation to
calculate the derivatives, the automatic differentiation returns the exact answer of the

179

180

Advances in subsurface data analytics

derivatives, though subject to the intrinsic rounding error. Machine learning software
such as TensorFlow and Pytorch all have the built-in implementation of AD as the core
functionality of backpropagation to optimize machine learning models. Accurate gradients are critical to the gradient-based optimizations used in both scientific computing
and machine learning.
In order to calculate the derivatives of any differentiable programs, AD needs to
store all operations on the execution path along with the intermediate results. It then
propagates derivatives backward from the final output for every single operation connected with the chain rule. For largescale applications, AD faces the challenge of meeting the demands of fast-growing storage in proportion to the executed operations.
Furthermore, the individual derivative function for each operation also slows down the
computation with intrinsic sequential execution. More work needs to be done if AD
can be directly applied to a real scientific application.
Computationally expensive scientific applications typically use the adjoint state
method to calculate the gradient of a function with much better computation efficiency,
although it is a numerical approximation. In FWI, the adjoint state method calculates
the derivative of a forward function J(m) that depends on the wavefield u(m). The forward function J can be defined using h, as following3:
J ( m ) = h (u ( m ) , m )

(1.9)

where m is the model parameter, which belongs to the model parameter space M and u
belongs to the state variable space, U.The the state variables, u follow the state equations
outlined with the mapping function, F, which is also known as the forward problem or
forward equation3:
F (u(m ), m ) = 0.

(1.10)

The mapping function F is mapping from U * M to U and is satisfied by the state
variable u. If the condition F(u, m) = 0 is satisfied, the state variable u becomes a physical
realization. Then, the adjoint state equation can be given as following, where λ is the
adjoint state variable and ũ is any element of U3:
*

 δ F ( u, m ) 
δ h ( u, m )
(1.11)

 λ=
δ u
 δ u 
This adjoint-state gradient calculation involves computing the reverse-time propagated residual wavefield, then combining with the saved forward-propagated wavefield
snapshots at specified time intervals to provide adjustments to the medium properties
(the gradient) at each spatial mesh point. In summary, the forward propagation computes
data observations representing the response of the model, and the residual between the
model response and actual observed data is backward propagated and combined with
the forward model response to compute adjustments to the current model estimate.

Applying scientific machine learning to improve seismic wave simulation and inversion

Intervening in the calculation of the gradient in this manner allows for management
of the required computational resources by saving the forward wave fields only as often
as numerically required, explicitly managing data resources through staging to disk or
check-pointing as needed, implementing shot-level parallelism, and other specially tailored techniques.

7.4.2 Extended automatic differentiation
A difficulty with the autodifferentiation (AD) procedure is that memory requirements for the back-propagation graph can become excessive, as noted by Richardson16.
Applying chain-rule differentiation on elemental network nodes over thousands of
RNN time steps for a large mesh of physical parameter values is a reasonably-sized task
for 1D problems, but the graph quickly becomes intractable for 2D and 3D models.This
issue usually renders impractical the use of pure AD for such model inversion problems.
An alternative solution to the problem, involves extending the AD backward process
using the PyTorch AD workflow to integrate the adjoint-state method for the more
efficient gradient calculation. In PyTorch, the AD workflow is extended by providing
a backward function to calculate the gradients of the corresponding forward function.
The required parameters of the forward function, the model parameters and loss function are provided to allow the backward function to pick up these parameters for the
adjoint-state calculation.
Control over this auto-differentiation process is available through use of a PyTorch
extension to the Autograd feature pictured conceptually in Fig. 7.5, wherein the RNN
layer of the network can be replaced by a forward propagation loop and corresponding adjoint back-propagation loop for an equivalent gradient calculation provided

Fig. 7.5 Adjoint gradient vs automatic differentiation. Derivatives with respect to model parameters
in the backward automatic differentiation are replaced by gradients from adjoint state.

181

182

Advances in subsurface data analytics

by the user. This alternative gradient calculation can take advantage of well-known
techniques in seismic inversion processing, enabling existing performance enhancements to be applied using the extended PyTorch capability for specially designed
back-propagation.
In the present case, the physical medium properties to be optimized are provided
to the “forward” wave propagation problem implemented using the publicly available
PySIT seismic inversion toolkit56, creating a simulated seismic response. This simulated
response is to be compared with the observed seismic trace data from the corresponding
actual field data recording (or recordings from a “true” model in our synthetic studies).
The corresponding “backward” propagation consists in using the residual wavefield
represented by the difference between the simulated data and the observed seismic trace
data, and implementing the “adjoint-state” solution to provide the required gradient of
the model parameters. Other implementations of wave propagation solutions may also
be used in this framework, such as spectral element methods57 for 2D, 3D, and spherical
3D wave propagation.
The beneficial end result is that traditional adjoint-state solution methods are incorporated into the AD workflow, so that seismic inversion calculations can be integrated
within the broader deep learning process with efficient calculation.

7.5 Seismic inversion
7.5.1 Seismic inversion using neural network
These new concepts from machine learning may now be applied to the problem of
seismic inversion as presented in Section 1.1. As described in Section 1.3 and Section
1.4, by reconstructing the forward problem using deep learning software, the seismic
inversion problem can be solved by the automatic differentiation package, a variety of
optimizers provided by PyTorch, and a customized loss function. The automatic differentiation package in PyTorch implements the methodology of automatic differentiation
by recording all the forward operations in sequence and performing backward derivative computation based on the chain rule.

7.5.2 Autoencoder for dimensionality reduction
The seismic inversion process needs to uncover the physical properties at every point
represented in the geological space, which quickly leads to a large number of model
parameters to optimize in the traditional FWI process. The nature of the nonlinear and
ill-posed inverse problem often falls into the local minimum traps. It is a sound solution
to apply the dimensionality reduction technique to reduce the optimization parameters
to improve the optimization accuracy by engaging with machine learning models.
Since the automatic differentiation workflow has been customized by integrating
the adjoint state method for the FWI gradients (described in Section 1.4), it is now

Applying scientific machine learning to improve seismic wave simulation and inversion

Fig. 7.6 Full waveform inversion enhanced with autoencoder dimensionality reduction.

feasible to integrate the machine learning models into the FWI workflow and keep the
program differentiable. Since the autoencoder A(x) is differentiable and the forward
model F (x) is differentiable, the composition of the F (A(x)) is differentiable. The autoencoder is applied as the dimensionality reduction method applied before the forward
model as shown in Fig. 7.6.
For the 60 × 80 2D model in the current example, the autoencoder contains
743,938 parameters as shown in Fig. 7.7A and B. The autoencoder is an unsupervised
learning model that compresses the information representation of the input data to a
sparse latent variable with less dimensions at the middle of the encoded layer. It then
reconstructs the data from the encoded latent variable to the original or enhanced data.
The compression process is called encoder and the reconstruction is called decoder. The
encoder learns how to compress the input data and describes it with the latent variable,
while the decoder learns how to reconstruct the data from the latent variable.
We start the autoencoder training by generating a large number of random seismic
velocity models. In this work, we are using some simple and flat velocity layers representing the velocities of different earth interiors including water and rocks. Specifically,
these models contain one or more low velocity layers in the middle or bottom of
these layers that is challenging for the low velocity inversion. All of these models have
the fixed dimensions of 60 × 80. As indicated in Fig. 7.7A, the autoencoder has two
components: a encoder and a decoder. The encoder compresses the input model with

183

Fig. 7.7 Traditional seismic velocity inversion.

184
Advances in subsurface data analytics

Applying scientific machine learning to improve seismic wave simulation and inversion

Fig. 7.8 The autoencoder training loss.

dimension of 60 × 80 to an encoded latent variable with dimension of 8 × 10, which is
1/60 of the original dimension.The latent variable is then decompressed by the decoder
to restore to its original dimension.
The loss function we used to train the autoencoder is the mean-squre-error (MSE)
loss and the optimizer is Adam with learning rate of 0.001. The batch size used is 128.
The loss values during the training process are shown in Fig. 7.8.
Fig. 7.6 shows the autoencoder-enhanced FWI process, where the autoencoder is
inserted before the forward function simulation starts. Note that the encoder is only
applied to the first iteration to get the encoded latent variable. For the remaining optimization iterations, the decoder is applied to decompress the encoded latent variable
to get a new velocity model with the original dimension. During the gradient-based
optimization process, the gradients are calculated with respect to the encoded latent
variable, instead of the original model, which reduces the dimensionality of the optimization search space by a factor of 60.

7.5.3 Results
PyTorch has a list of optimizers, including Adam55, RMSprop58, stochastic gradient
descent (SGD), Adadelta59, Adagrad60, LBFGS, and their variants. The learning rate,
scheduler and regularizations can be specified to fit different optimization problems.
There are also multiple regression and classification loss functions implemented in
PyTorch. All of these packages provide a rich environment to solve inverse problems.
The present implementation has demonstrated how to invoke the extended automatic gradient calculation for the velocity model. The Adam optimizer and the MSE

185

186

Advances in subsurface data analytics

Fig. 7.9 Traditional seismic velocity inversion.

loss function are used to compare the misfit of the simulated traces and observed traces
after each iteration of the forward model. The partial derivative (the gradient) of the loss
function with respect to the initial model and the encoded latent variable is calculated by
the automatic differentiation process, which is applied by the optimizer to minimize the
misfit. These iterations gradually find an approximation of the true velocity distribution.
Fig. 7.9 and Fig. 7.10 show the differences of the traditional FWI and the autoencoder enhanced FWI results. Within each figure (1) shows the initial model, the true
model (flat layers with a low velocity feature around 1 km depth), and the inverted

Applying scientific machine learning to improve seismic wave simulation and inversion

Fig. 7.10 The autoencoder-enhanced seismic velocity inversion.

model; the loss graph (B) shows the loss values (at different scales) after each optimization iteration, and Figure (C) shows the difference between the inverted model and the
initial model (top), as well as the difference between the inverted model and the true
model. It appears that the traditional FWI does not optimize well in the low velocity
layer case after 40 iterations ended with a high loss value, which falls into a local trap.
The autoencoder-enhanced FWI discovers the low velocity layer very well and continues to optimize the misfit for all 100 iterations. The difference graphs also confirm that
the autoencoder case identifies all layers well showing less structured misfits. Noticeably,

187

188

Advances in subsurface data analytics

there are also less artifacts introduced in the autoencoder-enhanced FWI compared
with the traditional FWI.
As described in Section 1.4, the automatic differentiation provided by the PyTorch
software does not provide sufficient efficiency to solve the FWI 2D problem.The gradients calculated for the whole program takes too long and too much space to store them.
The hybrid method described in Section 1.4.2 overcomes the problem by incorporating the adjoint state calculation. As the result, the gradient calculation using the hybrid
approach achieves both accuracy and efficiency, which is feasible for use in a large scale
scientific computation problem integrating with machine learning models.

7.6 Discussion
There are a few points that worth noting for the work. The first is that the automatic
differentiation is key for differentiable programming, which can bridge the physicsbased scientific computing with the machine learning (ML)/artificial intelligence (AI)
technologies. ML/AI methods do not have physics principles built-in, which may create
an infeasible solution given the fact that most of the scientific inverse problems may
be ill-posed. In our prior work61, the convergence of ML with a scientific application
without differentiable programming may not find a generalized solution since optimizations of the two different methods are disconnected.
The second point is that the automatic differentiation needs additional improvements to make it feasible to other applications. In the present method, integrating the
adjoint-state method makes it feasible to solve a large case; however, the solution is an
approximation. If the automatic differentiation method can be more memory-efficient
and parallelizable, it can be much more useful to compute the exact gradients for the
large complex problems.
The last point is the deep learning model autoencoder requires a revisit to reduce
the loss during decoding. Although it reduces the dimension by compressing the input
data into a sparse latent variable, the reconstruction is not lossless. There are some errors
introduced during the reconstruction process that may hinder the optimization process.
There is a trade-off to take into consideration when designing the convergence of ML/
AI with scientific computing.

7.7 Conclusions
Two cases have been demonstrated of restructuring the wave equation using finite
difference method in a deep learning RNN model framework and an autoencoder
enhanced FWI process. The benefits of the work include fully utilizing the highperformance tensor processing and optimization capabilities implemented in the deep
learning package PyTorch, as well as the deep integration of machine learning models
with the inverse problem. By integrating an HPC application with a deep learning

Applying scientific machine learning to improve seismic wave simulation and inversion

framework with differentiable programming, we can explore a large number of combinations of machine learning models with physical numerical solutions to achieve better
accuracy and efficiency.
Moreover, the work can be extended to more applications in reservoir characterization, identification of sweet spots, and reservoir simulation. With the scientific principles
embedded in the machine learning, the method enhances the interpretability and reliability of machine learing models in the engineering fields.

7.8 Acknowledgment
This research work is supported by the US National Science Foundation (NSF) awards ##1649788,
#1832034 and by the Office of the Assistant Secretary of Defense for Research and Engineering
(OASD(R&E)) under agreement number FA8750-15-2-0119. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation
thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the
US NSF, or the Office of the Assistant Secretary of Defense for Research and Engineering (OASD(R&E))
or the U.S. Government. This work used the Extreme Science and Engineering Discovery Environment
(XSEDE), which is supported by National Science Foundation grant number ACI-1548562. Specifically,
it used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh
Supercomputing Center (PSC).

References
1. Baker N, Alexander F, Bremer T, Hagberg A, Kevrekidis Y, Najm H, et al., Workshop report on basic
research needs for scientific machine learning: core technologies for artificial intelligence. http://doi.
org/10.2172/1478744. https://www.osti.gov/biblio/1478744.
2. Schuster G, Seismic Inversion, Society of Exploration Geophysicists, 2017. doi: http://doi.
org/10.1190/1.9781560803423.
3. Plessix R-E. A review of the adjoint-state method for computing the gradient of a functional
with geophysical applications. Geophys J Int 2006;167(2):495–503. http://doi.org/10.1111/j.1365246X.2006.02978.x.
4. Adler J, Oktem O, Solving ill-posed inverse problems using iterative deep neural networks. https://
arxiv.org/pdf/1704.04058.pdf. Accessed Feb. 15, 2021.
5. Virieux J, Operto S. An overview of full-waveform inversion in exploration geophysics. Geophysics
2009;74(6):WCC127–52. http://doi.org/10.1190/1.3238367.
6. Moseley B, Markham A, Nissen-Meyer T, Fast approximate simulation of seismic waves with deep
learning. https://arxiv.org/pdf/1807.06873.pdf. Accessed Feb. 15, 2021.
7. Baker N, Alexander F, Bremer T, Hagberg A, Kevrekidis Y, Najm H, et al., Workshop report on basic
research needs for scientific machine learning: core technologies for artificial intelligence. http://doi.
org/10.2172/1478744.
8. Baydin AG, Pearlmutter BA, Radul AA, Siskind JM. Automatic differentiation in machine learning: a
survey. J. Mach. Learn. Res. 2017;18(1):5595–637.
9. Poggio T, Mhaskar H, Rosasco L, Miranda B, Liao Q. Why and when can deep-but not shallownetworks avoid the curse of dimensionality: a review. Int J Autom Comput 2017;14(5):503–19. http://
doi.org/10.1007/s11633-017-1054-2.
10. Raissi M, Perdikaris P, Karniadakis G. Physics-informed neural networks: a deep learning framework
for solving forward and inverse problems involving nonlinear partial differential equations. J Comput
Phys 2019;378:686–707. https://doi.org/10.1016/j.jcp.2018.10.045.

189

190

Advances in subsurface data analytics

11. Kharazmi E, Zhang Z, Karniadakis GE, Variational physics-informed neural networks for solving
partial differential equations (2019). arXiv: 1912.00873.
12. Rackauckas C, Ma Y, Martensen J, Warner C, Zubov K, Supekar R, Skinner D, et al., Universal differential equations for scientific machine learning, arXiv preprint arXiv:2001.04385.
13. Greydanus S, Dzamba M, Yosinski J, Hamiltonian neural networks (2019). arXiv:1906.01563.
14. Mattheakis M, Protopapas P, Sondak D, Giovanni MD, Kaxiras E, Physical symmetries embedded in
neural networks (2020). arXiv:1904.08991.
15. Chen RTQ, Rubanova Y, Bettencourt J, Duvenaud DK. Neural ordinary differential equationsBengio S, Wallach H, Larochelle H, Grauman K, CesaBianchi N, Garnett R. Advances in Neural
Information Processing Systems, Curran Associates, Inc. 2018:6571–83. https://proceedings.neurips.
cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf.
16. Richardson A, Seismic full-waveform inversion using deep learning tools and techniques. https://
arxiv.org/pdf/1801.07232v2.pdf. Accessed Feb. 15, 2021.
17. Zhu W, Xu K, Darve E, Beroza1 GC, A general approach to seismic inversion with automatic differentiation. https://arxiv.org/pdf/2003.06027.pdf. Accessed Feb. 15, 2021.
18. Blanning RW. The construction and implementation of metamodels. Simulation 1975;24(6):177–84.
http://doi.org/10.1177/003754977502400606.
19. O’Hagan A, Bayesian analysis of computer code outputs: a tutorial, the Fourth International
Conference on Sensitivity Analysis of Model Output (SAMO 2004), Reliab Eng Syst Saf.
2006;91(10)1290–1300. https://doi.org/10.1016/j.ress.2005.11.025. URL http://www.sciencedirect.com/science/article/pii/S0951832005002383.
20. Willcox K, Peraire J. Balanced model reduction via the proper orthogonal decomposition. AIAA J
2002;40(11):2323–30. http://doi.org/10.2514/2.1570.
21. Wilson KC, Durlofsky LJ. Computational optimization of shale resource development using reducedphysics surrogate models, the SPE Western Regional Meeting. March 2012. Bakersfield, California,
USA: Society of Petroleum Engineers. https://doi.org/10.2118/152946-MS.
22. Sethian JA. A fast marching level set method for monotonically advancing fronts. Proc Natl Acad Sci
1996;93(4):1591–5. http://doi.org/10.1073/pnas.93.4.1591.
23. Robinson TD, Eldred MS, Willcox KE, Haimes R. Surrogate-based optimization using multifidelity
models with variable parameterization and corrected space mapping. AIAA J 2008;46(11):2814–22.
24. Regis RG, Shoemaker C. Constrained global optimization of expensive black box functions using
radial basis functions. J Glob Optim 2005;31:153–71.
25. Sandvol E, Seber D, Calvert A, Barazangi M. Grid search modeling of receiver functions: implications for crustal structure in the middle east and north africa. J Geophys Res Solid Earth
1998;103(B11):26899–917. http://doi.org/10.1029/98JB02238.
26. Sen MK, Stoffa PL. Nonlinear one-dimensional seismic waveform inversion using simulated annealing. Geophysics 1991;56(10):1624–38.
27. Zhao L-S, Sen MK, Stoffa P, Frohlich C. Application of very fast simulated annealing to the determination of the crustal structure beneath Tibet. Geophys J Int 1996;125(2):355–70.
28. Press F. Earth models obtained by monte carlo inversion. Journal of Geophysical Research (1896-1977)
1968;73(16):5223–34.
29. Fernández Martínez JL, García Gonzalo E, Fernández Álvarez JP, Kuzma HA, Menéndez Pérez CO.
Pso: a powerful algorithm to solve geophysical inverse problems: application to a 1d-dc resistivity case.
J Appl Geophys 2010;71(1):13–25. https://doi.org/10.1016/j.jappgeo.2010.02.001.
30. Kramer MA. Nonlinear principal component analysis using autoassociative neural networks. AlChE
J 1991;37(2):233–43.
31. Moseley B, Nissen-Meyer T, Markham A. Deep learning for fast simula-tion of seismic waves in
complex media. Solid Earth 2020;11(4):1527–49. http://doi.org/10.5194/se-11-1527-2020.
32. van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, Kalchbrenner N, et al.,
WaveNet: a generative model for raw audio, arXiv e-prints (2016) arXiv: 1609.03499arXiv:1609.
03499.
33. Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. In: Proceedings
of the 27th International Conference on International Conference on Machine Learning, ICML’10,
Omnipress, Madison, WI, USA 2010:807–14.

Applying scientific machine learning to improve seismic wave simulation and inversion

34. Gao Z, Li C, Liu N, Pan Z, Gao J, Xu Z. Large-dimensional seismic inversion using global optimization with autoencoder-based model dimensionality reduction. IEEE Trans Geosci Remote Sens
2020:1–5. https://doi.org/10.1109/TGRS.2020.2998035.
35. Storn R, Price K. Differential evolution – a simple and efficient heuristic for global optimization over
continuous spaces. J. Glob Optim 1997;11(4):341–59. http://doi.org/10.1023/A:1008202821328.
36. ChenY, Schuster GT. Seismic inversion by newtonian machine learning. Geophysics 2020;85(4):WA185–
200.
37. Sun B, Alkhalifah T, Ml-descent: an optimization algorithm for FWI using machine learning,
GEOPHYSICS 85:R477-R492. arXiv: https://doi.org/10.1190/geo2019-0641.1.
38. Kingma DP, Welling M, Auto-encoding variational bayes, CoRR abs/1312.6114.
39. Versteeg R. The marmousi experience: velocity model determination on a synthetic complex data
set. The Leading Edge 1994;13(9):927–36.
40. Appendix B. The SEG-EAGE salt data set, 2012, pp. 217–8. arXiv: https://library.seg.org/doi/
pdf/10.1190/1.9781560801689.appendixb.
41. Mosser L, Dubrule O, Blunt MJ, Stochastic seismic waveform inversion using generative adversarial
networks as a geological prior (2018). https://arxiv.org/abs/1806.03720. Accessed Feb 15, 2021.
42. Arjovsky M, Chintala S, Bottou L, Wasserstein GAN (2017). https://arxiv.org/abs/1701.07875.
Accessed Feb. 15, 2021
43. Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville A, Improved training of wasserstein gans
(2017). https://arxiv.org/abs/1704.00028. Accessed Feb. 15, 2021.
44. Tolstikhin I, Bousquet O, Gelly S, Scholkopf B, Wasserstein autoencoders. https://arxiv.org/
pdf/1711.01558.pdf. Accessed Feb. 15, 2021.
45. Roberts GO, Rosenthal JS. Optimal scaling of discrete approximations to langevin diffusions. J R Stat
Soc: Series B StatMethodol 1998;60(1):255–68.
46. Swischuk R, Mainini L, Peherstorfer B, Willcox K. Projection-based model reduction: Formulations
for physics-based machine learning. Comput Fluids 2019;179:704–17. http://doi.org/10.1016/j.
compfluid.2018.07.021.
47. Baydin AG, Pearlmutter BA, Radul AA, Automatic differentiation in machine learning: a survey,
CoRR abs/1502.05767. arXiv:1502.05767. http://arxiv.org/abs/1502.05767. Accessed Feb. 15,
2021.
48. Margossian CC, A review of automatic differentiation and its efficient implementation, CoRR
abs/1811.05031. arXiv:1811.05031. http://arxiv.org/abs/1811.05031. Accessed Feb. 15, 2021.
49. Innes M, Edelman A, Fischer K, Rackauckas C, Saba E, Shah VB, et al., A differentiable programming system to bridge machine learning and scientific computing, CoRR abs/1907.07587.
arXiv:1907.07587. http://arxiv.org/abs/1907.07587. Accessed Feb. 15, 2021.
50. Bezanson J, Edelman A, Karpinski S, Shah VB. Julia: a fresh approach to numerical computing. SIAM
Rev 2017;59(1):65–98. https://doi.org/10.1137/141000671.
51. Zhu W, Xu K, Darve EF, Beroza G, A general approach to seismic inversion with automatic differentiation, arXiv: Computational Physics. https://arxiv.org/abs/2003.06027. Accessed Feb. 15, 2021.
52. Xu K, Li D, Darve EF, Harris JM, Learning hidden dynamics using intelligent automatic differentiation, ArXiv abs/1912.07547. https://arxiv.org/abs/1912.07547. Accessed Feb. 15, 2021.
53. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput 1997;9(8):1735–80.
54. Chung J, Gulcehre C, Cho K, Bengio Y, Empirical evaluation of gated recurrent neural networks
on sequence modeling (2014). arXiv:1412.3555. https://arxiv.org/abs/1412.3555. Accessed Feb. 15,
2021.
55. Kingma DP, Ba J, Adam: A method for stochastic optimization (2014). arXiv:1412.6980. https://arxiv.
org/abs/1412.6980. Accessed Feb. 15, 2021.
56. Hewett RJ, Demanet L, Team TP, PySIT: python seismic imaging toolbox (2020). http://doi.
org/10.5281/zenodo.3603367. Accessed Feb. 15, 2021.
57. Tromp J, Komatitsch D, Liu Q. Spectral-element and adjoint methods in seismology. Commun Comput
Phys 2008;3(1):1–32.
58. Ruder S, An overview of gradient descent optimization algorithms (2016). arXiv:1609.04747.
https://arxiv.org/abs/1609.04747. Accessed Feb. 15, 2021.

191

192

Advances in subsurface data analytics

59. Zeiler MD, Adadelta: An adaptive learning rate method (2012). arXiv: 1212.5701. https://arxiv.org/
abs/1212.5701. Accessed Feb. 15, 2021.
60. Duchi J, Hazan E, Singer Y. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res. 2011;12:2121–59.
61. Huang L, Polanco M, Clee TE. Initial experiments on improving seismic data inversion with
deep learning. New York Scientific Data Summit (NYSDS) 2018;2018:1–3. http://doi.org/10.1109/
NYSDS.2018.8538956.

