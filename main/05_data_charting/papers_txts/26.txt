Deep Learning Seismic Substructure Detection Using the Frozen
Gaussian Approximation
James C. Hateley, Jay Roberts, Kyle Mylonakis, Xu Yang

arXiv:1810.06610v4 [physics.geo-ph] 5 Nov 2019

Department of Mathematics, University of California, Santa Barbara, CA 93106, USA

Abstract
We propose a deep learning algorithm for seismic interface and pocket detection with neural
networks trained by synthetic high-frequency displacement data efficiently generated by the
frozen Gaussian approximation (FGA). In seismic imaging high-frequency data is advantageous since it can provide high resolution of substructures. However, generation of sufficient
synthetic high-frequency data sets for training neural networks is computationally challenging. This bottleneck is overcome by a highly scalable computational platform built upon the
FGA, which comes from the semiclassical theory and approximates the wavefields by a sum
of fixed-width (frozen) Gaussian wave packets.
Training data for deep neural networks is generated from a forward simulation of the elastic wave equation using the FGA. This data contains accurate traveltime information (from
the ray path) but not exact amplitude information (with asymptotic errors not shrinking to
zero even at extremely fine numerical resolution). Using this data we build convolutional
neural network models using an open source API, GeoSeg, developed using Keras and Tensorflow. On a simple model, networks, despite only being trained on data generated by the
FGA, can detect an interface with a high success rate from displacement data generated by
the spectral element method. Benchmark tests are done for P-waves (acoustic) and P- and Swaves (elastic) generated using the FGA and a spectral element method. Further, results with
a high accuracy are shown for more complicated geometries including a three-layered model,
a sine interface, and a 2D-pocket model where the neural networks are trained by both clean
and noisy data.
Keywords: seismic tomography, convolutional neural network, elastic wave equation,
high-frequency wavefield, frozen Gaussian approximation, image segmentation
1. Introduction
Various geophysical aspects, e.g., tectonics and geodynamics [1, 20, 19, 29], can be better understood by images of substructures (e.g. locations of seismic interfaces) of the Earth
generated by seismic tomography. Neural networks excel at recognizing shapes, patterns, and
∗

Corresponding author
Email addresses: hateleyjc@gmail.com (James C. Hateley), jayroberts@math.ucsb.edu (Jay
Roberts), kmylonakis@math.ucsb.edu ( Kyle Mylonakis), xuyang@math.ucsb.edu (Xu Yang)

Preprint submitted to Elsevier

November 6, 2019

sorting relevant from irrelevant data; this makes them good for image recognition and classification. In particular, convolutional neural networks allowed for rapid advances in image
classification and object detection [14], and in fact networks have been created for specific
tasks, such as, fault detection [2], earthquake detection, ConvNetQuake [18], DeepDetect [26]
and seismic phase arrival times, PhaseNet [30]. One obstacle in building a neural network to
detect seismic structures is having an ample data set for training. There is constant waveform
data being collected by seismic stations across the globe, and generating data by resampling
of this seismic data to train a network can be done, but is limited by the Nyquist frequency.
Seismic data can not be resampled with a Nyquist frequency lower than the highest usable
frequency in the data, thus high frequency data is usually preferred as it tends to lead to improved resolution of the substructures. Other difficulties of gathering an ample data lie within
the differences in geological locations, natural phenomenon (e.g. earthquakes) and unnatural phenomenon (e.g. fracking). Using these data sets to train a general neural network is a
daunting task, and thus it is natural to use synthetic data for the training of neural networks.
The dominant frequency of a typical earthquake is around 5 Hz [17] leading to demanding,
and at times, unaffordable computational cost. This makes generation of sufficient synthetic
high-frequency data sets for training neural networks computationally challenging to wellknown methods. We overcome this difficulty by building a highly scalable computational
platform upon the frozen Gaussian approximation (FGA) method for the elastic wave equation
[9], which comes from the semiclassical theory. The FGA approximates the wavefields by a
sum of fixed-width (frozen) Gaussian wave packets. The dynamics of each Gaussian wave
packet follow ray paths with the prefactor amplitude equation derived from an asymptotic
expansion on the phase plane. The whole set of governing equations are decoupled for each
Gaussian wave packet, and thus, in theory, each corresponding ODE system can be solved on
its own process, making the algorithm embarrassingly parallel.
Using synthetic data, Araya-Polo et al. perform inverse tomography via fully connected
neural networks with great success in [3] . Their networks use low dimensional features
extracted from seismic data as input. Using deeper convolutional neural networks trained
on seismogram data may allow the network to pick up on previously unknown signals. The
increase in input dimensionality necessitates more sophisticated deep learning techniques than
those presented in [3].
In this paper, we propose a deep learning algorithm for seismic interface detection, with
the neural networks trained by synthetic high-frequency seismograms. We first generate the
time series of synthetic seismogram data by the FGA, which we use to train neural networks
made with an open source API, GeoSeg, developed using Keras and Tensorflow. Despite only
being trained on FGA generated data we observe the networks are able to detect a 1D interface with a high success rate on data generated by spectral element method. This method
more acucurately represents true seismic signals when fine time step and mesh sizes are used
in the computation. We conjecture that this robustness is due to the fact that although FGA
does not carry exact amplitude information (with asymptotic errors proportional to the ratio
of wavelength over domain size), it contains accurate traveltime information. For this simple problem it is straight-forward in geophysics to identify the traveltime as a key factor in
interface location; however, this is not built into the network and so its use must be learned.
With the success of the 1D interface detection, we further apply the deep learning algorithms
for geometries with more complicated structures, including a three layered model and a 2D
2

pocket model, both of which show a high accuracy. We also investigate the effect of noise by
studying the performance of deep learning algorithms on noisy validation data, with the neural
networks trained using clean and noisy data, respectively.
The paper is outlined as follows: In Section 2, we review briefly the mathematical background of FGA and describe how the synthetic data is generated. In Section 3, we describe
the details of the network design including network and block architectures. In Section 4 we
show the performance of various networks on a series of geometries with different substructures, using both clean and noisy data. Concluding remarks are made in Section 5.
2. Frozen Gaussian approximation
We summarize the mathematical theory of FGA in this section; for full exposition and
details for the elastic wave equation, see [9]; and for the acoustic wave equation, see [5].
The core idea of the FGA is to approximate seismic wavefields by fixed-width Gaussian
wave packets whose dynamics follow ray paths with the prefactor amplitude equation derived
from an asymptotic expansion on the phase plane. The ODE system governing the dynamics
for each wave packet are decoupled. In theory, each ODE system can be solved on its own
process, hence it is embarrassingly parallel. The implementation, as in previous works [9], is
with Fortran using message passage interface (MPI). The implementation has a speed up factor
of approximately 1.94; hence, doubling the number of cores nearly halves the computational
time. The equation for the forward modeling to generate the training data set we use is the
elastic wave equation [7],
ρ∂t2 u = (λ + µ)∇(∇ · u) + µ∆u + F,

(1)

where ρ, λ, µ, : R3 → R is the material density, the first and second Lamé parameters respectively and u : R × R3 → R3 is displacement. The differential operators are taken in terms of
the spacial variables. Eq. (1) has a natural separation into divergence and curl free components
and can also be written as
∂t2 u = c2p ∇(∇ · u) − c2s ∇ × ∇ × u + Fρ .

(2)

This decomposition represents P-wave, and S-wave respectively with velocities
c2p (x) =

λ(x) + 2µ(x)
,
ρ(x)

c2s (x) =

µ(x)
,
ρ(x)

(3)

with cp (x) representing the P-wave speed and cs (x) representing the S-wave speed.
2.1. The FGA Formulation
Presented below is an outline for the FGA. For derivation and benchmarking tests we
refer to [9, 5]. We introduce the FGA formula for the elastic wave equation (2), with initial
conditions
(
u(0, x) = f k (x),
(4)
∂t u(0, x) = gk (x),

3

where the superscript k represents the wavenumber. For a sake of simplicity and clarity, we
shall also use the following notations:
√
• i = −1 : the imaginary unit;
• subscripts/superscripts “p” and “s” indicate P- and S-waves, respectively;
• ± indicates the two-way wave propagation directions correspondingly;
• N̂ p,s (t): unit vectors indicating the polarized directions of P- and S-waves;
• n̂p,s : the initial directions of P- and S-waves.
The FGA approximates the wavefield uk (t, x) in eq. (1) by a summation of dynamic frozen
Gaussian wave packets,
X

ukF (t, x) ≈

ap N̂ p ψpk ikP p ·(x−Q )− k |x−Q |2
p
p δqδp
2
e
9/2
p (2π/k)

(q,p)∈G±

X

+

as N̂ s ψsk ikP s ·(x−Qs )− k |x−Qs |2
2
e
δqδp,
(2π/k)9/2
s

(5)

(q,p)∈G±

with the weight functions
k
ψp,s
(q, p) =

Z

k
αp,s
(y, q, p) =

k

2

k
αp,s
(y, q, p)e−ikp·(y−q)− 2 |y−q| dy,

1


k
k
kf
(y)c
|p|
±
ig
(y)
· n̂p,s .
p,s
2kcp,s |p|3

(6)
(7)

In eq. (5), Gp,s
± refers to the initial sets of Gaussian center q and propagation vector p for Pand S-waves, respectively. In eq. (7), the “±” on the right-hand-side of the equation indicate
k
correspond to (q, p) ∈ Gp,s
that the αp,s
± . We refer [9] for the derivation, accuracy and explanation of FGA, and only summarize the formulation as follows. The ray path is given by the
Hamiltonian system with Hamiltonian H(Q, P ) = ±cp,s (Q)|P |. The “±” give the two-way
wave propagation directions; e.g. for the “+” wave propagation, (q, p) ∈ Gp,s
+ , the Gaussian
center Qp,s (t, q, p) and propagation vector P p,s (t, q, p) follow the ray dynamics

dQp,s
P p,s


= cp,s (Qp,s )
,
dt
|P p,s |

 dP p,s = −∂ c (Q )|P |,
Q p,s
p,s
p,s
dt

(8)

with initial conditions
Qp,s (0, q, p) = q

and P p,s (0, q, p) = p.

4

(9)

Wave Reconstruction

Offset

Time Propagation

Transmitted

Reflected
Incident

Initial Decomposition

Offset

Figure 1: A cartoon illustration of FGA algorithms: Step 1, decompose the initial wavefield into a sum of
Gaussian wave packets with corresponding weights given by (6); Step 2, propagate Gaussian wave packets
following (8), (10), (11) and (12), with the reflection-transmission conditions described in Section 2.2; Step 3,
reconstruct the wavefield by summing all Gaussian wave packets using (5).

The prefactor amplitudes ap,s (t, q, p) satisfy the following equations, where S-waves have
been decomposed into SH- and SV-waves,



∂Qp cp · P p 1
dap
−1 dZp
= ap ±
+ tr Zp
,
(10)
dt
|P p |
2
dt



∂Qs cs · P s 1
dN̂ sh
dasv
−1 dZs
= asv ±
+ tr Zs
− ash
· N̂ sv ,
(11)
dt
|P s |
2
dt
dt



∂Qs cs · P s 1
dash
dN̂ sh
−1 dZs
= ash ±
+ tr Zs
· N̂ sv ,
(12)
+ asv
dt
|P s |
2
dt
dt
with the initial conditions ap,sv,sh = 23/2 , and N̂ sv and N̂ sh are the two unit directions perpendicular to P s , referring to the polarized directions of SV- and SH-waves, respectively. With
the short-hand notations,
∂z = ∂q − i∂p ,

Zp,s = ∂z (Qp,s + iP p,s ).

(13)

We illustrate the algorithm by Figure 1, and refer to the Figures 5 and 6 in [9] for the
performance of efficiency of FGA.
2.2. Interface conditions
Interface conditions are important as the direct and reflected waves from an interface are
picked up by the receiver, which records the time series of wavefield at certain location. This
gives travel time information; which in turn enables the depth of an interface to be computed.
For this exposition we only consider a flat interface, in general, we can use tangential-normal
5

z
Gtrs
φt

Gtrp

θt

Layer 2 (c∨
p,s )
z = z0

x

Layer 1 (c∧
p,s )
θi

θr
φr

Gin
p

Gre
p
Gre
s

Figure 2: Cartoon illustration of an incident Gaussian wave packet for P-wave hitting the interface at z = z0 ,
stands for
and then reflected and transmitted as Gaussian wave packets for P- and SV-waves. Here the Gin,re,tr
p,s
the Gaussian wave packet for the incident, reflected and transmitted P- and SV-waves, respectively. We denote
θi , θr , θt to be the incident, reflection and transmission angles of P-waves, and φr , φt to be the reflection and
transmission angles of SV-waves, respectively.

coordinates. The derivation was detailed in Appendix B in [9], with the idea of using the
continuity of level set functions corresponding to the Hamiltonian dynamics (8). A cartoon
illustration on the behavior of Gaussian wave packet is given in Figure 2. For a flat interface
z = z0 , the wave speeds of the two layers near the interface are assumed to be,
(
(
c∨s (x) z > z0
c∨p (x) z > z0
.
(14)
,
c
(x)
=
cp (x) =
s
c∧s (x) z < z0
c∧p (x) z < z0
As a Gaussian wave packet hits an interface, several of its quantities need to be defined. First,
ap,s and P p,s , are determined by Snell’s Law and the Zoeppritz equations [27]. If one denotes
θi , θr , θt to be the P-wave incident, reflection and transmission angles, and φr , φt to be the
SV-wave reflection and transmission angles, respectively, then the Zoeppritz equations read as
 re  

ap
cos(θr )
 are
  sin(θr )  in
s 


(15)
M
tr
 ap  =  cos(2φr )  ap ,
cos(2θr )
atr
s
with the matrix M as

c∧
p
cos(θr )
sin(φr )
c∧
s

∧
cp
 − sin(θ )
cos(φr )

r
c∧
s
M =
 − cos(2φr )
− sin(2φr )

c∧
sin(2θr ) −( cp∧ )2 cos(2φr )
s

c∧
p
cos(θt )
c∨
p
∧
cp
sin(θt )
c∨
p
ρ2
cos(2φt )
ρ1
∨ 2
ρ2 (c∧
p cs )
∧ 2 sin(2θt )
ρ1 (c∨
p cs )

6

c∧

− cp∨ sin(φt )
s



c∧
p
cos(φt ) 

c∨
s
,
ρ2
− ρ1 sin(2φt ) 

2
ρ2 (c∧
p)
cos(2φ
)
t
ρ1 (c∧ )2
s



(16)

where ρ1,2 are the densities for the layers 1 and 2, respectively. Let N denote the normal to
the interface at the point of incidence then Qin,re,tr is the Gaussian center at the point of incidence, and Pin,re,tr corresponds to the propagation vector of incident, reflected and transmitted
Gaussian wave packet for either P- or S-waves. Qin = Qre = Qtr and Pre,tr is updated as
follows

q
in
tr,re
in |ntr,re − |Pin | − (Pin · N)2 − (P · N) N,
|P
(17)
Ptr,re
=
P
+
sgn(P
)
p,s
p,s
p,s

tr
∨ ∧
where ntr,re
p,s denotes the index of refraction for the new respective direction, e.g. np = cp /cp .
Also Zp,s needs to be updated, requiring use of conservation of level set functions defined in
the Eulerian frozen Gaussian approximation formula [15, 25].

∂z Qre,tr = ∂z Qin F,

|Pre,tr |
re,tr
re,tr
in
in
∂
Q
·
∇c(Q
)
−
∂
Q
·
∇c(Q
)
N,
z
z
c(Qre,tr )Pre,tr · N
(18)
T
−1
F and W are two 3 × 3 matrices, F = W , and



2
1
0
0
c(Qre,tr )


0
1
0
.
F =
 , with κ =
re,tr
c(Qin )
(κ − 1) ppinx (κ − 1) ppiny κ ppz in
∂z Pre,tr = ∂z Pin W −

z

z

z

2.3. Advantage of FGA for Generating Training Data
The data points used for our experiments are generated from the forward simulation of the
elastic wave equation using the FGA. We record the displacement data from the wavefield at
various points near the surface; these points represent the receiver locations. Given an initial
condition, as in eq. (4), the initial wave packet decomposition can be saved for generating
a data set for training. That is, the same data can be loaded as the parameters which vary
from data point to data point; e.g. interface height, pocket location, pocket size, etc. Furthermore, if the initial condition is independent of the wave velocities, the same wave packet
decomposition can be used to generate data from simulation with varying velocities.
For a single forward simulation; after the initial wave packet decomposition generated and
saved, loading the initial wave packet decomposition, running an ODE solver, and recording
the displacement are the only tasks required to generate a data point. For generation of a data
set, the simulation can be restarted at t = 0 with another set of parameters. As the initial wave
packet decomposition is already loaded in memory, all that is required to generate the rest of
the data set is running an ODE solver, and recording the displacement. The ODE system for
the FGA is uncoupled for each wave packet, the speed of a single simulation greatly benefits
from a parallel implementation.
3. Network Design
The goal of Full Waveform Inversion (FWI) is to extract wave speed data from seismic
data. In its purest form, this is a regression type problem and was addressed with fully con7

nected networks in [3]. Our work approaches the problem from a segmentation perspective.
We address a simplified version of FWI and attempt to detect subsurface structures by classifying them as regions of low or high wavespeed, thus transforming the regression problem
into a segmentation problem. These sorts of segmentation problems have been addressed with
great success by CNNs [22]. Semantic segmentation of images is the process of labeling each
pixel in an image with a class label for which it belongs. In semantic segmentation problems
the correct pixel label map is referred to as the ground truth. In our work the “image is the
n-dimensional slice in the depth direction which is partitioned into N bins. The i, j th “pixel”
is the signal value from receiver i at depth bin j.
Each bin is then labeled depending on whether it came from a region of high or low velocity. These velocity regions are our classes. Our work diverges substantially from traditional
semantic segmentation of images, as our input is time series data which must be transformed
by the network. This is opposed to the traditional case where the input itself is labeled. The
goal of our network is to infer the presence of high and low wavespeed regions and the interfaces between them from seismogram data. The input to the network is X ∈ RM ×d×r , where
M is the number of timesteps, d is the spatial dimension of media, and r is the the number of
receiver. The output of the network is
N (X) = (pki1 ...in ) ∈ RM1 ×···×Mn ×N ,

ij ∈ {1, ..., Mj }
,
k ∈ {1, . . . , N }

(19)

where pki1 ···in is the probability that bin i1 · · · in belongs to the k th class. In this paper d = 3,
n = 1, 2, and N = 1, 2, 3.
For example, possible output and groundtruth could be


 
0.1 0.9
1
 0.2 0.8  , 1 .
0.55 0.45
1
Here, at depth indexed by 1, the network believes with 10% probability that this bin is a low
speed region and with 90% probability that it is a high speed region, and similarly for the other
rows. The accuracy of a given inference is found by taking the argmax along the last axis of
the output tensor and comparing against the groundtruth. Taking a max along the last axis
recovers the probability, interpreted as a confidence of the prediction. The above example has
66.67% accuracy, and the confidence is [0.9, 0.8, 0.55]T .
In [3], Araya-Polo et al. perform inverse tomography via Deep Learning and achieve
impressive results. Our model is fundamentally different than GeoDNN in that: GeoDNN
is a fully connected network whereas GeoSeg’s is fully convolutional, and GeoDNN uses
semblance panels from CMP data as features for the network and GeoSeg uses the raw seismograph data. Moreover, Araya-Polo et al. address the FWI problem and provide the wave
speeds in a two dimensional region and we tackle high and low velocity detection, shifting the
problem from regression to segmentation.
1
The networks were built using an open source API, GeoSeg , developed using Keras and
1

https://github.com/KyleMylonakis/GeoSeg

8

Tensorflow. GeoSeg supports UNet, fully convolutional segmentation network, or feed forward CNNs as a base meta-architecture, using any of residual, dense, or convolutional blocks,
with or without batch normalization [21, 22, 11, 12, 13]. GeoSeg also allows for easy hyperparameter selection for network and block architectures, and for training optimizers and parameters. The optimizers used were NADAM with default parameters [6], sometimes followed
by minibatch stochastic gradient descent (SGD), or SGD alone. The network structures are
described by their meta-architecture and their blocks. The meta-architecture describes the
global topology of the network and how the blocks interact with each-other. Each block either
begins or ends with a decoding or encoding transition layer respectively. Encoding transition
layers downsample their inputs with a strided convolution. Decoding transition layers upsample thier inputs with a strided deconvolution. Tranistion layers will not have dropout.
Meta-Architectures. While GeoSeg supports many kinds of feed-forward CNN’s and
Encoder-Decoder Networks with different choices of blocks, UNet architectures with dense
blocks performed the best and will be the only type of network reported.
GeoDUDe-L refers to a UNet architecture from [21]. These architectures have proven
highly efficient at image segmentation for road detection [28] and in biomedical applications [21]. These networks feed their input into a transfer branch, then an encoder branch
of length L, bridge block, and then a decoder branch of length L. The last layer is a convolutional layer followed by a softmax which outputs predictions as described above. The defining
feature of these networks are the “rungs” connecting the encoder and decoder branches (see
Figure 3). In this way, the network can incorporate both low and high resolution data [21, 28].
For the one dimensional problems the transfer branch is not necessary and can be omitted.
Convolutional Layers. The layer is broken first into a bottleneck convolution followed by
the main convolution. The bottleneck is a convolution which uses a 1x1 kernel to expand the
number of feature channels before performing the full convolution. It is suggested in [10, 23]
that such a bottleneck can reduce the number of necessary feature maps and so improve computational efficiency. We use Rectified Linear Units (ReLUs) [8] for our activation and size
3x3 (3x1 for 1D interface problems) filter kernels for our convolutions. As in [12], we use
Batch-Normalization [13] to help smooth training. The setup is shown in Figure 4
Dense Blocks. Though GeoSeg supports multiple block types, all the networks reported
in this paper use dense blocks. These are stacks of convolutional layers as shown in Figure 4.
The defining features of these blocks, introduced in [12] is that every layer receives input
from all previous layers in the block via concatenation. Such architectures have been shown
to greatly improve results in image classification while reducing computational burden [12].
Transfer Branch. All of our meta-architectures preserve resolution of their input and so
our detection resolution is limited by input resolution. This is not a problem in the temporal
axis, which translates to the z axis in output, since we have a large number of time samples;
however, the x-axis resolution is limited by the number of receivers we have for our input. To
increase the resolution in this direction, we place a small l-layer CNN before the main network
which upsamples the receiver axis, via strided deconvolutions, by a factor of 2l .

9

Figure 3: Meta-architecture of a two-layered UNet, GeoDUDe-2, with Transfer Branch used in deep learning
algorithms. For 2D problems the input is upsampled along the receiver axis by deconvolutions in the Transfer
Branch. UNet’s have “rungs” that connects the encoder and decoder branches. In this way, the network can
incorporate both low and high resolution data.

(a) Convolutional Layer

(b) Dense Block

Figure 4: The type of blocks used in GeoSeg for this paper: (a) Block compositions of a basic convolutional
layer using a bottleneck convolution to expand the filter channels before the full convolution; (b) a corresponding
dense block. Each layer of the block recieves input from all previous layers allowing information to flow through
the whole block.

10

4. Numerical Experiments
Here we present the performance of deep learning algorithms for the three detection experiments: 1D interface problme, three-layered media model, and a 2D single cylindrical pocket
model. The architecture used for all experiments is a UNet with Dense Blocks (GeoDUDe).
Each dense block will be made of four constituent bottle-necked convolutional layers with a
bottle neck factor of 4. For all 1D networks the dense blocks’ convolutions use a kernel size
of 3 × 1 in the base of the block and 2 × 1 at each transition layer, while for the 2D networks a
3 × 3 kernel size is used in the base block with a 2 × 2 kernel size in the transition layer. The
meta-architectures had 16 filter channels except for the 1D interface model with P-wave data
which only used 4.
Our primary evaluation metric is accuracy which is the number of correctly predicted
pixels over total pixels, i.e.,
accuracy =

number of correct pixels
,
total number of pixels

where we set the ground truth as follows for measuring accuracy; if any part of a pixel contains
a low velocity region, that pixel is counted as part of the low velocity region.
For the 2D pocket model, we will also consider the Intersection Over Union metric which
better captures segmentation performance.
In the 2D pocket model, a two-layer transfer branch was used. Each layer was a convolution, two-strided in the receiver direction with a kernel size of 3 × 3 with 4 filter channels.
During training, these layers had a drop out probability of 0.2.
The wavespeeds cp and cs are given by (3), which will be specified as piecewise linear
functions (or constants) detailed in each numerical example. The initial P-Wave data is generated with source function

fjk (x) = cos(k(xj − x0,j )) exp − 2k|x − x0 |2 ,
(20)
and the P,S-Wave initial data is generated from the Green’s function
fjk (x) =

3
X
(xi − xi,0 )(xj − xj,0 )
i=1

4πρc2p r3

Fj (t0 − r/cp )+

r2 δij − (xi − xi,0 )(xj − xj,0 )
Fj (t0 − r/cs )+
4πρc2s r3
Z
3(xi − xi,0 )(xj − xj,0 ) − r2 δij r/cs
sFj (t0 − s) ds,
4πρr5
r/cp

(21)

p
where Fj (t) = cos (kt) exp(−2kt2 ), δij is the Kronecker delta, and t0 = 2 1/k,
x = (x1 , x2 , x3 ), x0 = (x0,1 , x0,2 , x0,3 ) is the location of the source, r = kx − x0 k and ρ is
the density.
The data is generated on the cluster, pod, at the center for scientific computing at UC Santa

11

2

Barbara using 64 processes with a 4th order Runge-Kutta solver for the ODE system. As the
initial condition is independent of the wavespeed, only one wave packet decomposition needs
to be computed and saved for all data points to be generated. This saves a tremendous amount
of time as only the ODE system needs to be solved for various wavespeeds and interface
heights. For example to generate the P-Wave data, when 804672 total beams are used, each
data point is generated in approximately 2.5 minutes. This is compared to SPECFEM3D
which takes is approximately 45 minutes to generate a data point.
All of the networks were trained on the Google Cloud Platform, or on the cluster Pod at the
center for scientific computing at UC Santa Barbara with Keras 2.2.2 and Tensorflow 1.10.0
as a backend using a single NVIDIA Tesla V100 GPU.
4.1. 1D Interface
To provide a proof of concept we first experimented with a two-layered flat interface
model. We also use this case to investigate whether our network is simply inverting the FGA
by comparing performance of a network trained on FGA but evaluated on data generated by
SEM.
4.1.1. P-Wave Data
Dataset. The P-wave data set is generated with a computation domain of [0, 2] km×[0, 2]
km×[0, 2.5] km with a source centered at x0 = (0.5, 0.5, 0.5) km and k = 128 in (20),
which corresponds to approximately 20.37 Hz. The stations are located on the surface at
S1 : (1.5, 1.5, 0) km, S2 : (1.8, 1.5, 0) km, S3 : (1.6, 1.9, 0) km. The interface is a plane,
z = z0 that varies from depth 1 km to 2.5 km. Above the interface the wavespeed cp varies
linearly from .78 km/s to 1.22 km/s, below the interface the wavespeed cp varies linearly from
1.29 km/s to 1.56 km/s. See Figure 5.
Each data point is a (6000, 3, 3) tensor. Prior to training, we further down sample the temporal dimension by a factor of 25 and normalize the amplitude of the seismogram data. There
were a total of 7790 examples. The mini-batch size during training was 256 examples.
Network Details. As described above our architecture was a 1D GeoDUDe-3 where each
convolutional layer in the dense block had 4 feature channels. The During training the dropout
probability was set to 0.5 and a NADAM optimizer was used with default parameters.
Results. Network evaluations were performed with data generated by the FGA and SPECFEM.
Notably, the networks are never trained on any SPECFEM data. This was to investigate
whether the network was sensitive to the asymptotic error produced by the FGA.
After 3500 epochs of training GeoDUDe-3 achieved a 96.97% evaluation accuracy on data
generated by the FGA. When evaluated on data generated by SPECFEM dataset GeoDUDe-3
achieved a 94.29% evaluation accuracy, only a 2.68% decrease. We remark in [16, 24], it
was shown even small perturbations in input can affect network classification results. This
suggests that the asymptotic errors present in the FGA do not greatly affect the segmentation
problem. Visualizations of the output for GeoDUDe-3 are shown in Figure 6. Figure 7 shows
the heatmap. Recall this displays the confidence the network places on the pixels prediction.
2

http://csc.cnsi.ucsb.edu/clusters/pod

12

(a) Computational Domain

(b) Seismogram

Figure 5: The locations of source and receivers, and the generated synthetic P-wave seismograms for the 1D
interface problem. We take k = 128 for generating the synthetic data. (a) The source is located at (.5,.5,.5) km
as a star and the 3 receivers are located on the surface. The interface presented is at a depth of 2 km. (b) A
visualization of typical data point, which is a collection of 3 seismograms from the forward simulation using the
FGA.

(a) FGA: Actual

(b) FGA: Predicted

(c) FGA: Difference

(d) SEM: Actual

(e) SEM: Predicted

(f) SEM: Difference

Figure 6: 1D interface predicted by GeoDUDe-3 using P-wave data. Each column of pixels represents a data
point. The value of each pixel describes whether the material at the depth corresponding to that pixel’s column
belongs to either the high or low velocity region. The blue pixels represent the low velocity region, while the
yellow represent the high velocity region. Subfigures (c), (f) show the difference between the predicted and
actual velocity profile, where the accuracy is measured by the wrong-labeled pixels (blue) over the total number
of pixels in the figures (c), (f). In fact, after 3500 epochs of training GeoDUDe-3 achieved a 96.97% evaluation
accuracy on data generated by the FGA.

13

(a) FGA

(b) SEM

Figure 7: P-wave confidence distribution comparison produced by GeoDUDe-3 for 1D interface problem. Regions of low confidence correspond to areas where an interface is likely. The color bar is a probability spectrum
from 0.5 to 1. In general, the closer the network gets to the interface the less confident its prediction becomes.

4.1.2. P,S-wave data set
Dataset. The P,S-wave dataset is generated with a computation domain of [0, 2] km×[0, 2]
km×[0, 3] km with a source centered at x0 = (0.5, 0.5, 0.5) km, and wavenumber k = 32, or
approximately 5.09 Hz. The stations lie in a plane and are located just below the surface at
S1 : (1.1, 0.5, 0.1) km, S2 : (1.4, 0.5, .1) km, S3 : (1.8, 0.5, 0.1) km. The interface is a plane,
z = z0 that varies from depth 1 km to 2 km. Above the interface cp varies linearly from 0.75
km/s to 1.10 km/s, below the interface cp varies linearly from 1.12 km/s to 1.48 km/s and we
fix cs = cp /1.7 (corresponding the case λ ≈ µ). See Figure 8. There are a total of 6,400
data points in the P,S-wave dataset. Each data point is a (2048,3,3) tensor. Prior to training
each example is down-sampled along the temporal axis by a factor of 8. Each network used a
mini-batch training size of 256. Similarly to the P-wave dataset, 100 additional samples were
generated using SPECFEM3D for evaluation after training.
Network Details. GeoDUDe-2 and GeoDUDe-3 with default parameters were used.
Both networks were trained using a NADAM optimizer with a dropout probability of 0.5.
Results. Both networks were trained for 3500 epochs. The most successful network
was GeoDUDe-2, with 98.26 % evaluation accuracy on FGA data, and 97.55 % evaluation
accuracy on the SPECFEM data . We find that the evaluation accuracy goes down for deeper
networks. In particular, GeoDUDe-3 performed worse with only a 92.34 % evaluation accuracy, especially compared to the same network architecture on the P-wave dataset. This is
likely due to overfitting of the data causing an increase in generalization error. Similarly to the
P-wave dataset, evaluation accuracies on SPECFEM3D data are only marginally worse than
their FGA counterparts, with a max difference of 1.17% between the datasets. See Table 1 for
the summary of the results and Figures 9, 10 and 11.

14

(a) Computational Domain

(b) Seismogram

Figure 8: The locations of source and receivers, and the generated synthetic P- and S-wave seismograms for the
1D interface problem. We take k = 32 for generating the synthetic data. (a) The source is located at (.5,.5,.5)
as a star and the 3 receivers are located on the surface. The interface presented is at a depth of 2 km. (b) A
visualization of typical data point, which is a collection of 3 seismograms from the forward simulation using the
FGA.

Table 1: P,S-Data Network Comparisons for 1D interface problem. Here Eval. Acc. = evaluation accuracy,
Train. Acc. = training accuracy, and SEM Acc. = evaluation accuracy tested by SEM synthetic data.

Network Eval. Acc. Train. Acc. SEM Acc.
GeoDUDe-2
98.26 %
99.97 %
97.55 %
GeoDUDe-3
97.64 %
99.90 %
96.47 %

(a) Evaluation Accuracy

(b) Training Accuracy

Figure 9: PS-wave training results for 1D interface problem, with synthetic data generated for k = 32 in (20):
The evaluation data set for this figure only contains data generated by the FGA.

15

(a) FGA: Actual

(b) FGA: Predicted

(c) FGA: Difference

(d) SEM: Actual

(e) SEM: Predicted

(f) SEM: Difference

Figure 10: 1D interface predicted by GeoDUDe-2 using P,S-wave data. Each column of pixels represents a
sample. The value of each pixel describes whether the material at the depth corresponding to that pixel’s column
belongs to either the high or low velocity region. The blue pixels represent the low velocity region, while the
yellow represent the high velocity region. Subfigures (c), (f) show the difference between the predicted and
actual velocity profile, where the accuracy is measured by the wrong-labeled pixels (blue) over the total number
of pixels in the figures (c), (f). We give the statistical accuracy in Table 1, which shows an accuracy of over 96%.

(a) GeoDUDe-2:FGA

(b) GeoDUDe-2:SEM

Figure 11: P,S-wave heat-map distribution comparison produced by GeoDUDe-2 for 1D interface problem.
Regions of low confidence correspond to areas where an interface is likely. The color bar is a probability spectrum
from 0.5 to 1. In general, the closer the network gets to the interface the less confident its prediction becomes.

16

(a) Computational domain

(b) seismograph

Figure 12: The locations of source and receivers, and the generated synthetic P- and S-wave seismograms for
the three-layered media model. We take k = 32 for generating the synthetic data. (a) The source is located at
(.5, 1, .5) km as a star, the 32 receivers are located on the surface on the plane y = 1 km, and the interfaces
presented are at a depth of 1.5 km and 2 km. (b) A visualization of typical data point, which is a collection of 32
seismograms from the forward simulation using the FGA.

4.2. Three-Layered Media
Dataset. A natural extension of the model is to include one or more low velocity regions
in the computational domain. For this experiment we consider a three-layered media with a
low velocity region in the middle, the velocities in each region will be fixed.The P-wave speed
is cp = 1.3, 0.9, 1.7 km/s for the top, middle, and bottom layers, receptively. The S-wave
speed is set to cs = cp /1.7 for each layer. The lower interface will be in a rage of 1.8 km and
2.8 km by an increment of 1 m. Similarly the upper interface will vary from .2 km to 1.2 km
by an increment of 1m. See Figure 12. There were 10201 samples with a batch size of 64.
Network Details. GeoDUDe-3 was used. During training the dropout probability was
0.12. Training was performed with stochastic gradient descent with a learning rate of 0.001.
Results. The network achieved a training accuracy of 99.51% and an evaluation accuracy
of 95.51% after 3000 epochs. See Figures 13 and 14.
4.3. Sine interface model
Dataset. For this experiment we consider a more complicated interface, which is given
by the level set function; F (x, z) = z − 0.1 sin(πf x). The level set values F (x, z) = D vary
between 0.3 km and 2.3 km by an increment of .001 km. And the phase factor f will ranges
from 1 to 2 by an increment of .001. The nondimensionalized is set to wavenumber k = 64
and cp = 1.1, 1.5 for the top and bottom layers will all be fixed. As before, cs will be a fixed
multiple of cp by 1.7. We record the displacement for 10 s; see Figure 15 for source, receiver
details.
Network Details. GeoDUDe-4 was used with a dropout probability of 0.2. Training
was performed with 1500 epochs using the NADAM optimizer followed by 1000 epochs of
17

(a) Actual

(b) Predicted

(c) Difference

Figure 13: Predictions for three-layered media by GeoDUDe-3: Each column of pixels represents a sample. The
value of each pixel describes whether the material at the depth corresponding to that pixel’s column belongs to
either the high or low velocity region. The color bar is a probability spectrum from 0.5 to 1. In general, the closer
the network gets to the interface the less confident its prediction becomes. There is a slight loss of confidence for
the network detecting the lower interface.

Figure 14: Confidence map for three-layered media model produced by GeoDUDe-3. Regions of low confidence
correspond to areas where an interface is likely. The color bar is a probability spectrum from 0.5 to 1. In general,
the closer the network gets to the interface the less confident its prediction becomes.

18

(a) Computational domain

(b) Displacement color channel

Figure 15: The locations of source and receivers, and the generated synthetic P- and S-wave seismograms for the
sine interface model. We take k = 64 for generating the synthetic data. (a) The source is located at (0.4, 1, 2.7)
km as a star, the 32 receivers are located on the surface on the plane y = 1 km, and the interfaces presented are
at a depth of 1.5 km. (b) Visualization of network input as image for sine interface model. Each color channel
(inverse RGB) represents a coordinate of the displacement.

stochastic gradient descent with a learning rate of 0.001. There were 10050 date points generated. 9150 data points are used for training with a mini-batch size of 25; 900 data points were
used for evaluation.
Results. The network achieved a training accuracy of 99.92% and an evaluation accuracy
of 99.28% after 2500 epochs. See Figure 16 for evaluation of a typical data point.
4.4. 2D Low Velocity Pocket
Dataset: We now investigate whether the network can learn more complex 2D geometries.
The considered models each will be a three-layered problem with a low velocity cylindrical
region in the middle layer. The source will be located at (.5, 1, 1.5) km. The interfaces located
at 1 km and 2.5 km will be fixed. A cylinder with center (x, z) and radius r will be randomly

(a) Actual

(b) Predicted

(c) Confidence map

Figure 16: Typical results from training phase factor f = 1.83, level set value D = .8898. (a), (b) Ground
truth and prediction for sine interface GeoDUDe-4. (c) Confidence map for sine interface model. Regions of low
confidence correspond to areas where an interface is likely.

19

(a) Computational domain

(b) Displacement color channel

Figure 17: The locations of source and receivers, and generated synthetic P- and S-wave seismograms for the
2D pocket model. We take k = 32 for generating the synthetic data. (a) The source is located at (.5, 1, 1.5) km
as a star and the receivers are located on the surface on the plane y = 1 km. The interfaces are fixed at a depth
of 1 km and 2.5 km. Visualization of network input as image for 2D pocket model. Each color channel (inverse
RGB) represents a coordinate of the displacement.

generated x ∈ [0.85, 1.65] km, z ∈ [1.35, 2.15] km, and r ∈ [.05, .3] km with samples taken
from a uniform distribution. See Figure 17. 11350 data points are generated with 1000 being
saved for evaluation. The P-wave speeds will be fixed and are cp = 1.1, 1.3, 1.7 km/s, for
the top, middle and bottom layers respectively. The S-wave speed, cs will be a fixed multiple
of cp by 1.7 for each layer. Inside the pocket the P-wave speed is set to cp = 0.5 km/s and
the S-wave speed is set to zero, cs = 0. Only P -waves will propagate through the cylinder;
However, S-wave can transmit to P-wave going in the pocket and P-wave can transmit to P,Swaves coming out of the pocket. Unlike previous models the goal is to identify a low velocity
region in a three layered media in a 2D slice of the computational domain. A batch size of 20
examples was used.
Network Design. A GeoDUDe-4 network was used with a two layer transfer branch
before its input. The dropout probability was 0.2.
Results. The network achieved a training accuracy of 99.95% and an evaluation accuracy
of 99.73% after 1428 epochs. In Figure 19 we see the networks are indeed learning geometry.
This is particularly interesting given that the network only ”sees” images like Figure 18(b).
These results suggest the network is transforming the data in some way which we hope to
explore in future work.
4.5. Effect of Noisy data
We now consider the 2D pocket example with additive white noise. Normally, noise is
added to the training data set to increase the size of the set and lead to a more robust network. We take an evaluation set of 1000 data points and add i.i.d. (independent identically
distributed) Gaussian noise to each time step of the displacement field data. For an individual

20

(a) x-coordinate

(b) y-coordinate

(c) z-coordinate

Figure 18: Visualization of network input using normalized displacement data for 2D pocket model.

(a) Actual

(b) Predicted

(c) Confidence map

Figure 19: 2D pocket results predicted by GeoDUDe-4, with a typical data point chosen for visualization. The
pocket is recovered with the networks confidence wavering on the boundary of the pocket.

21

(a) No noise

(b) Noise strength of 1% of maximum recorded data

Figure 20: Comparison of seismograms with noise and no noise for the 2D pocket model. (a) Seismogram with
no noise. (b) Additive Gaussian white noise at 1% of max |u|. This shows that 1% of the maximum recorded
displacement is enough to mask the reflected data from the pocket.

data point, the noise strength can be calculated by
Wi =

σ
,
R max |ur |

(22)

where R is the reflection coefficient and max |ur | is the maximum displacement from the reflected wave. The noise strength will be given by W , which is the approximate average value
of Wi across the data set. The standard deviation σ is chosen so that W can be interrupted as
a percentage of the reflected wave displacement, e.g., W = 20 gives of the a noise strength of
20% of the average max displacement of the reflected wave. We notice that with noise generated with a strength of 1% of the maximum of direct recorded displacement, the reflected
data from the pocket is the same order of magnitude of the noise, effectively masking it. See
Figure 20.
Network Design: To compare results, we use the same model as in the previous Section 4.4 and train a network with the same parameters, with a noise strength of W = 20.
Results. A GeoDUDe-4 was trained for 2000 epochs with additional noise for a final
evaluation accuracy of 99.731% evaluation accuracy. However, evaluation accuracy can be
a misleading metric for network performance in pocket detection since assigning the high
velocity class to every pixel could get an accuracy up to 80% on some samples. Instead
intersection over union (IOU) is used (see [4] for a more detailed explanation). Figures 21
and 22 show the histograms the IOU scores of networks trained with and without white noise
evaluated on the evaluation data with no additional noise, additional noise strength W =
10, and additional noise strength W = 50 respectively. While both networks display good
IOU scores on the unperturbed data and when the data is only perturbed with noise strength
W = 10, the benefits of additional noise in training become clear when the noise strength
is increased to W = 50: the IOU scores of the network trained without noise on noisy data
22

Unperturbed Perturbed by W = 10 Perturbed by W = 50
Trained without Noise
0.8163
0.7335
0.1308
Trained with Noise
0.8706
0.7576
0.5249
Table 2: IOU Scores for GeoDUDe-4 trained with and without noise for the 2D pocket model.

(a) ground truth

(b) IOU=0.1403

(c) IOU=0.2052

Figure 21: Visualization of IOUs by GeoDUDe-4 for the 2D pocket model. Results taken from network trained
with noise. Data is augmented with noise with a noise strength of 50%. (a) ground truth for comparision. (b)
IOU=0.1403. (c) IOU= 0.2052. For each displayed results, the networks are able to detect the location of the
pocket. With additional noise the network is unable to resolve the geometry.

plummets, effectively misclassifying almost every pocket, while the IOU score of the network
trained with noise decreases, but maintains many correct classifications. The average IOU
scores are summarized in Table 2. Evaluating on higher noise strength collapses the network’s
output to no pocket detected.
4.6. Structured noise
We now consider the 2D pocket example with additive structured noise. From our noiseless
evaluation data set, we add structured low frequency noise to each receiver. Numbering each

(a) No noise

(b) W=10

(c) W=50

Figure 22: Network with trained without noise, 1000 data points are plotted in each Histogram. Subfigures (a),
(b), (c) show the IOU metric with no noise, 10% noise strength, and 50% noise strength respectively.

23

(a) No noise

(b) W=10

(c) W=50

Figure 23: Performance of the Network GeoDUDe-4 trained with noise strength at 20% of the average max
displacement of the reflected wave for the 2D pocket model. 1000 data points are plotted in each Histogram.
Subfigures (a), (b), (c) show the IOU metric, with no noise, 20% noise strength, and 50% noise strength, respectively.

(a) Actual

(b) Predicted, IOU=0.7585

(c) Confidence map

Figure 24: A visualization of a result from the structured noise (23) added to the noiseless evaluation dataset.
The network used is GeoDUDe-4 trained with noise from section 4.5. (a) Ground truth. (b) Prediction, with an
IOU value of 0.7585. (c) Confidence map.

receiver, Sj = (j2/31, 1, 0), for j = 0, . . . , 31 we add noise to receiver Sj as
a
4
cos(2t)
,
2
3(32 − j)

(23)

where the amplitude a is modulus of the maximum displacement of the wavefield reflected
from the interface. We add this noise to each component of the wavefield. We remark that at
receiver at x = 2, the noise is the strongest at with an amplitude of 2a/3. That is, the noise
strength is two thirds of the height of the modulus of the maximum displacement wavefield
reflected from the interface. This is a stronger noise than the W = 50 case of additive white
noise. The structured noise decreases to an amplitude of a/48 at the receiver located at x = 0
The network is able to detect the pocket with good success. The network has full confidence, as can be seen from Fig. 24c; however, the boundary for the prediction is perturbed.
This gives a slight false result which is reflected in the IOU metric, see Fig. 25. With the
structured noise, the prediction has a lower average IOU value of .7443 compared to 0.8706
with no noise.
24

Figure 25: The IOU metric for structured noise as the input into GeoDUDe-4 for the 2D pocket model trained
with noise from section 4.5. The average IOU is .7443.

5. Conclusions and future work
The use of the FGA to generate large amounts of seismic data provides a quick way to generate labeled synthetic data for statistical learning of the inverse tomography problem. Casting
the inverse problem as a segmentation problem resulted in high evaluation accuracy networks
for piecewise constant two-layer models on both FGA and SEM datasets. The UNet architectures with dense blocks displayed superior accuracy compared to simpler network architectures, however, deeper networks did not necessarily outperform their shorter counterparts. On
the two layer benchmark problem the networks exhibited good invariance of prediction in regard to which numerical method was used to generate the dataset, likely because the FGA and
SEM exhibit the same traveltime information. Having a network independent of numerical
method is important, and the FGA can help to train such a network as it generates synthetic
seismic data that carries the correct traveltime information of the real-world data. Further,
analogous meta-architectures also exhibit high evaluation and IOU accuracy for pocket detection in noisy data.
The success of the networks on the substructure geometries in the paper act as a stepping
stone to tackle more complicated and realistic geological models. By developing the API
GeoSeg, available at https://github.com/KyleMylonakis/GeoSeg, it is easy to implement neural networks designed for the reported example models and more general segmentation problems of seismogram data than those discussed in this paper. Together with the FGA, the task
of training a deep neural network on sufficiently large amounts of seismogram data becomes a
computationally affordable task. Immediate future directions to be explored are multi-pocket
models, multi-nonlinear interface models with and without pockets present. Long term goal
is to develop a neural network model to tackle fully 3D substructure geometries and develop
a neural network trained on synthetic seismic data capable of making inferences from real
seismic data.

25

Acknowledgement
We acknowledge support from the Center for Scientific Computing from the CNSI, MRL:
an NSF MRSEC (DMR-1720256) and NSF CNS-1725797. The work was partially supported
by the NSF grant DMS-1818592. XY also thanks Professors Haizhao Yang and Kui Ren for
useful discussions.
References
[1] K. Aki and W. Lee. Determination of the three-dimensional velocity anomalies under a
seismic array using first P arrival times from local earthquakes 1. A homogeneous intial
model. J. Geophys. Res., 81:4381–4399, 1976.
[2] M. Araya-Polo, T. Dahlke, C. Frogner, C. Zhang, T. Poggio, and D. Hohl. Automated
fault detection without seismic processing. The Leading Edge, 36(3):208–214, 2017.
[3] M. Araya-Polo, J. Jennings, A. Adler, and T. Dahlke. Deep-learning tomography. The
Leading Edge, 37(1):58–66, 2018.
[4] M. Atiqur Rahman and Y. Wang. Optimizing intersection-over-union in deep neural
networks for image segmentation. volume 10072, pages 234–244, 12 2016.
[5] L. Chai, P. Tong, and X. Yang. Frozen Gaussian approximation for 3-D seismic wave
propagation. Geophysical Journal International, 208(1):59–74, 2017.
[6] T. Dozat. Incorporating nesterov momentum into adam. 2016.
[7] A. M. Dziewonski and D. L. Anderson. Preliminary reference earth model. Physics of
the earth and planetary interiors, 25(4):297–356, 1981.
[8] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS,
2011.
[9] J. C. Hateley, X. Yang, L. Chai, and P. Tong. Frozen Gaussian approximation for 3D elastic wave equation and seismic tomography. Geophysical Journal International,
216(2):1394–1412, 11 2019.
[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.
CoRR, abs/1512.03385, 2015.
[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages
770–778, 2016.
[12] G. Huang, Z. Liu, and K. Q. Weinberger. Densely connected convolutional networks.
CoRR, abs/1608.06993, 2016.
[13] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. CoRR, abs/1502.03167, 2015.
26

[14] Y. LeCun, Y. Bengio, and G. E. Hinton. Deep learning. Nature, 521(7553):436–444,
2015.
[15] J. Lu and X. Yang. Frozen Gaussian approximation for general linear strictly hyperbolic
systems: Formulation and Eulerian methods. Multiscale Model. Simul., 10:451–472,
2012.
[16] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturbations. CoRR, abs/1610.08401, 2016.
[17] H. Nakamichi, H. Hamaguchi, S. Tanaka, S. Ueki, T. Nishimura, and A. Hasegawa.
Source mechanisms of deep and intermediate-depth low-frequency earthquakes beneath
iwate volcano, northeastern japan. Geophysical Journal International, 154(3):811–828,
2003.
[18] T. Perol, M. Gharbi, and M. Denolle. Convolutional neural network for earthquake
detection and location. Science Advances, 4(2), 2018.
[19] N. Rawlinson, S. Pozgay, and S. Fishwick. Seismic tomography: A window into deep
Earth. Phys. Earth Planet. Inter., 178(3-4):101–135, 2010.
[20] B. Romanowicz. Seismic tomography of the Earth’s mantle. Annu. Rev. Earth Planet.
Sci., 19:77–99, 1991.
[21] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical
image segmentation. CoRR, abs/1505.04597, 2015.
[22] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional networks for semantic segmentation. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 3431–3440, 2015.
[23] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Riedmiller. Striving for simplicity: The all convolutional net. CoRR, abs/1412.6806, 2014.
[24] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
[25] D. Wei and X. Yang. Eulerian gaussian beam method for high frequency wave propagation in heterogeneous media with discontinuities in one direction. Commun. Math. Sci.,
10:1287–1299, 2012.
[26] Y. Wu, Y. Lin, Z. Zhou, D. C. Bolton, J. Liu, and P. Johnson. Deepdetect: A cascaded
region-based densely connected network for seismic event detection. IEEE Transactions
on Geoscience and Remote Sensing, pages 1–14, 2018.
[27] O. Yilmaz. Seismic Data Analysis: Processing, Inversion, and Interpretation of Seismic
Data. Society of Exploration Geophysicists, 2001.
[28] Z. Zhang, Q. Liu, and Y. Wang. Road extraction by deep residual u-net. CoRR,
abs/1711.10684, 2017.
27

[29] D. Zhao. Tomography and dynamics of Western-Pacific subduction zones. Monogr.
Environ. Earth Planets, 1:1–70, 2012.
[30] W. Zhu and G. C. Beroza. PhaseNet: A Deep-Neural-Network-Based Seismic Arrival
Time Picking Method. ArXiv e-prints, Mar. 2018.

28

