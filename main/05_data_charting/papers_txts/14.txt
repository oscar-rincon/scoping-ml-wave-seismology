Received May 14, 2020, accepted May 24, 2020, date of publication May 27, 2020, date of current version June 26, 2020.
Digital Object Identifier 10.1109/ACCESS.2020.2997921

A Physics-Based Neural-Network Way to Perform
Seismic Full Waveform Inversion
YUXIAO REN

1 , XINJI XU

2 , SENLIN YANG1 , LICHAO NIE

1 School of Qilu Transportation, Shandong University, Jinan 250061, China

2 , AND YANGKANG CHEN

3

2 Geotechnical and Structural Engineering Research Center, Shandong University, Jinan 250061, China
3 School of Earth Sciences, Zhejiang University, Hangzhou 310027, China

Corresponding authors: Xinji Xu (xuxinji1990@163.com) and Lichao Nie (lichaonie@163.com)
This work was supported in part by the National Key Research and Development Plan under Grant 2016YFC0401801, in part by the
National Natural Science Foundation of China under Grant 51739007 and Grant U1806226, in part by the Fundamental Research Funds of
Shandong University under Grant 2017JC002 and Grant 2018GN019. The work of Yangkang Chen was supported by the Starting Funds
from Zhejiang University.

ABSTRACT Seismic full waveform inversion is a common technique that is used in the investigation of
subsurface geology. Its classic implementation involves forward modeling of seismic wavefield based on a
certain type of wave equation, which reflects the physics nature of subsurface seismic wavefield propagation.
However, obtaining a good inversion result using traditional seismic waveform inversion methods usually
comes with a high computational cost. Recently, with the emerging popularity of deep learning techniques
in various computer vision tasks, deep neural network (DNN) has demonstrated an impressive ability in
dealing with complex nonlinear problems, including seismic velocity inversion. Now, extensive efforts
have been made in developing a DNN architecture to tackle the problem of seismic velocity inversion,
and promising results have been achieved. However, due to the dependence of a labeled dataset, i.e., the
barely accessible true velocity model corresponding to real seismic data, the current supervised deep learning
inversion framework may suffer from limitations on generalization. One possible solution to mitigate this
issue is to impose the governing physics into this kind of purely data-driven method. Thus, following
the procedures of traditional seismic full waveform inversion, we propose a seismic waveform inversion
network, namely SWINet, based on wave-equation-based forward modeling network cells. By treating the
single-shot observation data and its corresponding shot position as training data pairs, the inverted velocity
model can be obtained as the trainable network parameters. Moreover, since the proposed seismic waveform
inversion method is performed in a neural-network way, its implementation and inversion effect could benefit
from some built-in tools in Pytorch, such as automatic differentiation, Adam optimizer and mini-batch
strategy, etc. Numerical examples indicate that the SWINet method may possess great potential in resulting
a good velocity inversion effect with relatively fast convergence and lower computation cost.
INDEX TERMS Acoustic wavefield modeling, deep learning inversion, seismic waveform inversion.
I. INTRODUCTION

Seismic full waveform inversion plays an important role in
the estimation of subsurface properties, such as geology,
lithology, rock mass quality, etc., and it has been of significant interest to exploration geophysicists for decades [1]‚Äì[5].
Waveform inversion is often achieved based on the forward
modeling of a particular wave equation to generate the synthetic observation data, which will be further compared with
the actual observation data to find the best description of the
subsurface properties.
The associate editor coordinating the review of this manuscript and
approving it for publication was Wei Liu.
112266

Particularly, seismic full waveform inversion (FWI) in its
conventional form is to reconstruct the velocity model that is
capable of matching the actual recorded data by minimizing
the data residual between synthetic data and actual data in
a L2 norm [6]‚Äì[8]. This inverse problem is usually ill-posed
and sensitive to initial model. Thus, the conventional FWI
often requires a good estimation of velocity as initial model
and utilizes an iterative optimization algorithm for updating
the model parameters. Moreover, due to the large amount of
model parameters (usually between 104 in 2D and 1010 in
3D) to invert, the computation cost is another constraining
factor that requires consideration in the research and practical
application of this method [9].

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

VOLUME 8, 2020

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

In recent years, the successful applications of deep learning
in various fields like computer vision (e.g. [10]‚Äì[12]) and
natural language processing (e.g. [13]‚Äì[15]), have attracted
attention from scholars in exploration geophysics community. Deep neural networks (DNNs) have demonstrated
good application effects on almost all kinds of geophysical
data (e.g. [16]‚Äì[20]). Particularly, some state-of-the-art deep
learning techniques have been widely applied in various seismic data problems, such as seismic noise attenuation (e.g.
[21]‚Äì[24]), automatic seismic event picking (e.g. [25]‚Äì[27])
and seismic structure interpretation (e.g. [28], [29]). All these
progresses indicate a great potential of DNN techniques and
collaboration between the deep learning community and the
geophysics community could lead to more achievements.
Especially in the domain of seismic data inversion,
the remarkable ability of DNN to accurately simulate
a non-linear and complex mapping has demonstrated
some advantages over traditional methods. For example,
Araya-Polo et al. [30] proposed a velocity prediction method
using a DNN with the calculated velocity feature semblance
as an input and a k-means method to post-process the output. Wu et al. [31] proposed a CNN-based network called
InversionNet to directly map the raw seismic data to the
corresponding seismic velocity model and it achieved good
inversion effect on simple fault models with flat or curved
subsurface layers. More recently, Li et al. [32] deeply analyzed the features of mapping the time-series seismic data to
a velocity image and then developed a novel DNN framework called SeisInvNet to perform the end-to-end velocity
inversion mapping with enhanced single-trace seismic data
as the input. In a word, various DNN frameworks have been
adopted into the task of seismic velocity inversion and some
of them have already outperformed the traditional FWI on
simple velocity models.
However, like all data-driven methods, the performance of
the aforementioned DNN-based seismic inversion methods
largely depends on the training dataset. Currently, almost all
of the DNN-based applications in seismic velocity inversion
is to simplify the velocity models into some classes and then
design an algorithm to generate enough velocity models for
training the deep learning network. Thus, the generalization
ability of the proposed deep learning inversion frameworks
to a more complex velocity model would be an issue. One
way to address this issue is to build or otherwise acquire
a set of realistic velocity models which is of a larger scale
and contain more complex structures such as salt bodies,
faults, several different curved layers with different velocity
distributions, etc. However, these kinds of improvements in
the aspects of velocity model size and complexity are usually
difficult, due to limitations of computation power such as
memories, CPU, GPU and etc. Thus, the extension of current
deep learning inversion philosophy to practical application
may require extensive research work in the future.
On the other hand, the promising inversion performance
of traditional FWI in synthetic and actual complex models, indicates the great power of considering the governing
VOLUME 8, 2020

wave equation in seismic inversion. Thus, another possible
solution to generalize the seismic deep learning inversion to
large complex models is to incorporate the relevant physical
nature rules into the purely data-driven methods and form
a physics-based deep learning concept [33], [34]. In this
concept, a seismic forward modeling operator needs to be
designed and imposed into the deep learning inversion architecture, which will relate the basic generation rules and features of seismic observation data with the learning process,
and as a result, may lead the optimization of network parameters to a more stable and reasonable direction. In fact, since
the traditional method achieves seismic waveform inversion
based entirely on the physics nature of wavefield propagation, i.e., the wave equation, its implementation from the
perspective of deep learning can be considered as a special
case of the physics-based deep learning scheme. Thus, the
physics-based seismic deep learning inversion may not only
have the ability of inverting a complex velocity model like
traditional FWI, but also maintain the potential of adopting
techniques from the deep learning community. For example,
the advanced tools like mini-batch strategy, various optimization algorithms and an easy access to GPU parallel computing
techniques will provide novel perspectives and convenience
for the study of seismic waveform inversion.
One key factor of the physics-based deep-learning inversion is to build a seismic forward-modeling operator that
can be incorporated into the deep-learning inversion network. By convolving with an angle-dependent wavelet,
Biswas et al. [33] proposed a forward modeling data generation method for updating the classic convolutional neural
network (CNN) in the task of elastic pre-stack inversion.
Richardson [35] achieved the simulation of acoustic wave
equation via a recurrent neural network (RNN) based on
TensorFlow platform [36], and undertook an initial attempt
on seismic FWI based on some useful deep learning tools.
In this work, we simulate seismic wave equation based
on a general DNN architecture using Pytorch [37], and the
implementation of a sponge absorbing boundary condition
in this framework is quite straightforward. Then for seismic
inversion, a comparison of the gradient calculation between
the adjoint state method that is widely used in FWI and
automatic differentiation in deep learning is conducted for
the discrete form of acoustic wave equation. Moreover, seismic waveform inversion is achieved in a neural network
optimization way, so that normal deep learning tools can
be adopted. As shown in the numerical examples, the proposed method demonstrates satisfactory inversion effect with
fast convergence and controllable computation cost. In the
end, we discuss the potential of the physics-based seismic
deep learning inversion and remark on some further possible
research directions along the line.
II. NETWORK FORMULATION FOR SEISMIC WAVEFIELD
EXTRAPOLATION

As stated in the INTRODUCTION section, the seismic forward modeling operator simulated via a neural network is
112267

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

an essential component for the physics-based seismic deep
learning inversion. Currently, one of the most popular and
accurate seismic wavefield extrapolation methods is based
on wave equation, which can be modified according to the
physical property of the wave propagation medium. Usually, for a homogeneous medium with constant density,
acoustic wave equation in time domain is used based on a
finite-difference discretization approach. Thus, for the convenience of demonstrating our work, we will start with the
basic one-dimensional acoustic wave equation and its corresponding network formulation. Similar implementations can
be extended to higher dimensions and other types of wave
equation.
A. PROBLEM DEFINITION AND BACKGROUND THEORY

FIGURE 1. The network architecture of seismic forward modeling (FM)
cells. The hexagon denotes a convolution operation with the Laplacian
operator L and the circles represent element-wise operations like
addition or multiplication with a number or matrix. Especially, matrix A(v)
contains the trainable parameter v of the network.

The acoustic wave equation in 1D time-space domain with a
source term s can be described as
‚àÇ 2p
‚àÇt 2

= v2 (z)

‚àÇ 2p
‚àÇz2

(1)

+ s(t, z),

where t and z denote time and depth, respectively, p(t, z)
usually denotes the pressure field and v(z) represents the
acoustic wave velocity. The finite-difference discretization in
the second order gives
pn+1
‚àí (2 + Œ±v2i ‚àá 2 )pni + pn‚àí1
= sn+1
.
i
i
i

(2)

1t
Here, Œ± = 1z
2 , vi denotes the acoustic velocity at all
nz model gird points and ‚àá 2 denotes the discrete Laplace
operator that conducts spatial discretization of the acoustic
wavefield p(t, z). By letting pn = [pn1 , pn2 , ¬∑ ¬∑ ¬∑ , pnnz ]T and
sn = [sn1 , sn2 , ¬∑ ¬∑ ¬∑ , snnz ]T represent the wavefield and source
at the time step n, respectively, the discrete acoustic wave
equation can be further rewritten in matrices as
2

pn+1 = Gpn ‚àí pn‚àí1 + sn+1 ,

(3)

where G = 2I + AL and A = diag{Œ±v2i }, L is the Laplacian
matrix of the form
Ô£Æ‚àí2
1
‚àí2
Ô£Ø1
..
L=Ô£Ø
.
Ô£∞

0

Ô£π
1
..
.
1

..

.
‚àí2
1

Ô£∫
Ô£∫
Ô£ª

1
‚àí2

.

(4)

nz√ónz

Notice that both matrix A and L are symmetric, while the
matrix G is not, which means that this discretization form
fails to maintain the self-adjoint property of acoustic wave
equation in the continuous form and extra attention is required
when calculating the inversion gradient based on the corresponding adjoint wave equation.
Furthermore, by denoting p = [p1 , p2 , . . . , pnt ]T , s =
1
[s , s2 , . . . , snt ]T , we can obtain the acoustic equation (3) in
a more compact form as Bp = s. Here
Ô£Æ I
Ô£π
‚àíG
I
Ô£Ø I
Ô£∫
‚àíG
I
Ô£∫.
B=Ô£Ø
(5)
Ô£∞
Ô£ª
..
..
..
.
.
.
I
‚àíG I
112268

As for the seismic acquisition geometry, we introduce the
matrix Ds to distribute the source wavelet w on every wavefield grids for all time steps, that is, s = Ds w. Moreover, data
collection process is denoted by the observation matrix Dr ,
which will extract the wavefield value at the predetermined
receiver position to an abstract data vector, i.e., d = Dr p =
Dr B‚àí1 s. Especially, when the seismic sources and receivers
are set on the same grids, we will have Ds = DTr . Actually, for
seismic data recorded on ground
surface, the

 corresponding
observation matrix is Dr = D1 D2 ¬∑ ¬∑ ¬∑ Dnt with submatrix
Dn of the size nt √ó nz in the following form
Ô£π
Ô£Æ
1 0 ¬∑¬∑¬∑ 0
0
0
¬∑
¬∑
¬∑
0
Ô£∫
Ô£Ø
D1 = Ô£∞ ..
..
Ô£ª,
..
. 0
.
.
0 0 ¬∑¬∑¬∑ 0
Ô£π
Ô£Æ
0 0 ¬∑¬∑¬∑ 0
Ô£Ø1 0 ¬∑ ¬∑ ¬∑ 0Ô£∫
Ô£∫
Ô£Ø
Ô£∫
Ô£Ø
(6)
D2 = Ô£Ø0 0 ¬∑ ¬∑ ¬∑ 0Ô£∫ , ¬∑ ¬∑ ¬∑ .
Ô£∫
Ô£Ø ..
..
..
Ô£∞.
. 0Ô£ª
.
0

¬∑¬∑¬∑

0

B. FORWARD MODELING NETWORK FORMULATION

Generally speaking, seismic wavefiled extrapolation is an
iterative process in time domain based on forward modeling
operators. In our work, considering the acoustic wave equation denoted in (3), we design a neural network with nt cells
and in each cell, wave extrapolation within one time step is
performed. That is, seismic wavefield extrapolation for nt
time steps can be achieved via the forward calculation of the
network. Moreover, the network parameters can be regarded
as the velocity values at every grid points and they can be
trained to arrive at their optimal points when provided with
suitable training setup and dataset.
More specially, the detailed network architecture in each
cell is designed by following the operations in the wave
equation (3). As shown in Fig. 1, the future wavefield pn+1
is calculated based on the wavefield on the previous two time
steps pn‚àí1 and pn as well as the future source sn+1 . Here, the
VOLUME 8, 2020

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

FIGURE 2. Illustration map for the (a) padding and (b) stacking
operations in absorbing boundary layers. The middle blue area denotes
the original velocity model.

circles represent element-wise operations. For example, the
circle with a number in it means element-wise multiplication
with that number and the circle A means element-wise multiplication between wavefield pn and the velocity model Œ±v.
The circle with a ‚Äò‚Äòplus‚Äô‚Äô sign denotes element-wise addition.
Moreover, the hexagon denotes the convolution operation of
wavefield pn with the discrete Laplace kernel of the corresponding dimension. For example, the vector [1, ‚àí2, 1] is
for 1D problems and 2D problems will adopt the matrix
filter
Ô£Æ
0
Ô£∞1
0

1
‚àí4
1

Ô£π
0
1Ô£ª .
0

(7)

The calculation within each cell can be accelerated by parallel
computing of the Laplace transform, which can be easily
achieved by calling a ready-made function in the Pytorch
platform.
C. ABSORBING BOUNDARY CONDITIONS

Usually, an absorbing boundary condition is considered in
seismic wavefield simulation to improve the calculation
efficiency by reducing the wavefield reflection from the
numerical boundaries. On the other hand, the addition of an
absorbing boundary layers should avoid changing the basic
architecture of the proposed forward modeling network and
maintain its differentiability, which is essential for accurately
computing the gradient when updating network parameters.
One of the most efficient and convenient ways to mitigate the
boundary reflection is to add several sponge layers outside
the original velocity models [38], [39].
In fact, adding sponge absorbing boundary layers includes
the padding of the outer boundary of the original velocity
model (see Fig. 2a) and then element-wise times with an
exponentially decaying factor to reduce the wavefield amplitudes in the extended layers. Thus, adding sponge layers can
be considered as putting a ‚Äò‚Äòmask‚Äô‚Äô over the whole wavefield,
and the ‚Äò‚Äòmask‚Äô‚Äô is a matrix whose element is one in the
original velocity area and the decaying factor in the extended
sponge area. With the absorbing boundary matrix M, the
governing matrix B for the acoustic forward modeling can
VOLUME 8, 2020

FIGURE 3. The network architecture of seismic forward modeling (FM)
cells with a sponge absorbing boundary condition. The circle M denotes
the sponge absorbing boundary condition, which indicates the ‚Äò‚Äòmask‚Äô‚Äô
matrix is element-wise multiplying with the corresponding wavefield.

be expressed as
Ô£Æ
I
Ô£Ø‚àíGM
2
Ô£Ø
B=Ô£Ø M
Ô£∞

Ô£π
I
‚àíGM
..
.

I
..
.
M2

Ô£∫
Ô£∫
Ô£∫.
Ô£ª

..

.
‚àíGM

(8)

I

Fortunately, the realization of the sponge absorbing boundary condition is quite straightforward. As shown in Fig. 3, the
two input previous wavefields are multiplied by the ‚Äò‚Äòmask‚Äô‚Äô
M in an element-wise form, which easily achieves the goal
of reducing wavefield amplitude without compromising the
differentiability of the proposed forward modeling network.
III. SEISMIC WAVEFORM INVERSION NETWORK

In this section, we build a neural network based on the proposed forward modeling (FM) cells and the velocity model
can be updated during the training process. It will be evident
to see that the way of optimizing velocity model is similar
to the traditional seismic inversion except for some adapted
concepts and techniques from the deep learning community.
A. NETWORK ARCHITECTURE

The aim of training a deep neural network is to find a set
of optimal DNN parameters that best fits the corresponding true observation data, and it is usually measured via a
loss function. In our case, based on the proposed forward
modeling cells, we can build a seismic waveform inversion
network, namely SWINet, whose trainable parameters are
the subsurface velocity distribution. The SWINet output is
the synthetic observation data that will be further compared
with the actual observation data to determine the velocity
update. Thus for a certain set of seismic data receivers, with
the shot positions acting as the network input, we can train
the SWINet to fit the actual observation data and obtain a
best guess of the subsurface velocity model.
Moreover, as shown in the network architecture illustration
map in Fig. 4, the input information about shot position is
transformed into seismic wavefield via the matrix Ds . In addition, nt forward modeling cells are adopted sequentially to
output the synthetic observation data dsyn for nt time steps.
112269

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

FIGURE 4. The network architecture of the seismic waveform inversion
network SWINet based on nt forward modeling cells.

Considering the fact that seismic data and its corresponding
shot position naturally come in pairs, it is not difficult to
acquire a training dataset containing seismic data generated
by thousands of or even millions of seismic shots in practice.
Similarly to the traditional seismic waveform inversion,
for every input data (i.e., single-shot seismic data), the loss
function can be defined as
1 XX
(dobs ‚àí dsyn )2 .
(9)
L=
2 nt nr
Here, dobs and dsyn are the observed data and the synthetic
data modeled by the neural network, respectively. nr and
nt denote the number of receivers and time steps, respectively. In this way, we can optimize the velocity model
by training the SWINet using all sorts of deep learning
advances, such as the Adam optimizer and mini-batch strategy in the numerical examples. It is worth noticing that
the requirement of a relatively good initial velocity model
in traditional FWI also applies to the training process of
SWINet, which means a good network initialization. It will
provide essential low-frequency information for the reconstruction of the velocity model and plays a vital role in
mitigating the cycle-skipping problem in seismic waveform
inversion [40]‚Äì[42].
B. GRADIENT CALCULATION

In the standard training process of DNN, network parameters
are usually optimized using gradient-based methods, which
can be found very convenient to use in the deep learning
platforms Pytorch and TensorFlow. The gradient can be calculated easily via the built-in automatic differentiation tools,
which are based on the backward propagation (BP) theory.
On the other hand, the gradient calculation in traditional
seismic inversion is based on the adjoint state method and
it has been theoretically proved to be coincident with BP
gradient in continuous form. Considering the discrete nature
of the finite-difference method, we demonstrate our proof in
the Appendices VI and VI from the perspective of matrix
calculation.
As shown in the Appendix A, the gradient calculation
involves computing the adjoint wavefield B‚àíT  with the
112270

data residual  as source. Normally, the adjoint wavefield is
obtained via the operator B‚àíT , and the adjointness between
them should be numerically verified by the dot-product test
[43]. When considering the absorbing boundary condition,
in addition to adopting the forward modeling operator shown
in (8), one should also consider the corresponding adjoint
operation of padding the boundary of the original velocity
model, which is the stacking of the absorbing layers onto
the corresponding boundary of the original velocity model
(see Fig. 2b for illustration). This can be understood via
matrix transposition where padding corresponds to a vertical vector a = [1, 1, ¬∑ ¬∑ ¬∑ , 1]T and stacking corresponds to
aT = [1, 1, ¬∑ ¬∑ ¬∑ , 1]. For example, when padding a boundary
velocity value vi for m times, the extended layers will have
a velocity value of vi a = P
[vi , vi , ¬∑ ¬∑ ¬∑ , vi ]T and the stacking
T
operation means a vi a = m vi = mvi . Thus, there will be
abnormal values on the outmost layers of the gradient and
the updated velocity model and this influence can be easily
neglected by cutting these abnormal layers during the training
process.
As mentioned before, using a handcraft code to calculate
the gradient for seismic velocity inversion usually requires
an adjoint test on the code [44], which is not easy to pass.
However, automatic differentiation calculates the gradient
based on the chain rule and its algorithm passes the adjoint
test automatically. Thus, considering the convenience of
implementing gradient calculation in deep learning platform,
we choose to use the automatic differentiation way for the
gradient computation in our work.
C. GPU PARALLEL COMPUTATION

Another advantage of using a neural network way to perform seismic waveform inversion is the easy implementation of GPU parallelization based on modern deep learning
platforms. In our case, the parallel computation is mainly
included in two processes: (a) parallel computing of the
Laplacian in acoustic extrapolation via convolution and
(b) parallel computing of the wavefield with different sources
on different GPUs. Both can be easily implemented by calling
the built-in functions in Pytorch. However, using GPU parallel computation also poses restrictions on the size of velocity
model due to the limited GPU memory. Thus, the effective
utilization of multiple GPUs in large 2D or even 3D velocity
models is worth further investigation.
In our work, we achieve the inversion of small 2D velocity models by implementing the SWINet on several GPUs
with 24G memory. As for a larger velocity, SWINet can be
implemented via CPU with larger memory. Moreover, experience from deep learning indicates that seismic inversion
can benefit from the gradient oscillations of the mini-batch
strategy and the adaptive gradient momentum of the Adam
optimizer, thereby obtaining a good inversion convergence.
Actually, the randomness in the gradient calculation is conducive for the gradient trajectory to escape from saddle points
or local minima during the optimization process. Considering
that Adam optimizer utilizes historical gradients and works
VOLUME 8, 2020

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

well with mini-batch strategy in various deep learning tasks,
we would like to expend their application scope to the field of
seismic inversion and impose them in the training process of
SWINet. Thus, the basic algorithm for training the SWINet
can be summarized in Alg. 1.
Algorithm 1 Training Process of SWINet
Inputs:
{dÃÇn }ns
n=1 : a set of real observation data; v0 : initial velocity
model; s: seismic source wavefield.
Notations:
p: seismic wavefield; P(¬∑): boundary padding of the
velocity model; L(¬∑): Laplacian of wavefield;
:
Hadamard product; M: absorbing boundary condition
mask.
Training process:
1: Init: Set trainable parameter v = v0 and p = 0;
2: for j = 1 : 1 : batchsize in GPUs do
3:
v ‚Üê P(v)
4:
for t = 1 : 1 : nt do
dt 2
2
5:
a ‚Üê dz
2 ¬∑v
6:
ptemp ‚Üê M p(t ‚àí 1)
7:
p(t) ‚Üê M p(t)
8:
p(t + 1) ‚Üê 2p(t) + a L(p(t)) ‚àí ptemp
9:
p(t + 1) ‚Üê p(t + 1) + s(j, t + 1)
10:
end for
11:
dj ‚Üê p(receiver position)
12:
loss ‚Üê L(dj , dÃÇj )
13: end for
14: v ‚Üê Adam(v, loss, lr)

IV. NUMERICAL EXAMPLES

One major goal of building the SWINet is to introduce some
popular strategies and techniques from the deep learning
community which can be experimented conveniently on seismic waveform inversion via deep learning platforms. In this
work, we focus on the approaches of Adam optimizer and
mini-batch strategy, and evaluate their application effect with
the proposed SWINet on the task of inverting the complex Marmousi model. This experiment is conducted on ten
NVIDIA Titan RTX with 24G memory.
A. DATASET AND TRAINING PARAMETER SETUP

The training dataset is constructed based on the synthetic data
simulated from the Marmousi model shown in Fig. 5a. This
model is down-sampled to a grid size of 112 √ó 384 in order to
fit the GPU memory. The smoothed velocity model in Fig. 5b
will be used for the initialization of the SWINet parameters.
Moreover, the grid spacing in both horizontal X and vertical
Z directions is 10m and time stepping interval is 1ms. We set
20 shots and 384 receivers evenly deployed on the ground
surface. The dominant frequency of seismic source Ricker
wavelet is 10Hz.
VOLUME 8, 2020

FIGURE 5. The Marmousi model. (a) True velocity and (b) smoothed
velocity. The scale-bar indicates the velocity value in m/s.

As for the network hyper-parameters, we follow the discussion in [34] and set the learning rate lr = 40 for the Adam
optimizer with the number of epochs equal to 50. Note that
there is no activation function or normalization applied in the
SWINet, a larger learning rate will benefit the convergence of
the Adam optimizer. To test the influence of the mini-batch
strategy in seismic waveform inversion, we design a series of
comparison experiments with different batch sizes and batch
iteration steps per epoch. Firstly, we fix the total computation
costs similar to the traditional full waveform inversion and
conduct the velocity inversion using SWINet with batch sizes
set as 2, 3, 4, 5, 6, 7, 10 and 20. The total computation cost
is defined as the number of gradient computations, which is
evaluated as batchsize √ó step √ó Nepoch. Considering that in
every batch iteration step, the traditional FWI usually utilizes
all of the observation data to update the velocity parameters,
its process is coincident with the trial with one batch iteration
step per epoch and batch size of 20. In other neural network
based waveform inversion, for batch size equal to 3, 6, 7,
we set batch step be 6, 3 and 2, respectively. In addition,
we remove the restriction regrading computation cost and set
batch steps equal to 2, 3, 4 and 5 to explore the inversion
effect of SWINet with more or limited computation power.
In order to obtain a reliable outcome of SWINet inversion
on Marmousi model, we repeat the aforementioned experiments for four times and their mean losses are summarized
in Table 1. Here, the data loss is computed via the L2 norm
of the difference between the forward modeling data and the
real observation data and the model loss is defined based on
the L2 norm of the difference between the inverted velocity
model and the truth velocity model corresponding to the real
observation data. According to the data loss, we mark the the
traditional FWI loss, the overall lowest loss, the lowest loss
with the fixed computation cost and the similar loss with lowest computation cost, respectively with color blue, red, green
and magenta. The model losses are also marked accordingly
112271

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

FIGURE 6. SWINet inversion results on Marmousi model corresponding to (a) traditional FWI (color blue in the table), (b) the overall lowest loss (color
red in the table), (c) the lowest loss with the fixed computation cost (color green in the table) and (d) the similar loss with lowest computation cost
(color magenta in the table).

TABLE 1. Statistics of the SWINet inversion loss with respect to different batch size (BS) and different batch iteration steps per epoch. According to the
data loss, we mark the the traditional FWI loss, the overall lowest loss, the lowest loss with the fixed computation cost and the similar loss with lowest
computation cost, respectively with color blue, red, green and magenta.

and their corresponding inversion results randomly selected
from the four repeated experiments are shown in Fig. 6.
B. RESULT ANALYSIS

Here, we use the conventional FWI result as a benchmark
to compare the SWINet inversion results in the view of data
and model misfit as well as computation costs. By comparing
the FWI result with the results of SWINet inversion based
on other combinations of batch size and batch iteration step,
we can see that the mini-batch strategy plays an essential role
in obtaining a good inversion outcome with fast convergence
and less computation power consumption.
112272

As shown in Fig. 6, the best inversion result (Fig. 6b)
comes from a batch size of ten and five batch iteration steps
per epoch, which gives a total of 2500 times of gradient
computations. Instead of 50 iterations, when conducting the
conventional FWI for 250 iterations, which corresponds to
a batch size of 20 and five batch iteration steps, one still
cannot obtain an inversion result as good as Fig.6b. On the
other hand, for a fixed computation power consumption (see
the first row of data and model loss in Table 1), the best
inversion result (Fig. 6c) comes from the case with batch size
of five, which also demonstrates the advantage of using a
mini-batch strategy. Moreover, it spends only 200 times of
VOLUME 8, 2020

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

FIGURE 7. SWINet inversion losses corresponding to the four figures in
Fig. 6, respectively. (a) data loss and (b) model loss. The ‚Äò‚ÄòSCC‚Äô‚Äô and ‚Äò‚ÄòLCC‚Äô‚Äô
in legend correspond to the inversion results using a similar computation
cost (SCC) to the conventional FWI and the lowest computation cost
(LCC), respectively.

gradient computations to get an inversion result (see Fig. 6d)
whose inversion effect is similar to the conventional FWI.
Noticing that the conventional FWI requires 1000 times gradient computations, which is another proof of the fast convergence when using mini-batch strategy in SWINet inversion.
It is worth mentioning that the data loss curve for cases of
setting batch size as two presents relatively severe oscillation
in Fig. 7a. Although it is not ideal in the perspective of loss
decaying, it implies a flexibility in updating the network
parameters and demonstrates a great potential in refining the
velocity model in the last stage of inversion. Thus, one may
consider using a very small batch size to further invert the
obtained inversion results in order to get an even better result.
Generally speaking, a smaller batch size leads to various
gradient direction with smaller memory storage requirement.
For a fixed batch size, larger batch iteration steps per epoch
usually results in a better convergence, but requires larger
computation cost. In our case, in order to determine an optimal batch size, we plot the losses in Fig. 8, where each curve
represents each row in the loss Table 1. Close observation of
these curves indicates that for a fixed batch step, both data
loss and model loss have a decreasing trend. However, the
downward trend seems to slow down at the batch size of 5.
Moreover, for a fixed computation cost, i.e., when the batch
size is defined as ‚Äò‚Äòfull‚Äô‚Äô, the data loss curve started to rise
after a turn point at batch size of 5. Thus, for a scenario
with limited computation resources, especially a limited GPU
memory, batch size set as 5 is potentially an efficient choice
to receive a good inversion result and it is empirical to adopt
four or five batch iteration steps in each epoch.
C. COMPARISON WITH SeisInvNet

In order to demonstrate the potential of the physics-based
deep learning scheme, we conduct a comparison experiment
to show the inversion effect of the SeisInvNet and the proposed SWINet. A dataset containing 18000 pairs of velocity
models and the corresponding observation data is numerically
generated. 15000 of them will be used to train the SeisInvNet
and we use another 1500 pairs of data for validation and 1500
for test. To make the comparison in a more realistic situation,
VOLUME 8, 2020

FIGURE 8. SWINet inversion losses with respect to different batch size.
(a) data loss and (b) model loss.

we design the velocity models to have some complex geology structures like folding layers and faults, and 20 seismic
sources and 32 receivers are evenly placed on the top layer
to generate the corresponding observation data. By following
the setup parameters shown in the reference [32], we train
the SeisInvNet for 200 epochs and its inversion effect on the
test dataset is compared with the SWINet, which is trained on
the observation data of each test velocity model based on the
same network setup as the color green indicates in Table 1.
An example of the comparison experiment result is shown
in Fig. 9. By comparing Figs. 9b and 9c, one can see that
SWINet seems to have a better inversion effect on the shallow
layers and the faulting structure while the inverted velocity
value within each layers seems more consistent in the SeisInvNet result than the SWINet result. However, we believe this
drawback of SWINet can be addressed via incorporating a
smooth regularization constraint in the network architecture.
Generally speaking, both methods have certain advantages.
Unlike the SeisInvNet, SWINet does not rely on the availability of the true velocity models as training labels. On the other
hand, the training process of SWINet requires a good estimated velocity model for network parameter initialization,
while SeisInvNet can be initialized randomly. As will be discussed later, a combination of both SWINet and SeisInvNet
could be studied to weaken the requirement of labelled dataset
(for SeisInvNet) and good network parameter initialization
(for SWINet) in the future.
V. DISCUSSION

The goal of this paper is to reformulate the traditional
FWI from a different perspective, e.g., in the format of
training a neural network, where the network parameters
become the subsurface velocity distribution. Thus in this
work, we demonstrate a physics-based neural network way
to perform seismic waveform inversion by proposing the
SWINet. After theoretically proving the equivalence of two
gradient calculation methods, i.e., the adjoint state method in
traditional seismic waveform inversion and the automatic differentiation widely used in deep learning, in a discrete matrix
form, we choose the latter due to its guarantee of adjointness and the convenient implementation in Pytorch. Numerical experiments with the Adam optimizer and mini-batch
strategy indicate that popular ideas from the deep learning
112273

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

FIGURE 9. A comparison experiment example. (a) True velocity model, (b) inversion result of SeisInvNet and (c) inversion result of SWINet.

community can also be beneficial to seismic full waveform
inversion and result in an improvement in the perspective of
convergence speed and computation cost.
Moreover, the numerical experiment results could be
further analyzed in terms of the possibility to obtain an uncertainty estimation [45] in the proposed framework. In a nutshell, the uncertainty estimation based on the Adam optimizer
is highly worth investigating, but we haven‚Äôt reached that
step. From the perspective of a general inverse problem, the
uncertainty estimation can be easily obtained by a bootstrapping method, i.e., we randomly select a fraction of the whole
input data for the inversion, and then calculate the standard
derivation of all the inverted result we have tested to be the
uncertainty of the inversion process.
Compared with the traditional FWI, the innovation of this
work lies in the sense of the implementation form and optimization strategy. This not only grants geophysicist an easy
access to latest advancements arised in all sorts of deep learning tasks, but also have a potential in solving the deep learning
problems with inspiring seismic approaches. For example,
when facing a high memory demand problem like seismic
inversion, where calculating the gradient using automatic
differentiation method requires the storage of the wavefield
from both forward modeling and backward propagation. One
possible way to greatly reduce the memory cost is to incorporate a wavefield reconstruction method [46]‚Äì[48] into the
automatic differentiation, which can rebuild the forward modeling wavefield and calculate the gradient while backward
propagation. Therefore, this work can be seen as a stepping
stone to deepen the cooperation among the researchers from
both geophysics and deep learning.
In addition, comparing the proposed SWINet with the
classic multi-layer perceptron (MLP) neural network, one can
conclude with two major differences in the network architecture design: in SWINet, (a) there is no need for activation
functions like the sigmoid function in accurately simulating
the wave equation, which plays an essential role in seismic
waveform inversion problem and (b) network parameters
(i.e., velocity model) are the same for all FM cells. That is,
there seems to be a very strong constraint on network parameters. In addition, instead of the single outcome of the last
layers, the loss function in seismic inversion problem involves
112274

the outcome of every layers. Thus, differently from the classic
deep learning network, where random initialization of network parameters works well [49], SWINet inversion depends
on a good initial model. How to modify the network architecture and tackle this issue may potentially lead to a major
breakthrough in seismic full waveform inversion.
Another potential of SWINet is to incorporate the FM cells
into the current purely data-driven seismic inversion network
such as the InversionNet [31] and the SeisInvNet [32]. In this
way, the physics nature of wave propagation can provide
guidance for the deep learning network to capture the true pattern between the training data pairs, thereby improve the generalization ability of seismic inversion network. Furthermore,
since the forward modeling data can be generated within the
network, how to construct an unsupervised seismic inversion
network and ease the requirement of true velocity model as a
label would be a hot topic worth further investigation.
VI. CONCLUSION

In this work, we constructed a seismic forward modeling
neural network and proposed the SWINet for seismic waveform inversion. With the velocity model acting as learnable
parameters, a set of single-shot observation data will be used
to train the SWINet. Numerical examples indicate that the
novel way to perform FWI provides an easy access to the
useful tools in deep learning society and the incorporation of
deep learning approaches, such as the Adam optimizer and
mini-batch strategy, can lead to an improvement on seismic
inversion like fast convergence and lower computation costs.
As illustrated in the discussion, there are some topics related
to this line of work that may be worth investigating.
APPENDIX A
GRADIENT DERIVATION VIA MATRIX CALCULUS

In this appendix, we will derive the velocity gradient in a
matrix calculus way, whose outcome is identical to the result
of adjoint state method that is massively used in traditional
seismic full waveform inversion. Following the notations in
the background acoustic wave equation theory, we have the
wavefield solver in the discretized form as
p = B‚àí1 s,

(10)
VOLUME 8, 2020

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

and the least-squares loss function is
1
(11)
L = (Dr p ‚àí Dr pÃÑ)T (Dr p ‚àí Dr pÃÑ) .
2
Here, p and pÃÑ denote the forward modeling wavefield and
the real wavefield, respectively. The goal is to calculate the
gradient of the loss function with respect to the velocity v.
Firstly, we can easily compute
dL(p)
= Dr T Dr (p ‚àí pÃÑ) := ,
(12)
dp
which is in fact the data residual distributed at all receiver
points. Then, based on the chain rule and considering that B
is a function of the velocity model v, for each vi we have

T
dp dL
dL
‚àí1 dB ‚àí1
=
=‚àí B
B s 
dvi
dvi dp
dvi
T

dB ‚àí1
B s B‚àíT .
(13)
=‚àí
dvi
Here,
Ô£Æ0
Ô£π
0
G
0
Ô£Ø i
Ô£∫
dB
Ô£Ø
Ô£∫
G0i
0
(14)
= ‚àíÔ£Ø
Ô£∫,
Ô£∞
Ô£ª
..
..
dvi
.
.
G0i 0
and
Ô£Æ

..

..

Ô£Ø .
Ô£Ø

0

dG
G0i =
= 2Œ±vi Ô£Ø
Ô£Ø
dvi
Ô£∞

.

..
0
1

.

Ô£π
0
‚àí2
0

1
0
..
.

0
..
.

..

Ô£∫
Ô£∫
Ô£∫.
Ô£∫
Ô£ª

.
(15)

Notice that, the subdiagonal matrix in (14) corresponds to
the one-time-step delay in forward modeling wavefield B‚àí1 s
when it cross-correlates with the backward propagated wavefield B‚àíT . The matrix G0 relates to the ith row of the Laplacian matrix, which can be further proved to be coincident
with the second-order time derivative of the forward model
wavefield in the conventional Born approximation theory.
APPENDIX B
GRADIENT CALCULATION VIA AUTO-DIFFERENTIATION

Here, we will calculate the gradient of network parameters
following the steps of automatic differentiation. In this case,
the least-squares loss function (11) can be rewritten as
nt

L=

1X n n
(Dr p ‚àí Dnr pÃÑn )2 .
2

(16)

n=1

Here, pn and pÃÑn are the forward modeling wavefield and
the real wavefield at the nth time step. According to the
back-propagation method, we need to firstly define a series of
middle variables according to the forward modeling equation
(3) as
‚àÇL
T
Œ¥ nt =
= Dnt (Dnt pnt ‚àí Dnt pÃÑnt ) =  nt ,
‚àÇpnt
VOLUME 8, 2020

‚àÇL ‚àÇpnt
‚àÇL
+ nt‚àí1
‚àÇpnt ‚àÇpnt‚àí1
‚àÇp
= GT Œ¥ nt +  nt‚àí1 ,
‚àÇL ‚àÇpnt‚àí1
‚àÇL
‚àÇL ‚àÇpnt
+
+ nt‚àí2
Œ¥ nt‚àí2 =
nt
nt‚àí2
nt‚àí1
nt‚àí2
‚àÇp ‚àÇp
‚àÇp
‚àÇp
‚àÇp
= GT Œ¥ nt‚àí1 ‚àí Œ¥ nt +  nt‚àí2 ,
¬∑¬∑¬∑¬∑¬∑¬∑
n
Œ¥ = GT Œ¥ n+1 ‚àí Œ¥ n+2 +  n .
(17)
Œ¥ nt‚àí1 =

Similarly to Appendix A, we obtain the data residual at the
‚àÇL
n
nth time step by  n = ‚àÇp
n . Œ¥ corresponds to the data residual
propagation backward in time based on the future wavefield
and it is in fact the adjoint wavefield B‚àíT  at the nth time
step.
Then based on the chain rule, the gradient of loss function
with respect to the ith network parameter vi can be calculated
as

nt 
X
dL dpn dG
dL
=
dvi
dpn dG dvi
n=0

=

nt 
X


(G0i pn‚àí1 )T Œ¥ n .

(18)

n=1

Here G0i is the same as the (15) in the Appendix A, pn‚àí1
denotes the forwarding modeling wavefield B‚àí1 s at the
(n ‚àí 1)th time step, which agrees with the one-time-step
delay caused by the subdiagonal matrix (14). In other
words, forward-propagating through the SWINet gives B‚àí1 s
and backward-propagating the data error gives B‚àíT , then
cross-correlating both wavefields according (18) is identical
to its theorical value shown in (13). Therefore, adopting the
automatic differentiation steps to calculate the gradient leads
to the same results as in the traditional seismic waveform
inversion using the adjoint state method.
REFERENCES
[1] C. Bunks, F. M. Saleck, S. Zaleski, and G. Chavent, ‚Äò‚ÄòMultiscale seismic waveform inversion,‚Äô‚Äô Geophysics, vol. 60, no. 5, pp. 1457‚Äì1473,
Sep. 1995.
[2] R. G. Pratt, ‚Äò‚ÄòSeismic waveform inversion in the frequency domain, part 1:
Theory and verification in a physical scale model,‚Äô‚Äô Geophysics, vol. 64,
no. 3, pp. 888‚Äì901, May 1999.
[3] Z. Pan, J. Wu, Z. Gao, and J. Gao, ‚Äò‚ÄòAdaptive differential evolution by
adjusting subcomponent crossover rate for high-dimensional waveform
inversion,‚Äô‚Äô IEEE Geosci. Remote Sens. Lett., vol. 12, no. 6, pp. 1327‚Äì1331,
Jun. 2015.
[4] S. Qu, E. Verschuur, and Y. Chen, ‚Äò‚ÄòFwi/jmi with an automatic directional
total variation constraint,‚Äô‚Äô Geophysics, vol. 84, no. 2, pp. 175‚Äì183, 2019.
[5] L. Mosser, O. Dubrule, and M. J. Blunt, ‚Äò‚ÄòStochastic seismic waveform
inversion using generative adversarial networks as a geological prior,‚Äô‚Äô
Math. Geosci., vol. 52, no. 1, pp. 53‚Äì79, Jan. 2020.
[6] P. Lailly, ‚Äò‚ÄòThe seismic inverse problem as a sequence of before stack
migrations,‚Äô‚Äô in Conference on Inverse Scattering: Theory and Application.
Philadelphia, PA, USA: SIAM, 1983, pp. 206‚Äì220.
[7] A. Tarantola, ‚Äò‚ÄòInversion of seismic reflection data in the acoustic approximation,‚Äô‚Äô Geophysics, vol. 49, no. 8, pp. 1259‚Äì1266, Aug. 1984.
[8] R.-E. Plessix, ‚Äò‚ÄòA review of the adjoint-state method for computing the
gradient of a functional with geophysical applications,‚Äô‚Äô Geophys. J. Int.,
vol. 167, no. 2, pp. 495‚Äì503, Nov. 2006.
112275

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

[9] Q. Zhang, W. Mao, H. Zhou, H. Zhang, and Y. Chen, ‚Äò‚ÄòHybrid-domain
simultaneous-source full waveform inversion without crosstalk noise,‚Äô‚Äô
Geophys. J. Int., vol. 215, no. 3, pp. 1659‚Äì1681, Dec. 2018.
[10] Y. LeCun, Y. Bengio, and G. Hinton, ‚Äò‚ÄòDeep learning,‚Äô‚Äô Nature, vol. 521,
pp. 436‚Äì444, May 2015.
[11] A. Kendall and Y. Gal, ‚Äò‚ÄòWhat uncertainties do we need in Bayesian deep
learning for computer vision?‚Äô‚Äô in Proc. Adv. Neural Inf. Process. Syst.,
2017, pp. 5574‚Äì5584.
[12] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, ‚Äò‚ÄòDeep
learning for computer vision: A brief review,‚Äô‚Äô Comput. Intell. Neurosci.,
vol. 2018, pp. 1‚Äì13, 2018.
[13] R. Collobert and J. Weston, ‚Äò‚ÄòA unified architecture for natural language
processing: Deep neural networks with multitask learning,‚Äô‚Äô in Proc. 25th
Int. Conf. Mach. Learn. (ICML), 2008, pp. 160‚Äì167.
[14] W. Yin, K. Kann, M. Yu, and H. Sch√§tze, ‚Äò‚ÄòComparative study of CNN and
RNN for natural language processing,‚Äô‚Äô 2017, arXiv:1702.01923. [Online].
Available: http://arxiv.org/abs/1702.01923
[15] T. Young, D. Hazarika, S. Poria, and E. Cambria, ‚Äò‚ÄòRecent trends in deep
learning based natural language processing,‚Äô‚Äô IEEE Comput. Intell. Mag.,
vol. 13, no. 3, pp. 55‚Äì75, Aug. 2018.
[16] L. Huang, X. Dong, and T. E. Clee, ‚Äò‚ÄòA scalable deep learning platform for
identifying geologic features from seismic attributes,‚Äô‚Äô Lead. Edge, vol. 36,
no. 3, pp. 249‚Äì256, Mar. 2017.
[17] G. Zhang, Z. Wang, and Y. Chen, ‚Äò‚ÄòDeep learning for seismic lithology
prediction,‚Äô‚Äô Geophys. J. Int., pp. 1368‚Äì1387, Aug. 2018.
[18] S. Lameri, F. Lombardi, P. Bestagini, M. Lualdi, and S. Tubaro, ‚Äò‚ÄòLandmine
detection from GPR data using convolutional neural networks,‚Äô‚Äô in Proc.
25th Eur. Signal Process. Conf. (EUSIPCO), Aug. 2017, pp. 508‚Äì512.
[19] B. Liu, Q. Guo, S. Li, B. Liu, Y. Ren, Y. Pang, X. Guo, L. Liu,
and P. Jiang, ‚Äò‚ÄòDeep learning inversion of electrical resistivity data,‚Äô‚Äô
IEEE Trans. Geosci. Remote Sens., early access, Feb. 11, 2020, doi:
10.1109/TGRS.2020.2969040.
[20] F. Wang, S. Chen, and Y. Liu, ‚Äò‚ÄòDeep learning for gravity and magnetic data
interpolation,‚Äô‚Äô in Proc. SEG Tech. Program Expanded Abstr., Aug. 2019,
pp. 2533‚Äì2537.
[21] H. Li, W. Yang, and X. Yong, ‚Äò‚ÄòDeep learning for ground-roll noise
attenuation,‚Äô‚Äô in Proc. SEG Tech. Program Expanded Abstr., Aug. 2018,
pp. 1981‚Äì1985.
[22] D. Liu, W. Wang, W. Chen, X. Wang, Y. Zhou, and Z. Shi, ‚Äò‚ÄòRandom-noise
suppression in seismic data: What can deep learning do?‚Äô‚Äô in Proc. SEG
Tech. Program Expanded Abstr., Aug. 2018, pp. 2016‚Äì2020.
[23] S. Yu, J. Ma, and W. Wang, ‚Äò‚ÄòDeep learning for denoising,‚Äô‚Äô Geophysics,
vol. 84, no. 6, pp. V333‚ÄìV350, Nov. 2019.
[24] O. Saad and Y. Chen, ‚Äò‚ÄòDeep denoising autoencoder for seismic random
noise attenuation,‚Äô‚Äô Geophysics, vol. 85, no. 4, doi: 10.1190/geo2019‚Äì
0468.1.2020.
[25] Y. Chen, ‚Äò‚ÄòAutomatic microseismic event picking via unsupervised
machine learning,‚Äô‚Äô Geophys. J. Int., vol. 212, no. 1, pp. 88‚Äì102, Jan. 2018.
[26] H. Wu, B. Zhang, F. Li, and N. Liu, ‚Äò‚ÄòSemiautomatic first-arrival picking of
microseismic events by using the pixel-wise convolutional image segmentation method,‚Äô‚Äô Geophysics, vol. 84, no. 3, pp. V143‚ÄìV155, May 2019.
[27] W. Zhu and G. C. Beroza, ‚Äò‚ÄòPhaseNet: A Deep-Neural-Network-Based
seismic arrival time picking method,‚Äô‚Äô Geophys. J. Int., pp. 261‚Äì273,
Oct. 2018.
[28] A. U. Waldeland, A. C. Jensen, L.-J. Gelius, and A. H. S. Solberg, ‚Äò‚ÄòConvolutional neural networks for automated seismic interpretation,‚Äô‚Äô Lead.
Edge, vol. 37, no. 7, pp. 529‚Äì537, Jul. 2018.
[29] H. Di, D. Gao, and G. AlRegib, ‚Äò‚ÄòDeveloping a seismic texture analysis
neural network for machine-aided seismic pattern recognition and classification,‚Äô‚Äô Geophys. J. Int., vol. 218, no. 2, pp. 1262‚Äì1275, Aug. 2019.
[30] M. Araya-Polo, J. Jennings, A. Adler, and T. Dahlke, ‚Äò‚ÄòDeep-learning
tomography,‚Äô‚Äô Lead. Edge, vol. 37, no. 1, pp. 58‚Äì66, 2018.
[31] Y. Wu, Y. Lin, and Z. Zhou, ‚Äò‚ÄòInversionnet: Accurate and efficient seismic
waveform inversion with convolutional neural networks,‚Äô‚Äô in Proc. SEG
Tech. Program Expanded Abstr., 2018, pp. 2096‚Äì2100.
[32] S. Li, B. Liu, Y. Ren, Y. Chen, S. Yang, Y. Wang, and P. Jiang, ‚Äò‚ÄòDeeplearning inversion of seismic data,‚Äô‚Äô IEEE Trans. Geosci. Remote Sens.,
vol. 58, no. 3, pp. 2135‚Äì2149, Oct. 2020.
[33] R. Biswas, M. K. Sen, V. Das, and T. Mukerji, ‚Äò‚ÄòPre-stack inversion
using a physics-guided convolutional neural network,‚Äô‚Äô in Proc. SEG Tech.
Program Expanded Abstr., Aug. 2019, pp. 4967‚Äì4971.
[34] J. Sun, Z. Niu, K. A. Innanen, J. Li, and D. O. Trad, ‚Äò‚ÄòA theory-guided deeplearning formulation and optimization of seismic waveform inversion,‚Äô‚Äô
Geophysics, vol. 85, no. 2, pp. R87‚ÄìR99, Mar. 2020.
112276

[35] A. Richardson, ‚Äò‚ÄòSeismic full-waveform inversion using deep learning
tools and techniques,‚Äô‚Äô 2018, arXiv:1801.07232. [Online]. Available:
http://arxiv.org/abs/1801.07232
[36] M. Abadi, ‚Äò‚ÄòTensorFlow: Large-scale machine learning on heterogeneous distributed systems,‚Äô‚Äô 2016, arXiv:1603.04467. [Online]. Available:
http://arxiv.org/abs/1603.04467
[37] A. Paszke, ‚Äò‚ÄòPytorch: An imperative style, high-performance deep learning
library,‚Äô‚Äô in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 8024‚Äì8035.
[38] C. Cerjan, D. Kosloff, R. Kosloff, and M. Reshef, ‚Äò‚ÄòA nonreflecting
boundary condition for discrete acoustic and elastic wave equations,‚Äô‚Äô
Geophysics, vol. 50, no. 4, pp. 705‚Äì708, Apr. 1985.
[39] C. Shin, ‚Äò‚ÄòSponge boundary condition for frequency‚Äìdomain modeling,‚Äô‚Äô
Geophysics, vol. 60, no. 6, pp. 1870‚Äì1874, Nov. 1995.
[40] N. Shah, M. Warner, T. Nangoo, A. Umpleby, I. Stekl, J. Morgan, and
L. Guasch, ‚Äò‚ÄòQuality assured full-waveform inversion: Ensuring starting model adequacy,‚Äô‚Äô in Proc. SEG Tech. Program Expanded Abstr.,
Sep. 2012, pp. 1‚Äì5.
[41] K. Jiao, D. Sun, X. Cheng, and D. Vigh, ‚Äò‚ÄòAdjustive full waveform
inversion,‚Äô‚Äô in Proc. SEG Tech. Program Expanded Abstr., Aug. 2015,
pp. 1091‚Äì1095.
[42] Y. Chen, H. Chen, K. Xiang, and X. Chen, ‚Äò‚ÄòGeological structure guided
well log interpolation for high-fidelity full waveform inversion,‚Äô‚Äô Geophys.
J. Int., vol. 207, no. 2, pp. 1313‚Äì1331, Nov. 2016.
[43] J. F. Claerbout, Earth Soundings Analysis: Processing Versus Inversion,
vol. 6. London, U.K.: Blackwell Scientific, 1992.
[44] M. Louboutin, P. Witte, M. Lange, N. Kukreja, F. Luporini, G. Gorman,
and F. J. Herrmann, ‚Äò‚ÄòFull-waveform inversion, part 2: Adjoint modeling,‚Äô‚Äô
Lead. Edge, vol. 37, no. 1, pp. 69‚Äì72, Jan. 2018.
[45] Z.-D. Zhang and T. Alkhalifah, ‚Äò‚ÄòRegularized elastic full-waveform inversion using deep learning,‚Äô‚Äô Geophysics, vol. 84, no. 5, pp. R741‚ÄìR751,
Sep. 2019.
[46] H. Liu, R. Ding, L. Liu, and H. Liu, ‚Äò‚ÄòWavefield reconstruction methods
for reverse time migration,‚Äô‚Äô J. Geophys. Eng., vol. 10, no. 1, Feb. 2013,
Art. no. 015004.
[47] E. B. Raknes and W. Weibull, ‚Äò‚ÄòEfficient 3D elastic full-waveform inversion using wavefield reconstruction methods,‚Äô‚Äô Geophysics, vol. 81, no. 2,
pp. 45‚Äì55, Mar. 2016.
[48] M. Vasmel and J. O. A. Robertsson, ‚Äò‚ÄòExact wavefield reconstruction on
finite-difference grids with minimal memory requirements,‚Äô‚Äô Geophysics,
vol. 81, no. 6, pp. 303‚Äì309, Nov. 2016.
[49] X. Glorot and Y. Bengio, ‚Äò‚ÄòUnderstanding the difficulty of training deep
feedforward neural networks,‚Äô‚Äô in Proc. 13th Int. Conf. Artif. Intell. Statist.,
2010, pp. 249‚Äì256.

YUXIAO REN received the bachelor‚Äôs degree in
mathematics from Shandong University, China,
in 2014, and the master‚Äôs degree in mathematics
from Loughborough University, U.K., in 2015.
He is currently pursuing the Ph.D. degree in civil
engineering with Shandong University. He is also
a Visiting Scholar with the Georgia Institute of
Technology under the supervision of Prof. Felix
Herrmann. His research interests include seismic
modeling and imaging, full-waveform inversion,
and deep-learning-based geophysical inversion.

XINJI XU was born in Jining, Shandong, China,
in 1990. He received the B.S. degree in city underground engineering and the Ph.D. degree from
Shandong University, Jinan, Shandong, in 2012
and 2017, respectively. Since 2017, he has been
working as an Assistant Research Fellow with the
Geotechnical and Structural Engineering Research
Center, Shandong University. His research interests include advance geological forecast of tunnel
and underground engineering, seismic data processing, and seismic imaging and inversion.
VOLUME 8, 2020

Y. Ren et al.: Physics-Based Neural-Network Way to Perform Seismic FWI

SENLIN YANG received the bachelor‚Äôs degree
in engineering from Shandong University, China,
in 2017, where he is currently pursuing the
Ph.D. degree. His research interests include deeplearning-based geophysical data processing and
inversion.

LICHAO NIE received the Ph.D. degree from the
Geotechnical and Structural Engineering Research
Center, Shandong University, China, in 2014.
He is currently an Associate Professor with the
School of Civil Engineering, Shandong University. He is mainly engaged in geophysical forward and inversion theory and method, geological forward-prospecting method and technology in tunnels, and tunnel boring machine
with forward-prospecting system as well as its
engineering application, and so on.

VOLUME 8, 2020

YANGKANG CHEN received the B.S. degree in
exploration geophysics from the China University of Petroleum, Beijing, in 2012, and the Ph.D.
degree in geophysics from The University of Texas
at Austin, in 2015, under the supervision of Prof.
Sergey Fomel. He joined with the Oak Ridge
National Laboratory, as a Distinguished Postdoctoral Research Associate, where he conducted
research on global adjoint tomography. He is currently an Assistant Professor with Zhejiang University. He has published more than 100 internationally renowned journal
articles and more than 80 international conference papers. His research
interests include seismic signal analysis, seismic modeling and inversion,
simultaneous-source data deblending and imaging, global adjoint tomography, and machine learning.

112277

