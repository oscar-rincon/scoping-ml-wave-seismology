Rapid Seismic Waveform Modeling and Inversion
with Neural Operators
Yan Yang, Angela F. Gao, Kamyar Azizzadenesheli, Robert W. Clayton, and Zachary E. Ross
Abstractâ€” Seismic waveform modeling is a powerful tool for
determining earth structure models and unraveling earthquake
rupture processes, but it is usually computationally expensive. We
introduce a scheme to vastly accelerate these calculations with a
recently developed machine learning paradigm called the neural
operator. Once trained, these models can simulate a full wavefield
at negligible cost. We use a U-shaped neural operator to learn a
general solution operator to the 2D elastic wave equation from an
ensemble of numerical simulations performed with random
velocity models and source locations. We show that full waveform
modeling with neural operators is nearly two orders of magnitude
faster than conventional numerical methods, and more
importantly, the trained model enables accurate simulation for
velocity models, source locations, and mesh discretization
distinctly different from the training dataset. The method also
enables convenient full-waveform inversion with automatic
differentiation.
Index Terms â€” Geophysics, machine learning, partial
differential equations (PDEs), waveform modeling, full-waveform
inversion

I. INTRODUCTION

T

HE seismic wave equation relates displacement fields
to external forces and the density and elastic structure in
the Earth. Solutions to the wave equation form the basis
of ground shaking simulations of large earthquakes [1]â€“[3] and
full waveform inversion for Earthâ€™s structure [4]â€“[6]. Due to the
highly heterogeneous nature of the Earth, as exemplified by
subduction zones and sedimentary basins, there are no exact
analytical solutions for these wavefields. Instead, approximate
solutions are made possible by approximating derivatives
through discretized spatial and time or frequency domains.
Finite difference methods (FDM) have been popular since the
early 80s due to their relatively straightforward formulation
[7]â€“[9]. The spectral-element method (SEM), a particular case
of finite element methods (FEM), which was introduced to
seismology in early 2000s, combined the flexibility of FEMs
with the accuracy of spectral approaches [10]â€“[13]. These
numerical solvers impose a tradeoff between resolution and
computation speed, with the computational cost proportional to
the fourth power of frequency [14]. Thus, the cost of wave
simulation is a major barrier to using full-waveform techniques
for seismic inversion and updating models of the subsurface
with new data.
A number of machine learning-based methods have been
proposed in the past few years to provide a faster alternative for
tackling seismological problems such as signal denoising [15]â€“
[17], event detection [18]â€“[20], and phase association [21],
[22]. Deep neural networks have also recently been used to
solve partial differential equations (PDEs), such as the Eikonal
equation and wave equation [23]â€“[27]. These approaches to

solving PDEs offer not only speedup in computational
capabilities, but also low-memory overhead, differentiability,
and on-demand solutions. Such advantages facilitate deep
learning being used for seismic inversion [28]â€“[32]. However,
one major limitation of these approaches is that the solutions
generated by these models are dependent on the specific spatial
and temporal discretization in the numerical simulation training
set.
Recently, a paradigm named â€˜neural operatorâ€™ was developed
to address the mesh-dependent shortcoming of classical neural
networks by creating a single deep learning model that can be
applied to different discretizations [33]â€“[36]. This is made
possible because neural operators can provably learn mappings
between infinite-dimensional function spaces [37] and
therefore are suitable for learning general solution operators to
PDEs, which are valid even when the PDE coefficients (e.g.
elastic properties) are varied. Since first introduced [33], a
variety of neural operator models have been developed. In
particular, the Fourier neural operator (FNO) is a model that
uses the fast Fourier transform as an integral operator, and has
been shown to outperform other neural operators in terms of
efficiency and accuracy [38]. The FNO has been applied to
many types of scientific problems including weather
forecasting [39], CO2 sequestration [40], and coastal flooding
[41].
Within the domain of seismology, neural operators were also
recently used to learn general solution operators to the 2-D
acoustic wave equation, a simplified case of the elastic wave
equation [42]. This pilot study demonstrated that it was possible
for a single FNO model to predict a complete wavefield given
an arbitrary velocity model and mesh discretization. The
success of this limited case highlights the potential of these
methods, however, extending the method from the acoustic
wave equation to the elastodynamic case requires substantially
increased model complexity. By comparison with neural
networks, FNO is not considered to be a deep architecture, and
is most analogous to the fully-connected neural networks
employed heavily until the 2010s. A U-shaped neural operator
(U-NO) was recently proposed to enable very deep neural
operators and facilitate fast training, data efficiency, and
hyperparameter selection robustness [43].
In this paper, we apply the U-NO architecture to full seismic
waveform modeling. We train a U-NO model to learn a general
solution operator to the 2D elastic wave equation and
demonstrate that the trained model enables fast and accurate
simulation for source locations, velocity structures and mesh
discretization beyond the training dataset. The trained U-NO
also allows for efficient full-waveform inversion with
automatic differentiation.

II. METHODS
A. Neural operator learning
Operators are maps between function spaces, and the purpose
of operator learning is to learn the operator given a dataset of
input-output pairs. In seismology, it is common to write
solutions to the wave equation, ğ‘ˆ(ğ‘¥) in terms of a linear
integral operator acting on a source function, ğ´(ğ‘¥),
ğ‘ˆ(ğ‘¥) = âˆ« ğº(ğ‘¥, ğ‘¦)ğ´(ğ‘¦)ğ‘‘ğ‘¦.
(1)
where ğ‘¥ âˆˆ â„! is the physical domain and ğº is a so-called
Greenâ€™s function defined for a particular velocity model.
Equation 1 holds so long as the velocity model is not varied
because the wave equation remains a linear operator.
Instead, if we consider the case where the input function,
ğ´(ğ‘¥), is a velocity model, the solution operator, â„’, relating this
to ğ‘ˆ(ğ‘¥) is nonlinear and cannot be written in the form of (1),
ğ‘ˆ(ğ‘¥) = (â„’ ğ´)(ğ‘¥).
The most general version of the nonlinear solution operator â„’
for the elastic wave equation is not known in closed form.
Neural operators are a class of models that aim to solve this
problem, as they provably can learn a wide array of nonlinear
operators. Their basic form consists of a composition of linear
operators with nonlinear activations. More specifically, a neural
operator with L layers can be written as:
ğ‘£" (ğ‘¥) = (ğ‘ƒ ğ´)(ğ‘¥),
(ğ‘¥)
(ğ‘¥)
ğ‘£#$%
= ğœ3ğ‘Š# ğ‘£#
+ âˆ« ğœ…# (ğ‘¥, ğ‘¦)ğ‘£# (ğ‘¦)ğ‘‘ğ‘¦7, ğ‘™ = 0, . . , ğ¿ âˆ’ 1
ğ‘ˆ(ğ‘¥) = (ğ‘„ ğ‘£& )(ğ‘¥).
(2)
where ğ‘£# is the input function at the ğ‘™'( layer, ğ‘ƒ is a pointwise
operator that lifts the input function to a higher dimensionality,
ğ‘„ is a pointwise operator that projects the function back to the
desired output dimensionality, ğ‘Š# is a linear pointwise
transformation that can keep track of non-periodic boundary
behavior, ğœ is a pointwise nonlinear activation operator, and ğœ…#
is a kernel function that acts along with the integral as a global
linear operator.
A neural operator is parameterized by ğ‘ƒ, ğ‘„, ğ‘Š# , and ğœ…# . A
critical aspect of this class of models is that these parameters
are independent of the numerical discretization of the physical
domain, i.e. they are shared across all possible discretizations
in a similar way that in convolutional networks, the parameters
are shared across neurons. It is this property that allows for the
learning of maps between infinite dimensional function spaces,
as the discretization can be chosen dynamically at inference
time independently of what was used for training.
If we are given a dataset of ğ‘ numerical simulations,
{ğ´) , ğ‘ˆ) }+
)*% , where the ğ´) are chosen to span the range of the
expected function space, we can train a neural operator in a
supervised fashion to map from arbitrary ğ´ into ğ‘ˆ.
Due to the expense of evaluating integral operators, neural
operators may lack the efficiency of convolutional or recurrent
neural networks in finite-dimensional settings. The FNO was

proposed to mitigate this difficulty through the fast Fourier
transform [38]. The kernel integral operator in (2) can be
considered a convolution operator, defined in Fourier space as:
âˆ« ğœ…# (ğ‘¥, ğ‘¦)ğ‘£# (ğ‘¦)ğ‘‘ğ‘¦ = â„± ,% (â„±(ğœ…# ) âˆ™ â„±(ğ‘£# )).
(3)
where â„± and â„± ,% denote Fourier Transform and its inverse,
respectively. However, FNO imposes that each layer is a map
between functions spaces with identical domain spaces, which
may cause a large memory usage. The U-NO, an analogy to the
U-net architectures, was proposed to allow progressively
transforming the input function space with respect to a sequence
of varying domains [43], [44]. After the lifting operator ğ‘ƒ, a
sequence of ğ¿% non-linear integral operators ğº) is applied to ğ‘£"
and map the input to a set of functions with decreasing
dimensional domain. Then a sequence of ğ¿- non-linear integral
operators ğº) is applied to ğ‘£&!$% and map the input to a set of
functions with increasing dimensional domain before the
projection operator ğ‘„. Skip connections [44] are included to
add vector-wise concatenation of ğ‘£&!$) and ğ‘£&!,) . The
contracting and expanding parts are symmetric. The
architecture of the U-NO used in this study is illustrated in
Figure 1, and we refer the interested readers to the references
[33]â€“[36], [38], [43] for more details.
B. Numerical simulation
We set up a training dataset of random source locations, Swave velocity (VS) models, and P- to S-wave velocity ratios
(VP/VS). We define the velocity model on a 64 Ã— 64 mesh with
0.16 km grid spacing. The source is set as an isotropic explosive
source randomly distributed on the mesh. The VS has an
average background of 3 km/s and perturbed by random fields
with a von KÃ¡rmÃ¡n covariance function with the following
parameters: Hurst exponent ğœ…=0.5â , correlation length ğ‘. =ğ‘/ =8
grids, and the fractional magnitude of the fluctuation ğœ€=10%
background velocityâ â . The power spectral density function of the
von KÃ¡rmÃ¡n type random field follows a power law (fractal
randomness) and can accurately represent the distribution of
Earthâ€™s heterogeneity [45]. The VP/VS is simplified to an
average background of 1.732 perturbed by a smooth Gaussian
random field with the following parameters: correlation length
ğœ†=32 grids, standard deviation ğœ=2% background. This work,
as our very first experiment to evaluate the feasibility of solving
2D elastic wave equations, wants to focus on the parameters
that the wavefield is most sensitive to. Therefore, we use the
empirical relation between density and VS to compute the
density [46]. Other input parameters such as density and
attenuation may be explored in future work. A total of 20,000
random sets of models are generated and each of them is input
to a GPU based 2D finite difference code in Cartesian
coordinates to simulate the 2-D displacement field [47]. For
simulation, the top boundary is set with a free-surface boundary
condition and the other three edges have absorbing boundary
conditions. A total of 4-sec wavefield with a time step of 0.01
sec and a major frequency content up to 6 Hz is simulated. Each
simulation takes about 1.23 sec with a GPU memory usage of
0.3 GB.

C. U-NO model training
We developed a framework that applies U-NO to the 2D
elastic wave equation. The architecture is depicted
schematically in Figure 1. U-NO takes the source location and
VP and VS as inputs, where VP is calculated from VS and VP/VS.
VP and VS are then passed through a point-wise lifting operator.
A sequence of non-linear integral operators (encoders) are
applied that gradually contract the physical domain size after
each inverse Fourier transform step, while simultaneously
increasing the number of channels in the co-domain. These
operators are followed by a sequence of non-linear integral
operators (decoders) that progressively expand the physical
domain, and decrease the number of channels. Finally, a pointwise projection operator leads to the output function [43]. The
output of the U-NO model is the complete horizontal and
vertical displacement wavefield function over the medium
domain, which can be queried at any mesh points desired,
regardless of the input and output training mesh used.
We describe the detailed parameters used in U-NO below
following the notations in Figure 1. The goal is to learn an
operator mapping from the input function ğ‘ to the output
function ğ‘¢. The training is on an input mesh of ğ‘‹01 Ã— ğ‘Œ01 Ã— ğ‘‡01
and an output mesh of ğ‘‹23' Ã— ğ‘Œ23' Ã— ğ‘‡23' Ã— ğ¶23' , where ğ‘‹01 =
ğ‘Œ01 = ğ‘‹23' = ğ‘Œ23' = 64, ğ‘‡01 = 3 representing source, VS, and
VP/VS distribution on the mesh, ğ‘‡23' = 128 for 32 Hz data
output, and ğ¶23' = 2 representing two displacement
components (horizontal and vertical). This work applies the UNO architecture designed for mapping between 3-D spatiotemporal function domains (ğ‘¥, ğ‘¦, ğ‘¡) without any recurrent
composition in time [43]. The fourth dimension ğ¶23' of the
output function ğ‘¢ can be created in the last step through the
projection operator ğ‘„. Constructing the operator to learn the
mapping between 3-D spatio-temporal function domains:
ğº: {ğ‘: [0,1]- Ã— [0, ğ‘‡01 ] â†’ â„4" }
â†’ {ğ‘¢: [0,1]- Ã— [0, ğ‘‡23' ] â†’ â„4# }.
(4)
The operators {ğº) }&)*" as shown in Figure 1 that are used to
construct the U-NO are defined as:
ğº) : {ğ‘£) : [0, ğ›¼) ]- Ã— ğ’¯) â†’ â„4$% }
&
â†’ Yğ‘£)$% : [0, ğ‘)5 ğ›¼) ]- Ã— ğ‘)6 ğ’¯) â†’ â„7% 4$% [.
(5)
where [0, ğ›¼) ]- Ã— ğ’¯) is the domain of the input function ğ‘£) to the
operator ğº) , and ğ‘)5 , ğ‘)6 , and ğ‘)7 are the expansion or contraction
factors for the spatial domain, temporal domain, and co-domain
for ğ‘– '( operator, respectively. Note that ğ’¯" = [0, ğ‘‡01 ] , ğ›¼" =
ğ›¼&$% = 1, and ğ’¯&$% = [0, ğ‘‡23' ]. In this work we set the number
of layers to ğ¿=8. The details of the expansion and contraction
factors ğ‘)5 , ğ‘)6 , and ğ‘)7 are in Figure 1. The lifting operator ğ‘ƒ to
convert the input to a higher dimension channel space is a fullyconnected neural network with channel number ğ‘‘" = 16. The
projection operator ğ‘„ to the output domain is also a fully
connected neural network. The activation function used in each
FNO block is the Gaussian Error Linear Unit (GELU) [48].
With the simulation dataset and the U-NO design, we train the
U-NO model in a supervised manner with the objective of
learning the general solution operator to the wave equation for
arbitrary inputs. We divide the training dataset into 90%

training and 10% validation. The model is trained with a batch
size of 8. After hyperparameter tuning, the loss function we use
in model training is the 90% relative L1 loss plus 10% relative
L2 loss. The incorporation of L1-norm loss is more resistant to
outliers. We use an Adam optimizer [49] with a learning rate of
10,8 and a weight decay of 10,9 . We trained for 100 epochs,
which takes approximately 40 minutes per epoch using a single
NVIDIA Tesla V100 GPU with 24GB memory usage. A 70%
of loss decrease is achieved in the first 10 epochs. Once the UNO model is trained, the model parameters require GPU usage
of 3.8 GB and the time for an evaluation on a new source and
velocity model takes only 0.02 sec with GPU usage of 0.9 GB.
III. RESULTS
A. The number of simulations needed for training
Once completely trained, the U-NO model can be evaluated
on a new input with very little computational cost (0.02 sec
compared to the FDM runtime of 1.23 sec). The number of
training simulations is the main factor in the computational
cost. In the training process, we split the entire training dataset
to 90% for training and 10% for validation. We test the
performance of the model on the velocity models out of the
training data set. We can see that the U-NO model trained on a
dataset of 5000 simulations can already predict the major phase
arrivals, while increasing the dataset size from 5000 simulations
to 20000 provides better fit to the amplitudes (Figure 2). With
a training dataset of 20000 simulations, the validation and
training loss are very close, indicating there is no overfitting of
the training data.
B. Generalizability to arbitrary velocity structure or
discretization
The U-NO model is trained on random velocity models
generated with the von Karman correlation function, which can
best mimic the Earthâ€™s heterogenous velocity distribution [45],
[50]. We show by example that the U-NO model, although
trained on random velocity models with some certain
parameters, is applicable to arbitrary velocity models. These
outcomes are in fact expected from theoretical grounds because
most physical functions can be approximated to arbitrary
accuracy by random fields.
Our first example is with velocity models from a von Karmantype random distribution, but with a different covariance
function than the one used for the training data. We increase the
roughness of the velocity structure by a factor of four by
decreasing the correlation length of VS and VP/VS to only onefourth that of the training data. As shown in Figure 3, the
wavefield snapshot has more coda than with the smoother
models because of the scattering from increased heterogeneity.
However, the coda waves are well modeled by U-NO when
compared to the ground truth simulation by FDM.
The velocity models used in the training data do not have
coherent structures with discontinuities as in the real Earth, but
wavefields for such models can still be simulated with our
method. As mentioned before, this is because discontinuous
functions can be approximated to arbitrary accuracy by random

fields. Figure 4 shows a simple model with a dipping â€˜slabâ€™
embedded in a homogeneous background. The slab has 20%
higher VS and 5% lower VP/VS. The wavefield snapshots show
that the reflections from the high velocity anomaly are clearly
predicted by U-NO. A more complex example is shown in
Figure 5, where a random subpanel of the Marmousi model, a
2D velocity model with complex vertical and horizontal
structures used in exploration studies [51], is used. The
reflected and refracted waves are very complicated due to the
presence of folding and faulting, but the U-NO predictions still
closely approximate the numerical solutions (Figure 5).
One of the most important advantages of a neural operator
compared with a neural network is its mesh-free nature, since it
intrinsically learns the mapping between function spaces. A
model trained on a particular mesh can be evaluated on any
other mesh, even at finer spacing. The Fourier layers may learn
from and evaluate functions on any discretization because
parameters are directly learned in Fourier space and resolving
the functions in physical space is simply projecting on the basis
[38]. The example in Figure 6 shows the U-NO trained on a grid
of 64*64 nodes applied to an input velocity model with
160*160 nodes. Here both the input velocity model and the
output wavefield can be seen at a much higher resolution, yet
U-NO provides comparable prediction with the FDM solver.
Note that if the resolution is increased by a factor of 2, a gridbased numerical solver like FDM takes about 6 times greater
computational time; however, the evaluation using U-NO takes
only about 2.5 times longer, providing additional computational
efficiency.
We evaluate the overall generalization performance of the
trained U-NO by performing a thousand random realizations on
each of these cases. The distribution of the relative L2-norm
misfit and cross-correlation coefficient are plotted in Figure 7.
In the case of the Marmousi model, the extended tail of the
histogram is attributed to the model's imbalanced complexity.
In general, however, we see a very high cross-correlation
coefficient (>0.95) between U-NO prediction and ground truth,
confirming its robust generalizability.
C. Application to full-waveform inversion
One of the most important applications of wavefield
simulations is in full-waveform inversion (FWI), which uses the
full recorded waveform to image the Earthâ€™s interior. The
adjoint-state method is the traditional approach for computing
the gradients of an objective function with respect to parameters
of interest [4], [5]. Neural operators are differentiable by
design, which enables gradient computation with reverse-mode
automatic differentiation. It has been shown that automatic
differentiation and the adjoint approach are mathematically
equivalent [28]. Hence, the trained U-NO model allows for
convenient FWI and the associated speed and accuracy should
depend only on the forward modeling part.
We demonstrate the inversion performance using the velocity
structure of random subpanels in the Marmousi model [51]. The
synthetic waveform data are simulated with FDM [47] using 14
sources distributed in a ring shape. In Figure 8, we use the true
source location, receivers on all 64*64 grids and noise-free

waveform data; the goal here is not to demonstrate resolution,
but rather the computational accuracy of the method. We then
invert for VP and VS simultaneously by starting with
homogeneous initial VP and VS models and forward
propagating the wavefield with the U-NO for each source. The
misfit is defined by the mean square error between the forward
modeled and true wavefield. The gradient of the misfit with
respect to VP and VS can be computed through automatic
differentiation. VP and VS are then iteratively updated with
gradient descent for 100 iterations using the Adam optimizer
[49] with a learning rate of 0.01. Each iteration takes only about
1.4 sec by taking advantage of U-NO forward computation. The
results in Figure 8 show a relative L2-norm misfit between the
true and inverted model of only 3%. This successful inversion,
in turn, further validates the accuracy of forward modeling with
U-NO.
Besides the fact that the inversion target velocity model is
quite different from the smooth random fields in the training
dataset, this experiment itself is difficult due to conventional
problems in full-waveform inversion, such as cycle skipping
(multiple local maxima in the least-squares misfit function). We
also show that if we only use 64 receivers on the surface, the
inversion results in the region with ray path coverage are still
reasonably accurate (Figure 9). Inversion with a biased
homogenous initial model is also capable of producing
relatively accurate results (see Supplementary Materials).
IV. DISCUSSION AND CONCLUSION
We use the relative L2 loss between the FDM and U-NO
predictions to evaluate the performance of the trained model for
generalization. The relative L2 loss is defined as the L2-norm
of the difference between the prediction and ground truth
divided by the L2-norm of the ground truth. This ratio is used
to evaluate the performance of the trained model. When using
the same mesh discretization as the training data, the relative L2
loss is around 10â€“20 percent, but this number rises to 30â€“40
percent when the tests are performed on finer grids (Figure 7).
These values are misleading, however, because the relative L2
loss imposes equal weights to the entire sparse matrix of
waveforms that is dominated by small amplitudes close to 0.
Alternatively, the cross-correlation coefficient is a quantity that
is more sensitive to the seismic phases with amplitudes larger
than background noise. A cross-correlation coefficient larger
than 0.95 suggests the coherence of the U-NO prediction is
excellent, even for the scenarios with large relative L2 loss
(Figure 7). In addition, the FWI results confirm that the large
L2 loss is not so important since even challenging models can
still be properly recovered (Figure 8).
Besides the more than an order of magnitude higher speed,
the most important advantage of the neural operator-based full
waveform modeling is its generalizability to arbitrary velocity
models or discretization. This is because the neural operator
learns a general solution operator to the wave equation instead
of a specific instance of input velocity models. Once the neural
operator is trained, it can be used by the entire seismology
community for any region of a similar size without the need for
retraining. Since the full waveform modeling with a neural

operator has easily accessible gradients for convenient FWI, we
anticipate that this approach will eventually make FWI as
affordable as travel time tomography.
One of the main limitations of the method is the domain
extent. For a trained neural operator, the function is defined on
a fixed domain extent (e.g. it could be a unit cube). We can
evaluate at a different grid size but cannot change the extent.
We are now working on an extension of the work, where we
recursively predict the wavefield. Through this way, a trained
neural operator is essentially taking the first few time steps as
input and then output the next few time steps, and there will be
no need for retraining.
The scalability of evaluation using a trained neural operator
with respect to the grid size and the number of time steps is a
little different from conventional FDM. Assuming the original
dimension is (ğ‘ğ‘¥% , ğ‘ğ‘¦% , ğ‘ğ‘¡% ), where ğ‘ğ‘¥% , ğ‘ğ‘¦% are the number
of grids in the x and y domain, respectively, and ğ‘ğ‘¡% is the
number of time steps. If the new dimension is (ğ‘ğ‘¥- , ğ‘ğ‘¦- , ğ‘ğ‘¡- ),
+. âˆ™+/ âˆ™+6
the memory becomes +.'âˆ™+/'âˆ™+6' times the original memory,
!

!

!

which is consistent with the FDM. In the example presented in
this paper, evaluation using U-NO takes 3 times the GPU
memory of the FDM approach, and this scaling should be
consistent with increasing grid points. In terms of
computational cost, the majority of it for UNO is on the Fourier
transform and its inverse. The computational cost of fast
Fourier transform with dimension (ğ‘ğ‘¥% , ğ‘ğ‘¦% , ğ‘ğ‘¡% ) is
proportional to ğ‘ğ‘¥% âˆ™ ğ‘ğ‘¦% âˆ™ ğ‘ğ‘¡% âˆ™ log(ğ‘ğ‘¥% âˆ™ ğ‘ğ‘¦% âˆ™ ğ‘ğ‘¡% ) , and
therefore, the new computational time becomes
+.' âˆ™+/' âˆ™+6' âˆ™;2<(+.' âˆ™+/' âˆ™+6' )
times the original computational
+. âˆ™+/ âˆ™+6 âˆ™;2<(+. âˆ™+/ âˆ™+6 )
!

!

!

!

!

!

time. This scaling is slightly higher than that of FDM, however,
considering the 60 times acceleration in the example presented
in this paper, UNO evaluation on an increased dimension of
1024*1024*1024 should still have ~40 times the acceleration.
The most compute- and memory-intensive part of the UNO
method is the one-time training process. The cost of training for
the 2D case is tractable on a single GPU. For the extension from
2D to 3D modeling, the computation and memory will increase
due to the larger dataset and the larger number of parameters to
learn. Therefore, the next step is to enhance data compression
and parallelization to accelerate the training process and reduce
the storage. Since this is a learning-based approach, the model
performance can be improved by fine-tuning the model
parameters and increasing the size of the training dataset. More
importantly, any future advancements made in neural operator
model architectures will be able to be directly incorporated into
the system as they occur. For example, the improvement from
linear layers of FNO to U-NO enables faster training
convergence. As a result, we should only take current
performance metrics as a starting point.
REFERENCES
[1]

R. W. Graves and A. Pitarka, â€œBroadband groundmotion simulation using a hybrid approach,â€ Bulletin of

the Seismological Society of America, vol. 100, no. 5 A,
pp. 2095â€“2123, 2010, doi: 10.1785/0120100057.
[2] A. J. Rodgers, A. Pitarka, N. Anders Petersson, B.
Sjogreen, D. B. McCallen, and N. Abrahamson,
â€œBroadband (0-5 Hz) fully deterministic 3D groundmotion simulations of a magnitude 7.0 Hayward fault
earthquake: Comparison with empirical ground-motion
models and 3D path and site effects from source
normalized intensities,â€ Seismological Research Letters,
vol. 90, no. 3, pp. 1268â€“1284, 2019, doi:
10.1785/0220180261.
[3] R. Graves and A. Pitarka, â€œKinematic ground-motion
simulations on rough faults including effects of 3D
stochastic velocity perturbations,â€ Bulletin of the
Seismological Society of America, vol. 106, no. 5, pp.
2136â€“2153, Oct. 2016, doi: 10.1785/0120160088.
[4] A. Fichtner, B. L. N. Kennett, H. Igel, and H. P. Bunge,
â€œFull seismic waveform tomography for upper-mantle
structure in the Australasian region using adjoint
methods,â€ Geophysical Journal International, vol. 179,
no. 3, pp. 1703â€“1725, Dec. 2009, doi: 10.1111/J.1365246X.2009.04368.X.
[5] C. Tape, Q. Liu, A. Maggi, and J. Tromp, â€œAdjoint
tomography of the southern california crust,â€ Science,
vol. 325, no. 5943, pp. 988â€“992, 2009, doi:
10.1126/SCIENCE.1175298.
[6] L. Gebraad, C. Boehm, and A. Fichtner, â€œBayesian
Elastic Full-Waveform Inversion Using Hamiltonian
Monte Carlo,â€ Journal of Geophysical Research: Solid
Earth, vol. 125, no. 3, Mar. 2020, doi:
10.1029/2019JB018428.
[7] K. B. Olsen, â€œSite Amplification in the Los Angeles
Basin from Three-Dimensional Modeling of Ground
Motion,â€ Bulletin of the Seismological Society of
America, vol. 90, no. 6B, pp. S77â€“S94, Dec. 2000, doi:
10.1785/0120000506.
[8] K. R. Kelly, R. W. Ward, S. Treitel, and R. M. Alford,
â€œSynthetic seismograms: a finite â€difference approach,â€
GEOPHYSICS, vol. 41, no. 1, pp. 2â€“27, Feb. 1976, doi:
10.1190/1.1440605.
[9] H. Igel, T. Nissen-Meyer, and G. Jahnke, â€œWave
propagation in 3D spherical sections: effects of
subduction zones,â€ Physics of the Earth and Planetary
Interiors, vol. 132, no. 1â€“3, pp. 219â€“234, Sep. 2002,
doi: 10.1016/S0031-9201(02)00053-5.
[10] A. Fichtner, H. Igel, H. P. Bunge, and B. L. N. Kennett,
â€œSimulation and inversion of seismic wave propagation
on continental scales based on a spectral-element
method,â€ Journal of Numerical Analysis, Industrial and
Applied Mathematics, vol. 4, no. 1â€“2, pp. 11â€“22, 2009.
[11] Q. Liu and Y. J. Gu, â€œSeismic imaging: From classical
to adjoint tomography,â€ Tectonophysics, vol. 566â€“567,
pp. 31â€“66, Sep. 2012, doi:
10.1016/J.TECTO.2012.07.006.
[12] D. Komatitsch and J. Tromp, â€œSpectral-element
simulations of global seismic wave propagation-I.
Validation,â€ Geophysical Journal International, vol.
149, no. 2, pp. 390â€“412, May 2002, doi:
10.1046/j.1365-246X.2002.01653.x.

[13] D. Komatitsch and J. Tromp, â€œSpectral-element
simulations of global seismic wave propagation-II.
Three-dimensional models, oceans, rotation and selfgravitation,â€ Geophysical Journal International, vol.
150, no. 1, pp. 303â€“318, Jul. 2002, doi: 10.1046/j.1365246X.2002.01716.x.
[14] O. Pell, J. Bower, R. Dimond, O. Mencer, and M. J.
Flynn, â€œFinite-Difference Wave Propagation Modeling
on Special-Purpose Dataflow Machines,â€ IEEE
Transactions on Parallel and Distributed Systems, vol.
24, no. 5, pp. 906â€“915, May 2013, doi:
10.1109/TPDS.2012.198.
[15] W. Zhu, S. M. Mousavi, and G. C. Beroza, â€œSeismic
Signal Denoising and Decomposition Using Deep
Neural Networks,â€ IEEE Transactions on Geoscience
and Remote Sensing, vol. 57, no. 11, pp. 9476â€“9488,
Nov. 2019, doi: 10.1109/TGRS.2019.2926772.
[16] L. Yang, X. Liu, W. Zhu, L. Zhao, and G. C. Beroza,
â€œToward improved urban earthquake monitoring
through deep-learning-based noise suppression,â€
Science Advances, vol. 8, no. 15, p. 3564, Apr. 2022,
doi:
10.1126/SCIADV.ABL3564/SUPPL_FILE/SCIADV.A
BL3564_MOVIES_S1_AND_S2.ZIP.
[17] C. Birnie, M. Ravasi, S. Liu, and T. Alkhalifah, â€œThe
potential of self-supervised networks for random noise
suppression in seismic data,â€ Artificial Intelligence in
Geosciences, vol. 2, pp. 47â€“59, Dec. 2021, doi:
10.1016/j.aiig.2021.11.001.
[18] W. Zhu and G. C. Beroza, â€œPhaseNet: A deep-neuralnetwork-based seismic arrival-time picking method,â€
Geophysical Journal International, vol. 216, no. 1, pp.
261â€“273, 2019, doi: 10.1093/gji/ggy423.
[19] S. M. Mousavi, W. L. Ellsworth, W. Zhu, L. Y. Chuang,
and G. C. Beroza, â€œEarthquake transformerâ€”an
attentive deep-learning model for simultaneous
earthquake detection and phase picking,â€ Nature
Communications, vol. 11, no. 1, pp. 1â€“12, 2020, doi:
10.1038/s41467-020-17591-w.
[20] Z. E. Ross, M. A. Meier, and E. Hauksson, â€œP Wave
Arrival Picking and First-Motion Polarity
Determination With Deep Learning,â€ Journal of
Geophysical Research: Solid Earth, vol. 123, no. 6, pp.
5120â€“5129, 2018, doi: 10.1029/2017JB015251.
[21] Z. E. Ross, Y. Yue, M. A. Meier, E. Hauksson, and T.
H. Heaton, â€œPhaseLink: A Deep Learning Approach to
Seismic Phase Association,â€ Journal of Geophysical
Research: Solid Earth, vol. 124, no. 1, pp. 856â€“869,
2019, doi: 10.1029/2018JB016674.
[22] W. Zhu, K. S. Tai, S. M. Mousavi, P. Bailis, and G. C.
Beroza, â€œAn End-To-End Earthquake Detection Method
for Joint Phase Picking and Association Using Deep
Learning,â€ Journal of Geophysical Research: Solid
Earth, vol. 127, no. 3, pp. 1â€“13, 2022, doi:
10.1029/2021JB023283.
[23] A. Siahkoohi, M. Louboutin, and F. J. Herrmann,
â€œNeural network augmented wave-equation simulation,â€
Sep. 2019, doi: 10.48550/arxiv.1910.00925.
[24] B. Moseley, T. Nissen-Meyer, and A. Markham, â€œDeep
learning for fast simulation of seismic waves in complex

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

media,â€ Solid Earth, vol. 11, no. 4, pp. 1527â€“1549,
Aug. 2020, doi: 10.5194/SE-11-1527-2020.
B. Moseley, A. Markham, and T. Nissen-Meyer, â€œFast
approximate simulation of seismic waves with deep
learning,â€ Jul. 2018, [Online]. Available:
http://arxiv.org/abs/1807.06873
B. Moseley, A. Markham, and T. Nissen-Meyer, â€œFinite
Basis Physics-Informed Neural Networks (FBPINNs): a
scalable domain decomposition approach for solving
differential equations,â€ Jul. 2021, [Online]. Available:
http://arxiv.org/abs/2107.07871
J. D. Smith, K. Azizzadenesheli, and Z. E. Ross,
â€œEikoNet: Solving the Eikonal Equation with Deep
Neural Networks,â€ IEEE Transactions on Geoscience
and Remote Sensing, vol. 59, no. 12, pp. 10685â€“10696,
Dec. 2021, doi: 10.1109/TGRS.2020.3039165.
W. Zhu, K. Xu, E. Darve, and G. C. Beroza, â€œA general
approach to seismic inversion with automatic
differentiation,â€ Computers & Geosciences, vol. 151, p.
104751, Jun. 2021, doi:
10.1016/J.CAGEO.2021.104751.
B. Sun and T. Alkhalifah, â€œML-descent: An
optimization algorithm for full-waveform inversion
using machine learning,â€ GEOPHYSICS, vol. 85, no. 6,
pp. R477â€“R492, Nov. 2020, doi: 10.1190/geo20190641.1.
X. Zhang and A. Curtis, â€œBayesian Geophysical
Inversion Using Invertible Neural Networks,â€ Journal
of Geophysical Research: Solid Earth, vol. 126, no. 7,
p. e2021JB022320, Jul. 2021, doi:
10.1029/2021JB022320.
M. Rashtâ€Behesht, C. Huber, K. Shukla, and G. E.
Karniadakis, â€œPhysicsâ€Informed Neural Networks
(PINNs) for Wave Propagation and Full Waveform
Inversions,â€ Journal of Geophysical Research: Solid
Earth, vol. 127, no. 5, pp. 1â€“21, 2022, doi:
10.1029/2021jb023120.
V. Kazei, O. Ovcharenko, P. Plotnitskii, D. Peter, X.
Zhang, and T. Alkhalifah, â€œMapping full seismic
waveforms to vertical velocity profiles by deep
learning,â€ GEOPHYSICS, vol. 86, no. 5, pp. R711â€“
R721, Sep. 2021, doi: 10.1190/geo2019-0473.1.
L. Lu, P. Jin, and G. E. Karniadakis, â€œDeepONet:
Learning nonlinear operators for identifying differential
equations based on the universal approximation theorem
of operators,â€ Nature Machine Intelligence, vol. 3, no.
3, pp. 218â€“229, Oct. 2019, doi: 10.1038/s42256-02100302-5.
K. Bhattacharya, B. Hosseini, N. B. Kovachki, and A.
M. Stuart, â€œModel Reduction and Neural Networks for
Parametric PDEs,â€ SMAI Journal of Computational
Mathematics, vol. 7, pp. 121â€“157, May 2020, doi:
10.48550/arxiv.2005.03180.
Z. Li et al., â€œMultipole Graph Neural Operator for
Parametric Partial Differential Equations,â€ Advances in
Neural Information Processing Systems, vol. 2020December, Jun. 2020, doi: 10.48550/arxiv.2006.09535.
Z. Li et al., â€œNeural Operator: Graph Kernel Network
for Partial Differential Equations,â€ Mar. 2020, doi:
10.48550/arxiv.2003.03485.

[37] N. Kovachki et al., â€œNeural Operator: Learning Maps
Between Function Spaces,â€ arXiv, arXiv:2108.08481,
Dec. 2021. Accessed: Aug. 19, 2022. [Online].
Available: http://arxiv.org/abs/2108.08481
[38] Z. Li et al., â€œFourier Neural Operator for Parametric
Partial Differential Equations,â€ Oct. 2020, doi:
10.48550/arxiv.2010.08895.
[39] J. Pathak et al., â€œFourCastNet: A Global Data-driven
High-resolution Weather Model using Adaptive Fourier
Neural Operators,â€ Feb. 2022, doi:
10.48550/arxiv.2202.11214.
[40] G. Wen, Z. Li, K. Azizzadenesheli, A. Anandkumar,
and S. M. Benson, â€œU-FNOâ€”An enhanced Fourier
neural operator-based deep-learning model for
multiphase flow,â€ Advances in Water Resources, vol.
163, p. 104180, May 2022, doi:
10.1016/J.ADVWATRES.2022.104180.
[41] P. Jiang et al., â€œDigital Twin Earth â€“ Coasts:
Developing a fast and physics-informed surrogate
model for coastal floods via neural operators,â€ Oct.
2021, doi: 10.48550/arxiv.2110.07100.
[42] Y. Yang, A. F. Gao, J. C. Castellanos, Z. E. Ross, K.
Azizzadenesheli, and R. W. Clayton, â€œSeismic Wave
Propagation and Inversion with Neural Operators,â€ The
Seismic Record, vol. 1, no. 3, pp. 126â€“134, Oct. 2021,
doi: 10.1785/0320210026.
[43] M. A. Rahman, Z. E. Ross, and K. Azizzadenesheli, â€œUNO: U-shaped Neural Operators,â€ Apr. 2022, doi:
10.48550/arxiv.2204.11127.
[44] O. Ronneberger, P. Fischer, and T. Brox, â€œU-net:
Convolutional networks for biomedical image
segmentation,â€ Lecture Notes in Computer Science
(including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics), vol.
9351, pp. 234â€“241, 2015, doi: 10.1007/978-3-31924574-4_28/COVER/.
[45] N. Nakata and G. C. Beroza, â€œStochastic
characterization of mesoscale seismic velocity
heterogeneity in Long Beach, California,â€ Geophysical
Journal International, vol. 203, no. 3, pp. 2049â€“2054,
2015, doi: 10.1093/gji/ggv421.
[46] T. M. Brocher, â€œEmpirical Relations between Elastic
Wavespeeds and Density in the Earthâ€™s Crust,â€ Bulletin
of the Seismological Society of America, vol. 95, no. 6,
pp. 2081â€“2092, Dec. 2005, doi: 10.1785/0120050077.
[47] D. Li, D. Helmberger, R. W. Clayton, and D. Sun,
â€œGlobal synthetic seismograms using a 2-D finitedifference method,â€ Geophysical Journal International,
vol. 197, no. 2, pp. 1166â€“1183, 2014, doi:
10.1093/gji/ggu050.
[48] D. Hendrycks and K. Gimpel, â€œGaussian Error Linear
Units (GELUs),â€ Jun. 2016, doi:
10.48550/arxiv.1606.08415.
[49] D. P. Kingma and J. L. Ba, â€œAdam: A Method for
Stochastic Optimization,â€ 3rd International Conference
on Learning Representations, ICLR 2015 - Conference
Track Proceedings, Dec. 2014, doi:
10.48550/arxiv.1412.6980.
[50] P. M. Mai and G. C. Beroza, â€œA Spatial Random Field
Model to Characterize Complexity in Earthquake Slip,â€

Journal of Geophysical Research: Solid Earth, vol. 107,
no. B11, p. ESE 10-1-ESE 10-21, 2002, doi:
10.1029/2001JB000588.
[51] R. Versteeg, â€œThe Marmousi experience: Velocity
model determination on a synthetic complex data set,â€
https://doi.org/10.1190/1.1437051, vol. 13, no. 9, pp.
927â€“936, Feb. 2012, doi: 10.1190/1.1437051.

Yan Yang is a Ph.D. student with the
Seismological Laboratory, California
Institute of Technology, Pasadena,
CA, USA. Her research interests
focus on seismic imaging and
monitoring of subsurface.

Angela Gao is a Ph.D. student with
the Computing and Mathematical
Sciences
Department
of
the
California Institute of Technology,
Pasadena, CA, USA. She is interested
in inverse problems, computational
photography, generative models, and
deep learning, with applications in
scientific imaging problems.
Kamyar Azizzadenesheli is a Senior Research Scientist at
Nvidia. Prior to his role at Nvidia, he
was an assistant professor at Purdue
University, department of computer
science. Prior to his faculty position,
he was at the California Institute of
Technology as a Postdoctoral
Scholar in the Department of
Computing
and
Mathematical
Sciences.
Robert W. Clayton is a Professor of
geophysics at California Institute of Technology, Pasadena,
CA, USA, where he works in the
areas of seismic wave propagation,
earth structure, and tectonics. He
has applied imaging methods to the
Los Angeles region and to
subduction zones around the world.
Zachary E. Ross is an Assistant
Professor of geophysics with the
California Institute of Technology,
Pasadena, CA, USA, where he uses
machine learning and signal
processing techniques to better
understand earthquakes and fault
zones. He is interested in the
dynamics of seismicity, earthquake
source properties, and fault zone
imaging.

Figures

Figure 1. Overview of our method for solving the elastic wave equation with neural operators. The inputs, a, to
the U-NO model are the P- and S-wave velocity (VP, VS) model and the source location (indicated by the white star).
VP is calculated from VS and VP/VS, examples of which are shown in the upper left panel. The outputs, u, are the
horizontal and vertical displacements at each time step, examples of which are shown in the upper right panel. In the
middle panel showing the U-NO architecture, orange circles P and Q denote point-wise operators, rectangles G denote
general operators, and smaller blue circles denote concatenations in function space. The lower panel shows the
architecture of each FNO layer, where v is the input of the layer, F and F-1 are Fourier transform and its inverse,
respectively, R and W are a linear transform, and ğœ is the nonlinear activation function. The expansion (or contraction)
8
%
!
8
%
8
5
5
5
5
6
6
factors in equation (5) are set as: ğ‘%,= ! , ğ‘8,!
= - , ğ‘9,@
= 2, ğ‘A,B
= 8 , ğ‘%6 = ! , ğ‘-6 = 8 , ğ‘8,!
= - , ğ‘9,@
= 2, ğ‘A6 = - , ğ‘B6 =
!
8

%

7
7
, ğ‘%,-,8,!
= 2, ğ‘9,@,A,B
= -.

Figure 2 Model performance as a function of the number of training samples. Left panel: relative L2 loss curves
for the training and validation data. Right panel: Example of simulated waveform comparison between FDM (black
solid) and U-NO (red dashed). N means the number of simulations in the training dataset, including 90% for training
and 10% for validation.

1

Figure 3 Model generalization experiments 1: random fields of Vs and Vp/Vs model with 4-times roughness of the
training data. The top row shows the VS and VP/VS. From the second row to the fifth row, the wavefield snapshots at
0.5 s-2.0 s are shown. From left to right, the first three columns show the horizontal displacement of the FD simulation,
U-NO prediction, and their misfit in the same color scale. The latter three columns show the vertical component. The
horizontal and vertical displacement waveforms at each grid are cross-correlated between FD and U-NO, with the
maximum cross-correlation value and its associated time shift shown in the bottom row. For this case, the relative L2
loss of the U-NO simulation is 0.182.

2

Figure 4 Model generalization experiments 2: Similar as Figure 3, but for a homogeneous background model
embedded with a â€˜slabâ€™ with 20% higher Vs and 5% lower Vp/Vs. Relative L2 loss of the U-NO simulation is 0.090.

3

Figure 5 Model generalization experiments 3: Similar as Figure 3, but for a random subpanel from the Marmousi
model. The velocity perturbation range is normalized to 30% of the average velocity. Relative L2 loss of the U-NO
simulation is 0.225.

4

Figure 6 Model generalization experiments 4: Similar as Figure 3, but Vp and Vs model mesh discretization is
increased from 64*64 to 160*160. Relative L2 loss of the U-NO simulation is 0.385.

5

Figure 7 Output evaluation. Distribution of Relative L2 loss (top) and correlation coefficient (bottom) between the
U-NO predictions and ground truth. From left to right, the columns are corresponding to the experiments in Figure 3,
5, 6. The red and black dashed lines mark the mean and standard deviation of the histograms.

Figure 8 Full waveform inversion. The inversion for a random subpanel from the Marmousi model. 14 sources are
placed in a ring shape (black stars) and receivers are placed at every node of the 64*64 grid. From left to right, the
columns represent true velocity model, initial model for inversion, inverted model without regularization, and inverted
model with 0th and 1st order Tikhonov regularization. The top and bottom rows are the models for Vs and Vp,
respectively.

6

Figure 9 Full waveform inversion. Same as Figure 8, but the receivers are only placed on the 64 grids on the surface
(blue line on the top of each subpanel). The gray shaded areas mask the areas without ray path coverage.

7

