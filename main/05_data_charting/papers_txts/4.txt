Journal of Geophysics and Engineering
Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

https://doi.org/10.1093/jge/gxac016

High-frequency wavefield extrapolation using the
Fourier neural operator
and Yanghua Wang

Centre for Reservoir Geophysics, Resource Geophysics Academy, Imperial College London, South
Kensingtion, London SW7 2BP, UK
Corresponding author: Yanghua Wang. E-mail: yanghua.wang@imperial.ac.uk
Received 15 October 2021, revised 11 January 2022
Accepted for publication 12 March 2022

Abstract
In seismic wave simulation, solving the wave equation in the frequency domain requires
calculating the inverse of the impedance matrix. The total cost strictly depends on the number of
frequency components that are considered, if using a finite-difference method. For the
applications such as seismic imaging and inversion, high-frequency information is always
required and thus the wave simulation is always a challenging task as it demands tremendous
computational cost for obtaining dispersion-free high-frequency wavefields for large subsurface
models. This paper demonstrates that a data-driven machine learning method, called the Fourier
neural operator (FNO), is capable of predicting high-frequency wavefields, based on a limited
number of low-frequency components. As the FNO method is for the first time applied to
seismic wavefield extrapolation, the experiment reveals three attractive features with FNO: high
efficiency, high accuracy and, importantly, the predicted high-frequency wavefields are dispersion
free.
Keywords: dispersion free, Fourier neural operator, high-frequency wavefield, machine
learning, wavefield extrapolation

1. Introduction
The finite-difference method is often used in seismic wave
simulation because of its straightforward concept and easy
implementation (Dablain 1986; Virieux et al. 2011). In the
frequency domain, the wave equation can be written in a
compact form of a Helmholtz equation:
L(x, f )u(x, xs , f ) = s(xs , f ),

(1)

where x is the spatial coordinate, xs denotes the source coordinates, f is the frequency, s(xs , f ) is the source function,
u(x, xs , f ) is the frequency-domain wavefield and L(x, f ) is
the impedance operator. After the finite-differencing discretisation, the impedance matrix L(x, f ) is a sparse matrix. Solving equation (1) requires computation of the matrix inverse
of L(x, f ), and an lower-upper (LU) decomposition method

which is a LU factorisation is often used factorising the matrix and to derive the solution recursively (Wang 2011).
The total time cost and total required memory are both
linearly proportional to the number of frequencies that are
considered.
In seismic full-waveform inversion and reverse-time migration, the same wave simulation engine is used many times,
and thus the time cost and memory requirement become
a significant issue in seismic application (Virieux & Operto 2009; Wang & Rao 2009; Wang 2016). Moreover, for
high-frequency wavefield solutions, a finer spatial sampling is
required to mitigate the numerical dispersion in the finitedifference method. Such a fine sampling might exceed the capabilities of the computational resources (Wu & Alkhalifah
2018). To address the numerical dispersion issue, many
methods have been proposed, such as high-order finite

¬© The Author(s) 2022. Published by Oxford University Press on behalf of the Sinopec Geophysical Research Institute. This is an Open Access article distributed under the terms of
the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium,
provided the original work is properly cited.

269

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Chao Song

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

trained for every different velocity model and frequency.
Frequency-domain seismic imaging and inversion methods
need multiple frequencies to provide sufficient wavenumber
components to refine the resolution of the model. However, it is difficult for PINN to generate high-frequency
components due to the spectra-bias issue, which indicates
the learning priority of low-frequency components over
high-frequency ones (Rahaman et al. 2019).
Regarding the data-driven perspective, machine learning
extracts key features from data and simulates the dynamic
evolution of underlying physics disciplines. For example, the
CNN framework is used to calculate fluid flow solutions corresponding to inviscid Euler equations efficiently (Tompson
et al. 2017), and to resolve Reynolds-averaged Navier‚ÄìStokes
(RANS) equations (Thuerey et al. 2020). A CNN-based autoencoder is used as a surrogate modelling engine for fast
seismic data simulation and several dynamical PDE systems
(Geneva & Zabaras 2020; Moseley et al. 2020). A neural operator, which is a different concept from CNN, is effective in
learning the mapping between function spaces with limited
training samples (Li et al. 2020a) and can be implemented in
the Fourier space domain to accelerate the training process
(Li et al. 2020b).
In this paper, we propose to use a data-driven machine
learning method to achieve fast high-frequency wavefield extrapolation free of numerical dispersion errors. The wavefields can be treated as solutions from PDEs parameterised by
the frequency and can be solved by a machine learning-based
PDE solver. This PDE solver is the Fourier neural operator
(FNO) (Li et al. 2020b), which will we use in this paper.
The rest of this paper will be organised as follows: first, we
introduce how to implement FNO for the wavefield extrapolation. Next, we explain the used data for training, validating
and testing. Then, we compare the high-frequency wavefield
result from FNO and that from the conventional finitedifference method. Finally, we draw the conclusion that
FNO is effective and efficient in generating high-frequency
wavefields.
2. FNO for wavefield extrapolation
Frequency-domain seismic inversion and imaging processes
need wavefields of multiple frequencies to provide sufficient
wavenumber components to reconstruct the model. We attempt to exploit a machine learning method from the datadriven perspective using the FNO. When we experiment the
FNO usage in wavefield extrapolation, the implementation
consists of four steps.
(i) We generate synthetic wavefields within a wide frequency range using the finite-difference method and
split these wavefields into small wavefield pieces.
(ii) We divide the resulting wavefield pieces into a lowfrequency wavefield group and a high-frequency group,
270

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

difference, compact finite difference and low-rank approximation (Dablain 1986; Yang et al. 2006; Fomel et al. 2013;
Wu & Alkhalifah 2014). In the current paper, we propose
to improve the efficiency of high-frequency wave simulation
using a machine learning method rather than solving the
wave equation (1) directly for all frequency components.
Machine learning methods are attractive because they can
deal with large-scale data and images, and still show excellent
performance. One of the popular machine learning methods
is the neural network-based method. For example, the deep
neural network method is effective in picking the Fresnel
zone (Sun et al. 2019), eliminating surface waves (Kaur et al.
2020a), improving the qualities of the migration images
(Kaur et al. 2020b) and enhancing the resolution of velocity models reconstructed by waveform inversion (Li et al.
2021). The convolutional neural network (CNN) method
is capable of extracting feature maps from images by using a
convolution filter and is applicable to detecting salt bodies
and faults from migration images (Li et al. 2019; Shi et al.
2019). CNN has also been used to predict low-frequency
components of the data that are needed for the waveform
inversion and for detecting microseismic events
(Ovcharenko et al. 2019).
The machine learning methods are also effective in solving
partial differential equations (PDEs). For seismic wave equations, either in the time or frequency domain, which are typical PDEs, the machine learning methods can be used from
two perspectives. One is to use the underlying physics disciplines as loss functions. For example, a framework called
the physics-informed neural network (PINN) can use automatic differentiation to calculate the partial derivatives of
target output functions with respect to spatial and temporal coordinates (Baydin et al. 2017; Raissi et al. 2019). The
other perspective is data-driven machine learning. The PDEs
of frequency-domain wave equations are parameterised by
frequency and subsurface property models. By using a large
amount of training data, data-driven machine learning methods are popularly used to solve PDEs by transforming the
question into an optimisation problem.
Regarding the physics-informed perspective, the enormous training cost for multiple PINNs corresponding to
different PDEs parameters will adversely affect their performances in real-life applications (Waheed et al. 2021). In
addition, many trial-and-error tests must be performed to
tune the hyperparameters in the networks to get reasonable
results. While Song et al. (2021) showed the flexibility and
versatility of PINNs for isotropic and anisotropic acoustic
media, and for models with irregular shapes, such as topography, in solving the scattered form of the Helmholtz equation,
they also stated the limitations of PINN-based Helmholtz
equation solvers, including the high computational cost of
training for small models and low resolution of resulting
wavefield solutions. In addition, a new network needs to be

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

By doing so, the trained network becomes a highfrequency wavefield generator that can produce highfrequency wavefields for large models. The proposed method
is purely data-driven. Even though large-scale models have
many details and complex structures, we can still divide them
into small sections and generate low and high wavefields for
training. If these complicated structures and corresponding
wavefields are included in the training, we will be able to
make accurate wavefield predictions in the test data for whole
large-scale models.
3. Data generation for training, validation and testing
We demonstrate the performance of FNO using three test
models. All three models are based on the Marmousi model
(figure 1), but each test model has a different level of complexity. Therefore, the generated wavefields will have different complexities and of course will have different implications in the FNO training and prediction. Test model I
(figure 1a) is a strongly smoothed model, smoothed by a 2D
Gaussian filter with a smoothing window of 40 grid points.
Test model II (figure 1b) is smoothed with a smoothing window of 20 grid points. Finally, test model III (figure 1c) is the
original Marmousi model.
The size of the Marmousi model is 300 √ó 1500 grids with
a 10-m spatial sampling interval in both vertical and horizontal directions. We randomly select five velocity portions
from the model. One of them is shown in the green frame
in figure 1a. The size of each selected velocity portion is
300 √ó 800 grids. We set 40 source points in each velocity
portion, and the source points are uniformly distributed on
the surface at the depth of 20 m. The total source number is
40 √ó 5 = 200. We use a nine-point finite-differencing scheme
to discretise the wave equation and generate the training
data ( Jo et al. 1996).
For these 200 source points, we generate the frequencydomain wavefields from 5 to 30 Hz with a frequency interval
of 1 Hz. These frequencies are in a practical frequency range
for waveform inversion, in which the extreme low-frequency
components are often missing from the recorded data or
contaminated by noise. For each wavefield, we divide it into
24 wavefield pieces with the size of 50 √ó 50. In total, there

4. Network training
In the training process, we extend data set A to five dimensions arranged as follows:
271

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

are 200 √ó 24 = 4800 wavefield pieces for each frequency.
Among these 4800 pieces, we use 4700 wavefield pieces as
the training data and the remaining 100 pieces as validation
data. Figure 2 displays 24 wavefield pieces arbitrarily selected
from training data, and all the wavefield pieces are plotted
along the same scale. We divide the training data into two
data sets: the wavefields ranging from 5 to 12 Hz (data set
A) and the wavefields ranging from 13 to 30 Hz (data set
B). As a result, the size of data set A is 4700 √ó 50 √ó 50 √ó 8,
and the size of data set B is 4700 √ó 50 √ó 50 √ó 18. We target
predicting data set B, which has 18 frequencies, using data
set A, which has eight frequencies. We consider this target
quite challenging as we try to predict 18 high-frequency
wavefields, based on only eight input low-frequency ones.
Our ultimate goal is to generate high-frequency wavefields
for the whole Marmousi model using test data. As the input wavefields are at low frequencies (5‚Äì12 Hz), we can generate test data using a coarse spatial grid sampling interval.
We downsample the original Marmousi model by half in
both vertical and horizontal directions and double the spatial grid sampling interval to 20 m. Thus, the survey area remains the same as 3 √ó 15 km2 . Now the size of the model
becomes 150 √ó 750 grids. We set a source located at (0.02,
7.5 km) and divide the resulting wavefield into small pieces
with the size of 50 √ó 50 as test data. As a result, there are
(150/50)√ó(750/50) = 45 sets of wavefield pieces. Considering the frequency number, the size of the test data set is
45 √ó 50 √ó 50 √ó 8.
As we know, for finite-differencing approximation to the
wave equation, the grid size depends on the maximum frequency considered to avoid numerical dispersion errors.
If the highest frequency is doubled, the grid size should
be half smaller. This would require significantly more storage and computation time. Thus, we effectively cut the
considered highest frequency by at least half in the finitedifferencing wave simulation when using a machine learning method to predict the high-frequency wavefields. This
procedure significantly reduces the complexity of numerical
computation.
We generate the training, validation and test data using a
delta function as the source signature, so trained networks
will act as Green‚Äôs function for generating high-frequency
wavefields. Because of the delta source signature, the amplitude and phase of predicted wavefields are the same for all
frequencies. Thus, in the realistic seismic surveys, we shall
recover the true wavefields by modification based on the
corresponding frequency components of the actual source
wavelet.

and train the networks to build the mapping relationship between the low-frequency and high-frequency
groups.
(iii) Once we complete the training of the networks, we
import a limited number of low-frequency wavefields
to the trained networks and predict multiple highfrequency wavefields.
(iv) Finally, we stitch the predicted wavefield pieces together to produce the final high-frequency wavefields
for the entire large velocity model.

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

(i) The first dimension corresponds to the number of
wavefield piece sets used in training; it is 4700 in this
case.
(ii) The second dimension corresponds to the index of vertical grids within the wavefield piece; it is 50 in this case.
(iii) The third dimension corresponds to the index of horizontal grids within the wavefield piece; it is also 50 in
this case.

(iv) The fourth dimension corresponds to the desired number of high-frequency wavefield frequencies; it is 18 in
this case.
(v) The fifth dimension corresponds to the number of
channels within the first block; it is equal to the sum of
the number of frequencies within data set A, the vertical index and the horizontal index, and the frequency
index; it is 11 (= 8 + 1 + 1 + 1) in this case. The three

272

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Figure 1. The Marmousi velocity model. (a) Test model I: a strongly smoothed model, using a 2D Gaussian smoothing filter with a window of 40 grid
points. (b) Test model II: a moderately smoothed model, using a 2D Gaussian smoothing filter with a window of 20 grid points. (c) Test model III: the
original Marmousi model. Three models have different complexity in wavefields that will lead to different accuracies in FNO prediction.

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

indexes (1 + 1 + 1) here are normalised between zero
and one.
For the internal output (data set B‚Äô) from FNO, the dimension is reduced to four since the channel number in the
fifth dimension is one and can be compressed and ignored.
The rest four dimensions of data set B‚Äô are the same as the
size of the training data set B, and this data set B‚Äô shall match
the data set B. After 250 epochs, the mismatch between B‚Äô
and B is sufficiently small, we complete the network training
successfully.
We adopt the Adam optimiser in the network training. The Adam optimiser is a gradient-based optimisation
method of stochastic objective functions, which is suitable
for optimisation problems with large-scale data (Kingma &
Ba 2014).
There are two key parameters in the Adam optimiser. The
first one is the learning rate, which is equivalent to a step
length in the optimisation problem. We determine the learning rate based on trial-and-error. We start the learning rate
with 0.0025 and reduce it by half after every 50 epochs. The
total epoch number is 250.
The second parameter is the size of a mini batch, which
is the number of wavefield piece sets taken in each training
step. After sequentially training the steps when all wavefield
piece sets are used, it completes an epoch in the machine
learning method. In this example, we set the size of the mini
batch to 50.
Figure 3 displays the training loss history curves for the
three test models using the unit of decibels (dB). The displayed training loss curves are normalised to dB. It is obvious that the training loss decreases faster and reaches a lower
loss value for a smoother velocity model, as in the curves for
test model I (solid black curve) and test model II (dashed
red curve). A sharper velocity model produces more scattering details in the wavefield solutions. Compare the smoothed
wavefields generated from test model I and from test model

Figure 3. The normalised training loss history curves (in dB) for the three
test models. The solid black curve corresponds to the test model I (figure 1a); the dashed red curve corresponds to the test model II (figure 1b)
and the dotted blue curve corresponds to the test model III (figure 1c).

II; the scattering wavefields generated from test model III
are more difficult to learn for the machine learning method.
However, we can still achieve the training convergence with
a slightly higher training loss, as shown by the dotted blue
curve in figure 3.
5. Performance of FNO for the validation data
After the networks are trained, we input the low-frequency
(5‚Äì12 Hz) wavefield pieces from the validation data into the
trained networks to produce the high-frequency (13‚Äì30 Hz)
wavefield ones. We arbitrarily select two wavefield piece sets
from validation data. We show one validation wavefield piece
comparison in figure 4a and another comparison in figure 4b.
From the left column to the right column, the displayed
wavefield frequencies are 15, 18, 21, 24, 27 and 30 Hz. In
figure 4a and b, the validation wavefield pieces generated
273

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Figure 2. Wavefield pieces (24) arbitrarily selected from the training data set. The entire traning data set consists of 4800 of such sample pieces.

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

5 √ó 20 = 100 m. According to the dependence of the lowest
velocity (v) and the longest wavelength (ùúÜ), the minimum
frequency of wavefields that are free of dispersion should be
15 Hz.
We compare the wavefields generated from the finitedifference method and from FNO using three frequencies,
13, 22 and 30 Hz, for the three test models used. For 13 Hz,
both finite-difference and FNO wavefields will not suffer
from dispersion errors; for 22 Hz, dispersion errors will appear in the wavefield from the finite-difference method; while
for 30 Hz, the finite-difference method will suffer from dispersion severely.
To quantitatively measure the similarity of the wavefields
from the finite-difference and the FNO methods, we calculate the correlation coefficients between them for each grid
point using the following formulation (Gao & Wang 2020):

from the finite-difference method are displayed in the top
row, and the FNO-predicted wavefield pieces are displayed in
the bottom row. Plotting the wavefield pieces from the finitedifference and the FNO methods on the same scale, we can
barely observe any difference between them for both comparisons in figure 4a and 4b, and this confirms the accuracy
of the FNO.

6. Performance of FNO for the test data
After confirming the accuracy of the FNO training using the validation data, we input the test data into the
trained FNO to get the predicted high-frequency wavefield
pieces with the size of 45 √ó 50 √ó 50 √ó 18. We stitch the
predicted wavefield pieces (13‚Äì30 Hz) to get the final highfrequency wavefields for the whole Marmousi model. To
compare the FNO-predicted wavefields with the numerical
solutions, we calculate 13‚Äì30 Hz wavefields using the finitedifference method from the downsampled Marmousi model
(150 √ó 750) with a coarse spatial grid sampling interval
(20 m). The reason for downsampling the Marmousi model
is that the memory requirement for the original Marmousi
model is unaffordable for our hardware equipment.
The minimal velocity value in the original Marmousi
model is 1500 m s-1 . To mitigate the dispersion errors for the
finite-difference method, there have to be at least five grid
points in each wavelength (Hall & Wang 2009). Considering the coarse spatial grid sampling interval of 20 m, the minimal wavelength that is immune to the dispersion error is

|
|
‚àë ‚àë l,k l,k
|
|
u
u
|
|
l
k FD FNO
|,
|
cor(i, j) = | ‚àö
|
| ‚àë ‚àë l,k 2 ‚àë ‚àë l,k 2 |
|
|
(u
)
(u
)
l
k
l
k
FD
FNO |
|

(2)

where uFD and uFNO represent the wavefields from the finitedifference and the FNO methods, respectively; i and j denote the indexes for the grid point in the vertical and horizontal directions, respectively, and l and k represent the
integral indexes within the ranges of [i ‚àí nw, i + nw] and
[j ‚àí nw, j + nw], respectively. nw is the half size of the window in which we calculate the correlation coefficients. In this
paper, we assume nw equal to 10 for all the cases. The resulting correlation coefficient values range from 0 to 1. A
274

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Figure 4. (a, b) Two comparisons of the acoustic wavefield piece. The wavefield displays six frequency components: 15, 18, 21, 24, 27, 30 Hz. There is
no obvious difference between wavefield pieces generated from the finite-difference method (top row) and from FNO (the bottom row).

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

large value close to 1 indicates high similarity between the
two wavefields, while a small value close to 0 indicates low
similarity.
In this section, we plot all the wavefields for the Marmousi
model in the same scale. First, we show the wavefields of
13 Hz from the finite-difference and the FNO methods for
the test model I (figure 1a) in figure 5a and b, respectively. We
observe that FNO can generate a reasonable wavefield prediction result (figure 5b) that is close to the finite-difference
solution (figure 5a). Figure 5c displays the correlation coefficients between the wavefields generated from the finitedifference and the FNO methods. In the whole model domain, the correlation coefficients are close to 1, which shows
high similarity between the 13 Hz wavefields from the finitedifference and the FNO methods. For the 13 Hz wavefield,
the finite-difference method should be accurate according to
our analysis, thus the high similarity in figure 5c indicates that
FNO is equally accurate to the finite-difference method.
Next, we show the 22 Hz wavefields from the finitedifference and the FNO methods in figure 6a and b, respectively. As discussed previously, the lowest frequency that is
free of dispersion error for the Marmousi model is 15 Hz,
so the accuracy of the resulting wavefield at 22 Hz from the
finite-difference method is compromised by the dispersion.

Although we do not observe obvious dispersion effects in
the wavefield from the finite-difference method in figure 6a,
these errors are revealed by the low correlation coefficients between the finite-difference wavefield (figure 6a)
and the FNO-predicted wavefield (figure 6b) shown in
figure 6c. These low correlation coefficients are located in
the shallow part of the velocity model, which is consistent with the low-velocity region that may cause dispersion
errors.
Next, figure 7a and b display the 30-Hz wavefields from
the finite-difference and the FNO methods, respectively. For
this high frequency, we observe obvious dispersion errors
(reflected in both phase shift and amplitude distortion in
figure 7a) in the wavefield solution from the finite-difference
method, as indicate by the arrows in figure 7a. By comparison, the FNO-predicted wavefield (figure 7b) is immune to
the dispersion effect, which shows FNO‚Äôs accuracy superiority over the finite-difference method. Figure 7c displays the
wavefield correlation coefficient between figure 7a and b. The
dispersion errors in figure 7a are reflected by low correlation
coefficients that are widely distributed in the low-velocity
areas in the shallow part.
To explore how FNO will perform for more complicated
wavefields, we consider a less smoothed Marmousi model
275

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Figure 5. Wavefields at 13 Hz from (a) the finite-difference method, (b) from FNO and (c) the correlation coefficients between (a) and (b) corresponding to test model I (figure 1a).

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Figure 6. The same plotting configuration as figure 5 for 22 Hz.

Figure 7. The same plotting configuration as figure 5 for 30 Hz. The arrows in (a) point to dispersion errors.
276

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

Figure 9. The same plotting configuration as figure 8 for 22 Hz.
277

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Figure 8. Wavefields at 13 Hz from (a) the finite-difference method, (b) from FNO and (c) the correlation coefficients between (a) and (b) corresponding to test model II (figure 1b).

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

Figure 11. Wavefields at 13 Hz from (a) the finite-difference method, (b) from FNO and (c) the correlation coefficients between (a) and (b) corresponding to test model III (figure 1c).
278

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Figure 10. The same plotting configuration as figure 9 for 30 Hz. The arrows in (a) point to dispersion errors, while the arrows in (b) point to discontinuities between wavefield piece boundaries.

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

method become obvious in the shallow part of the model,
as shown in figure 10a (pointed by the arrows). Figure 10b
displays the dispersion-free FNO-predicted wavefield. Although we observe some discontinuities on wavefield piece
boundaries in Figure 10b (indicated by the arrows), these
mild discontinuities will not affect the applications of the
FNO-predicted wavefield in the waveform inversion as they
can be smeared by smoothing filters focusing on the horizontal direction in the velocity gradient calculation. We show the
wavefield correlation coefficient between figure 10a and b in
figure 10c, and small correlation coefficients are distributed
in large areas that correspond to low velocity.
Finally, we directly show the applications of the finitedifference and FNO methods for the test model III. Using
the same training setup, we obtain the 13-Hz wavefields from
the finite-difference and the FNO methods and display them
in figure 11 parts a and b, respectively. It is obvious that the
wavefields get more complicated as more detailed structures
are considered in the velocity model. The FNO-predicted
wavefield (figure 11b) is very close to the finite-difference solution (figure 11a), and this is confirmed by the large correlation coefficient shown in figure 11c.
Figure 12 displays the 22-Hz wavefields from the finitedifference and the FNO methods for the test model III. Although the wavefield becomes more complex due to the increase in the model complexity, FNO is still able to generate a

(figure 1b). Repeating the training, validation, and test data
generation and the training of the network using the test
model II, we directly show the wavefield results for test data.
Figure 8 parts a and b display the 13 Hz wavefields from the
finite-difference and the FNO methods for the test model II,
respectively. FNO can generate equally accurate wavefields
with the finite-difference method for relatively low frequencies. The wavefield correlation coefficient between figure 8a
and b is shown in figure 8c, which is equal to one in most
areas despite some low correlation coefficients on the edge
of the model. With the increase of test model complexity,
the wavefield complexity also increases and the amplitude
of the wavefield becomes unbalanced in the model domain.
The wavefield in the regions with very weak amplitudes is
difficult to recover, which is indicated by the low correlation
coefficient on the edges.
Figure 9 displays the 22-Hz wavefields from the finitedifference and FNO methods for test model II. The general
wave shapes of figure 9a and b are similar, but we can capture
the dispersion errors in figure 9a by the low correlation coefficients shown in figure 9c. Comparing figure 9c with figure 7c,
we observe that low correlation coefficients are located in the
same area in the shallow part of the model caused by the low
velocity.
When we increase the frequency to 30 Hz, the dispersion
effects in the wavefield calculated by the finite-difference
279

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Figure 12. The same plotting configuration as figure 11 for 22 Hz.

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

reasonably good wavefield solution. In the original Marmousi
model (figure 1c), the low-velocity layers are not smeared but
distributed in the whole model domain. As a result, the dispersion errors caused by low-velocity layers become more severe, as indicated by the low correlation coefficient between
the wavefields in figure 12a and b, shown in figure 12c.
For 30 Hz, the finite-difference method suffers notably
from the dispersion error, especially in the shallow area of low
velocity (figure 13a). By comparison, FNO is immune to dispersion errors in spite of some discontinuities in the FNOpredicted wavefield (figure 13b). The correlation coefficient
between wavefields from the finite-difference and the FNO
methods is shown in figure 13c, which is small spanning the
whole model space. When the wavefield gets complicated,
it gets more difficult to recover the details of the wavefield.
We shall increase the number of FNO blocks to improve the
wavefield prediction accuracy. When the number of the FNO
blocks is increased, the training computational cost will be increased linearly, whereas the efficiency for the wavefield generation is kept the same.
From the computational efficiency aspect, FNO is almost two orders of magnitude faster than the finite-difference
method after the training process. For future applications,
we can start the training from a pre-trained model based on
the theory of transfer learning (Pan & Yang 2009; Song et al.

2022). Consequently, we can further reduce the training cost
and retain the efficiency feature of the FNO.
7. Conclusions
We have experimented for the first time to extrapolate
high-frequency wavefield using the Fourier neural operator
(FNO). The aim was to build a mapping relationship between low- and high-frequency wavefields. Considering the
limit of computer capacity, we have divided the model into
small pieces, trained the FNO and conducted predictions using small pieces. We have demonstrated that only a limited
number of low frequencies are needed to complete the task
of extrapolation from low- to high-frequency wavefields. By
stitching all the small pieces of predictions together, we have
obtained the high-frequency wavefields for large models that
a computer using the conventional finite-difference method
cannot cope with.
Acknowledgements
The authors are grateful to the sponsors of the Centre for Reservoir
Geophysics, Imperial College London, for supporting this research.

Conflict of interest statement: Authors declare that there is
no conflict of interest.
280

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Figure 13. The same plotting configuration as figure 12 for 30 Hz.

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang

this series Rùúô can be parameterised directly as a complexvalued tensor, to replace the Fourier transform of ùúÖùúô . In the
parameterisation of Rùúô , we treat Rùúô as a low-pass filter, to suppress unnecessary high-frequency components in F[pj (x)],
and thus truncate the series. In this way, we effectively reduce
the number of trainable parameters and effectively improve
the efficiency of training. We define the process of equation
(A.1) as one FNO block (figure A.1).

Appendix. Fourier neural operator (FNO)

References
Baydin, A.G., Pearlmutter, B.A., Radul, A.A. & Siskind, J.M., 2017. Automatic differentiation in machine learning: a survey, Journal of Machine
Learning Research, 18, 5595‚Äì5637.
Dablain, M.A., 1986. The application of high-order differencing to the
scalar wave equation. Geophysics, 51, 54‚Äì66.
Fomel, S., Ying, L. & Song, X., 2013. Seismic wave extrapolation using lowrank symbol approximation, Geophysical Prospecting, 61, 526‚Äì536.
Gao, F. & Wang, Y., 2020. Radiation pattern analyses for seismic multiparameter inversion of HTI anisotropic media, Journal of Geophysics and
Engineering, 17, 65‚Äì75.
Geneva, N. & Zabaras, N., 2020. Modelling the dynamics of PDE systems with physics-constrained deep auto-regressive networks, Journal of
Computational Physics, 403, 109056.
Hall, F. & Wang, Y., 2009. Elastic wave modelling by an integrated finite
difference method, Geophysical Journal International, 177, 104‚Äì114.
Jo, C.H., Shin, C. & Suh, J.H., 1996. An optimal 9-point, finite-difference,
frequency-space, 2-D scalar wave extrapolator, Geophysics, 61,
529‚Äì537.
Kaur, H., Fomel, S. & Pham, N., 2020a. Seismic ground-roll noise attenuation using deep learning, Geophysical Prospecting, 68, 2064‚Äì2077.
Kaur, H., Pham, N. & Fomel, S., 2020b. Improving resolution of migrated
images by approximating the inverse Hessian using deep learning, Geophysics, 85, WA173‚ÄìWA18.
Kingma, D.P. & Ba, J., 2014. ADAM: a method for stochastic optimization.
arXiv preprint arXiv:1412.6980.
Li, S., Yang, C., Sun, H. & Zhang, H., 2019. Seismic fault detection using
an encoder‚Äìdecoder convolutional neural network with a small training
set, Journal of Geophysics and Engineering, 16, 175‚Äì189.
Li, Y., Alkhalifah, T. & Zhang, Z., 2021. Deep-learning assisted regularized
elastic full waveform inversion using the velocity distribution information from wells, Geophysical Journal International, 226, 1322‚Äì1335.
Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart,
A. & Anandkumar, A., 2020a. Neural operator: graph kernel network
for partial differential equations, arXiv preprint, arXiv:2003.03485.
Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart,
A. & Anandkumar, A., 2020b. Fourier neural operator for parametric
partial differential equations, arXiv preprint, arXiv:2010.08895.
Moseley, B., Nissen-Meyer, T. & Markham, A., 2020. Deep learning for fast
simulation of seismic waves in complex media, Solid Earth, 11, 1527‚Äì
1549.
Ovcharenko, O., Kazei, V., Kalita, M., Peter, D. & Alkhalifah, T., 2019. Deep
learning for low-frequency extrapolation from multi-offset seismic data,
Geophysics, 84, R989‚ÄìR1001.
Pan, S.J. & Yang, Q., 2009. A survey on transfer learning. IEEE Transactions
on Knowledge and Data Engineering, 22, 1345‚Äì1359.
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht,
F., Bengio, Y. & Courville, A., 2019. On the spectral bias of neural
networks, Proceedings of the 36th International Conference on Machine
Learning, arXiv:1806.08734.

with
(K(ùúô)pj (x) =

‚à´

ùúÖùúô (x, y)pj (y)dy,

(A.2)

where ùúÖùúô is a neural network parameterised by weights in the
network ùúô, and W is a linear transformation. Both W and ùúô
can be learned from the training data. The activation function
ùúé(x) is the non-linear element-wise activation function.
To accelerate the integral process of equation (A.2), one
could impose the condition ùúÖùúô (x, y) = ùúÖùúô (x ‚àí y) and turn
the integral into a convolution
(K(ùúô)pj )(x) = ‚à´ ùúÖùúô[(x ‚àí y)pj (y)dy ]
= F ‚àí1 F[ùúÖùúô (x)]F[pj (x)] ,

(A.3)

where F denotes the Fourier transform, and F ‚àí1 denotes the
inverse Fourier transform. Because of the Fourier transform
implementation, this neural operator is referred to as FNO
(Li et al. 2020b).
While ùúÖùúô is defined in the Fourier transform space,
Rùúô = F[ùúÖùúô (x)],

(A.4)

Figure A.1. Block of FNO. F denotes the forward fast Fourier transform;
F ‚àí1 denotes the inverse Fourier transform; Rùúô denotes a linear transform
filters out the high-frequency components; W denotes a local linear transform; ùúé denotes a non-linear activation function and pj and pj+1 are the
input and output of the current block, respectively.
281

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

FNO is one of the neural operator methods. The concept of
the neural operator was developed for the mapping between
function spaces, and especially is applicable to solve PDEs.
For instance, if we want to find a non-linear mapping operator
G to connect two functions A = A(a) and B = B(b), the procedure consists of two local transformations. In the first local
transformation, the input a ‚àà A is lifted to a higher dimension by a shallow fully connected neural network p0 = P(a)
as the initial state and, then, an iterative architecture p0 ‚Üí
p1 ... ‚Üí pj ... ‚Üí pT is used to evolve their state until the connection with the output. In the second local transformation,
using another shallow fully connected neural network Q , the
last state output pT is projected to the dimension of output
using b = Q (pT ).
The iterative update from state pj to pj+1 is defined by an
activation function ùúé(x) as
(
)
pj+1 (x) = ùúé Wpj (x)+(K(ùúô)pj )(x) ,
(A.1)

Journal of Geophysics and Engineering (2022) 19, 269‚Äì282

Song and Wang
Virieux, J. & Operto, S., 2009. An overview of full-waveform inversion in
exploration geophysics, Geophysics, 74, WCC1‚ÄìWCC26.
Virieux, J., Calandra, H. & Plessix, R.√â., 2011. A review of the spectral, pseudo-spectral, finite-difference and finite-element modelling
techniques for geophysical imaging, Geophysical Prospecting, 59,
794‚Äì813.
Waheed bin, U., Haghighat, E., Alkhalifah, T., Song, C. & Hao, Q., 2021.
PINNeik: Eikonal solution using physics-informed neural networks,
Computers & Geosciences, 155, 104833.
Wang, Y. & Rao, Y., 2009. Reflection seismic waveform tomography, Journal of Geophysical Research, 114, B03304.
Wang, Y., 2011, Seismic, waveform modelling and tomography, in Encyclopedia
of Solid Earth Geophysics, Springer Verlag, 1290‚Äì1301.
Wang, Y., 2016. Seismic Inversion: Theory and Applications, John Wiley &
Sons.
Wu, Z. & Alkhalifah, T., 2014. The optimized expansion based low-rank
method for wavefield extrapolation, Geophysics, 79, T51‚ÄìT60.
Wu, Z. & Alkhalifah, T., 2018. A highly accurate finite-difference method
with minimum dispersion error for solving the Helmholtz equation,
Journal of Computational Physics, 365, 350‚Äì361.
Yang, D., Peng, J., Lu, M. & Terlaky, T., 2006. Optimal nearly analytic discrete approximation to the scalar wave equation, Bulletin of the Seismological Society of America, 96, 1114‚Äì1130.

282

Downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by Universidad Eafit user on 28 August 2024

Raissi, M., Perdikaris, P. & Karniadakis, G.E., 2019. Physics-informed neural networks: a deep learning framework for solving forward and inverse
problems involving nonlinear partial differential equations, Journal of
Computational Physics, 378, 686‚Äì707.
Shi, Y., Wu, X. & Fomel, S., 2019. SaltSeg: automatic 3D salt segmentation
using a deep convolutional neural network, Interpretation, 7, SE113‚Äì
SE122.
Song, C., Alkhalifah, T. & Waheed, U.B., 2021. Solving the frequencydomain acoustic VTI wave equation using physics-informed neural networks, Geophysical Journal International, 225, 846‚Äì859.
Song, C., Alkhalifah, T. & Waheed, U.B., 2022. A versatile framework to
solve the Helmholtz equation using physics-informed neural networks,
Geophysical Journal International, 228, 1750‚Äì1762.
Sun, H., Zhang, H., Song, M., Li, S. & Lu, Y., 2019. Automatic Fresnel zone
picking in the dip-angle domain using deep neural networks, Journal of
Geophysics and Engineering, 16, 136‚Äì145.
Thuerey, N., Wei√üenow, K., Prantl, L. & Hu, X., 2020. Deep learning methods for Reynolds-averaged Navier‚ÄìStokes simulations of airfoil flows,
AIAA Journal, 58, 25‚Äì36.
Tompson, J., Schlachter, K., Sprechmann, P. & Perlin, K., 2017. Accelerating Eulerian fluid simulation with convolutional networks, Proceedings of the 34th International Conference on Machine Learning,
arXiv:1607.03597.

