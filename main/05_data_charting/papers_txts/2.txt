Geophys. J. Int. (2023) 232, 1503–1514
Advance Access publication 2022 October 11
GJI Marine geosciences and applied geophysics

https://doi.org/10.1093/gji/ggac399

Simulating seismic multifrequency wavefields with the Fourier feature
physics-informed neural network
Chao Song

1,2

and Yanghua Wang

2

1 College of Geo-exploration Science and Technology, Jilin University, Changchun 130021, China
2 Centre for Reservoir Geophysics, Resource Geophysics Academy, Imperial College London, South Kensington, London SW7 2BP, UK.
E-mail: yanghua.wang@imperial.ac.uk

SUMMARY
To simulate seismic wavefields with a frequency-domain wave equation, conventional numerical methods must solve the equation sequentially to obtain the wavefields for different
frequencies. The monofrequency equation has the form of a Helmholtz equation. When solving
the Helmholtz equation for seismic wavefields with multiple frequencies, a physics-informed
neural network (PINN) can be used. However, the PINN suffers from the problem of spectral bias when approximating high-frequency components. We propose to simulate seismic
multifrequency wavefields using a PINN with an embedded Fourier feature. The input to the
Fourier feature PINN for simulating multifrequency wavefields is 4-D, namely the horizontal
and vertical spatial coordinates of the model, the horizontal position of the source, and the
frequency, and the output is multifrequency wavefields at arbitrary source positions. While
an effective Fourier feature initialization strategy can lead to optimal convergence in training
this network, the Fourier feature PINN simulates multifrequency wavefields with reasonable
efficiency and accuracy.
Key words: Neural networks, fuzzy logic; Numerical modeling; Physics-informed neural
network; Wave propagation.

1 I N T RO D U C T I O N
Calculating the seismic wavefield by solving the wave equation is
an essential step in seismic imaging applications, such as reverse
time migration (RTM), least squares RTM (LSRTM), and full waveform inversion (FWI; Tarantola 1984; Nemeth et al. 1999; Virieux
& Operto 2009; Wang 2011). LSRTM and FWI are optimization
problems based on data fitting. They require solving the wave equation to obtain the entire wavefields for the computation of the image
and the velocity gradient, and this step usually needs to be repeated
hundreds or even thousands of times to achieve convergence of the
optimization. In the frequency domain implementations, LSRTM
and FWI require many frequency components to cover the entire
wavenumbers of the model (Virieux & Operto 2009; Wang & Rao
2009; Wang 2016). Therefore, the computational costs of LSRTM
and FWI are linearly dependent on the number of frequencies considered and the iterations of model updating. Moreover, the computational costs of LSRTM and FWI increase dramatically in complex
media, such as elastic media, anisotropic media and poroelastic media (Brossier et al. 2009; Lee et al. 2010; Yang & Malcolm 2021).
The wave equation in the frequency domain is a Helmholtz equation for whose numerical solution the finite difference method is
often used (Dablain 1986; Virieux et al. 2011). It discretizes the
wave equation to form a large sparse impedance matrix which is

multiplied by the solution of the target wavefield. To solve the wavefield in the frequency domain, we need to calculate the inverse of
this large impedance matrix. To achieve this goal, the LU decomposition is usually used. For realistic large models, the calculation
of the wavefields requires an enormous amount of computation. We
can look at the wavefield solutions from a different perspective.
Machine learning is making rapid progress in a variety of scientific and technical fields due to the rapid development of computing
resources and the availability of data. In geophysics, a widely used
machine learning method called neural network has been applied
in many important areas, such as first arrival and phase selection
from seismic waveforms (Dai & MacBeth 1995; Gentili & Michelini 2006; Zhu & Beroza 2019), seismogram quality analysis and
evaluation (Valentine & Trampert 2012), fault and salt body detection from seismic images (Shi et al. 2019; Wu et al. 2019), seismic
source characterization and localization (Van den Ende & Ampuero
2020; Wang & Alkhalifah 2021), resolution improvement for migrated images and inverted velocity models (Kaur et al. 2020; Li
et al. 2021), dispersion curve extraction (Dai et al. 2021), etc.
In addition to these applications, neural networks have also been
used to solve partial differential equations (PDEs). A framework
called physics-informed neural networks (PINNs) uses the underlying physical disciplines as loss functions to solve PDEs. PINNs
accept spatial and temporal coordinate values as input and set the


C The Author(s) 2022. Published by Oxford University Press on behalf of The Royal Astronomical Society. This is an Open Access
article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which
permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

1503

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

Accepted 2022 October 1. Received 2022 October 1; in original form 2022 February 22

1504

C. Song and Y. Wang

A(x, m, ω)u(xs , ω, xs ) − s(x, xs ) = 0

(2)

by optimization. We solve this optimization problem with the help
of a machine learning system, the so-called PINN.
The source function vector s(x, xs ) is very sparse, and therefore
does not provide enough samples for training the network. We use
scattering theory and represent the wave equation in a perturbation
form as
ω2 m(x)δu(x, ω, xs ) + ∇ 2 δu(x, ω, xs ) = −ω2 δm(x)u 0 (x, ω, xs ),
(3)
where u 0 (x, ω, xs ) is the background wavefield corresponding to a
background modelm 0 (x), δm(x) = m(x) − m 0 (x) is the model perturbation, and δu(x, ω, xs ) is the scattered wavefield. In scattering
theory, the virtual source function −ω2 δm(x)u 0 (x, ω, xs ) in eq. (3),
which is related to δm(x) and u 0 (x, ω, xs ), extends over the entire
spatial domain.
The background model m0 (x) is set as an infinite isotropic homogeneous model and the source function s(x, xs ) is a delta function. Thus, the background wavefield u 0 (x, ω, xs ) can be obtained
by an analytical solution. For the Helmholtz equation (eq. 1), its
√
(1)
solutionu 0 (x, ω, xs ) = i 14 H0 (ω m 0 |x − xs |), is given in terms of
(1)
H0 , the 1st-kind Hankel function of order 0 (Engquist & Zhao
2018).

2.1.2 PINN

2 T H E O RY
2.1 Solving wave equation using PINN
2.1.1 Wave equation in scattering theory
The propagation of seismic waves can be simulated in the frequency
domain by solving the acoustic wave equation as follows:
ω2 mu(x, ω, xs ) + ∇ 2 u(x, ω, xs ) = s(x, xs ),

where x={x, z} is a vector of spatial coordinates for 2-D media, xs
denotes the coordinate of the source, ω is the angular frequency, m
is the squared slowness, s(x, xs ) is the source function vector, ∇ 2 =
∂ 2 /∂ x 2 + ∂ 2 /∂z 2 is the Laplacian operator and u(x, ω, xs ) is the
wavefield in the frequency domain. Each element of the wavefield
vector μ is complex.
Eq. (1) is a Helmholtz equation and can be written in a compact
form, A(x, m, ω)u(x, ω, xs ) = s(x, xs ), where A(x, m, ω)=ω2 m +
∇ 2 is the modeling operator. The modeling operator A(x, m, ω)
can be discretized into a sparse matrix, using a finite-difference
method. The inverse of such a sparse matrix A(x, m, ω) can be
calculated using the LU decomposition method. The LU decomposition factorizes the sparse matrix A(x, m, ω) as the product of
a lower triangular matrix (L) and an upper triangular matrix (U)
(Schwarzenberg-Czerny 1995). The inverse of the stored lower part
(L) and upper part (U) can be computed efficiently and applied
repeatedly to different source functions. The computational cost of
solving the Helmholtz equation based on the LU decomposition increases cubically with the increase in model size (Trefethen & Bau
1997). Therefore, the LU decomposition is the largest cost factor
in conventional wavefield simulation. For each frequency, a new
impedance matrix A(x, m, ω) must be created and its inverse calculated. The total costs for solving multifrequency wavefields thus
depend linearly on the number of frequencies.
To improve efficiency, instead of calculating the inverse of matrix
A(x, m, ω), we try to solve the linear system

(1)

Neural networks are capable of representing universal functions
(Hornik et al. 1989; Leshno et al. 1993; Van der Baan & Jutten
2000). Assume that a neural network with L + 1 layers is supposed
to approximate the wavefield solution u, and it starts with layer 0
and ends with layer L − 1. The number of hidden layers is L − 1.
In the lth hidden layer, there are kl neurons. A weighting parameter
wij l is used to connect the ith neuron in layer l − 1 and the jth
neuron in layer l. This weighting parameter controls the strength of

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

objective function as output. In geophysics, PINNs have been successfully used to solve eikonal equations (Smith et al. 2020; Waheed
et al. 2021), Maxwell’s equations (Huang et al. 2021), wave equations in the time and frequency domains for isotropic and anisotropic
media (Karimpouli & Tahmasebi 2020; Moseley et al. 2020; Voytan
& Sen 2020; Alkhadhr et al. 2021; Song et al. 2021, 2022; Alkhalifah et al. 2021a). Based on the traveltime solution of the Eikonal
equation, PINN has been used to localize microseismic events and
for seismic tomography (Grubas et al. 2021; Izzatullah et al. 2022;
Smith et al. 2022). PINN has also been applied to FWI based on
the wavefield solution of the wave equation (Rasht-Behesht et al.
2022; Song & Alkhalifah 2022).
If solving the Helmholtz equation with PINN, a new network
must be trained for each individual source and frequency. Alkhalifah et al. (2021b) suggested including the locations of the sources
and the frequency components as additional inputs to the network
so that the network can generate wavefields for multiple sources and
frequencies. However, it is difficult for a single network to generate
equally accurate wavefields for different frequencies because networks prefer low-frequency components when learning, which is
also known as spectral bias (Rahaman et al. 2019; Xu et al. 2019).
Spectral bias describes a weakness of PINN, meaning that PINN
can only represent low-frequency features, while it is unable to approximate high-frequency function (Wang et al. 2021). To solve this
problem, the position encoding method has been proposed to transform the input coordinates into Fourier basis functions (sinusoids)
with logarithmically spaced, axis-aligned frequencies (Mildenhall
et al. 2020; Huang et al. 2021). Tancik et al. (2020) also proposed
a Fourier feature network that transforms the input coordinates into
sinusoids with off-axis frequency distributions. In this way, the target output function can be represented by its Fourier decomposition,
which facilitates optimization of the network compared to using the
input coordinates directly.
In this paper, we propose to use the Fourier-feature PINN to
solve a Helmholtz equation to simulate seismic wavefields with
multiple frequencies and multiple sources. To convey this idea, we
structure the rest of this paper as follows. In the theory section, we
first introduce the Helmholtz equation and its scattered form. Then
we demonstrate the problem of spectral bias in predicting highfrequency seismic waves using neural networks. Next, we explain
how to use Fourier feature PINNs to solve this problem and propose an effective Fourier feature embedding initialization strategy.
In the network building and training section, we explain how to
define the network architecture and train the network. In the results
section, we show the significant improvements of Fourier feature
PINNs compared to simple PINNs in terms of accuracy in generating multifrequency wavefields. Finally, we discuss the limitations
and future prospects of the proposed method and summarize our
contribution.

Simulating seismic multifrequency wavefields
the connection of two neurons. A bias term bi is also used to shift
the output. The output for the kth neuron in the lth layer (u lk ) can
be determined by a weighted sum of the inputs (outputs in the last
layer ul–1 ) as follows (Bishop 2006):
⎛
⎞
kl −1

l−1
l
l
l
wk j u j + bk ⎠ ,
uk = φ ⎝
(4)
j=1

f =

1 
ω2 m(x)[δu R (x, ω, xs ) + δu I (x, ω, xs )]
N
+ ∇ 2 [δu R (x, ω, xs ) + δu I (x, ω, xs )]
2
+ ω2 δm(x)[u 0,R (x, ω, xs ) + u 0,I (x, ω, xs )] ,
2

(5)

where N is the number of training samples, and
[δu R (x, ω, xs ), δu I (x, ω, xs )] is the scattered wavefield in
complex value. The scattered wavefield is a function that depends
on the model coordinates (x), the frequency (f), and the source
coordinates (xs ). We use these parameters as inputs for layer 0
to predict multifrequency scattered wavefields at different source
locations in the last layer. We use only the horizontal source coordinate as input since sources in realistic seismic surveys are usually
placed at the surface with a fixed depth. The inputs to the network
and the resulting scattered wavefields thus contain four dimensions:
the horizontal (x) and vertical (z) spatial coordinates of the model,
the horizontal position of the source (xs ) and the frequency (f). The
second-order partial derivatives of [δu R (x, ω, xs ), δu I (x, ω, xs )]
with respect to the spatial coordinates of the input are evaluated by
automatic differentiation (Baydin et al. 2017). In the loss function,
the background wavefields ([δu R (x, ω, xs ), δu I (x, ω, xs )]) are
calculated according to the input model coordinate (x), the
angular frequency (ω = 2π f ), and the source coordinate (xs ). The
velocity model parameters (m, δm) are interpolated at random
model coordinates (x). Background wavefields and velocity model
parameters are non-trainable parameters that are included in the
loss function.

2.2 Spectral bias and Fourier feature PINN

This function u = eikx can be used as a solution of 1-D harmonic
plane wave, where k denotes the wavenumber. According to Euler’s
formula, u = eikx can be expressed as
eikx = cos(kx) + i sin(kx).

(6)

In this case, the seismic waves can be considered as a combination
of sine and cosine waves with different wavenumbers k. We use gR (x)
and gI (x) to denote the real and imaginary parts of eq. (6) which
sum over a group of wavenumbers, expressed as

cos(ki x),
(7)
g R (x) =
i

g I (x) =



sin(ki x),

(8)

i

where i is the index of the wavenumber. If the wavenumbers are
given as k = {5, 10, 15, 20, 25, 30, 35, 40, 45, 50}, we get function
g R (x) and function g I (x) shown in Figs 1(a, left-hand side) and (b,
left-hand side), respectively. Both g R (x) and g I (x) functions show
complicated waveforms with multiple frequency components. We
calculate their spectra,
G R (k) and G I (k), using the Fourier transform

G(k)=F[g(x)]= g(x)e−ikx dx, where F denotes the operation of
Fourier transform. The magnitude of G(k) is denoted as |G(k)| or
|F[g(x)]|, where | · | means the modulus taken from a complex
number. Figs 1(a, right-hand side) and (b, right-hand side) show
the magnitude of spectra G R (k) and G I (k), and they show consistent wavenumber components in g R (x) and g I (x) with the given
parameters.
We use a neural network to regress g R (x) and g I (x). This network
contains six hidden layers, and each hidden layer has 256 neurons.
We use the Adam optimizer with the full-batch gradient descent
method to train the network. The learning rate is 0.0003. A fullbatch training scheme means all training samples are passed to the
optimizer at once. We input 200 samples for the x-axis coordinate
into the network. These samples are sampled uniformly between 0
and 1 m. The 2-D output of this network contains two functions,
denoted by p R (x) and p I (x). They are designed to correspond to
g R (x) and g I (x), respectively. As the number of epochs increases,
the normalized magnitudes of F[ p R (x)] and F[ p I (x)] at different
wavenumbers during training are shown in Figs 2(a) and (b), respectively. An epoch in machine learning means a complete run of all
training samples by the optimizer. For all experiments in this paper,
we use a Quadro Tesla P100 16 GB GPU to train the networks and
obtain the predicted results. In this example, each training epoch
takes 0.003 s, so it takes 3 min to perform 60 000 training epochs.
In Fig. 2, the x-axis represents the wavenumber, and the y-axis
the number of training epochs. It is obvious that the low-frequency
components can be recovered quickly with a limited number of
epochs. In comparison, the high-frequency components can only be
partially recovered even with a large number of epochs. The problem
arises from spectral bias, which indicates that the learning priority
of low-frequency features takes precedence over high-frequency
ones in a neural network. This is a major challenge for neural
networks to simulate the propagation of seismic waves that contain
high-frequency information.

2.2.1 Spectral bias
Neural networks prefer low-frequency components when learning
the features of the target functions of interest, and this phenomenon
is called spectral bias (Rahaman et al. 2019). This problem will
cause difficulties in generating high-frequency wavefields. We will
demonstrate this problem in the reconstruction of seismic waves
using neural networks with a simple example.

2.2.2 Fourier feature neural networks
To mitigate the problem of spectral bias in predicting the highfrequency wavefield, we propose to use the Fourier feature
PINN to simulate multifrequency wavefields (Tancik et al. 2020;
Wang et al. 2021). In Fourier feature PINN, the input data

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

where φ is the nonlinear activation function. The activation function
decides which neuron to activate and pass on to the next layer. A
loss function should be defined to provide the connection between
the input and the target output function. Training the neural network means an optimization process to find the optimal weighting
parameter wij l that minimizes the defined loss function.
In this paper, we use a fully connected neural network to represent
the real and imaginary parts of the scattered wavefields, denoted as
δu R and δu I , respectively. Unlike conventional data-driven neural
networks that require a large amount of data to create the mapping
between input and output data, we use a PINN based on physical
laws to train the neural network. To solve for scattered wavefields,
we use the scattering form of the wave equation (eq. 3) as the loss
function. As the scattered wavefield is complex-valued, δu R and δu I
are combined together to form one loss function, in the L2 norm as

1505

1506

C. Song and Y. Wang

Figure 2. The spectrum evolution of the neural network output function for each frequency against the training iteration. Panel (a) corresponds to G R (k) and
panel (b) corresponds to G I (k). The x-axis represents the wavenumber, and the y-axis represents the training epoch number.

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

Figure 1. The (a) g R and (b) g I functions and their corresponding spectrums. In both panels, the left-hand part displays the functions, and the right-hand part
displays the corresponding spectrum using fast Fourier transform (F). The given wavenumber k is {5, 10, 15, 20, 25, 30, 35, 40, 45, 50}.

Simulating seismic multifrequency wavefields

1507

(v = {x, z, xs , f}) are lifted to a higher dimension which is made up
of a series of sinusoids using the function γ , stated as
γ (v) =

cos(ϕv)
,
sin(ϕv)

(9)

where ϕ represent the wavenumbers in the Fourier basis used to approximate the wavefield solution. This process is called Fourier feature embedding in the Fourier feature PINN, as shown in Fig. 2(a).
It is worth noting that the Fourier feature ϕ is not a trainable parameter, so the Fourier feature PINN does not introduce any additional
trainable parameters compared to the simple PINN. Therefore, the
training cost for the Fourier feature PINN is similar to that of the
simple PINN. After this step of embedding the Fourier feature, γ (v)
is passed to the network. In the first layer of the Fourier feature
PINN, the process is described as follows:
H1 = φ (W1 γ (v) + b1 ) ,

(10)

where W1 is the weight matrix; b1 is the bias vector. In γ (v), ϕ
is equivalent to the wavenumber k in eq. (6), and we can sample ϕ
from a uniform distribution. The maximum value of this distribution
corresponds to the maximum target wavenumber of the predicted
wavefield.
To understand the mechanism behind the Fourier feature PINN,
let us make the connection to the Fourier decomposition. If we consider the output function uv (v) as an approximation to the solution
of the wavefield u, the Fourier decomposition of u can be expressed
as follows:

(11)
u(x) =
u k eikx ,
where u k represents the Fourier coefficients for Fourier basis. In
principle, u k is equivalent to the weights W in the Fourier feature PINN, and eikx is equivalent to the Fourier basis γ (v) transferred from the input coordinate values using Fourier feature embedding. The weight matrix W1 acts as the Fourier coefficients
of the Fourier basis which can be optimized when training the
network.

3 NETWORK BUILDING AND
TRAINING
The simple PINN and the Fourier feature PINN we use in this
work contain eight hidden layers with {128, 128, 64, 64, 32, 32,

32, 32} neurons from the shallow layer to the deep layer, as shown
in Figs 3(a) and (b). We use a large number of neurons in the
shallow layer to extract low-level features. Then, these features are
passed to subsequent deep layers with fewer neurons. Deep, thinner layers can effectively and efficiently learn high-level features
from the extracted low-level features (Song & Alkhalifah 2022).
The number of neurons and hidden layers is determined by trial
and error. We initialize the weights in the hidden layers using a
Xavier initialization with a uniform distribution (Glorot & Bengio 2010). We use the inverse tangent function as the activation
function to connect layers in the neural network, except for the
last hidden layer which connects the output. In the Fourier feature
PINN (Fig. 3b), the inputs are transferred into a series of sinusoids
using the Fourier feature embedding before being passed on to the
network.
Our goal is to simulate seismic wavefields for multiple frequencies at multiple source locations. Thus, each of the input samples
for the neural network comprises four dimensions, vj = {xj , zj , xsj ,
fj }, where j is the sample index, xs denotes the source location in the
horizontal direction and f denotes the frequency (in Hz), ω = 2π f .
We do not input the vertical position of the source into the neural
network because we place the sources at the surface (at the same
depth), which mimics a realistic seismic survey at the surface. The
outputs are real (δu R ) and imaginary (δu I ) parts of the scattered
wavefields.
We optimize the networks in Figs 3(a) and (b) using an Adam
optimizer with a stochastic gradient descent method (Kingma
& Ba 2014) followed by optimization with the Limited-memory
Broyden–Fletcher–Goldfarb–Shanno algorithm (L-BFGS) (Liu &
Nocedal 1989; Rao & Wang 2017; Rao et al. 2019). The Adam
optimizer is based on the first-order derivative gradient, while LBFGS is a quasi-Newton algorithm that depends on the secondorder derivative. L-BFGS can accelerate the convergence rate. The
training of the L-BFGS optimizer stops when the maximum iteration number is reached or the convergence is achieved. The
main problem of the L-BFGS optimizer is that it often does not
converge for randomly initialized networks. Therefore, we use the
Adam optimizer primarily to provide a good starting point for LBFGS and to speed up training. Both optimizers use a gradientbased optimization algorithm based on a full batch. For all experiments conducted in the Results section, the number of training
epochs for the Adam optimizer is 100 000 and training is automatically terminated after about 50 000 L-BFGS updates; the

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

Figure 3. (a) The plain PINN architecture. The input contains 4-D parameters, (x, z, xs , f). The outputs are real (δu R ) and imaginary (δu I ) parts of the scattered
wavefield. (b) The Fourier feature PINN architecture. The only difference between the Fourier feature PINN and the plain PINN is that the inputs are transferred
to a series of sinusoids using Fourier feature embedding.

1508

C. Song and Y. Wang
the scattered wavefields with the simple PINN. This shows that the
simple PINN cannot predict multifrequency wavefields.

4.1.2 Performance of Fourier feature PINN with the theoretical ϕ
sampling range

learning rate is 0.001. These hyper-parameters were determined
by trial and error.

4 R E S U LT S
4.1 A velocity section from the Sigsbee2A model
We test the proposed method on a layered model, extracted from the
Sigsbee2A model (Paffenholz et al. 2002), as shown in Fig. 4. The
size of the model is 101 × 101 gridpoints with a spatial sampling
of 25 m in both vertical and horizontal directions. We refer to these
gridpoints representing the model as regular gridpoints. We generate
50 000 samples of {x, z, xs , f} as training data. These samples are
randomly generated from a uniform distribution. For x, z and xs ,
the range of the training samples is between 0 and 2.5; while, for
the frequency f, the range is between 5 and 10. The training data
are input into the networks to satisfy the scatter wave equation
in eq. (5).
In Fig. 5(a), we show the reference total wavefields calculated
by a numerical method. The numerical solver is an optimal 9-point
frequency-domain finite-difference (FDFD) operator for the wave
equation (Jo et al. 1996). It takes 10.93 s to obtain the wavefields
from 5 to 10 Hz with an interval of 1 Hz for nine sources. Fig. 5(b)
shows the background wavefields generated from the analytical solutions for the same source locations and frequencies as in Fig. 5(a).
The computational cost of calculating the background wavefields is
almost negligible. We obtain the reference scattered wavefields by
calculating the wavefield difference between Figs 5(a) and (b), as
shown in Fig. 5(c).

4.1.1 Performance of the plain PINN
To compare the performance of plain PINN and Fourier feature
PINN, we first train the simple PINN architecture in Fig. 3(a) with
the Adam optimizer and then with the L-BFGS optimizer. In this
example, each epoch of the Adam optimizer takes 0.16 s and each
epoch of the L-BFGS optimizer takes 0.12 s. In total, it takes 6.11
hr to complete the training. After training the network, we input
the regular gridpoints for the numerical solutions into the trained
network to obtain the predicted scattered wavefields, and this takes
only 0.063 s. Compared to the FDFD method, the computational
cost of a trained PINN in generating the wavefields is more than 150
times less. The resulting predicted scattered wavefields of the simple
PINN are shown in Fig. 6(a). It is clear that we find no evidence of

4.1.3 Performance of Fourier feature PINN with calibrated ϕ
sampling ranges
To demonstrate the importance of using the effective and simple
sample strategy for ϕ, we first calibrate the maximum ϕ to 0.02.
Using the same training setup and training data as the previous
experiments, the predicted wavefields from the Fourier feature PINN
with uniformly sampled ϕ from a narrower range of [−0.02, 0.02]
are shown in Fig. 6(c). We find that the scattered wavefields are well
recovered at low frequencies (5−8 Hz), while there are errors at the
edges of the predicted wavefields at high frequencies (9 and 10 Hz).
Then, we set the maximum ϕ to 0.1 and train the same network
with the same setup. The predicted wavefields from the Fourier
feature PINN with uniformly sampled ϕ from this wider range of
[−0.1, 0.1] are shown in Fig. 6(d). Due to the effect of spectral bias,
only the low-frequency scattered wavefields are well predicted, and
the errors for the high-frequency ones (9–10 Hz) are large. In this
experiment, we sample ϕ from a wider range with a uniform distribution. Since this wider range goes beyond the target range, we
may get unwanted samples that are not n the target range. Consequently, this leads to insufficient sampling for the target ϕ, resulting
in poorer training convergence in network optimization compared
to using the theoretical ϕ range.
In both Figs 7(c) and (d), we see that the main errors are distributed at the edges of the high-frequency wavefields. This is because in the training data, the gird points at the edges are not as
well sampled as those in the central region, resulting in inadequate
training. Due to the problem of spectral bias, the errors occur mainly
in the predicted high-frequency wavefields.
To further verify the accuracy of the wavefields predicted by
PINNs, we calculate the difference between the reference scattered wavefields and PINN-predicted scattered wavefields. Fig. 7(a)
shows a large difference between scattered wavefields and indicates the low accuracy of the simple PINN in generating

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

Figure 4. A layer velocity model extracted from the Sigsbee2A model.

With the Fourier feature PINN, it is important to choose sample
ϕ (the wavenumber in the Fourier basis) wisely. As we mentioned
earlier, ϕ represents the wavenumber of the seismic waves. So, we
need to sample ϕ within the range of the wavenumber for the target
wavefields. According to the relationship between wavenumber,
frequency and velocity, the maximum wavenumber of the target
wavefield can be calculated by kmax = 2π f /vmin , where f max is the
maximum frequency of the target wavefield (in this case 10 Hz)
and vmin is the minimum velocity value (in this case 1830 m s–1 ).
So the maximum wavenumber of the target wavefield is 0.0343.
To ensure that the maximum wavenumber is sampled, we set the
maximum value of ϕ slightly larger than 0.0343, in this case 0.04.
Consequently, all wavenumbers are sampled equally in the range of
[−0.04, 0.04]. We call this ϕ sampling range the theoretical range.
We train the Fourier feature PINN in Fig. 3(b) with the same
training setup and training data as the simple PINN experiment.
In Fig. 6(b), we observe the wavefields predicted with the Fourier
feature PINN in the same configuration as in Fig. 5(a). We see that
scattered wavefields from 5 to 10 Hz are well recovered for three
different sources.

Simulating seismic multifrequency wavefields

1509

Figure 6. Predicted scattered wavefields (a) from the plain PINN, from Fourier feature PINN with ϕ uniformly sampled from the range of (b) [−0.04, 0.04],
(c) [−0.02, 0.02] and (d) [−0.1, 0.1]. The elliptical circles in (c) and (d) indicate the poor prediction results for high-frequency wavefields. The frequencies
and source locations displayed are the same in Fig. 5.

multifrequency wavefields. Quantitatively, the L2 norm wavefield
difference in Fig. 7(a) is 687.37. In Fig. 7(b), we find that the
difference is almost zero for all the displayed wavefields, and the
L2 norm wavefield difference for Fig. 7(b) is 13.20. This small

wavefield difference confirms the accuracy of the Fourier feature
PINN predicted wavefields using the theoretical ϕ sampling range.
By comparison, the accuracy of Fourier feature PINN predicted
wavefields will be compromised using a smaller ([−0.02, 0.02])

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

Figure 5. (a) Reference wavefields from the finite-difference method, (b) background wavefields from the analytical solution, and (c) reference scattered
wavefields [difference between figures (a) and (b)] at different frequencies and different source locations. In each figure, from the left column to the right
column, the displayed frequencies are from 5 to 10 Hz with a 1 Hz interval; each row displays the wavefields for three different source locations (S1 , S2 , S3 ) on
the surface.

1510

C. Song and Y. Wang

Figure 8. The training loss of different networks (blue dashed line: plain
PINN; orange solid line: Fourier feature PINN with ϕ uniformly sampled
from the theoretical range of [−0.04, 0.04]; red dash-dotted line: Fourier
feature PINN with ϕ uniformly sampled from a narrower range of [−0.02,
0.02]; purple dotted line: Fourier feature PINN with ϕ uniformly sampled
from a wider range of [−0.1, 0.1].)

or a larger ([−0.1, 0.1]) ϕ sampling range, as shown in Figs 7(c)
and (d). They show larger differences in scattered wavefields than
in Fig. 7(b), especially at relatively high frequencies (9 and 10 Hz).
The L2 norm wavefield difference for Figs 7(c) and (d) are 234.62
and 121.82, respectively, which are 17 and 9 times larger than
those of Fig. 7(b).
Fig. 8 shows the training loss curves for different experiments.
We note that the training loss for the simple PINN decreases very
slowly (blue dashed line). It indicates that simple PINN is not able
to solve for scattered wavefields that satisfy the Helmholtz equation

Figure 9. Another velocity model extracted from the Sigsbee2A model.

with multiple frequencies. In comparison, the Fourier feature PINN
can achieve training convergence and provide reasonably accurate
results of scattered wavefield prediction. When embedding Fourier
features, it is important to choose a good sampling strategy for ϕ.
From our experiments, we know that it is good to choose the
maximum value of ϕ slightly larger than the maximum wavenumber of the target wavefields. In Fig. 8, it can be seen that the best
convergence for network training can be achieved when the theoretical range of [−0.04, 0.04] is sampled for ϕ (according to the
sampling strategy we introduced), as shown by the orange solid
curve. In comparison, sampling ϕ from a narrower or wider range
than the theoretical one causes the training optimization to converge
to a higher loss.

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

Figure 7. (a) Scattered wavefield difference between the reference wavefields (Fig. 5c) and the Plain PINN-predicted wavefields. Scattered wavefields difference
between the reference wavefields (Fig. 5c) and Fourier feature PINN-predicted wavefields with ϕ uniformly sampled from the range of (b) [−0.04, 0.04], (c)
[−0.02, 0.02] and (d) [−0.1, 0.1]. The frequencies and source locations displayed are the same in Fig. 5.

Simulating seismic multifrequency wavefields

1511

Table 1. Computational time (seconds) comparison between PINN and FDFD for different numbers of frequencies (f)
and sources (xs ). For PINN, the computation time is a sum of the training time of the network (1188.05 s) and the time
for wavefield prediction.
Method/numbers
PINN
FDFD

6 (f)×9 (xs )

51 (f)×9 (xs )

501 (f)×9 (xs )

501 (f)×50 (xs )

1188.05 + 0.063
10.93

1188.05 + 0.52
102.01

1188.05 + 5.43
1205.03

1188.05 + 26.46
2044.02

4.2 Transferring to another section from the Sigsbee2A
model
In the previous experiments, we have shown that the Fourier feature
PINN can solve a Helmholtz equation with multiple frequencies
for arbitrary source locations. However, the trained network is only
applicable for the specific velocity model used in the training. For
a new velocity model, we need to retrain the network. Transfer
learning is a machine learning technique that allows us to use a
previously trained model as a starting point for a new model specified for a new task (Pan & Yang 2009; Weiss et al. 2016). Based
on the concept of transfer learning, we can start training with the
pre-trained network obtained from a specific velocity model. Since
the pre-trained network has already learned the features of the wavefield solution, a very limited number of L-BFGS iterations is sufficient to adjust the weights that take into account a new velocity
model.
For a new velocity model shown in Fig. 9, we start training
with the trained Fourier feature PINN using the theoretical sampling strategy for the velocity model in Fig. 4. In this case, we
use only 10 000 iterations of the L-BFGS optimization to train
the network. In total, it takes 19.80 min (1188.05 s) to complete the training. By using transfer learning, we can reduce the
training cost of this new velocity model by 18 times. We show
the reference scattered wavefields from the numerical method
in Fig. 10(a). The scattered wavefields from the newly trained
Fourier feature PINN are shown in Fig. 10(b). The difference

between Figs 10(a) and (b) is almost not noticeable as shown
in Fig. 10(c).
To simulate wavefields at nine sources in the range of 5−10 Hz
with an interval of 1 Hz, the Fourier feature PINN still takes more
time than the FDFD method, even with the help of transfer learning.
However, if we want to analyze the wavefield signals in the time
domain, we need a large number of wavefields in the frequency
domain to perform the inverse Fourier transform and convert them
to the time domain. In addition, field seismic surveys require a large
number of sources to image the subsurface structures. The computational efficiency of the Fourier feature PINN becomes apparent
as the number of frequencies and sources for the target wavefields
increases. Table 1 shows the computation time of PINN and FDFD
for different number of frequencies (f) and sources (xs ). For PINN,
the training time of the network is a constant (1188.05 s), and the
total computation time is equal to this training time plus the time
for wavefield prediction. We see that PINN can be more efficient
in generating wavefield solutions for a relatively large number of
frequencies and sources.
Figs 11 and 12 show the snapshots of the scattered wavefields
in the time domain, which are transformed from the numerical
calculated and the Fourier feature PINN-predicted wavefields in the
frequency domain over the frequency range from 5 to 10 Hz with
an interval of 0.01 Hz. At different time steps, the snapshots of the
wavefield from the FDFD method and the Fourier feature PINN
are almost identical, confirming the accuracy of the Fourier feature
PINN.

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

Figure 10. (a) Reference scattered wavefields from the finite-difference method, (b) predicted scattered wavefield from the Fourier feature PINN with ϕ
uniformly sampled from the range of [−0.04, 0.04] and (c) scattered wavefield difference [difference between panels (a) and (b)] at different frequencies and
different source locations for the velocity model in Fig. 9. The frequencies and source locations displayed are the same in Fig. 5.

1512

C. Song and Y. Wang

Figure 12. Time-domain scattered wavefield snapshots at (a) 0.6 s, (b) 0.8 , and (c) 1 s transformed from Fourier feature PINN-predicted frequency-domain
wavefields ranging from 5 to 10 Hz with a 0.01 Hz interval.

5 DISCUSSION
In this paper, we try to solve the scattered wavefields instead of
the real wavefields directly. The solution of the scattered wavefield is related to the background wavefield that we provide. We
recommend using the analytical wavefield solution corresponding
to an infinite isotropic homogeneous model and a delta source
function. The velocity of the homogeneous background model
should be the same as that at the source gridpoint. This way, the
scattered wavefields around the source will not be dramatically
large. We use a delta function as the source function so that the
trained networks become generators of Green’s function. For the
predicted wavefields at all frequencies, the amplitude and phase
are identical. In realistic seismic applications, we need to recover
the actual frequency component of the source information for real
wavefields.
The main contribution of this work is that we have developed a
machine learning based method to simulate multifrequency wavefields directly from the Helmholtz equation. This feature surpasses
the capabilities of conventional numerical methods. In the commonly used FDFD method, we have to solve the Helmholtz equation repeatedly for wavefields at different frequencies, which is very
costly for a large number of frequencies. PINN-based Helmholtz
equation solvers have been developed to retrieve the wavefield solutions from the machine learning perspective (Song et al. 2021,
2022; Alkhalifah et al. 2021a), and this work paves the way for
the new method presented in this paper. Unlike the previous PINNbased Helmholtz equation solvers, the Fourier feature PINN-based
solver uses the Fourier feature embedding step to establish a physical connection between input coordinates and output wavefields. It
overcomes the problem of spectral bias in PINN and manages to
simulate accurate multi-frequency wavefields.

PINN’s training cost for solving the Helmholtz equation is relatively high. However, if we take advantage of the theory of transfer
learning, the training cost of a new velocity model using a pretrained network can be significantly reduced. We can obtain this
trained network with a certain velocity before the actual seismic
surveys. We recommend using a velocity model that is close to the
true model in the study area. If this condition cannot be met, the
corresponding trained network can learn basic features of the target wavefields, even if a certain velocity reflects limited geological
background information. If we need wavefields in the time domain,
hundreds or thousands of frequencies are needed to obtain enough
time samples. The proposed method is faster than the numerical
solver in generating a large number of frequencies for frequency
domain wavefields. In large seismic surveys, thousands of sources
are usually used to obtain sufficient illumination of the subsurface.
In this case, the proposed method can simulate wavefields for thousands of sources in a very short time.
Moreover, the proposed method is flexible and versatile for wave
equations in different media, such as anisotropic media, elastic
media, fluid-saturated porous media, etc. For numerical methods,
researchers need to rewrite the codes of wave equation solvers
for different media. In the framework of PINN, we can easily adjust the loss functions corresponding to different wave equations.
Moreover, it is easy to convert the PINN-based Helmholtz solver
from 2-D models to 3-D models by introducing an additional input for the third-dimensional coordinate. For FDFD wave equation
solvers in complex media or in 3-D models, we need to extend
the impedance matrix to introduce more physical parameters or a
high spatial dimension to simulate seismic waves. As a result, the
computational cost increases significantly. In these applications, the
proposed method will prove its attractiveness in terms of computational efficiency.

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

Figure 11. Time-domain scattered wavefield snapshots at (a) 0.6 s, (b) 0.8 s and (c) 1 s transformed from numerical frequency-domain wavefields ranging
from 5 to 10 Hz with a 0.01 Hz interval.

Simulating seismic multifrequency wavefields

6 C O N C LU S I O N
We have proposed to simulate seismic multifrequency wavefields
using the Fourier feature PINN. Since this version of PINN exploits
the Fourier feature, it is able to simulate wavefields with different
frequencies and source locations.
The four dimensions are the horizontal and vertical spatial coordinates of the model, the horizontal position of the source, and the
frequency Each dimension is represented by a series of sinusoidal
functions that vary with different wavenumbers. These sinusoidal
functions are the Fourier basis, and the associated coefficients are
embedded in PINN.
For embedding the Fourier feature, we proposed a theoretical
wavenumber sampling strategy to achieve optimal training convergence. Numerical experiments have shown the efficiency and accuracy of the method, in generating wavefields of different frequencies
with arbitrary source locations.

AC K N OW L E D G M E N T S
The authors are grateful to the sponsors of the Centre for Reservoir Geophysics, Imperial College London, for supporting this research. The research is also supported by the Special Fund of the
Key Laboratory of Geoghysical Exploration Equipment, Ministry
of Education (Jilin University).

D ATA AVA I L A B I L I T Y
There are no field data associated with this paper.

CONFLICT OF INTEREST
The authors acknowledge that there are no conflicts of interest
recorded.

REFERENCES
Alkhadhr, S., Liu, X. & Almekkawy, M., 2021. Modelling of the forward
wave propagation using physics-informed neural networks, in Proceedings of the 2021 IEEE International Ultrasonics Symposium, pp. 1–4.
Alkhalifah, T., Song, C., bin Waheed, U. & Hao, Q., 2021a. Wavefield
solutions from machine learned functions constrained by the Helmholtz
equation, Artif. Intell. Geosci., 2, 11–19.
Alkhalifah, T., Song, C. & Huang, X., 2021b. High-dimensional wavefield
solutions based on neural network functions, in Proceedings of the First
International Meeting for Applied Geoscience & Energy, Society of Exploration Geophysicists, pp. 2440–2444.
Baydin, A.G., Pearlmutter, B.A., Radul, A.A. & Siskind, J.M., 2017. Automatic differentiation in machine learning: a survey, J. Mach. Learn. Res.,
18, 5595–5637.
Bishop, C.M., 2006. Pattern Recognition and Machine Learning, Springer.
Brossier, R., Operto, S. & Virieux, J., 2009. Seismic imaging of complex onshore structures by 2D elastic frequency-domain full-waveform inversion,
Geophysics, 74(6), WCC105–WCC118.
Dablain, M.A., 1986. The application of high-order differencing to the scalar
wave equation, Geophysics, 51, 54–66.
Dai, H. & MacBeth, C., 1995. Automatic picking of seismic arrivals in local
earthquake data using an artificial neural network, Geophys. J. Int., 120,
758–774.
Dai, T., Xia, J., Ning, L., Xi, C., Liu, Y. & Xing, H., 2021. Deep learning
for extracting dispersion curves, Surv. Geophys., 42, 69–95.
Engquist, B. & Zhao, H., 2018. Approximate separability of the Green’s
function of the Helmholtz equation in the high frequency limit, Commun.
Pure appl. Math., 71, 2220–2274.
Gentili, S. & Michelini, A., 2006. Automatic picking of P and S phases
using a neural tree, J. Seismol., 10, 39–63.
Glorot, X. & Bengio, Y., 2010. Understanding the difficulty of training
deep feed forward neural networks, in Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics, pp. 249–
256.
Grubas, S., Yaskevich, S. & Duchkov, A., 2021. Localization of microseismic events using the physics-informed neural-network for traveltime
computation, in Proceedings of the 82nd EAGE Annual Conference &
Exhibition, pp.1–5.
Hornik, K., Stinchcombe, M. & White, H., 1989. Multilayer feedforward
networks are universal approximators, Neural Netw., 2, 359–366.
Huang, X., Alkhalifah, T. & Song, C., 2021. A modified physics-informed
neural network with positional encoding, in The First International Meeting for Applied Geoscience & Energy, Society of Exploration Geophysicists, pp.2480–2484.
Huang, X., Liu, H., Shi, B.,Wang, Z., Yang, K., Li, Y., Weng, B., Wang, M.,
Chu, H., Zhou, J., Yu, F., Hua, B., Chen, L. & Dong, B., 2021. Solving
partial differential equations with point source based on physics-informed
neural networks, arXiv:2111.01394.
Izzatullah, M., Yildirim, I.E., Waheed, U.B. & Alkhalifah, T., 2022. Laplace
HypoPINN: physics-informed neural network for hypocenter localization
and its predictive uncertainty, Mach. Learn. Sci. Technol., 3, 045001.
Jo, C.-H., Shin, C. & Suh, J.H., 1996. An optimal 9-point, finite-difference,
frequency-space, 2-D scalar wave extrapolator, Geophysics, 61, 529–537.
Karimpouli, S. & Tahmasebi, P. 2020. Physics informed machine learning:
seismic wave equation, Geosci. Front., 11, 1993–2001.
Kaur, H., Pham, N. & Fomel, S., 2020. Improving the resolution of migrated images by approximating the inverse Hessian using deep learning,
Geophysics, 85(4), WA173–WA183.
Kingma, D.P. & Ba, J., 2014. Adam: a method for stochastic optimization,
arXiv:1412.6980.
Lee, H.Y., Koo, J.M., Min, D. J., Kwon, B.D. & Yoo, H.S., 2010. Frequencydomain elastic full waveform inversion for VTI media, Geophys. J. Int.,
183, 884–904.
Leshno, M., Lin, V.Y., Pinkus, A. & Schocken, S., 1993. Multilayer feedforward networks with a nonpolynomial activation function can approximate
any function, Neural Netw., 6, 861–867.

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

In terms of accuracy, the finite difference method requires explicit
meshing to approximate the spatial derivatives, which can lead to
severe numerical dispersion errors if a coarse-mesh scheme is used.
These errors are almost inevitable in acoustic anisotropic media
due to the slow propagating shear waves. In comparison, PINN
computes spatial derivatives using a mesh-free technique, automatic
differentiation, which is immune to dispersion errors (Song et al.
2021).
FWI is an important method for inverting the high-resolution
velocity model. PINN has been proposed to perform waveformbased inversion (Rasht-Behesht et al. 2022; Song & Alkhalifah
2022). In FWI, multiple frequency components are needed to cover
the entire wavenumber components of the velocity model. Our
proposed method is able to provide wavefield solutions for multiple frequencies, which could potentially become an alternative
wave equation solver for FWI, especially for complex media. For
FWI, the velocity model will not change dramatically during update iterations, so a very limited number of L-BFGS trainings
will suffice. FWI requires a large number of wavefields for multiple sources and frequencies. In this case, the proposed method
is very fast in generating wavefields for multiple frequencies and
sources.

1513

1514

C. Song and Y. Wang
let networks learn high frequency functions in low dimensional domains,
Adv. Neural. Inf. Process. Syst., 33, 7537–7547.
Tarantola, A., 1984. Inversion of seismic reflection data in the acoustic
approximation, Geophysics, 49, 1259–1266.
Trefethen, L.N. & Bau, D., 1997. Numerical Linear Algebra, Vol. 50, SIAM.
Valentine, A.P. & Trampert, J., 2012. Data space reduction, quality assessment and searching of seismograms: autoencoder networks for waveform
data, Geophys. J. Int., 189, 1183–1202.
Van den Ende, M.P. & Ampuero, J.P., 2020. Automated seismic source
characterization using deep graph neural networks, Geophys. Res. Lett.,
47(17), e2020GL088690.
Van der Baan, M. & Jutten, C., 2000. Neural networks in geophysical applications, Geophysics, 65, 1032–1047.
Virieux, J., Calandra, H. & Plessix, R.É., 2011. A review of the spectral, pseudo-spectral, finite-difference and finite-element modelling techniques for geophysical imaging, Geophys. Prospect., 59, 794–813.
Virieux, J. & Operto, S., 2009. An overview of full-waveform inversion in
exploration geophysics, Geophysics, 74(6), WCC1–WCC26.
Voytan, D. & Sen, M.K., 2020. Wave propagation with physics informed
neural networks, in The Expanded Abstracts of SEG International Exposition and Annual Meeting, pp.3477–3481.
Waheed, U. B., Haghighat, E., Alkhalifah, T., Song, C. & Hao, Q. 2021. PINNeik: eikonal solution using physics-informed neural networks, Comput.
Geosci., 155, 104833.
Wang, H. & Alkhalifah, T., 2021. Direct microseismic event location and
characterization from passive seismic data using convolutional neural
networks, Geophysics, 86(6), KS109–KS121.
Wang, S., Wang, H. & Perdikaris, P., 2021. On the eigenvector bias of
Fourier feature networks: from regression to solving multi-scale PDEs
with physics-informed neural networks, Comput. Meth. Appl. Mech. Eng.,
384, 113938.
Wang, Y., 2011. Seismic, waveform modelling and tomography, in Encyclopedia of Solid Earth Geophysics, Springer Verlag, pp.1290–1301.
Wang, Y., 2016. Seismic Inversion: Theory and Applications, John Wiley &
Sons.
Wang, Y. & Rao, Y., 2009. Reflection seismic waveform tomography, J.
geophys. Res., 114(B3), B03304.
Weiss, K., Khoshgoftaar, T. M. & Wang, D., 2016. A survey of transfer
learning, J. Big. Data, 3(1), 1–40.
Wu, X., Liang, L., Shi, Y. & Fomel, S., 2019. FaultSeg3D: using synthetic
data sets to train an end-to-end convolutional neural network for 3D
seismic fault segmentation, Geophysics, 84(3), IM35–IM45.
Xu, Z.J., Zhang, Y. & Xiao, Y., 2019. Training behavior of deep neural
network in frequency domain, in International Conference on Neural
Information Processing, pp.264–274.
Yang, Q. & Malcolm, A., 2021. Frequency domain full-waveform inversion in a fluid-saturated poroelastic medium, Geophys. J. Int., 225,
68–84.
Zhu, W. & Beroza, G.C., 2019. PhaseNet: a deep-neural-network-based
seismic arrival-time picking method, Geophys. J. Int., 216, 261–273.

Downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by Universidad Eafit user on 24 August 2024

Li, Y., Alkhalifah, T. & Zhang, Z., 2021. Deep-learning assisted regularized
elastic full waveform inversion using the velocity distribution information
from wells, Geophys. J. Int., 226, 1322–1335.
Liu, D. C. & Nocedal, J., 1989. On the limited memory BFGS method for
large scale optimization, Math. Program., 45, 503–528.
Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R. &
Ng, R., 2020. NeRF: representing scenes as neural radiance fields for view
synthesis, in European Conference on Computer Vision, pp. 405–421.
Moseley, B., Markham, A. & Nissen-Meyer, T., 2020. Solving the wave
equation with physics-informed deep learning, arXiv:2006.11894.
Nemeth, T., Wu, C. & Schuster, G. T., 1999. Least-squares migration of
incomplete reflection data, Geophysics, 64, 208–221.
Paffenholz, J., Stefani, J., McLain, B. & Bishop, K., 2002. Sigsbee 2A
synthetic subsalt dataset-image quality as function of migration algorithm
and velocity model error, in Proceedings of the 64th EAGE Conference
& Exhibition, cp-5-00108.
Pan, S. J. & Yang, Q. 2009. A survey on transfer learning, IEEE Trans.
Knowl. Data Eng., 22, 1345–1359.
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F. &
Courville, A., 2019. On the spectral bias of neural networks, Proceedings
of Machine Learning Research, 97, 5301–5310.
Rao, Y. & Wang, Y., 2017. Seismic waveform tomography with shotencoding using a restarted L-BFGS algorithm, Sci. Rep., 7, 8494.
Rao, Y., Wang, Y. & Han, D., 2019. Seismic waveform tomography with
simplified restarting scheme, IEEE Geosci. Remote Sens. Lett., 16, 135–
139.
Rasht-Behesht, M., Huber, C., Shukla, K. & Karniadakis, G.E., 2022.
Physics-informed neural networks (PINNs) for wave propagation and
full waveform inversions, J. geophys. Res., 127(5), e2021JB023120.
Schwarzenberg-Czerny, A., 1995. On matrix factorization and efficient least
squares solution, Astron. Astrophys. Suppl. Ser., 110, 405.
Shi, Y., Wu, X. & Fomel, S., 2019. SaltSeg: automatic 3D salt segmentation
using a deep convolutional neural network, Interpretation, 7(3), SE113.
Smith, J.D., Azizzadenesheli, K. & Ross, Z.E., 2020. Eikonet: solving the
eikonal equation with deep neural networks, IEEE Trans. Geosci. Remote
Sens., 59, 10 685–10 696.
Smith, J.D., Ross, Z.E., Azizzadenesheli, K. & Muir, J.B., 2022. HypoSVI:
hypocentre inversion with Stein variational inference and physics informed neural networks, Geophys. J. Int., 228, 698–710.
Song, C. & Alkhalifah, T., 2022. Wavefield reconstruction inversion via
physics-informed neural networks, IEEE Trans. Geosci. Remote Sens.,
60, 5908012.
Song, C., Alkhalifah, T. & Waheed, U.B., 2021. Solving the frequencydomain acoustic VTI wave equation using physics-informed neural networks, Geophys. J. Int., 225, 846–859.
Song, C., Alkhalifah, T. & Waheed, U.B., 2022. A versatile framework to
solve the Helmholtz equation using physics-informed neural networks,
Geophys. J. Int., 228, 1750–1762.
Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N.,
Singhal, U., Ramamoorthi, R., Barron, J. & Ng, R., 2020. Fourier features

