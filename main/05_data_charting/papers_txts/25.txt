arXiv:2006.09550v1 [physics.comp-ph] 16 Jun 2020

An encoder-decoder deep surrogate for reverse time
migration in seismic imaging under uncertainty
Rodolfo S. M. Freitasa , Carlos H. S. Barbosaa , Gabriel M. Guerrab , Alvaro L.
G. A. Coutinhoa and Fernando A. Rochinhaa,âˆ—
a COPPE, Federal University of Rio de Janeiro, Rio de Janeiro, Rio de Janeiro, 21941-598, Brazil
b Department of Mechanical Engineering, Federal Fluminense University, NiterÃƒÅŸi, Brazil

ARTICLE INFO
Keywords:
Reverse time migration
Deep Learning
Surrogate Modeling
Uncertainty Quantification

ABSTRACT
Seismic imaging faces challenges due to the presence of several uncertainty
sources. Uncertainties exist in data measurements, source positioning, and
subsurface geophysical properties. Reverse time migration (RTM) is a highresolution depth migration approach useful for extracting information such
as reservoir localization and boundaries. RTM, however, is time-consuming
and data-intensive as it requires computing twice the wave equation to generate and store an imaging condition. RTM, when embedded in an uncertainty
quantification algorithm (like the Monte Carlo method), shows a many-fold
increase in its computational complexity due to the high input-output dimensionality. In this work, we propose an encoder-decoder deep learning surrogate model for RTM under uncertainty. Inputs are an ensemble of velocity
fields, expressing the uncertainty, and outputs the seismic images. We show
by numerical experimentation that the surrogate model can reproduce the
seismic images accurately, and, more importantly, the uncertainty propagation from the input velocity fields to the image ensemble.

1. Introduction
Seismic imaging is employed to delineate the salient geological features of the Earth subsurface. Imaging methods are popular in the Oil & Gas industry as they are designed to be focused on
the more essential characteristics: the horizons bounding the regions of interest. They can also be
used in conjunction with inverse methods such as Full Waveform Inversion [1]. Imaging methods
are designed and built departing from the integration of specialized optical (illuminating) principles and physics-based models describing the wave propagation through heterogeneous media.
A critical aspect arising from such arrangement is the potential computational cost required, as
a large domain is to be illuminated, which implies solving partial differential equations (PDEs)
âˆ— Corresponding

author.
Email:
rodolfosmfreitas@gmail.com (Rodolfo S. M. Freitas)
faro@mecanica.coppe.ufrj.br (Fernando A. Rochinha)
ORCID (s): 0000-0001-6036-8534 (R.S.M. Freitas); 0000-0002-1420-9118 (C.H.S. Barbosa);
0000-0002-5430-3475 (G.M. Guerra); 0000-0002-4764-1142 (A.L.G.A. Coutinho); 0000-0001-8035-9651
(F.A. Rochinha)

R. S. M. Freitas et al.

Page 1 of 26

An encoder-decoder deep surrogate for reverse time migration

associated with the wave models in that area. The situation tends to be more complicated as the
excitation signals bear high-frequency content, which demands very fine grids in space and time.
Such time-consuming tasks often hamper the use of high-fidelity codes constructed upon physicsbased models. That becomes a more critical issue whenever one faces many-query applications
like sensitivity analysis, design, optimization, or uncertainty quantification.
In this work, we develop a machine-learning model to alleviate computational costs to provide
seismic images with quantified uncertainty [2]. In this context, we propose a Monte Carlo method
(MC) to sweep a large ensemble of plausible velocity fields obtained by approximate methods,
and, therefore, prone to uncertainties, to compute an ensemble of images aiming at characterizing
the propagated uncertainties along with the seismic image processing. Moreover, we embed the
MC sampling as an outer loop of a larger computational workflow proposed in [2] and detailed
in Algorithm 1. This algorithm is structured in three sequential stages, enabling a probabilistic
framework for seismic imaging.
The first stage aims at generating plausible subsurface velocity fields honoring seismic data.
Probabilistic inversion, such as Bayesian tomography [2, 3, 4, 5], and stochastic FWI [6, 7, 8, 9, 10]
can provide a velocity field ensemble used as input to the second stage. Hence, in Stage 2, an
imaging technique migrates the seismogram information using each velocity field sample. This
strategy wraps a seismic migration tool into an MC algorithm aiming to build a set of migrated
seismic images. We have chosen the Reverse Time Migration (RTM) as the seismic migration
technique to localize the seismic reflectors in the correct depth location in the subsurface [11]. RTM
is a depth migration approach based on the two-way wave equation, frequently used in industry,
that provides reliable subsurface high-resolution seismic images useful for seismic interpretation
and reservoir characterization [11]. The last stage of the workflow post-processes the RTM seismic
images ensemble, calculating uncertainty maps and extracting features, such as horizons and faults,
that characterizes uncertainty in the resulting images.
Algorithm 1 Workflow for seismic imaging with quantified uncertainty
Input: source signals, seismograms, and spatial domain (raw data).
Output: ensemble of seismic images.
Stage 1: Generate an ensemble of velocity fields:
â€¢ Bayesian inversion with simplified physics-based models
Stage 2: Propagate uncertainties â€“ migrate the seismograms for the velocity field ensemble using
RTM, producing a corresponding ensemble of seismic images
â€¢ Monte Carlo loop over samples produced in Stage 1
Stage 3: Post-process the RTM seismic images
â€¢ Uncertainty maps;
â€¢ Automatic features (horizons) detection;
â€¢ Probabilistic characterization of such features;
Due to its flexibility by design, it is possible to generate different workflow versions, by, for
instance, replacing components within the stages (e.g., different strategies for the uncertain veR. S. M. Freitas et al.

Page 2 of 26

An encoder-decoder deep surrogate for reverse time migration

locity fields estimation in Stage 1) targeting to accommodate different demands or efficiency requirements. Nonetheless, we would still be facing a time-consuming computational task in the
many-query UQ analysis of Stage 2. That is what motivates us to follow a consolidated trend,
replacing the original physics-based model by a cheap-to-evaluate surrogate. Recently, Machine
Learning techniques, like Gaussian Processes [12, 13, 14, 15, 16, 17] and Deep Neural Networks
(DNNs), [18, 19, 20, 21, 22, 23, 24, 25] have deployed efficient surrogates for UQ analysis. Gaussian Processes have achieved considerable success with computer models with inputs and outputs
of moderate dimensionality, including the ability to blend data of different sources, leading to multifidelity approaches [15, 16]. On the other hand, Deep Neural Networks have gained traction due
to their unique profile of being flexible and scalable nonlinear function approximators. Another
aspect worth highlighting is the substantial amount of computer libraries and tools available to
enable their use.
Here, we apply the deep learning surrogate architecture proposed in [18] for systems governed
by PDEs cast as an image-to-image problem. The performance of such architecture was tested in
uncertainty quantification of flows in heterogeneous media [19], extended to semi-supervised learning in [20], and inverse problems in [21], with excellent results. This architecture is composed of
convolutional layers and dense blocks, following an encoder-decoder neural network arrangement
to handle the potential high-dimensionality of inputs and outputs. More specifically, we employ
the deep learning architecture for constructing efficient proxies for RTM imaging by avoiding the
high costs of solving twice a wave propagation equation in a heterogeneous medium. Such surrogates are nonlinear mappings linking the uncertain velocity field to the seismic images. It is
worth to highlight that differently from usual surrogates, we do not replace only a forward solver
associated to a PDE, but the whole more expensive imaging process. The surrogate can handle
the high-dimensional inputs (velocity fields) and outputs (seismic images), leading to cost savings
in processing and memory storage. We demonstrate through two examples that such an approach
enables producing seismic images with quantified uncertainty. Indeed, it can accurately reproduce
the ensemble of images resulting from the MC uncertain propagation with much less computational
effort. Moreover, it uses a limited training data as expected, which was confirmed by our results
and efficiency estimation.
The remainder of this paper is organized as follows. The next section details the RTM mathematical problem along with its computational implementation within Stage 2 MC uncertainty
propagation. Section 3 presents our deep learning, surrogate architecture and training strategy. In
Section 4, we present numerical experiments where we investigate the accuracy, convergence, and
cost-effectiveness of our surrogate model to replace the high-fidelity RTM under uncertainty. The
paper ends with a summary of our main findings.

2. Reverse Time Migration under Uncertainty
RTM is a high-resolution depth migration technique providing useful subsurface images for
extracting information such as reservoir localization and boundaries [11]. The raw data for RTM
consists of recorded seismic signals induced by a seismic source (a shot). The group of seismic
signals represents a seismogram that captures information related to reflections coming from the
subsurface. RTM relies on the two-way wave propagation equation, resulting in an imaging condition (IC) [26] computed over the space-time domain to be imaged. More specifically, the wave
R. S. M. Freitas et al.

Page 3 of 26

An encoder-decoder deep surrogate for reverse time migration

equation is solved twice, the first time to compute the forward-propagated wave due to seismic
sources, followed by the computation of the backward-propagated wave induced by the recorded
seismograms. Both solutions are needed to compute the IC. We calculate the forward wave isotropic
acoustic case by solving,
1 ğœ• 2 ğ‘(r, ğ‘¡)
= ğ‘“ (ğ«ğ‘  , ğ‘¡),
ğ‘£2 (r) ğœ•ğ‘¡2
ğ‘(r, ğ‘¡) = 0 on ğœ•Î©D and îˆ®p(r, t) = 0 on ğœ•Î©inf ,
ğœ•ğ‘(r, 0)
ğ‘(r, 0) = 0 and
= 0, r âˆˆ Î©,
ğœ•ğ‘¡
âˆ‡2 ğ‘(r, ğ‘¡) âˆ’

(1)

where Î© âŠ‚ â„3 denotes the domain to be imaged, ğœ•Î© = ğœ•Î©ğ· âˆª ğœ•Î©ğ‘–ğ‘›ğ‘“ âŠ‚ â„2 is the domain boundary
and ğœ•Î©ğ· and ğœ•Î©ğ‘–ğ‘›ğ‘“ are non-overlapping boundary partitions. ğœ•Î©ğ· is the portion of the boundary
where Dirichlet boundary conditions are applied, representing, for instance, the free-surface. The
operator îˆ® represents the non-reflecting boundary condition [27] applied on ğœ•Î©ğ‘–ğ‘›ğ‘“ . The pressure
(the forward-propagated source wavefield) ğ‘(r, ğ‘¡) is defined at the position r = (ğ‘Ÿğ‘¥ , ğ‘Ÿğ‘¦ , ğ‘Ÿğ‘§ ) âˆˆ Î© and
time ğ‘¡ âˆˆ [0, ğ‘‡ ]. Moreover, ğ‘£(ğ«) is the compressional wave velocity spatial field, and ğ‘“ (ğ«ğ‘  , ğ‘¡) is the
seismic source. The vector ğ«ğ‘  represents the seismic source position. The backward-propagated
wavefield is calculated solving,
Ì„ ğœ)
1 ğœ• 2 ğ‘(r,
= ğ‘ (ğ«ğ‘Ÿ , ğœ),
2
ğ‘£ (r) ğœ•ğœ 2
ğ‘(r,
Ì„ ğœ) = 0 on ğœ•Î©D and îˆ®Ì„p(r, ğœ) = 0 on ğœ•Î©inf
ğœ• ğ‘(r,
Ì„ 0)
ğœ•ğ‘(r, ğ‘‡ )
ğ‘(r,
Ì„ 0) = ğ‘(r, ğ‘‡ ) and
=
, r âˆˆ Î©.
ğœ•ğœ
ğœ•ğœ
âˆ‡2 ğ‘(r,
Ì„ ğœ) âˆ’

(2)

which is an equation similar to (1), but with a different source ğ‘ (ğ«ğ‘Ÿ , ğœ), that is, the recorded signals
at the receivers positioned in ğ«ğ‘Ÿ . Besides, the evolution in Eq. (2) is over the reverse time ğœ = ğ‘‡ âˆ’ ğ‘¡.
Thus, the backward-propagated wavefield ğ‘(r,
Ì„ ğœ) is defined in Î© and ğœ âˆˆ [0, ğ‘‡ ].
The IC dictates the quality and fidelity of the final RTM image. There are several possibilities,
for instance, excitation ICs [28, 29, 30], extend ICs [31, 32, 33], wavefield decomposition ICs [34],
and the zero-lag cross-correlation ICs [11, 35]. We have chosen the zero-lag cross-correlation
between the forward and backward propagated waves at each point in Î©,
ğ‘‡

ğ¼(r) =

âˆ«0

ğ‘(r, ğ‘¡) ğ‘(r,
Ì„ ğœ) ğ‘‘ğ‘¡.

(3)

The IC amplitudes in equation (3) do not provide an explicit physical relationship with the
reflection coefficients. In [35], we find a detailed explanation of the relation between the imaging
condition and the reflection coefficient. Nevertheless, the resulting image provides the correct
amplitude contrast locations of the geological interfaces of rocks with different physical properties
[11]. The amplitude contrast patterns are the main feature of the migrated seismic images explored
R. S. M. Freitas et al.

Page 4 of 26

An encoder-decoder deep surrogate for reverse time migration

in the present work.
We apply an explicit 2ğ‘›ğ‘‘ -order in time and 4ğ‘¡â„ -order in space finite difference numerical scheme
[36] to approximate equations (1) and (2), leading to the vector ğ¯, the discrete version of the velocity
Ì„ ğ¬, ğŸ at each time step. Note that the vectors ğ©, ğ©,
Ì„ and ğ¯ have
field, and similarly the vectors ğ©, ğ©,
the same dimension, that is ğ‘ = ğ‘ğ‘¥ âˆ— ğ‘ğ‘¦ (or ğ‘ = ğ‘ğ‘¥ âˆ— ğ‘ğ‘¦ âˆ— ğ‘ğ‘§ in 3D), where ğ‘ğ‘¥ , ğ‘ğ‘¦
(and ğ‘ğ‘§ ) are the number of grid points in each Cartesian direction. Each discrete seismogram is
a vector of size ğ‘ğ‘Ÿğ‘’ğ‘ âˆ— (ğ‘ğ‘¡ + 1), where ğ‘ğ‘Ÿğ‘’ğ‘ is the number of receivers, and ğ‘ğ‘¡ = ğ‘‡ âˆ•Î”ğ‘¡, with
Î”ğ‘¡ is the time step. The seismic source ğŸ has dimension ğ‘ğ‘¡ . RTM is not only computationally
intensive but also data-intensive due to the high dimensional inputs, the amount of data to manage,
for instance, storing and retrieving ğ©, and the computational costs associated with imposing the
stability and dispersion conditions in the discrete two-way wave equation [11]. The dispersion
relation takes into account the number of grid points per wavelength and the medium properties,
which in our isotropic acoustic case is the P-wave velocity. Thus, complexity in estimating the
migrated image increases with high heterogeneous media and seismic source cutoff frequency.
Least-squares RTM (LSRTM) extends the basic method to an iterative method that minimizes a
data misfit term [26, 37]. However, in the present work, we restrict ourselves to the standard RTM
technique.
As we wrap RTM into a sampling method in Algorithm 1, for taking into consideration the
input uncertainties, the overall computational cost of Stage 2 rises proportionally to ğ‘ğ‘€ğ¶ , the
cardinality of the ensemble of possible velocity fields. Typically, seismic raw data recording sets
are split into multiple steps (ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  ) to cope with the potentially high spatial dimensions to be
covered and to enhance the signal to noise ratio (SNR) in processing stages. Each step covering
fully or partially the domain corresponds to a different arrangement of sources and receivers. It is
essential to mention that RTM iterates over the number of shots producing partial migrated images
characterized by equation (3). When this loop ends, a process called staking sums the partially
migrated seismic images into a single one [38, 39], gathering into this single image all information
related to the seismogram set. Algorithm 2 details the generation of the RTM images ensemble,
where a set of seismograms, {ğ¬1 , â‹… â‹… â‹…, ğ¬ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  }, a set of velocity fields, {ğ¯1 , â‹… â‹… â‹…, ğ¯ğ‘ğ‘€ğ¶ }, and a seismic
source (ğŸ ) are given as inputs. The indexes ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  and ğ‘ğ‘€ğ¶ represent the number of shots and the
number of samples for the MC method. For each MC iteration, we solve the wave equation twice,
one for the seismic source and other for the seismograms associated with it. The computation of
the imaging condition uses both solutions (source wavefield, and receiver wavefield), retrieving
from persistent storage the source wavefield to build the migrated seismic section and stacking
the partial results over time (ğˆâˆ‘ ğ‘¡ ), and over the number of seismograms (ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ ). At the end of
Algorithm 2, we have the discrete imaging condition set {ğˆ1 , ğˆ2 , â‹¯ , ğˆğ‘ğ‘€ğ¶ } where each ğˆğ‘– is a vector
in â„ğ‘ . It is usual to filter each image to sharpen its features. Nevertheless, we do not apply any
filter to the ensemble of seismic images. Summarizing, migrations of seismograms for the set of
velocity fields produce the corresponding set of migrated seismic images, where each one has a
direct relation with one velocity sample.
Different strategies can be pursued in order to make feasible Algorithm 1 by reducing the inherent computational costs of processing and storage. They could rely, for instance, on data compression [40, 41, 42, 43] or more effective stochastic sampling [44], but here, as mentioned before,
our option is for using deep learning surrogates for the RTM imaging, what we describe in the
following section.
R. S. M. Freitas et al.

Page 5 of 26

An encoder-decoder deep surrogate for reverse time migration

Algorithm 2 Reverse Time Migration under Uncertainty.
Require: {ğ¯1 , â‹… â‹… â‹…, ğ¯ğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  }, {ğ¬1 , â‹… â‹… â‹…, ğ¬ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  }, and ğŸ
1: function RTM _ UQ ( vectors {ğ¯1 , â‹… â‹… â‹…, ğ¯ğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  }, vectors {ğ¬1 , â‹… â‹… â‹…, ğ¬ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  }, vector ğŸ )
2:
for ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’_ğ‘–ğ‘‘ = 1 to ğ‘ğ‘€ğ¶ do
3:
read ğ¯ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’_ğ‘–ğ‘‘ , and ğŸ
4:
initialize image condition ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ = 0
5:
for ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ = 1 to ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  do
6:
initialize ğ‘›ğ‘¡ = 0
7:
apply initial conditions for ğ‘–ğ‘¡ = 0
8:
for ğ‘–ğ‘¡ = 1 to ğ‘ğ‘¡ do
9:
ğ‘›ğ‘¡ = ğ‘›ğ‘¡ + ğ‘–ğ‘¡ âˆ— Î”ğ‘¡
10:
solve equation (1)
âŠ³ source wavefield
11:
store ğ©ğ‘›ğ‘¡
12:
end for
13:
initialize ğ‘›ğœ = 0, and ğˆâˆ‘ ğœ = 0
14:
apply initial conditions for ğ‘–ğœ = 0
15:
for ğ‘–ğœ = 1 to ğ‘ğ‘¡ do
16:
ğ‘›ğœ = ğ‘ğ‘¡ âˆ’ (ğ‘›ğœ + ğ‘–ğœ âˆ— Î”ğœ)
âŠ³ reverse time
17:
read ğ©ğ‘›ğœ , and ğ¬ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘
18:
solve equation (2)
âŠ³ receiver wavefield
âˆ‘
âˆ‘
19:
calculate ğˆ ğ‘›ğœ = ğˆ ğ‘›ğœ + ğ©ğ‘›ğœ ğ©Ì„ ğ‘›ğœ
âŠ³ imaging condition
20:
end for
21:
stack ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ = ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ + ğˆâˆ‘ ğ‘›ğœ
âŠ³ stacking
22:
end for
23:
store ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ âˆ€ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’_ğ‘–ğ‘‘ âˆˆ 1 â‰¤ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’_ğ‘–ğ‘‘ â‰¤ ğ‘ğ‘€ğ¶
24:
end for
25: end function

3. Deep Learning Surrogate
The main goal of surrogate models is to replicate the multivariate input-output mapping provided by physical models governed by PDEs to save computational costs. Performing uncertainty
quantification in such conditions is often hampered whenever one faces high-dimensionality, the
so-called curse of dimensionality. As pointed out in the literature, DNNs have proved successful
in such situations by exploiting low-dimensional latent spaces and sophisticated training methods
[45]. We aim to construct and evaluate the performance of DNNs acting as a surrogate model
for the RTM imaging under uncertainty, using as baseline the encoder-decoder architecture proposed by [18] and designed for problems cast as image-to-image regressions. We briefly review
the building blocks of the network in this section.
Figure 1, inspired in [18], provides the big picture of our end-to-end solution, depicting the
main components of the encoder-decoder network. In our particular application, the input for the
deep learning surrogate is the ensemble of heterogeneous velocity fields, and the outputs are the
corresponding imaging conditions given by Eq. (3) for each sample of the velocity field ensemble.
Inputs and outputs are high-dimensional spatial fields, and the surrogate modeling cast as a fieldR. S. M. Freitas et al.

Page 6 of 26

An encoder-decoder deep surrogate for reverse time migration

Figure 1: RTM deep convolutional encoder-decoder network architecture. Inputs: ensemble of velocity
fields ğ¯ âˆˆ â„ğ‘ . Outputs: corresponding ensemble of image conditions ğˆ âˆˆ â„ğ‘ .

to-field regression. This approach is image-inspired. Then, it relies on connecting each pixel of
the input field to an output pixel, where pixels correspond to grid points in the input computational
mesh and output fields. The trained network maps the velocity ğ¯ âˆˆ â„ğ‘ into the IC field ğˆ âˆˆ â„ğ‘ .
The encoder-decoder architecture displayed in Fig. 1 consists of two sequential main phases.
The first is the encoder, wherein dimension reduction occurs, followed by the decoder that allows
expressing the network output with its original dimension. Alternating dense blocks and transition
layers constitute both. Indeed, this architecture merges key characteristics of fully connected with
convolutional networks, suited for the present application. On one side, convolutional networks are
quite effective in dimension reduction [45] and are capable of capturing spatial correlations present
in the input velocity fields. Fully connected layers enhance the information transmitted across the
network, improving the overall efficiency reflected in reasonable sizes of the needed training data
set [46].
Dense-blocks connect all layers directly to each other, helping the training process with the
improvement of information flow and gradients across the network [46]. Inputs of the ğ‘™-th layer
are the concatenated outputs from the previous layers, that is, zğ‘™ = ğ» ğ‘™ ([z0 , z1 , â€¦ , zğ‘™âˆ’1 ]), with zğ‘™
the output of ğ‘™-th layer, and [z0 , z1 , â€¦ , zğ‘™âˆ’1 ] refers to their concatenation, and [0, â€¦ , ğ‘™ âˆ’ 1]. ğ» ğ‘™ is
a non-linear transformation. Here, ğ» ğ‘™ results from applying three consecutive operations, batch
normalization [47] followed by a ReLU [48] and, convolution. The dense-block has two design
parameters, the number of layers ğ¿, and the growth rate, ğ¾, the number of output features of every
single layer. The transition layers here, in the encoder (decoder), are convolutional (deconvolutional) and, therefore, handle the dimension inputs or outputs of dense blocks. As shown in Figure
1, during the encoder phase, the high dimensional velocity fields are immersed in an alternating
series of layers of dense blocks and encoders. The last layer of the encoder phase produces lowdimensional feature maps that characterize the high dimensional field, as shown in the purple maps.
Such maps are immersed in the decoder phase, which is composed of an alternating series of layers
of dense-blocks and decoders, returning ICs to its (high) dimension.
The surrogate ğ  is expressed formally in a compact notation as,
R. S. M. Freitas et al.

Page 7 of 26

An encoder-decoder deep surrogate for reverse time migration

ğ²Ì‚ = ğ (ğ±; ğ°),

(4)

where ğ²Ì‚ is the surrogate prediction (imaging condition, ğˆ) for an input ğ± (that is, a velocity field ğ¯),
and vector ğ° contains the parameters of the neural network. Training the neural network means
learning parameters ğ° using data from the set îˆ° = {(ğ±ğ‘– , ğ²ğ‘– )}, ğ‘– = 1 â‹¯ ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› obtained from simulations with the RTM algorithm, where ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› is the number of samples in the training set. The
stochastic gradient descent algorithm computes the unknown network elements for a given loss
function [49]. We consider for training our surrogate, the following îˆ¸2 regularized mean squared
error (MSE) function,
ğ¿ğ‘€ğ‘†ğ¸ =

1

âˆ‘

ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›

ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ğ‘–=1

â€–ğ²Ì‚ âˆ’ ğ² â€–2 + ğ›¼ Î©(ğ°)
ğ‘– â€–2
â€– ğ‘–

(5)

where ğ²Ì‚ ğ‘– = ğ ğ‘– (ğ±ğ‘– , ğ°). Here the penalty function is given by Î©(ğ°) = 12 ğ°ğ‘‡ ğ° for the îˆ¸2 regularization.
Moreover, the root mean squared error (ğ‘…ğ‘€ğ‘†ğ¸) is used for monitoring the convergence of the
training error. The ğ‘…ğ‘€ğ‘†ğ¸ is given by,
âˆš
âˆš
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
âˆš 1 ğ‘âˆ‘
â€–ğ² âˆ’ ğ²Ì‚ â€–2 .
(6)
ğ‘…ğ‘€ğ‘†ğ¸ = âˆš
ğ‘– â€–2
â€– ğ‘–
ğ‘
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ğ‘–=1

4. Numerical experiments
Here, we present two examples to demonstrate the ability of the encoder-decoder surrogate
in replacing the original two-way wave equation RTM algorithm efficiently. In these numerical
experiments, we mimic Stage 1 outputs of Algorithm 1. That is, we need to generate an ensemble
of velocities. Hence, we assign to the different geological layers random velocity magnitudes for
producing synthetic data to train the neural network and perform the uncertainty analysis. The first
example deals with a medium containing two flat geological layers of constant velocity. We increase
the difficulty for the surrogate in the second example, by employing a more complex medium, in
which the five geological layers are no longer flat, implying in horizontal heterogeneity. The seismic
source term considered in the present work is a Ricker-type wavelet [50].
The encoder-decoder networks are constructed using the open platform Tensorflow [51]. The
Adam optimizer algorithm [52] is employed for parameter learning, considering a weight decay of
1Ã—10âˆ’5 , and an initial learning rate of 1Ã—10âˆ’3 , with a learning rate scheduler, that is used dropping
two times on plateau of the rooted mean squared error. We compute a total of 1300 samples by
considering the velocity magnitude constant in the interior of each geological layer. Therefore,
each velocity field in the ensemble has the form,
ğ¯=

ğ‘›ğ‘™
âˆ‘

ğ‘£ğ‘™ (1 + ğœğ‘™ ğœ‰ğ‘™ ) ğğ‘™

(7)

ğ‘™=1

where ğ‘›ğ‘™ is the number of geological layers, ğ‘£ğ‘™ is the mean velocity within each geological layer, ğœğ‘™ is
the standard deviation, here assumed as 5%, ğœ‰ğ‘™ âˆ¼ ğ•Œ[âˆ’1, 1] is a random variable following a uniform
distribution, and ğğ‘™ is a ğ‘-dimensional vector containing 1 in the components corresponding to the
R. S. M. Freitas et al.

Page 8 of 26

An encoder-decoder deep surrogate for reverse time migration

ğ‘™-th geological layer grid points and 0 otherwise. After that, we apply a moving harmonic average
to ğ¯ with a window length of 20 grid points to mimic the velocity fields computed by parameter
model building techniques like tomography or full-waveform inversion. To analyze the surrogate
training convergence, out of 1300 samples computed with the original RTM model, we create four
training datasets with 200, 400, 600, and 800 samples each. We used the remaining ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ = 500
samples to test the trained network.
Accuracy is measured using the distance between predictions with the surrogate model and
those computed with the RTM original model. To evaluate the surrogate model quality, we consider
the coefficient of determination (ğ‘…2 -score) metric [53]. We define the coefficient of determination
as,
âˆ‘ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ â€–
2
ğ² âˆ’ ğ²Ì‚ ğ‘– â€–
â€–2
ğ‘–=1 â€– ğ‘–
2
ğ‘… = 1 âˆ’ âˆ‘ğ‘
2
ğ‘¡ğ‘’ğ‘ ğ‘¡ â€–
ğ² âˆ’ ğ²â€–
â€–2
ğ‘–=1 â€– ğ‘–

(8)

âˆ‘ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡
where ğ² = ğ‘1
ğ²ğ‘– is the mean of test samples. The ğ‘…2 -score metric represents the normalized
ğ‘–=1
ğ‘¡ğ‘’ğ‘ ğ‘¡
error, allowing the comparison between surrogate models trained by different datasets, with values
close to 1 corresponding to the surrogate models best accuracy. Here, we consider 0.95 a good
target. Also, we intend that the surrogate model returns not only good predictions of seismic images
but also accurate estimations of quantities that characterize the uncertainties in such images. To
measure the degree of uncertainty in the seismic images, we follow the approach proposed in [54].
In their approach, the degree of uncertainty is expressed by a confidence index that represents the
pointwise normalized standard deviation, where low values represent high variabilities and high
values the opposite. The confidence index is,
ğ‘(r) =

ğœğ‘šğ‘ğ‘¥ âˆ’ ğœ(r)
,
ğœğ‘šğ‘ğ‘¥ âˆ’ ğœğ‘šğ‘–ğ‘›

(9)

where ğ‘(r) is the confidence index at position r, and ğœğ‘šğ‘–ğ‘› and ğœğ‘šğ‘ğ‘¥ are the minimum and maximum
field standard deviations, respectively. Another form of measuring the degree of uncertainty is the
coefficient of variation, defined as the pointwise ratio between the standard deviation and the mean,
cğ‘£ (r) =

ğœ(r)
ğœ‡(r)

(10)

where ğœ‡(r) is the expected value at position r.

4.1. A simple geologic scenario: efficiency and convergence analysis
In this first example, designed to evaluate the efficacy and efficiency of the proposed surrogate,
we assume a simple geologic scenario in which two horizontal homogeneous geological layers
separated by a flat horizon parallel to the surface composes the subsurface, as shown in Fig 2.
This domain has 1000 m of depth and 1000 m of lateral extension, where the velocity in the first
geological layer is 3000 m/s, and the velocity in the deeper geological layer is 4500 m/s.
We produce synthetic data using the wave propagation forward solver with the reference velocity field of Fig 2, with a seismic source with cutoff frequency of 30 Hz.In such modeling, we
simulate a fixed split-spread acquisition [38] comprising nine shots, where receivers are positioned
near the surface for each shot and equally spaced of 20 meters. The seismic source is also placed
R. S. M. Freitas et al.

Page 9 of 26

An encoder-decoder deep surrogate for reverse time migration

0

3000
3200
3400

velocity [m/s]

Depth [m]

200
400

3600
3800

600

4000

800
1000

4200
4400
0

200

400 600
Length [m]

800 1000

Figure 2: Simple geologic setup: two horizontal geological layers.
Table 1
RTM numerical parameters.
Parameter

Value

Description (Unit)

â„
Î”ğ‘¡
ğ‘¡ğ‘
ğ‘ğ‘¥ Ã— ğ‘ğ‘¦
ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¥ , ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¦
ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¥ , ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¦

20
2.22 Ã— 10âˆ’3
0.5
50 Ã— 50
([5:5:45], 5)
( [1:1:50], 5)

Spatial discretization (m)
Temporal discretization (s)
Total acquisition time (s)
Number of grid points
Source positions
Receiver positions

near the surface and moved 100 meters for each shot, covering the entire domain with nine shots.
Table 1 shows the parameters used in the numerical modeling of the wave acoustic equations and
the positioning of the seismic source and receivers given by the index ranges [ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¥ , ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¦ ] for the
sources, and [ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¥ , ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¦ ] for the receivers. The grid size and time step respect the numerical dispersion and stability criteria [36].
Table 2 details the neural network architecture. The first layer is convolutional, with kernel
size equals to 4 and stride 2. This first layer captures spatial relations from the velocity input.
The number of features maps after the first convolutional layer is 48. The neural network has 3
dense-blocks with ğ¿ = 4 and ğ¾ = 16. Dense-blocks have a kernel size equals 3, and a stride of 1.
Encoder-decoder layers have a kernel size of 3 and a stride of 2. In the decoding layer, we introduce
a transposed convolution, allowing the expression of the output with its original dimensionality,
equal to the computational grid. A final ReLU layer [48] imposes that the outputs are positive
numbers, naturally constraining the network to output ğ¼ğ¶ > 0 at each grid point. Thus, the neural
network has 218, 425 parameters to be estimated.
Figure 3 shows the decay of the ğ‘…ğ‘€ğ‘†ğ¸ as a function of the number of epochs during the
R. S. M. Freitas et al.

Page 10 of 26

An encoder-decoder deep surrogate for reverse time migration
Table 2
Neural Network Architecture. "Outputs" represents the number of features maps and "Dimension" is
the dimension of the features maps.
Layers

Output

Dimension

Input
Convolution
Dense-block 1
Encoding
Dense-block 2
Decoding 1
Dense-block 3
Decoding 2
ReLU

1
48
112
56
120
60
124
1
1

50 Ã— 50
24 Ã— 24
24 Ã— 24
12 Ã— 12
12 Ã— 12
24 Ã— 24
24 Ã— 24
50 Ã— 50
50 Ã— 50
Train = 200 samples
Train = 400 samples
Train = 600 samples
Train = 800 samples

0.30

RMSE

0.25
0.20
0.15
0.10
0.05
0.00

0

25

50

75 100 125 150 175 200
Epoch

Figure 3: RMSE decay with number of epochs.

training process, for training data sets ranging from 200 to 800 samples. Note that the ğ‘…ğ‘€ğ‘†ğ¸
stabilizes after 150 epochs for all cases and that for smaller data sets, we see higher error values.
Key characteristics one is seeking when replacing the original expensive computational model by a
surrogate are efficiency and accuracy. We estimate efficiency as the ratio between ğ‘ğ‘† , the number
of samples in the training set, and ğ‘ğ‘€ğ¶ , the number of samples of the MC method to achieve
a prescribed level of accuracy (ğ‘…2 â‰¥ 0.95). Thus, we introduce the following index to evaluate
efficiency,
(
)
ğ‘ğ‘†
Efficiency = 1 âˆ’ ğœ‚
Ã— 100
(11)
ğ‘ğ‘€ğ¶
Then, the index in equation (11) represents the percentage of the saved computational costs, and
ğœ‚ is an adjustment factor accounting for the time spent in the construction, training, and making
predictions with the surrogate model. Without loss of generality, we assume for now ğœ‚ = 1.0. For
less optimistic conditions, we recognize that ğœ‚ > 1.0. We calculate the coefficient of determination
ğ‘…2 to assess the accuracy of the neural network with the remaining 500 samples. We observe that
the surrogate model accuracy is good even in small training data scenarios, reaching ğ‘…2 â‰¥ 0.95,
R. S. M. Freitas et al.

Page 11 of 26

An encoder-decoder deep surrogate for reverse time migration
96.5

0.995
0.990

95.5

0.985

R 2-score

Efficiency [%]

96.0

95.0

0.980

94.5

0.975

94.0

0.970

93.5

200 300 400 500 600 700 800
Number of samples for training

Figure 4: Test ğ‘…2 -score and efficiency in function of the number of training samples.

as shown in Fig. 4. Furthermore, Fig. 4 also depicts the surrogate efficiency. Here it is worth
highlighting that 600 RTM runs are necessary to compute the variance with a relative error of
1 Ã— 10âˆ’3 . Thus, we see that the efficiency is higher than 90%, even for the larger data set with 800
samples.
To further illustrate how the surrogate model approximates the predictions of the original model
with good accuracy, Fig. 5 shows comparisons for two realizations chosen randomly from the test
set. We observe that the surrogate presents good results, returning good predictions of the image
condition. We also depict a comparison between the standard deviation, ğœ(ğ«), the confidence index,
ğ‘(ğ«), and the coefficient of variation, ğ‘ğ‘£ (ğ«), predicted by the original and surrogate models with
ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘” = 500 testing samples, see Fig. 6. Besides, we introduce a discrete version of a ğ¿2 relative
error between two spatial fields ğ‘“ , one computed with the RTM and the other by the surrogate as,
1 âˆ‘
=
ğ‘ ğ‘–=1
ğ‘

ğ‘’ğ‘”

2

( ğ‘–
)2
ğ‘”ğ‘…ğ‘‡ ğ‘€ âˆ’ ğ‘”ğ‘ ğ‘–
ğ‘–
ğ‘”ğ‘…ğ‘‡
ğ‘€

(12)

where the subscripts refer to how we compute the field ğ‘”. This index is an average of the pointwise
relative error for all ğ‘ grid points. Figure 5 compares randomly selected seismic images produced
by RTM with the corresponding ones obtained with the deep learning surrogate. The visual resemblance is quantified using equation (12), leading to relative errors that stay below 2%. We extend
further our assessment of the surrogate effectiveness by comparing the uncertainty indexes computed with the two techniques displayed in Fig. (6). For all indexes, the relative errors computed
with equation (12) are less than 1%.

4.2. A non-flat medium with five geological layers
To challenge the encoder-decoder surrogate, we use a synthetic geologic model with five homogeneous layers similar to the one proposed in [55]. The 2D velocity model consists of a water
layer with velocity 1500 m/s, and four mini sedimentary basins with velocities of 2000 m/s, 2500
m/s, 3000 m/s, and 4000 m/s, respectively. Figure 7 display a schematic view of the velocity field
with 1000 m of depth and 1000 m of lateral extension.
Two synthetic seismograms are generated for the velocity fields shown in Fig 7 considering now
the seismic sources with cutoff frequencies of 30 and 45Hz.Table 3 shows the RTM parameters and
R. S. M. Freitas et al.

Page 12 of 26

1.0

0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

Depth [m]

1000

1000

0

0

200

200

400 600
Length [m]

400 600
Length [m]

800 1000

800 1000

Depth [m]

0

Depth [m]

Depth [m]

An encoder-decoder deep surrogate for reverse time migration

0

0

(a) RTM model

200

200

400 600
Length [m]

400 600
Length [m]

800 1000

800 1000

0.0

(b) Surrogate model

Figure 5: Randomly selected samples of seismic images in the test data set computed by the RTM (a)
and the surrogate model (b). The relative errors in the image condition, ğ‘’ğ¼ , are lower than 2%.
Table 3
RTM numerical parameters.
Parameter

ğ‘“ğ‘ğ‘¢ğ‘¡ğ‘œğ‘“ ğ‘“ = 30ğ»ğ‘§

ğ‘“ğ‘ğ‘¢ğ‘¡ğ‘œğ‘“ ğ‘“ = 45ğ»ğ‘§

Description (Unit)

â„
Î”ğ‘¡
ğ‘¡ğ‘
ğ‘ğ‘¥ Ã— ğ‘ğ‘¦
ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¥ , ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¦
ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¥ , ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¦

10
1.25 Ã— 10âˆ’3
2.0
100 Ã— 100
([5:10:95], 5)
( [1:1:100], 5)

6.666
8.333 Ã— 10âˆ’4
2.0
150 Ã— 150
([7:15:142], 7)
([1:1:150], 7)

Spatial discretization (m)
Temporal discretization (s)
Total acquisition time (s)
Number of grid points
Source positions
Receiver positions

the positioning of sources and receivers. For the cutoff frequency of 45Hz, due to the imposition of
the stability and dispersion conditions in the discrete two-way wave equation, there is a significant
increase in the input dimensionality, that bears the potential to hamper the neural network training.
The next sub-sections present results for the scenarios involving the two frequencies of excitation.
R. S. M. Freitas et al.

Page 13 of 26

An encoder-decoder deep surrogate for reverse time migration
0
200

0.15
0.10

800
200

400 600
Length [m]

Depth [m]

1000

800 1000

Standard deviation - RTM model

0.15
0.10
0.05
0

200

400 600
Length [m]

800 1000

Standard deviation - Surrogate model

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

1000

0

200

400 600
Length [m]

800 1000

Confidence index - RTM model
0

0.20
0.15
0.10

1000

0.05
0

200

400 600
Length [m]

800 1000

Coefficient of variation - RTM model

Depth [m]

0.25

800

400 600
Length [m]

800 1000

0.0

0.40
0.35

200

0.30

600

200

0

0.35

400

0

Confidence index - Surrogate model
0.40

200
Depth [m]

0.20

800

0.05
0

0.25

600

Depth [m]

Depth [m]

0.20

600

0.30

400

Depth [m]

0.25

0.35

200

0.30

400

1000

0

0.35

0.30
0.25

400

0.20

600

0.15
0.10

800
1000

0.05
0

200

400 600
Length [m]

800 1000

Coefficient of variation - Surrogate model

Figure 6: UQ indexes - standard deviation, ğœ(ğ«), confidence index, ğ‘(ğ«), and coefficient of variation,
ğ‘ğ‘£ (ğ«) - predicted by the RTM (left) and surrogate models (right). The relative errors to the RTM model
are lower than 1%.

4.2.1. Cutoff frequency - 30Hz
We detail the architecture of the neural network for this scenario in Table 4. It contains five
dense blocks, leading to 412, 210 parameters to be trained. We can see in Figure 8 the ğ‘…ğ‘€ğ‘†ğ¸
decay as the number of epochs increase for all training sets. We verify the accuracy of the surrogate
R. S. M. Freitas et al.

Page 14 of 26

0

1500

200

2000

400

2500
P3

600

P2

800
1000

0

200

velocity [m/s]

Depth [m]

An encoder-decoder deep surrogate for reverse time migration

P1

3000

P4

3500

400 600
Length [m]

800 1000

4000

RMSE

Figure 7: Geologic model with 5 layers adapted from [55]. ğ‘ƒ1 , ğ‘ƒ2 , ğ‘ƒ3 , and ğ‘ƒ4 are control points in the
geological model to compute statistics.
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00

Train = 200 samples
Train = 400 samples
Train = 600 samples
Train = 800 samples

0

25

50

75 100 125 150 175 200
Epoch

Figure 8: RMSE decay with number of epochs.

by computing the ğ‘…2 score for the 500 testing samples. We find that, as expected, for networks
trained with larger data sets, ğ‘…2 values are closer to 1.0, as shown in Figure 9(a). Due to limitations
imposed by the high cost of generating samples using the full RTM model for this example, we
develop a different efficiency analysis extrapolating from the basic conditions used for the network
training. We assume conservatively that ğ‘ğ‘€ğ¶ is of the same order of the case in section 4.1. Thus,
we start from a scenario where only 5,000 samples are needed to characterize uncertainties in
the seismic images and extrapolate to more expensive potential scenarios requiring hypothetically
till 50,000 samples. Here, ğ‘ğ‘† is equal to 1100, 600 samples to train the neural network with an
accuracy of ğ‘…2 â‰¥ 0.95, and 500 to test the surrogate model. Figure 9(b) depicts the efficiency
analysis in function of ğ‘ğ‘€ğ¶ . We note, for the worst scenario, an efficiency of around 78%, and for
the scenarios where ğ‘ğ‘€ğ¶ is higher than 10,000 samples, the efficiencies reach values greater than
90%. For the most expensive scenario, we see an efficiency close to 98%.
Fig. 10 shows comparisons between images randomly selected from the test set and the corresponding images produced with the full RTM model. We observe that the surrogate model returns
R. S. M. Freitas et al.

Page 15 of 26

An encoder-decoder deep surrogate for reverse time migration

Layers

Output

Dimension

Input
Convolution
Dense-block 1
Encoding
Dense-block 2
Encoding
Dense-block 3
Decoding
Dense-block 4
Decoding
Dense-block 5
Decoding
ReLU

1
48
112
56
120
60
124
62
126
63
127
1
1

100 Ã— 100
48 Ã— 48
48 Ã— 48
24 Ã— 24
24 Ã— 24
12 Ã— 12
12 Ã— 12
24 Ã— 24
24 Ã— 24
48 Ã— 48
48 Ã— 48
100 Ã— 100
100 Ã— 100

0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.90

Efficiency [%]

R 2-score

Table 4
Neural Network Architecture. "Outputs" represents the number of features maps and "Dimension" is
the spatial dimension of the features maps.

200

300

400

500 600
Samples

700

(a) ğ‘…2 -score for the trained networks

800

97.5
95.0
92.5
90.0
87.5
85.0
82.5
80.0
77.5

10000

20000 30000 40000
Number of samples for MC

50000

(b) Efficiency

Figure 9: Test ğ‘…2 -score and efficiency in function of the number of MC samples.

excellent predictions of the imaging condition. We can also note that the surrogate model predicts
the UQ figures - standard deviation, ğœ(ğ«), confidence index, ğ‘(ğ«), and coefficient of variation, ğ‘ğ‘£ (ğ«) with good accuracy, as seen in Fig 11, where the relative error between the surrogate predictions and
the RTM model are lower than 6%. Results in Figures 10 and 11 shows that the encoder-decoder
surrogate extrapolates the replication of the IC training targets, delivering remarkable results to
assist in UQ analysis.
Next, we deepen our investigation of the surrogateâ€™s ability to reproduce the probabilistic characterization of the IC fields by plotting the probability density functions (PDFs) of the imaging
condition at control points in the domain, as displayed in Fig. 7. We place the control points in
regions of low and high uncertainties. As the reference solution, we use PDFs obtained by the
RTM model with the 500 test samples to compare the accuracy of the surrogate model trained
R. S. M. Freitas et al.

Page 16 of 26

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

Depth [m]

1000

1000

0

0

0

200

200

200

400 600
Length [m]

400 600
Length [m]

400 600
Length [m]

800 1000

800 1000

800 1000

(a) RTM model

Depth [m]

Depth [m]

1000

Depth [m]

0

Depth [m]

Depth [m]

An encoder-decoder deep surrogate for reverse time migration

0

0

0

200

200

200

400 600
Length [m]

400 600
Length [m]

400 600
Length [m]

800 1000

800 1000

800 1000

0.0

0.0

0.0

(b) Surrogate model

Figure 10: Randomly selected images from the test data set computed by the RTM model (a) and the
surrogate model (b) trained with 600 samples. The relative errors in the image condition, ğ‘’ğ¼ , are lower
than 6%.

with different datasets to estimate the PDFs at the control points. Figure 12 shows the imaging
condition PDFs estimated with the surrogate models trained with 400, 600, 800 samples, together
with the reference PDFs. We observe that the PDFs obtained with the surrogate model are in good
agreement with the reference ones for all control points, particularly at the deeper point (ğ‘ƒ 4).
Now we exemplify a possible surrogate use in the feature extraction and interpretation of seismic images, Stage 3 of Algorithm 1. We provide, using the surrogate, a view of the uncertainties
R. S. M. Freitas et al.

Page 17 of 26

An encoder-decoder deep surrogate for reverse time migration
0

0.35

0

0.35

200

0.30

200

0.30

0.15

800
1000

0

200

400 600
Length [m]

800 1000

0.10

800

0.05

1000

0.10
0

200

400 600
Length [m]

800 1000

0.05

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

1000

0

200

400 600
Length [m]

800 1000

Confidence index - RTM model
0

0.5

600

0.4
0.3
0.2
0

200

400 600
Length [m]

800 1000

400 600
Length [m]

800 1000

0.0

0.1

Coefficient of Variation - RTM model

0.8
0.7

200

0.6

400

1000

200

0

0.7

800

0

Confidence index - Surrogate model
0.8

200
Depth [m]

0.15

Standard deviation - Surrogate model

Depth [m]

Depth [m]

Standard deviation - RTM model

0.20

600

Depth [m]

Depth [m]

0.20

600

0.25

400

Depth [m]

0.25

400

0.6

400

0.5

600

0.4

800

0.2

1000

0.3

0

200

400 600
Length [m]

800 1000

0.1

Coefficient of variation - Surrogate model

Figure 11: UQ indexes - standard deviation, ğœ(ğ«), confidence index, ğ‘(ğ«), and coefficient of variation,
ğ‘ğ‘£ (ğ«) - predicted by the RTM model (left) and the surrogate model (right). The relative errors between
the surrogate predictions to the RTM model for the UQ indexes are less than 6%.

associated with specific seismic targets, the interfaces of geological layers. This view can reveal
how the propagated uncertainties can directly impact the images posterior interpretation. Figure 13
provides the IC mean value and associate confidence bands for the four interfaces. In the right part
of the figure, we give an idea of the uncertainties spatial distribution, having as background a randomly selected image from the ensemble. To promote visual perception, we plotted amplified IC
R. S. M. Freitas et al.

Page 18 of 26

An encoder-decoder deep surrogate for reverse time migration

5

RTM model
Surrogate - 400 samples
Surrogate - 600 samples
Surrogate - 800 samples

8
6

4

PDF

4

PDF

3

2

1

0

2

0.0

0.2

0.4

0.6

0.8

1.0

0

0.0

0.2

0.4

I

ğ‘ƒ1

0.8

1.0

0.6

0.8

1.0

ğ‘ƒ2

5

4

4

3
PDF

PDF

3

2
1

1

0

0.6
I

2

0.0

0.2

0.4

0.6

0.8

1.0

0

0.0

0.2

0.4
I

I

ğ‘ƒ3

ğ‘ƒ4

Figure 12: Comparison between PDFs predicted by the RTM model and the surrogate model.

confidence bands associated with each interface. Those bands reflect the IC value dispersion amid
the image ensemble, and, therefore, might lead to a lack of confidence in the reflector placement.

4.2.2. Cutoff frequency - 45Hz
We now stress the proposed deep learning surrogate by testing its performance in a scenario not
considered for the training, but still focusing on the subsurface geology of Fig.7. The RTM image
comprises a certain number of shots, each designed for illuminating the same subsurface using
different seismic sources conditions. Here we analyze a situation involving a higher excitation
frequency, ğ‘“ = 45ğ»ğ‘§, implying, due to numerical requirements, in the necessity of a finer grid. In
such a case, input and output images have different dimensions compared to the previous scenario.
Still, the intrinsic dimensionality is the same for the input as we are imaging the same velocity
field as before. Instead of seeking for a new architecture, we slightly changed the previous one
by replacing the first and last network layers and adapting the initial convolutional layer to ensure
that an integer number defines the kernel. We do not expect to obtain the surrogateâ€™s optimal
performance by employing such a strategy, but that can be quite useful in practical terms if it works.
Table 5 shows the neural network architecture for the 45 Hz scenario. The network architecture
is the same as in section 4.2.1 with small changes. The first convolutional layer has a kernel size
R. S. M. Freitas et al.

Page 19 of 26

An encoder-decoder deep surrogate for reverse time migration

Confidence bands on the interfaces for ğ‘“ = 30Hz

Confidence bands over image condition, ğ‘“ =
30Hz

Figure 13: IC confidence bands over the interface between geological layers (left), and IC confidence
bands superimposed over a randomly selected image (right) for ğ‘“ = 30 Hz.
Table 5
Neural Network Architecture. "Outputs" represents the number of features maps and "Dimension" is
the dimension of the features maps.
Layers

Output

Dimension

Input
Convolution
Dense-block 1
Encoding
Dense-block 2
Encoding
Dense-block 3
Decoding
Dense-block 4
Decoding
Dense-block 5
Decoding
ReLU

1
48
112
56
120
60
124
62
126
63
127
1
1

150 Ã— 150
72 Ã— 72
72 Ã— 72
36 Ã— 36
36 Ã— 36
18 Ã— 18
18 Ã— 18
36 Ã— 36
36 Ã— 36
72 Ã— 72
72 Ã— 72
150 Ã— 150
150 Ã— 150

equal to 7 and a stride of 2. The total number of parameters in the network is 416, 390. Figure 14
shows ğ‘…ğ‘€ğ‘†ğ¸ decay as a function of the number of epochs in the training process.
We can see in Fig. 15a the ğ‘…2 score for different training sets showing for this more difficult scenario a slight decrease in the neural network quality. Confirming our initial expectations
of a non-optimal but acceptable performance, the coefficients of determination ğ‘…2 for all training
R. S. M. Freitas et al.

Page 20 of 26

An encoder-decoder deep surrogate for reverse time migration
0.40

Train = 200 samples
Train = 400 samples
Train = 600 samples
Train = 800 samples

0.35

RMSE

0.30
0.25
0.20
0.15
0.10
0.05
0

25

50

75 100 125 150 175 200
Epoch

100

0.88
0.86
0.84
0.82
0.80
0.78
0.76
0.74

90
Efficiency [%]

R 2-score

Figure 14: RMSE decay with number of epochs.

80
70

Î· = 1.0
Î· = 1.5
Î· = 2.0
Î· = 2.5
Î· = 3.0

60
50
40

200

300

400

500 600
Samples

700

800

(a) ğ‘…2 -score for the trained networks

10000

20000 30000 40000
Number of samples for MC

50000

(b) Efficiency

Figure 15: ğ‘…2 -score and efficiency for the trained networks.

datasets are lower than 0.90. Moreover, we estimate the efficiency of the surrogate model in the
same manner as in the previous case. However, here we consider values for the adjustment factor
ğœ‚ > 1.0. More precisely, the adjustment factor tries to estimate the time spent in search of the
neural network hyperparameters to optimize the surrogate model accuracy and to generate larger
training sets. Without loss of generality, we assume that the number of samples ğ‘ğ‘† to train the
neural network is equal to 1100, 600 to train, and 500 to test the surrogate model. Figure 15(b)
shows the efficiency in function of ğ‘ğ‘€ğ¶ , for several adjustment factors. Note that for scenarios
with ğ‘ğ‘€ğ¶ â‰¤ 10, 000, the efficiency drops significantly for higher adjustment factors. However, for
scenarios where ğ‘ğ‘€ğ¶ â‰¥ 20, 000 samples the efficiency reaches values close to 80-90%. For scenarios where ğ‘ğ‘€ğ¶ â‰¥ 40, 000 we observe an efficiency close to 90% even for the higher adjustment
factor.
Despite the lower accuracy presented in this scenario, the surrogate model could reach satisfactory predictions of the imaging condition, as we can see in Fig. 16. In this Figure, we show
the three randomly selected images from the test data set computed by the RTM model and the
surrogate model. Note, however, that the image produced by the RTM model may not be the best
image we can compute for these conditions. The grid is adjusted only to satisfy the stability and
dispersion criteria for the 45 Hz cutoff frequency. We do not optimize the domain size for a proper
R. S. M. Freitas et al.

Page 21 of 26

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

Depth [m]

1000

1000

0

0

0

200

200

200

400 600
Length [m]

400 600
Length [m]

400 600
Length [m]

800 1000

800 1000

800 1000

(a) RTM model

Depth [m]

Depth [m]

1000

Depth [m]

0

Depth [m]

Depth [m]

An encoder-decoder deep surrogate for reverse time migration

0

0

0

200

200

200

400 600
Length [m]

400 600
Length [m]

400 600
Length [m]

800 1000

800 1000

800 1000

0.0

0.0

0.0

(b) Surrogate model

Figure 16: Randomly selected images from the test data set computed by the RTM model (a) and the
surrogate model (b) trained with 600 samples. The relative errors ğ‘’ğ¼ = are lower than 10%.

representation of the non-reflecting boundary conditions and source/receiver arrangement. Furthermore, Figure 17 shows a comparison between the standard deviation, ğœ(ğ«), confidence index,
ğ‘(ğ«), and coefficient of variation, ğ‘ğ‘£ (ğ«), computed by the RTM and surrogate models. We observe
that the surrogate model predicts the UQ indexes with satisfactory accuracy. The relative errors
between the surrogate predictions to the RTM model for the UQ indexes are lower than 6%.
We now investigate the probability density functions (PDFs) of the imaging condition at the
control points in Fig. 7. We use again as reference solution PDFs obtained by the RTM model
R. S. M. Freitas et al.

Page 22 of 26

An encoder-decoder deep surrogate for reverse time migration
0.35

0.30

400

0.25

400

0.20
0.15

1000

Depth [m]

200

800
0

200

400 600
Length [m]

800 1000

0.05

0.25
0.20

600

0.15

800

0.10

Standard deviation - RTM model

1000

0.10
0

200

400 600
Length [m]

800 1000

0.05

Standard deviation - Surrogate model

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

1000

0

200

400 600
Length [m]

800 1000

Confidence index - RTM model

Depth [m]

Depth [m]

0.35

0.30

600

Depth [m]

0

200

0

200

400 600
Length [m]

800 1000

0.0

Confidence index - Surrogate model

0

0.8

0

0.8

200

0.7

200

0.7

400

0.5

0.6

400

0.5
0.4

600

1000

0

200

400 600
Length [m]

800 1000

0.6

600

0.4

0.2

800

0.2

0.1

1000

0.3

800

Depth [m]

Depth [m]

0

Coefficient of variation - RTM model

0.3

0

200

400 600
Length [m]

800 1000

0.1

Coefficient of Variation - Surrogate model

Figure 17: UQ indexes - standard deviation, ğœ(ğ«), confidence index, ğ‘(ğ«), and coefficient of variation,
ğ‘ğ‘£ (ğ«) - predicted by the RTM model (left) and the surrogate model (right). The relative errors between
the surrogate predictions to the RTM model for the UQ indexes are lower than 6%.

with 500 test samples to verify the accuracy of the surrogate models trained with different datasets
to estimate the PDFs at the control points. Figure 18 depicts the imaging condition PDFs at the
control points estimated by the surrogate model trained with 200, 400, 600, 800 samples, together
with the reference PDFs. We observe that the PDFs obtained with the surrogate model capture well
the reference PDFs in all control points, particularly for large training datasets.
R. S. M. Freitas et al.

Page 23 of 26

An encoder-decoder deep surrogate for reverse time migration

10

RTM model
Surrogate - 400 samples
Surrogate - 600 samples
Surrogate - 800 samples

6

4
3
PDF

PDF

8

2

4

1

2
0

0.0

0.2

0.4

0.6

0.8

0

1.0

0.0

0.2

0.4

ğ‘ƒ1

0.8

1.0

0.6

0.8

1.0

ğ‘ƒ2

4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0

PDF

PDF

6
5
4
3
2
1
0

0.6
I

I

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

I

ğ‘ƒ3

I

ğ‘ƒ4

Figure 18: Comparison between PDFs predicted by the RTM model and the surrogate model.

5. Conclusions
We propose a deep learning model based on an encoder-decoder architecture to replace the
costly RTM technique on producing seismic images. This approach naturally fits the framework
of a computational workflow to produce seismic images with quantified uncertainty in [2]. This
surrogate model builds a scalable imageâ€“toâ€“image mapping, coping with the high dimensionality
of both the heterogeneous velocity fields that serve as inputs and images outputs. Such surrogate
has revealed to be very efficient in the context of UQ many-query tasks, as demonstrated by our
numerical examples. Indeed, that was observed even in cases where we employ a non-optimal
neural network architecture.
We place our contribution in the emerging area of physics-informed machine learning, where
the final model, in many different ways, blends two main components: often expensive computational models relying on first principles and phenomenological closure equations, and machine
learning data-driven tools. Such combination not only suits perfectly to the needs required by the
workflow mentioned earlier but also offers a broad spectrum of opportunities to improve performance, like employing more powerful training strategies and automatic hyperparameters optimization.

R. S. M. Freitas et al.

Page 24 of 26

An encoder-decoder deep surrogate for reverse time migration

Acknowledgements
This study was financed in part by CAPES, Brasil Finance Code 001. This work is also partially
supported by FAPERJ (grant E-26/203.018/2017), CNPq (grant 302489/2016-9), and Petrobras
(grant 2017/00148-9).

References
[1] Yu Chen, Kai Gao, Eric S. Davis, Dipen N. Sinha, Cristian Pantea, and Lianjie Huang. Full-waveform inversion and least-squares reversetime migration imaging of collimated ultrasonic-beam data for high-resolution wellbore integrity monitoring. Applied Physics Letters,
113(7):071903, 2018.
[2] Carlos HS Barbosa, Liliane NO Kunstmann, RÃ´mulo M Silva, Charlan DS Alves, Bruno S Silva, Marta Mattoso, Fernando A Rochinha,
Alvaro LGA Coutinho, et al. A workflow for seismic imaging with quantified uncertainty. arXiv preprint arXiv:2001.06444, 2020.
[3] A. Botero, A Gesret, T. Romary, M. Noble, and C. Maisons. Stochastic seismic tomography by interacting markov chains. Geophysical
Journal International, 207:374â€“392, 2016.
[4] J. Belhadj, T. Romary, A. Gesret, M. Noble, , and B. Figliuzzi. New parametrizations for bayesian seismic tomography. Inverse Problems,
34:33, 2018.
[5] N. Brantut. Time-resolved tomography using acoustic emissions in the laboratory, and application to sandstone compaction. Geophysical
Journal International, 213:2177â€“2192, 2018.
[6] James Martin, Lucas C Wilcox, Carsten Burstedde, and Omar Ghattas. A stochastic newton mcmc method for large-scale statistical inverse
problems with application to seismic inversion. SIAM Journal on Scientific Computing, 34(3):A1460â€“A1487, 2012.
[7] Hejun Zhu, Siwei Li, Sergey Fomel, Georg Stadler, and Omar Ghattas. A bayesian approach to estimate uncertainty for full-waveform
inversion using a priori information from depth migration. Geophysics, 81(5):R307â€“R323, 2016.
[8] Reetam Biswas and Mrinal Sen. 2d full-waveform inversion and uncertainty estimation using the reversible jump hamiltonian monte carlo.
In SEG Technical Program Expanded Abstracts 2017, pages 1280â€“1285. Society of Exploration Geophysicists, 2017.
[9] Lars Gebraad, Christian Boehm, and Andreas Fichtner. Bayesian elastic full-waveform inversion using hamiltonian monte carlo. EarthArXiv,
page qftn5, 2019.
[10] Zeyu Zhao and Mrinal K Sen. A gradient based mcmc method for fwi and uncertainty analysis. In SEG Technical Program Expanded Abstracts
2019, pages 1465â€“1469. Society of Exploration Geophysicists, 2019.
[11] H-W. Zhou, H. Hu, Z. Zou, Y. Wo, and O. Youn. Reverse time migration: A prospect of seismic imaging methodology. Earth-Science
Reviews, 179:207â€“227, 2018.
[12] C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. 2006.
[13] Ilias Bilionis and Nicholas Zabaras. Multi-output local gaussian process regression: Applications to uncertainty quantification. Journal of
Computational Physics, 231(17):5718 â€“ 5746, 2012.
[14] Ilias Bilionis, Nicholas Zabaras, Bledar A Konomi, and Guang Lin. Multi-output separable gaussian process: Towards an efficient, fully
bayesian paradigm for uncertainty quantification. Journal of Computational Physics, 241:212â€“239, 2013.
[15] L. Parussini, D. Venturi, P. Perdikaris, and G.E. Karniadakis. Multi-fidelity gaussian process regression for prediction of random fields.
Journal of Computational Physics, 337:36â€“50, 2017.
[16] P. Perdikaris, D. Venturi, and G. E. Karniadakis. Multi-fidelity information fusion algorithms for high-dimensional systems and massive data
sets. SIAM J. Sci. Comput., 38:B521 â€“ B538, 2016.
[17] G. S. H. Pau, Y. Zhang, and S. Finsterle. Reduced order models for many-query subsurface flow applications. Computational Geosciences,
17:705â€“721, 2013.
[18] Y. Zhu and N. Zabaras. Bayesian deep convolutional encoder-decoder networks for surrogate modeling and uncertainty quantification. Journal
of Computational Physics, 366:415â€“447, 2018.
[19] S. Mo, Y. Zhu, N. Zabaras, X. Shi, and J. Wu. Deep convolutional encoderÃ¢Ä‚Å˜decoder networks for uncertainty quantification of dynamic
multiphase flow in heterogeneous media. Water Resources Research, 55:703â€“728, 2019.
[20] Y. Zhu, N. Zabaras, P.S. Koutsourelakis, and P. Perdikaris. Physics-constrained deep learning for high-dimensional surrogate modeling and
uncertainty quantification without labeled data. Journal of Computational Physics, 394:56â€“81, 2019.
[21] S. Mo, N. Zabaras, X. Shi, and J. Wu. Integration of adversarial autoencoders with residual dense convolutional networks for estimation of
nonÃ¢Ä‚Å˜gaussian hydraulic conductivities. Water Resources Research, 56:e2019WR026082, 2020.
[22] Y. Yang and P. Perdikaris. Conditional deep surrogate models for stochastic, high-dimensional, andmulti-fidelity systems. Computational
Mechanics, 64:417â€“434, 2019.
[23] M. Tang, Y. Liu, and L. J.Durlofsky. A deep-learning-based surrogate model for data assimilation in dynamic subsurface flow problems.
Journal of Computational Physics, 413:109456, 2020.
[24] Adar Kahana, Eli Turkel, Shai Dekel, and Dan Givoli. Obstacle segmentation based on the wave equation and deep learning. Journal of
Computational Physics, 413:109458, 2020.
[25] Jian Sun, Zhan Niu, Kristopher A. Innanen, Junxiao Li, , and Daniel O. Trad. A theory-guided deep-learning formulation and optimization
of seismic waveform inversion. Geophysics, 85(2):R87â€“R99, 2020.
[26] G. T. Schuster. Seismic inversion. Society Exploration Geophysicists, 1 edition, 2017.
[27] Charles Cerjan, Dan Kosloff, Ronnie Kosloff, and Moshe Reshef. A nonreflecting boundary condition for discrete acoustic and elastic wave
equations. Geophysics, 50(4):705â€“708, 1985.

R. S. M. Freitas et al.

Page 25 of 26

An encoder-decoder deep surrogate for reverse time migration
[28] Wen-Fong Chang and George A McMechan. Reverse-time migration of offset vertical seismic profiling data using the excitation-time imaging
condition. Geophysics, 51(1):67â€“84, 1986.
[29] Wen-Fong Chang and George A McMechan. Elastic reverse-time migration. Geophysics, 52(10):1365â€“1375, 1987.
[30] Bao D Nguyen and George A McMechan. Excitation amplitude imaging condition for prestack reverse-time migration. Geophysics,
78(1):S37â€“S46, 2013.
[31] Paul Sava and Sergey Fomel. Time-shift imaging condition in seismic migration. Geophysics, 71(6):S209â€“S217, 2006.
[32] Bin Wang, Chuck Mason, Manhong Guo, Kwangjin Yoon, Jun Cai, Jean Ji, and Zhiming Li. Subsalt velocity update and composite imaging
using reverse-time-migration based delayed-imaging-time scan. Geophysics, 74(6):WCA159â€“WCA166, 2009.
[33] Paul Sava and Ivan Vasconcelos. Extended imaging conditions for wave-equation migration. Geophysical Prospecting, 59(1):35â€“55, 2011.
[34] Faqi Liu, Guanquan Zhang, Scott A Morton, and Jacques P Leveille. An effective imaging condition for reverse-time migration using wavefield
decomposition. Geophysics, 76(1):S29â€“S39, 2011.
[35] Sandip Chattopadhyay and George A McMechan. Imaging conditions for prestack reverse-time migration. Geophysics, 73(3):S81â€“S89, 2008.
[36] John C. Strikwerda. Finite Difference Schemes and Partial Differential Equations, Second Edition. SIAM: Society for Industrial and Applied
Mathematics.
[37] Toktam Zand, Hamid R. Siahkoohi, Alison Malcolm, Ali Gholami, and Alan Richardson. Consensus optimization of total variationâ€“based
reverse time migration. Computational Geosciences, 2020.
[38] Philip Kearey, Michael Brooks, and Ian Hill. An introduction to geophysical exploration. John Wiley & Sons, 2013.
[39] Ã–z Yilmaz. Seismic data analysis: Processing, inversion, and interpretation of seismic data. Society of exploration geophysicists, 2001.
[40] Lanshu Bai, Huiyi Lu, and Yike Liu. High-efficiency observations: compressive sensing and recovery of seismic waveform data. Pure and
Applied Geophysics, 177(1):469â€“485, 2020.
[41] Navjot Kukreja, Jan Huckelheim, Mathias Louboutin, Kaiyuan Hou, Fabio Luporini, Paul Hovland, and Gerard Gorman. Combining checkpointing and data compression for large scale seismic inversion. arXiv preprint arXiv:1810.05268, 2018.
[42] Peter Lindstrom, Po Chen, and En-Jui Lee. Reducing disk storage of full-3d seismic waveform tomography (f3dt) through lossy online
compression. Computers & Geosciences, 93:45â€“54, 2016.
[43] Philipp A Witte, Mathias Louboutin, Fabio Luporini, Gerard J Gorman, and Felix J Herrmann. Compressive least-squares migration with
on-the-fly fourier transforms. Geophysics, 84(5):R655â€“R672, 2019.
[44] Marco Ballesio, Joakim Beck, Anamika Pandey, Laura Parisi, Erik von Schwerin, and RaÃºl Tempone. Multilevel monte carlo acceleration of
seismic wave propagation under uncertainty. GEM-International Journal on Geomathematics, 10(1):22, 2019.
[45] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.
[46] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2017.
[47] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In
Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICMLÃ¢Ä‚Å¹15, page
448Ã¢Ä‚Å456. JMLR.org, 2015.
[48] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Geoffrey Gordon, David Dunson, and Miroslav
DudÃƒÅ‹k, editors, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings
of Machine Learning Research, pages 315â€“323, Fort Lauderdale, FL, USA, 11â€“13 Apr 2011. PMLR.
[49] F. Chollet. Deep Learning with Python. Manning Publications Company, 2017.
[50] Norman Ricker. The form and laws of propagation of seismic wavelets. Geophysics, 18(1):10â€“40, 01 1953.
[51] MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean,
Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz
Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion ManÃ©, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda ViÃ©gas, Oriol
Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015. Software available from tensorflow.org.
[52] P. Kingma Diederik and Ba Jimmy. Adam: A method for stochastic optimization. preprint arXiv:1412.6980, 2014.
[53] Sanford Weisberg. Applied Linear Regression. John Wiley & Sons, Inc., 2005.
[54] Y. Li and J. Sun. 3d magnetization inversion using fuzzy c-means clustering with application to geology differentiation. Geophysics, 81:J61â€“
J78, 2016.
[55] T. Huang, Y. Zhang, and H. Zhang. The benefit of tti reverse time migration for subsalt imaging, gulf of mexico. 2009.

R. S. M. Freitas et al.

Page 26 of 26

