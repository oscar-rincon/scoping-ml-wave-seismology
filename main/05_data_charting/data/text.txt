arxiv:2006.09550v1 [physics.comp-ph] 16 jun 2020

an encoder-decoder deep surrogate for reverse time
migration in seismic imaging under uncertainty
rodolfo s. m. freitasa , carlos h. s. barbosaa , gabriel m. guerrab , alvaro l.
g. a. coutinhoa and fernando a. rochinhaa,âˆ—
a coppe, federal university of rio de janeiro, rio de janeiro, rio de janeiro, 21941-598, brazil
b department of mechanical engineering, federal fluminense university, niterÃ£ÅŸi, brazil

article info
keywords:
reverse time migration
deep learning
surrogate modeling
uncertainty quantification

abstract
seismic imaging faces challenges due to the presence of several uncertainty
sources. uncertainties exist in data measurements, source positioning, and
subsurface geophysical properties. reverse time migration (rtm) is a highresolution depth migration approach useful for extracting information such
as reservoir localization and boundaries. rtm, however, is time-consuming
and data-intensive as it requires computing twice the wave equation to generate and store an imaging condition. rtm, when embedded in an uncertainty
quantification algorithm (like the monte carlo method), shows a many-fold
increase in its computational complexity due to the high input-output dimensionality. in this work, we propose an encoder-decoder deep learning surrogate model for rtm under uncertainty. inputs are an ensemble of velocity
fields, expressing the uncertainty, and outputs the seismic images. we show
by numerical experimentation that the surrogate model can reproduce the
seismic images accurately, and, more importantly, the uncertainty propagation from the input velocity fields to the image ensemble.

1. introduction
seismic imaging is employed to delineate the salient geological features of the earth subsurface. imaging methods are popular in the oil & gas industry as they are designed to be focused on
the more essential characteristics: the horizons bounding the regions of interest. they can also be
used in conjunction with inverse methods such as full waveform inversion [1]. imaging methods
are designed and built departing from the integration of specialized optical (illuminating) principles and physics-based models describing the wave propagation through heterogeneous media.
a critical aspect arising from such arrangement is the potential computational cost required, as
a large domain is to be illuminated, which implies solving partial differential equations (pdes)
âˆ— corresponding

author.
email:
rodolfosmfreitas@gmail.com (rodolfo s. m. freitas)
faro@mecanica.coppe.ufrj.br (fernando a. rochinha)
orcid (s): 0000-0001-6036-8534 (r.s.m. freitas); 0000-0002-1420-9118 (c.h.s. barbosa);
0000-0002-5430-3475 (g.m. guerra); 0000-0002-4764-1142 (a.l.g.a. coutinho); 0000-0001-8035-9651
(f.a. rochinha)

r. s. m. freitas et al.

page 1 of 26

an encoder-decoder deep surrogate for reverse time migration

associated with the wave models in that area. the situation tends to be more complicated as the
excitation signals bear high-frequency content, which demands very fine grids in space and time.
such time-consuming tasks often hamper the use of high-fidelity codes constructed upon physicsbased models. that becomes a more critical issue whenever one faces many-query applications
like sensitivity analysis, design, optimization, or uncertainty quantification.
in this work, we develop a machine-learning model to alleviate computational costs to provide
seismic images with quantified uncertainty [2]. in this context, we propose a monte carlo method
(mc) to sweep a large ensemble of plausible velocity fields obtained by approximate methods,
and, therefore, prone to uncertainties, to compute an ensemble of images aiming at characterizing
the propagated uncertainties along with the seismic image processing. moreover, we embed the
mc sampling as an outer loop of a larger computational workflow proposed in [2] and detailed
in algorithm 1. this algorithm is structured in three sequential stages, enabling a probabilistic
framework for seismic imaging.
the first stage aims at generating plausible subsurface velocity fields honoring seismic data.
probabilistic inversion, such as bayesian tomography [2, 3, 4, 5], and stochastic fwi [6, 7, 8, 9, 10]
can provide a velocity field ensemble used as input to the second stage. hence, in stage 2, an
imaging technique migrates the seismogram information using each velocity field sample. this
strategy wraps a seismic migration tool into an mc algorithm aiming to build a set of migrated
seismic images. we have chosen the reverse time migration (rtm) as the seismic migration
technique to localize the seismic reflectors in the correct depth location in the subsurface [11]. rtm
is a depth migration approach based on the two-way wave equation, frequently used in industry,
that provides reliable subsurface high-resolution seismic images useful for seismic interpretation
and reservoir characterization [11]. the last stage of the workflow post-processes the rtm seismic
images ensemble, calculating uncertainty maps and extracting features, such as horizons and faults,
that characterizes uncertainty in the resulting images.
algorithm 1 workflow for seismic imaging with quantified uncertainty
input: source signals, seismograms, and spatial domain (raw data).
output: ensemble of seismic images.
stage 1: generate an ensemble of velocity fields:
â€¢ bayesian inversion with simplified physics-based models
stage 2: propagate uncertainties â€“ migrate the seismograms for the velocity field ensemble using
rtm, producing a corresponding ensemble of seismic images
â€¢ monte carlo loop over samples produced in stage 1
stage 3: post-process the rtm seismic images
â€¢ uncertainty maps;
â€¢ automatic features (horizons) detection;
â€¢ probabilistic characterization of such features;
due to its flexibility by design, it is possible to generate different workflow versions, by, for
instance, replacing components within the stages (e.g., different strategies for the uncertain ver. s. m. freitas et al.

page 2 of 26

an encoder-decoder deep surrogate for reverse time migration

locity fields estimation in stage 1) targeting to accommodate different demands or efficiency requirements. nonetheless, we would still be facing a time-consuming computational task in the
many-query uq analysis of stage 2. that is what motivates us to follow a consolidated trend,
replacing the original physics-based model by a cheap-to-evaluate surrogate. recently, machine
learning techniques, like gaussian processes [12, 13, 14, 15, 16, 17] and deep neural networks
(dnns), [18, 19, 20, 21, 22, 23, 24, 25] have deployed efficient surrogates for uq analysis. gaussian processes have achieved considerable success with computer models with inputs and outputs
of moderate dimensionality, including the ability to blend data of different sources, leading to multifidelity approaches [15, 16]. on the other hand, deep neural networks have gained traction due
to their unique profile of being flexible and scalable nonlinear function approximators. another
aspect worth highlighting is the substantial amount of computer libraries and tools available to
enable their use.
here, we apply the deep learning surrogate architecture proposed in [18] for systems governed
by pdes cast as an image-to-image problem. the performance of such architecture was tested in
uncertainty quantification of flows in heterogeneous media [19], extended to semi-supervised learning in [20], and inverse problems in [21], with excellent results. this architecture is composed of
convolutional layers and dense blocks, following an encoder-decoder neural network arrangement
to handle the potential high-dimensionality of inputs and outputs. more specifically, we employ
the deep learning architecture for constructing efficient proxies for rtm imaging by avoiding the
high costs of solving twice a wave propagation equation in a heterogeneous medium. such surrogates are nonlinear mappings linking the uncertain velocity field to the seismic images. it is
worth to highlight that differently from usual surrogates, we do not replace only a forward solver
associated to a pde, but the whole more expensive imaging process. the surrogate can handle
the high-dimensional inputs (velocity fields) and outputs (seismic images), leading to cost savings
in processing and memory storage. we demonstrate through two examples that such an approach
enables producing seismic images with quantified uncertainty. indeed, it can accurately reproduce
the ensemble of images resulting from the mc uncertain propagation with much less computational
effort. moreover, it uses a limited training data as expected, which was confirmed by our results
and efficiency estimation.
the remainder of this paper is organized as follows. the next section details the rtm mathematical problem along with its computational implementation within stage 2 mc uncertainty
propagation. section 3 presents our deep learning, surrogate architecture and training strategy. in
section 4, we present numerical experiments where we investigate the accuracy, convergence, and
cost-effectiveness of our surrogate model to replace the high-fidelity rtm under uncertainty. the
paper ends with a summary of our main findings.

2. reverse time migration under uncertainty
rtm is a high-resolution depth migration technique providing useful subsurface images for
extracting information such as reservoir localization and boundaries [11]. the raw data for rtm
consists of recorded seismic signals induced by a seismic source (a shot). the group of seismic
signals represents a seismogram that captures information related to reflections coming from the
subsurface. rtm relies on the two-way wave propagation equation, resulting in an imaging condition (ic) [26] computed over the space-time domain to be imaged. more specifically, the wave
r. s. m. freitas et al.

page 3 of 26

an encoder-decoder deep surrogate for reverse time migration

equation is solved twice, the first time to compute the forward-propagated wave due to seismic
sources, followed by the computation of the backward-propagated wave induced by the recorded
seismograms. both solutions are needed to compute the ic. we calculate the forward wave isotropic
acoustic case by solving,
1 ğœ• 2 ğ‘(r, ğ‘¡)
= ğ‘“ (ğ«ğ‘  , ğ‘¡),
ğ‘£2 (r) ğœ•ğ‘¡2
ğ‘(r, ğ‘¡) = 0 on ğœ•Ï‰d and îˆ®p(r, t) = 0 on ğœ•Ï‰inf ,
ğœ•ğ‘(r, 0)
ğ‘(r, 0) = 0 and
= 0, r âˆˆ Ï‰,
ğœ•ğ‘¡
âˆ‡2 ğ‘(r, ğ‘¡) âˆ’

(1)

where Ï‰ âŠ‚ â„3 denotes the domain to be imaged, ğœ•Ï‰ = ğœ•Ï‰ğ· âˆª ğœ•Ï‰ğ‘–ğ‘›ğ‘“ âŠ‚ â„2 is the domain boundary
and ğœ•Ï‰ğ· and ğœ•Ï‰ğ‘–ğ‘›ğ‘“ are non-overlapping boundary partitions. ğœ•Ï‰ğ· is the portion of the boundary
where dirichlet boundary conditions are applied, representing, for instance, the free-surface. the
operator îˆ® represents the non-reflecting boundary condition [27] applied on ğœ•Ï‰ğ‘–ğ‘›ğ‘“ . the pressure
(the forward-propagated source wavefield) ğ‘(r, ğ‘¡) is defined at the position r = (ğ‘Ÿğ‘¥ , ğ‘Ÿğ‘¦ , ğ‘Ÿğ‘§ ) âˆˆ Ï‰ and
time ğ‘¡ âˆˆ [0, ğ‘‡ ]. moreover, ğ‘£(ğ«) is the compressional wave velocity spatial field, and ğ‘“ (ğ«ğ‘  , ğ‘¡) is the
seismic source. the vector ğ«ğ‘  represents the seismic source position. the backward-propagated
wavefield is calculated solving,
Ì„ ğœ)
1 ğœ• 2 ğ‘(r,
= ğ‘ (ğ«ğ‘Ÿ , ğœ),
2
ğ‘£ (r) ğœ•ğœ 2
ğ‘(r,
Ì„ ğœ) = 0 on ğœ•Ï‰d and îˆ®Ì„p(r, ğœ) = 0 on ğœ•Ï‰inf
ğœ• ğ‘(r,
Ì„ 0)
ğœ•ğ‘(r, ğ‘‡ )
ğ‘(r,
Ì„ 0) = ğ‘(r, ğ‘‡ ) and
=
, r âˆˆ Ï‰.
ğœ•ğœ
ğœ•ğœ
âˆ‡2 ğ‘(r,
Ì„ ğœ) âˆ’

(2)

which is an equation similar to (1), but with a different source ğ‘ (ğ«ğ‘Ÿ , ğœ), that is, the recorded signals
at the receivers positioned in ğ«ğ‘Ÿ . besides, the evolution in eq. (2) is over the reverse time ğœ = ğ‘‡ âˆ’ ğ‘¡.
thus, the backward-propagated wavefield ğ‘(r,
Ì„ ğœ) is defined in Ï‰ and ğœ âˆˆ [0, ğ‘‡ ].
the ic dictates the quality and fidelity of the final rtm image. there are several possibilities,
for instance, excitation ics [28, 29, 30], extend ics [31, 32, 33], wavefield decomposition ics [34],
and the zero-lag cross-correlation ics [11, 35]. we have chosen the zero-lag cross-correlation
between the forward and backward propagated waves at each point in Ï‰,
ğ‘‡

ğ¼(r) =

âˆ«0

ğ‘(r, ğ‘¡) ğ‘(r,
Ì„ ğœ) ğ‘‘ğ‘¡.

(3)

the ic amplitudes in equation (3) do not provide an explicit physical relationship with the
reflection coefficients. in [35], we find a detailed explanation of the relation between the imaging
condition and the reflection coefficient. nevertheless, the resulting image provides the correct
amplitude contrast locations of the geological interfaces of rocks with different physical properties
[11]. the amplitude contrast patterns are the main feature of the migrated seismic images explored
r. s. m. freitas et al.

page 4 of 26

an encoder-decoder deep surrogate for reverse time migration

in the present work.
we apply an explicit 2ğ‘›ğ‘‘ -order in time and 4ğ‘¡â„ -order in space finite difference numerical scheme
[36] to approximate equations (1) and (2), leading to the vector ğ¯, the discrete version of the velocity
Ì„ ğ¬, ğŸ at each time step. note that the vectors ğ©, ğ©,
Ì„ and ğ¯ have
field, and similarly the vectors ğ©, ğ©,
the same dimension, that is ğ‘ = ğ‘ğ‘¥ âˆ— ğ‘ğ‘¦ (or ğ‘ = ğ‘ğ‘¥ âˆ— ğ‘ğ‘¦ âˆ— ğ‘ğ‘§ in 3d), where ğ‘ğ‘¥ , ğ‘ğ‘¦
(and ğ‘ğ‘§ ) are the number of grid points in each cartesian direction. each discrete seismogram is
a vector of size ğ‘ğ‘Ÿğ‘’ğ‘ âˆ— (ğ‘ğ‘¡ + 1), where ğ‘ğ‘Ÿğ‘’ğ‘ is the number of receivers, and ğ‘ğ‘¡ = ğ‘‡ âˆ•Î´ğ‘¡, with
Î´ğ‘¡ is the time step. the seismic source ğŸ has dimension ğ‘ğ‘¡ . rtm is not only computationally
intensive but also data-intensive due to the high dimensional inputs, the amount of data to manage,
for instance, storing and retrieving ğ©, and the computational costs associated with imposing the
stability and dispersion conditions in the discrete two-way wave equation [11]. the dispersion
relation takes into account the number of grid points per wavelength and the medium properties,
which in our isotropic acoustic case is the p-wave velocity. thus, complexity in estimating the
migrated image increases with high heterogeneous media and seismic source cutoff frequency.
least-squares rtm (lsrtm) extends the basic method to an iterative method that minimizes a
data misfit term [26, 37]. however, in the present work, we restrict ourselves to the standard rtm
technique.
as we wrap rtm into a sampling method in algorithm 1, for taking into consideration the
input uncertainties, the overall computational cost of stage 2 rises proportionally to ğ‘ğ‘€ğ¶ , the
cardinality of the ensemble of possible velocity fields. typically, seismic raw data recording sets
are split into multiple steps (ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  ) to cope with the potentially high spatial dimensions to be
covered and to enhance the signal to noise ratio (snr) in processing stages. each step covering
fully or partially the domain corresponds to a different arrangement of sources and receivers. it is
essential to mention that rtm iterates over the number of shots producing partial migrated images
characterized by equation (3). when this loop ends, a process called staking sums the partially
migrated seismic images into a single one [38, 39], gathering into this single image all information
related to the seismogram set. algorithm 2 details the generation of the rtm images ensemble,
where a set of seismograms, {ğ¬1 , â‹… â‹… â‹…, ğ¬ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  }, a set of velocity fields, {ğ¯1 , â‹… â‹… â‹…, ğ¯ğ‘ğ‘€ğ¶ }, and a seismic
source (ğŸ ) are given as inputs. the indexes ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  and ğ‘ğ‘€ğ¶ represent the number of shots and the
number of samples for the mc method. for each mc iteration, we solve the wave equation twice,
one for the seismic source and other for the seismograms associated with it. the computation of
the imaging condition uses both solutions (source wavefield, and receiver wavefield), retrieving
from persistent storage the source wavefield to build the migrated seismic section and stacking
the partial results over time (ğˆâˆ‘ ğ‘¡ ), and over the number of seismograms (ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ ). at the end of
algorithm 2, we have the discrete imaging condition set {ğˆ1 , ğˆ2 , â‹¯ , ğˆğ‘ğ‘€ğ¶ } where each ğˆğ‘– is a vector
in â„ğ‘ . it is usual to filter each image to sharpen its features. nevertheless, we do not apply any
filter to the ensemble of seismic images. summarizing, migrations of seismograms for the set of
velocity fields produce the corresponding set of migrated seismic images, where each one has a
direct relation with one velocity sample.
different strategies can be pursued in order to make feasible algorithm 1 by reducing the inherent computational costs of processing and storage. they could rely, for instance, on data compression [40, 41, 42, 43] or more effective stochastic sampling [44], but here, as mentioned before,
our option is for using deep learning surrogates for the rtm imaging, what we describe in the
following section.
r. s. m. freitas et al.

page 5 of 26

an encoder-decoder deep surrogate for reverse time migration

algorithm 2 reverse time migration under uncertainty.
require: {ğ¯1 , â‹… â‹… â‹…, ğ¯ğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  }, {ğ¬1 , â‹… â‹… â‹…, ğ¬ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  }, and ğŸ
1: function rtm _ uq ( vectors {ğ¯1 , â‹… â‹… â‹…, ğ¯ğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘  }, vectors {ğ¬1 , â‹… â‹… â‹…, ğ¬ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  }, vector ğŸ )
2:
for ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’_ğ‘–ğ‘‘ = 1 to ğ‘ğ‘€ğ¶ do
3:
read ğ¯ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’_ğ‘–ğ‘‘ , and ğŸ
4:
initialize image condition ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ = 0
5:
for ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ = 1 to ğ‘ğ‘ â„ğ‘œğ‘¡ğ‘  do
6:
initialize ğ‘›ğ‘¡ = 0
7:
apply initial conditions for ğ‘–ğ‘¡ = 0
8:
for ğ‘–ğ‘¡ = 1 to ğ‘ğ‘¡ do
9:
ğ‘›ğ‘¡ = ğ‘›ğ‘¡ + ğ‘–ğ‘¡ âˆ— Î´ğ‘¡
10:
solve equation (1)
âŠ³ source wavefield
11:
store ğ©ğ‘›ğ‘¡
12:
end for
13:
initialize ğ‘›ğœ = 0, and ğˆâˆ‘ ğœ = 0
14:
apply initial conditions for ğ‘–ğœ = 0
15:
for ğ‘–ğœ = 1 to ğ‘ğ‘¡ do
16:
ğ‘›ğœ = ğ‘ğ‘¡ âˆ’ (ğ‘›ğœ + ğ‘–ğœ âˆ— Î´ğœ)
âŠ³ reverse time
17:
read ğ©ğ‘›ğœ , and ğ¬ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘
18:
solve equation (2)
âŠ³ receiver wavefield
âˆ‘
âˆ‘
19:
calculate ğˆ ğ‘›ğœ = ğˆ ğ‘›ğœ + ğ©ğ‘›ğœ ğ©Ì„ ğ‘›ğœ
âŠ³ imaging condition
20:
end for
21:
stack ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ = ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ + ğˆâˆ‘ ğ‘›ğœ
âŠ³ stacking
22:
end for
23:
store ğˆâˆ‘ ğ‘ â„ğ‘œğ‘¡_ğ‘–ğ‘‘ âˆ€ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’_ğ‘–ğ‘‘ âˆˆ 1 â‰¤ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’_ğ‘–ğ‘‘ â‰¤ ğ‘ğ‘€ğ¶
24:
end for
25: end function

3. deep learning surrogate
the main goal of surrogate models is to replicate the multivariate input-output mapping provided by physical models governed by pdes to save computational costs. performing uncertainty
quantification in such conditions is often hampered whenever one faces high-dimensionality, the
so-called curse of dimensionality. as pointed out in the literature, dnns have proved successful
in such situations by exploiting low-dimensional latent spaces and sophisticated training methods
[45]. we aim to construct and evaluate the performance of dnns acting as a surrogate model
for the rtm imaging under uncertainty, using as baseline the encoder-decoder architecture proposed by [18] and designed for problems cast as image-to-image regressions. we briefly review
the building blocks of the network in this section.
figure 1, inspired in [18], provides the big picture of our end-to-end solution, depicting the
main components of the encoder-decoder network. in our particular application, the input for the
deep learning surrogate is the ensemble of heterogeneous velocity fields, and the outputs are the
corresponding imaging conditions given by eq. (3) for each sample of the velocity field ensemble.
inputs and outputs are high-dimensional spatial fields, and the surrogate modeling cast as a fieldr. s. m. freitas et al.

page 6 of 26

an encoder-decoder deep surrogate for reverse time migration

figure 1: rtm deep convolutional encoder-decoder network architecture. inputs: ensemble of velocity
fields ğ¯ âˆˆ â„ğ‘ . outputs: corresponding ensemble of image conditions ğˆ âˆˆ â„ğ‘ .

to-field regression. this approach is image-inspired. then, it relies on connecting each pixel of
the input field to an output pixel, where pixels correspond to grid points in the input computational
mesh and output fields. the trained network maps the velocity ğ¯ âˆˆ â„ğ‘ into the ic field ğˆ âˆˆ â„ğ‘ .
the encoder-decoder architecture displayed in fig. 1 consists of two sequential main phases.
the first is the encoder, wherein dimension reduction occurs, followed by the decoder that allows
expressing the network output with its original dimension. alternating dense blocks and transition
layers constitute both. indeed, this architecture merges key characteristics of fully connected with
convolutional networks, suited for the present application. on one side, convolutional networks are
quite effective in dimension reduction [45] and are capable of capturing spatial correlations present
in the input velocity fields. fully connected layers enhance the information transmitted across the
network, improving the overall efficiency reflected in reasonable sizes of the needed training data
set [46].
dense-blocks connect all layers directly to each other, helping the training process with the
improvement of information flow and gradients across the network [46]. inputs of the ğ‘™-th layer
are the concatenated outputs from the previous layers, that is, zğ‘™ = ğ» ğ‘™ ([z0 , z1 , â€¦ , zğ‘™âˆ’1 ]), with zğ‘™
the output of ğ‘™-th layer, and [z0 , z1 , â€¦ , zğ‘™âˆ’1 ] refers to their concatenation, and [0, â€¦ , ğ‘™ âˆ’ 1]. ğ» ğ‘™ is
a non-linear transformation. here, ğ» ğ‘™ results from applying three consecutive operations, batch
normalization [47] followed by a relu [48] and, convolution. the dense-block has two design
parameters, the number of layers ğ¿, and the growth rate, ğ¾, the number of output features of every
single layer. the transition layers here, in the encoder (decoder), are convolutional (deconvolutional) and, therefore, handle the dimension inputs or outputs of dense blocks. as shown in figure
1, during the encoder phase, the high dimensional velocity fields are immersed in an alternating
series of layers of dense blocks and encoders. the last layer of the encoder phase produces lowdimensional feature maps that characterize the high dimensional field, as shown in the purple maps.
such maps are immersed in the decoder phase, which is composed of an alternating series of layers
of dense-blocks and decoders, returning ics to its (high) dimension.
the surrogate ğ  is expressed formally in a compact notation as,
r. s. m. freitas et al.

page 7 of 26

an encoder-decoder deep surrogate for reverse time migration

ğ²Ì‚ = ğ (ğ±; ğ°),

(4)

where ğ²Ì‚ is the surrogate prediction (imaging condition, ğˆ) for an input ğ± (that is, a velocity field ğ¯),
and vector ğ° contains the parameters of the neural network. training the neural network means
learning parameters ğ° using data from the set îˆ° = {(ğ±ğ‘– , ğ²ğ‘– )}, ğ‘– = 1 â‹¯ ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› obtained from simulations with the rtm algorithm, where ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› is the number of samples in the training set. the
stochastic gradient descent algorithm computes the unknown network elements for a given loss
function [49]. we consider for training our surrogate, the following îˆ¸2 regularized mean squared
error (mse) function,
ğ¿ğ‘€ğ‘†ğ¸ =

1

âˆ‘

ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›

ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ğ‘–=1

â€–ğ²Ì‚ âˆ’ ğ² â€–2 + ğ›¼ Ï‰(ğ°)
ğ‘– â€–2
â€– ğ‘–

(5)

where ğ²Ì‚ ğ‘– = ğ ğ‘– (ğ±ğ‘– , ğ°). here the penalty function is given by Ï‰(ğ°) = 12 ğ°ğ‘‡ ğ° for the îˆ¸2 regularization.
moreover, the root mean squared error (ğ‘…ğ‘€ğ‘†ğ¸) is used for monitoring the convergence of the
training error. the ğ‘…ğ‘€ğ‘†ğ¸ is given by,
âˆš
âˆš
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
âˆš 1 ğ‘âˆ‘
â€–ğ² âˆ’ ğ²Ì‚ â€–2 .
(6)
ğ‘…ğ‘€ğ‘†ğ¸ = âˆš
ğ‘– â€–2
â€– ğ‘–
ğ‘
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ğ‘–=1

4. numerical experiments
here, we present two examples to demonstrate the ability of the encoder-decoder surrogate
in replacing the original two-way wave equation rtm algorithm efficiently. in these numerical
experiments, we mimic stage 1 outputs of algorithm 1. that is, we need to generate an ensemble
of velocities. hence, we assign to the different geological layers random velocity magnitudes for
producing synthetic data to train the neural network and perform the uncertainty analysis. the first
example deals with a medium containing two flat geological layers of constant velocity. we increase
the difficulty for the surrogate in the second example, by employing a more complex medium, in
which the five geological layers are no longer flat, implying in horizontal heterogeneity. the seismic
source term considered in the present work is a ricker-type wavelet [50].
the encoder-decoder networks are constructed using the open platform tensorflow [51]. the
adam optimizer algorithm [52] is employed for parameter learning, considering a weight decay of
1Ã—10âˆ’5 , and an initial learning rate of 1Ã—10âˆ’3 , with a learning rate scheduler, that is used dropping
two times on plateau of the rooted mean squared error. we compute a total of 1300 samples by
considering the velocity magnitude constant in the interior of each geological layer. therefore,
each velocity field in the ensemble has the form,
ğ¯=

ğ‘›ğ‘™
âˆ‘

ğ‘£ğ‘™ (1 + ğœğ‘™ ğœ‰ğ‘™ ) ğğ‘™

(7)

ğ‘™=1

where ğ‘›ğ‘™ is the number of geological layers, ğ‘£ğ‘™ is the mean velocity within each geological layer, ğœğ‘™ is
the standard deviation, here assumed as 5%, ğœ‰ğ‘™ âˆ¼ ğ•Œ[âˆ’1, 1] is a random variable following a uniform
distribution, and ğğ‘™ is a ğ‘-dimensional vector containing 1 in the components corresponding to the
r. s. m. freitas et al.

page 8 of 26

an encoder-decoder deep surrogate for reverse time migration

ğ‘™-th geological layer grid points and 0 otherwise. after that, we apply a moving harmonic average
to ğ¯ with a window length of 20 grid points to mimic the velocity fields computed by parameter
model building techniques like tomography or full-waveform inversion. to analyze the surrogate
training convergence, out of 1300 samples computed with the original rtm model, we create four
training datasets with 200, 400, 600, and 800 samples each. we used the remaining ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ = 500
samples to test the trained network.
accuracy is measured using the distance between predictions with the surrogate model and
those computed with the rtm original model. to evaluate the surrogate model quality, we consider
the coefficient of determination (ğ‘…2 -score) metric [53]. we define the coefficient of determination
as,
âˆ‘ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ â€–
2
ğ² âˆ’ ğ²Ì‚ ğ‘– â€–
â€–2
ğ‘–=1 â€– ğ‘–
2
ğ‘… = 1 âˆ’ âˆ‘ğ‘
2
ğ‘¡ğ‘’ğ‘ ğ‘¡ â€–
ğ² âˆ’ ğ²â€–
â€–2
ğ‘–=1 â€– ğ‘–

(8)

âˆ‘ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡
where ğ² = ğ‘1
ğ²ğ‘– is the mean of test samples. the ğ‘…2 -score metric represents the normalized
ğ‘–=1
ğ‘¡ğ‘’ğ‘ ğ‘¡
error, allowing the comparison between surrogate models trained by different datasets, with values
close to 1 corresponding to the surrogate models best accuracy. here, we consider 0.95 a good
target. also, we intend that the surrogate model returns not only good predictions of seismic images
but also accurate estimations of quantities that characterize the uncertainties in such images. to
measure the degree of uncertainty in the seismic images, we follow the approach proposed in [54].
in their approach, the degree of uncertainty is expressed by a confidence index that represents the
pointwise normalized standard deviation, where low values represent high variabilities and high
values the opposite. the confidence index is,
ğ‘(r) =

ğœğ‘šğ‘ğ‘¥ âˆ’ ğœ(r)
,
ğœğ‘šğ‘ğ‘¥ âˆ’ ğœğ‘šğ‘–ğ‘›

(9)

where ğ‘(r) is the confidence index at position r, and ğœğ‘šğ‘–ğ‘› and ğœğ‘šğ‘ğ‘¥ are the minimum and maximum
field standard deviations, respectively. another form of measuring the degree of uncertainty is the
coefficient of variation, defined as the pointwise ratio between the standard deviation and the mean,
cğ‘£ (r) =

ğœ(r)
ğœ‡(r)

(10)

where ğœ‡(r) is the expected value at position r.

4.1. a simple geologic scenario: efficiency and convergence analysis
in this first example, designed to evaluate the efficacy and efficiency of the proposed surrogate,
we assume a simple geologic scenario in which two horizontal homogeneous geological layers
separated by a flat horizon parallel to the surface composes the subsurface, as shown in fig 2.
this domain has 1000 m of depth and 1000 m of lateral extension, where the velocity in the first
geological layer is 3000 m/s, and the velocity in the deeper geological layer is 4500 m/s.
we produce synthetic data using the wave propagation forward solver with the reference velocity field of fig 2, with a seismic source with cutoff frequency of 30 hz.in such modeling, we
simulate a fixed split-spread acquisition [38] comprising nine shots, where receivers are positioned
near the surface for each shot and equally spaced of 20 meters. the seismic source is also placed
r. s. m. freitas et al.

page 9 of 26

an encoder-decoder deep surrogate for reverse time migration

0

3000
3200
3400

velocity [m/s]

depth [m]

200
400

3600
3800

600

4000

800
1000

4200
4400
0

200

400 600
length [m]

800 1000

figure 2: simple geologic setup: two horizontal geological layers.
table 1
rtm numerical parameters.
parameter

value

description (unit)

â„
Î´ğ‘¡
ğ‘¡ğ‘
ğ‘ğ‘¥ Ã— ğ‘ğ‘¦
ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¥ , ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¦
ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¥ , ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¦

20
2.22 Ã— 10âˆ’3
0.5
50 Ã— 50
([5:5:45], 5)
( [1:1:50], 5)

spatial discretization (m)
temporal discretization (s)
total acquisition time (s)
number of grid points
source positions
receiver positions

near the surface and moved 100 meters for each shot, covering the entire domain with nine shots.
table 1 shows the parameters used in the numerical modeling of the wave acoustic equations and
the positioning of the seismic source and receivers given by the index ranges [ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¥ , ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¦ ] for the
sources, and [ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¥ , ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¦ ] for the receivers. the grid size and time step respect the numerical dispersion and stability criteria [36].
table 2 details the neural network architecture. the first layer is convolutional, with kernel
size equals to 4 and stride 2. this first layer captures spatial relations from the velocity input.
the number of features maps after the first convolutional layer is 48. the neural network has 3
dense-blocks with ğ¿ = 4 and ğ¾ = 16. dense-blocks have a kernel size equals 3, and a stride of 1.
encoder-decoder layers have a kernel size of 3 and a stride of 2. in the decoding layer, we introduce
a transposed convolution, allowing the expression of the output with its original dimensionality,
equal to the computational grid. a final relu layer [48] imposes that the outputs are positive
numbers, naturally constraining the network to output ğ¼ğ¶ > 0 at each grid point. thus, the neural
network has 218, 425 parameters to be estimated.
figure 3 shows the decay of the ğ‘…ğ‘€ğ‘†ğ¸ as a function of the number of epochs during the
r. s. m. freitas et al.

page 10 of 26

an encoder-decoder deep surrogate for reverse time migration
table 2
neural network architecture. "outputs" represents the number of features maps and "dimension" is
the dimension of the features maps.
layers

output

dimension

input
convolution
dense-block 1
encoding
dense-block 2
decoding 1
dense-block 3
decoding 2
relu

1
48
112
56
120
60
124
1
1

50 Ã— 50
24 Ã— 24
24 Ã— 24
12 Ã— 12
12 Ã— 12
24 Ã— 24
24 Ã— 24
50 Ã— 50
50 Ã— 50
train = 200 samples
train = 400 samples
train = 600 samples
train = 800 samples

0.30

rmse

0.25
0.20
0.15
0.10
0.05
0.00

0

25

50

75 100 125 150 175 200
epoch

figure 3: rmse decay with number of epochs.

training process, for training data sets ranging from 200 to 800 samples. note that the ğ‘…ğ‘€ğ‘†ğ¸
stabilizes after 150 epochs for all cases and that for smaller data sets, we see higher error values.
key characteristics one is seeking when replacing the original expensive computational model by a
surrogate are efficiency and accuracy. we estimate efficiency as the ratio between ğ‘ğ‘† , the number
of samples in the training set, and ğ‘ğ‘€ğ¶ , the number of samples of the mc method to achieve
a prescribed level of accuracy (ğ‘…2 â‰¥ 0.95). thus, we introduce the following index to evaluate
efficiency,
(
)
ğ‘ğ‘†
efficiency = 1 âˆ’ ğœ‚
Ã— 100
(11)
ğ‘ğ‘€ğ¶
then, the index in equation (11) represents the percentage of the saved computational costs, and
ğœ‚ is an adjustment factor accounting for the time spent in the construction, training, and making
predictions with the surrogate model. without loss of generality, we assume for now ğœ‚ = 1.0. for
less optimistic conditions, we recognize that ğœ‚ > 1.0. we calculate the coefficient of determination
ğ‘…2 to assess the accuracy of the neural network with the remaining 500 samples. we observe that
the surrogate model accuracy is good even in small training data scenarios, reaching ğ‘…2 â‰¥ 0.95,
r. s. m. freitas et al.

page 11 of 26

an encoder-decoder deep surrogate for reverse time migration
96.5

0.995
0.990

95.5

0.985

r 2-score

efficiency [%]

96.0

95.0

0.980

94.5

0.975

94.0

0.970

93.5

200 300 400 500 600 700 800
number of samples for training

figure 4: test ğ‘…2 -score and efficiency in function of the number of training samples.

as shown in fig. 4. furthermore, fig. 4 also depicts the surrogate efficiency. here it is worth
highlighting that 600 rtm runs are necessary to compute the variance with a relative error of
1 Ã— 10âˆ’3 . thus, we see that the efficiency is higher than 90%, even for the larger data set with 800
samples.
to further illustrate how the surrogate model approximates the predictions of the original model
with good accuracy, fig. 5 shows comparisons for two realizations chosen randomly from the test
set. we observe that the surrogate presents good results, returning good predictions of the image
condition. we also depict a comparison between the standard deviation, ğœ(ğ«), the confidence index,
ğ‘(ğ«), and the coefficient of variation, ğ‘ğ‘£ (ğ«), predicted by the original and surrogate models with
ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘” = 500 testing samples, see fig. 6. besides, we introduce a discrete version of a ğ¿2 relative
error between two spatial fields ğ‘“ , one computed with the rtm and the other by the surrogate as,
1 âˆ‘
=
ğ‘ ğ‘–=1
ğ‘

ğ‘’ğ‘”

2

( ğ‘–
)2
ğ‘”ğ‘…ğ‘‡ ğ‘€ âˆ’ ğ‘”ğ‘ ğ‘–
ğ‘–
ğ‘”ğ‘…ğ‘‡
ğ‘€

(12)

where the subscripts refer to how we compute the field ğ‘”. this index is an average of the pointwise
relative error for all ğ‘ grid points. figure 5 compares randomly selected seismic images produced
by rtm with the corresponding ones obtained with the deep learning surrogate. the visual resemblance is quantified using equation (12), leading to relative errors that stay below 2%. we extend
further our assessment of the surrogate effectiveness by comparing the uncertainty indexes computed with the two techniques displayed in fig. (6). for all indexes, the relative errors computed
with equation (12) are less than 1%.

4.2. a non-flat medium with five geological layers
to challenge the encoder-decoder surrogate, we use a synthetic geologic model with five homogeneous layers similar to the one proposed in [55]. the 2d velocity model consists of a water
layer with velocity 1500 m/s, and four mini sedimentary basins with velocities of 2000 m/s, 2500
m/s, 3000 m/s, and 4000 m/s, respectively. figure 7 display a schematic view of the velocity field
with 1000 m of depth and 1000 m of lateral extension.
two synthetic seismograms are generated for the velocity fields shown in fig 7 considering now
the seismic sources with cutoff frequencies of 30 and 45hz.table 3 shows the rtm parameters and
r. s. m. freitas et al.

page 12 of 26

1.0

0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

depth [m]

1000

1000

0

0

200

200

400 600
length [m]

400 600
length [m]

800 1000

800 1000

depth [m]

0

depth [m]

depth [m]

an encoder-decoder deep surrogate for reverse time migration

0

0

(a) rtm model

200

200

400 600
length [m]

400 600
length [m]

800 1000

800 1000

0.0

(b) surrogate model

figure 5: randomly selected samples of seismic images in the test data set computed by the rtm (a)
and the surrogate model (b). the relative errors in the image condition, ğ‘’ğ¼ , are lower than 2%.
table 3
rtm numerical parameters.
parameter

ğ‘“ğ‘ğ‘¢ğ‘¡ğ‘œğ‘“ ğ‘“ = 30ğ»ğ‘§

ğ‘“ğ‘ğ‘¢ğ‘¡ğ‘œğ‘“ ğ‘“ = 45ğ»ğ‘§

description (unit)

â„
Î´ğ‘¡
ğ‘¡ğ‘
ğ‘ğ‘¥ Ã— ğ‘ğ‘¦
ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¥ , ğ‘–ğ‘ ğ‘Ÿğ‘ğ‘¦
ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¥ , ğ‘–ğ‘ ğ‘–ğ‘ ğ‘¦

10
1.25 Ã— 10âˆ’3
2.0
100 Ã— 100
([5:10:95], 5)
( [1:1:100], 5)

6.666
8.333 Ã— 10âˆ’4
2.0
150 Ã— 150
([7:15:142], 7)
([1:1:150], 7)

spatial discretization (m)
temporal discretization (s)
total acquisition time (s)
number of grid points
source positions
receiver positions

the positioning of sources and receivers. for the cutoff frequency of 45hz, due to the imposition of
the stability and dispersion conditions in the discrete two-way wave equation, there is a significant
increase in the input dimensionality, that bears the potential to hamper the neural network training.
the next sub-sections present results for the scenarios involving the two frequencies of excitation.
r. s. m. freitas et al.

page 13 of 26

an encoder-decoder deep surrogate for reverse time migration
0
200

0.15
0.10

800
200

400 600
length [m]

depth [m]

1000

800 1000

standard deviation - rtm model

0.15
0.10
0.05
0

200

400 600
length [m]

800 1000

standard deviation - surrogate model

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

1000

0

200

400 600
length [m]

800 1000

confidence index - rtm model
0

0.20
0.15
0.10

1000

0.05
0

200

400 600
length [m]

800 1000

coefficient of variation - rtm model

depth [m]

0.25

800

400 600
length [m]

800 1000

0.0

0.40
0.35

200

0.30

600

200

0

0.35

400

0

confidence index - surrogate model
0.40

200
depth [m]

0.20

800

0.05
0

0.25

600

depth [m]

depth [m]

0.20

600

0.30

400

depth [m]

0.25

0.35

200

0.30

400

1000

0

0.35

0.30
0.25

400

0.20

600

0.15
0.10

800
1000

0.05
0

200

400 600
length [m]

800 1000

coefficient of variation - surrogate model

figure 6: uq indexes - standard deviation, ğœ(ğ«), confidence index, ğ‘(ğ«), and coefficient of variation,
ğ‘ğ‘£ (ğ«) - predicted by the rtm (left) and surrogate models (right). the relative errors to the rtm model
are lower than 1%.

4.2.1. cutoff frequency - 30hz
we detail the architecture of the neural network for this scenario in table 4. it contains five
dense blocks, leading to 412, 210 parameters to be trained. we can see in figure 8 the ğ‘…ğ‘€ğ‘†ğ¸
decay as the number of epochs increase for all training sets. we verify the accuracy of the surrogate
r. s. m. freitas et al.

page 14 of 26

0

1500

200

2000

400

2500
p3

600

p2

800
1000

0

200

velocity [m/s]

depth [m]

an encoder-decoder deep surrogate for reverse time migration

p1

3000

p4

3500

400 600
length [m]

800 1000

4000

rmse

figure 7: geologic model with 5 layers adapted from [55]. ğ‘ƒ1 , ğ‘ƒ2 , ğ‘ƒ3 , and ğ‘ƒ4 are control points in the
geological model to compute statistics.
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00

train = 200 samples
train = 400 samples
train = 600 samples
train = 800 samples

0

25

50

75 100 125 150 175 200
epoch

figure 8: rmse decay with number of epochs.

by computing the ğ‘…2 score for the 500 testing samples. we find that, as expected, for networks
trained with larger data sets, ğ‘…2 values are closer to 1.0, as shown in figure 9(a). due to limitations
imposed by the high cost of generating samples using the full rtm model for this example, we
develop a different efficiency analysis extrapolating from the basic conditions used for the network
training. we assume conservatively that ğ‘ğ‘€ğ¶ is of the same order of the case in section 4.1. thus,
we start from a scenario where only 5,000 samples are needed to characterize uncertainties in
the seismic images and extrapolate to more expensive potential scenarios requiring hypothetically
till 50,000 samples. here, ğ‘ğ‘† is equal to 1100, 600 samples to train the neural network with an
accuracy of ğ‘…2 â‰¥ 0.95, and 500 to test the surrogate model. figure 9(b) depicts the efficiency
analysis in function of ğ‘ğ‘€ğ¶ . we note, for the worst scenario, an efficiency of around 78%, and for
the scenarios where ğ‘ğ‘€ğ¶ is higher than 10,000 samples, the efficiencies reach values greater than
90%. for the most expensive scenario, we see an efficiency close to 98%.
fig. 10 shows comparisons between images randomly selected from the test set and the corresponding images produced with the full rtm model. we observe that the surrogate model returns
r. s. m. freitas et al.

page 15 of 26

an encoder-decoder deep surrogate for reverse time migration

layers

output

dimension

input
convolution
dense-block 1
encoding
dense-block 2
encoding
dense-block 3
decoding
dense-block 4
decoding
dense-block 5
decoding
relu

1
48
112
56
120
60
124
62
126
63
127
1
1

100 Ã— 100
48 Ã— 48
48 Ã— 48
24 Ã— 24
24 Ã— 24
12 Ã— 12
12 Ã— 12
24 Ã— 24
24 Ã— 24
48 Ã— 48
48 Ã— 48
100 Ã— 100
100 Ã— 100

0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.90

efficiency [%]

r 2-score

table 4
neural network architecture. "outputs" represents the number of features maps and "dimension" is
the spatial dimension of the features maps.

200

300

400

500 600
samples

700

(a) ğ‘…2 -score for the trained networks

800

97.5
95.0
92.5
90.0
87.5
85.0
82.5
80.0
77.5

10000

20000 30000 40000
number of samples for mc

50000

(b) efficiency

figure 9: test ğ‘…2 -score and efficiency in function of the number of mc samples.

excellent predictions of the imaging condition. we can also note that the surrogate model predicts
the uq figures - standard deviation, ğœ(ğ«), confidence index, ğ‘(ğ«), and coefficient of variation, ğ‘ğ‘£ (ğ«) with good accuracy, as seen in fig 11, where the relative error between the surrogate predictions and
the rtm model are lower than 6%. results in figures 10 and 11 shows that the encoder-decoder
surrogate extrapolates the replication of the ic training targets, delivering remarkable results to
assist in uq analysis.
next, we deepen our investigation of the surrogateâ€™s ability to reproduce the probabilistic characterization of the ic fields by plotting the probability density functions (pdfs) of the imaging
condition at control points in the domain, as displayed in fig. 7. we place the control points in
regions of low and high uncertainties. as the reference solution, we use pdfs obtained by the
rtm model with the 500 test samples to compare the accuracy of the surrogate model trained
r. s. m. freitas et al.

page 16 of 26

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

depth [m]

1000

1000

0

0

0

200

200

200

400 600
length [m]

400 600
length [m]

400 600
length [m]

800 1000

800 1000

800 1000

(a) rtm model

depth [m]

depth [m]

1000

depth [m]

0

depth [m]

depth [m]

an encoder-decoder deep surrogate for reverse time migration

0

0

0

200

200

200

400 600
length [m]

400 600
length [m]

400 600
length [m]

800 1000

800 1000

800 1000

0.0

0.0

0.0

(b) surrogate model

figure 10: randomly selected images from the test data set computed by the rtm model (a) and the
surrogate model (b) trained with 600 samples. the relative errors in the image condition, ğ‘’ğ¼ , are lower
than 6%.

with different datasets to estimate the pdfs at the control points. figure 12 shows the imaging
condition pdfs estimated with the surrogate models trained with 400, 600, 800 samples, together
with the reference pdfs. we observe that the pdfs obtained with the surrogate model are in good
agreement with the reference ones for all control points, particularly at the deeper point (ğ‘ƒ 4).
now we exemplify a possible surrogate use in the feature extraction and interpretation of seismic images, stage 3 of algorithm 1. we provide, using the surrogate, a view of the uncertainties
r. s. m. freitas et al.

page 17 of 26

an encoder-decoder deep surrogate for reverse time migration
0

0.35

0

0.35

200

0.30

200

0.30

0.15

800
1000

0

200

400 600
length [m]

800 1000

0.10

800

0.05

1000

0.10
0

200

400 600
length [m]

800 1000

0.05

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

1000

0

200

400 600
length [m]

800 1000

confidence index - rtm model
0

0.5

600

0.4
0.3
0.2
0

200

400 600
length [m]

800 1000

400 600
length [m]

800 1000

0.0

0.1

coefficient of variation - rtm model

0.8
0.7

200

0.6

400

1000

200

0

0.7

800

0

confidence index - surrogate model
0.8

200
depth [m]

0.15

standard deviation - surrogate model

depth [m]

depth [m]

standard deviation - rtm model

0.20

600

depth [m]

depth [m]

0.20

600

0.25

400

depth [m]

0.25

400

0.6

400

0.5

600

0.4

800

0.2

1000

0.3

0

200

400 600
length [m]

800 1000

0.1

coefficient of variation - surrogate model

figure 11: uq indexes - standard deviation, ğœ(ğ«), confidence index, ğ‘(ğ«), and coefficient of variation,
ğ‘ğ‘£ (ğ«) - predicted by the rtm model (left) and the surrogate model (right). the relative errors between
the surrogate predictions to the rtm model for the uq indexes are less than 6%.

associated with specific seismic targets, the interfaces of geological layers. this view can reveal
how the propagated uncertainties can directly impact the images posterior interpretation. figure 13
provides the ic mean value and associate confidence bands for the four interfaces. in the right part
of the figure, we give an idea of the uncertainties spatial distribution, having as background a randomly selected image from the ensemble. to promote visual perception, we plotted amplified ic
r. s. m. freitas et al.

page 18 of 26

an encoder-decoder deep surrogate for reverse time migration

5

rtm model
surrogate - 400 samples
surrogate - 600 samples
surrogate - 800 samples

8
6

4

pdf

4

pdf

3

2

1

0

2

0.0

0.2

0.4

0.6

0.8

1.0

0

0.0

0.2

0.4

i

ğ‘ƒ1

0.8

1.0

0.6

0.8

1.0

ğ‘ƒ2

5

4

4

3
pdf

pdf

3

2
1

1

0

0.6
i

2

0.0

0.2

0.4

0.6

0.8

1.0

0

0.0

0.2

0.4
i

i

ğ‘ƒ3

ğ‘ƒ4

figure 12: comparison between pdfs predicted by the rtm model and the surrogate model.

confidence bands associated with each interface. those bands reflect the ic value dispersion amid
the image ensemble, and, therefore, might lead to a lack of confidence in the reflector placement.

4.2.2. cutoff frequency - 45hz
we now stress the proposed deep learning surrogate by testing its performance in a scenario not
considered for the training, but still focusing on the subsurface geology of fig.7. the rtm image
comprises a certain number of shots, each designed for illuminating the same subsurface using
different seismic sources conditions. here we analyze a situation involving a higher excitation
frequency, ğ‘“ = 45ğ»ğ‘§, implying, due to numerical requirements, in the necessity of a finer grid. in
such a case, input and output images have different dimensions compared to the previous scenario.
still, the intrinsic dimensionality is the same for the input as we are imaging the same velocity
field as before. instead of seeking for a new architecture, we slightly changed the previous one
by replacing the first and last network layers and adapting the initial convolutional layer to ensure
that an integer number defines the kernel. we do not expect to obtain the surrogateâ€™s optimal
performance by employing such a strategy, but that can be quite useful in practical terms if it works.
table 5 shows the neural network architecture for the 45 hz scenario. the network architecture
is the same as in section 4.2.1 with small changes. the first convolutional layer has a kernel size
r. s. m. freitas et al.

page 19 of 26

an encoder-decoder deep surrogate for reverse time migration

confidence bands on the interfaces for ğ‘“ = 30hz

confidence bands over image condition, ğ‘“ =
30hz

figure 13: ic confidence bands over the interface between geological layers (left), and ic confidence
bands superimposed over a randomly selected image (right) for ğ‘“ = 30 hz.
table 5
neural network architecture. "outputs" represents the number of features maps and "dimension" is
the dimension of the features maps.
layers

output

dimension

input
convolution
dense-block 1
encoding
dense-block 2
encoding
dense-block 3
decoding
dense-block 4
decoding
dense-block 5
decoding
relu

1
48
112
56
120
60
124
62
126
63
127
1
1

150 Ã— 150
72 Ã— 72
72 Ã— 72
36 Ã— 36
36 Ã— 36
18 Ã— 18
18 Ã— 18
36 Ã— 36
36 Ã— 36
72 Ã— 72
72 Ã— 72
150 Ã— 150
150 Ã— 150

equal to 7 and a stride of 2. the total number of parameters in the network is 416, 390. figure 14
shows ğ‘…ğ‘€ğ‘†ğ¸ decay as a function of the number of epochs in the training process.
we can see in fig. 15a the ğ‘…2 score for different training sets showing for this more difficult scenario a slight decrease in the neural network quality. confirming our initial expectations
of a non-optimal but acceptable performance, the coefficients of determination ğ‘…2 for all training
r. s. m. freitas et al.

page 20 of 26

an encoder-decoder deep surrogate for reverse time migration
0.40

train = 200 samples
train = 400 samples
train = 600 samples
train = 800 samples

0.35

rmse

0.30
0.25
0.20
0.15
0.10
0.05
0

25

50

75 100 125 150 175 200
epoch

100

0.88
0.86
0.84
0.82
0.80
0.78
0.76
0.74

90
efficiency [%]

r 2-score

figure 14: rmse decay with number of epochs.

80
70

Î· = 1.0
Î· = 1.5
Î· = 2.0
Î· = 2.5
Î· = 3.0

60
50
40

200

300

400

500 600
samples

700

800

(a) ğ‘…2 -score for the trained networks

10000

20000 30000 40000
number of samples for mc

50000

(b) efficiency

figure 15: ğ‘…2 -score and efficiency for the trained networks.

datasets are lower than 0.90. moreover, we estimate the efficiency of the surrogate model in the
same manner as in the previous case. however, here we consider values for the adjustment factor
ğœ‚ > 1.0. more precisely, the adjustment factor tries to estimate the time spent in search of the
neural network hyperparameters to optimize the surrogate model accuracy and to generate larger
training sets. without loss of generality, we assume that the number of samples ğ‘ğ‘† to train the
neural network is equal to 1100, 600 to train, and 500 to test the surrogate model. figure 15(b)
shows the efficiency in function of ğ‘ğ‘€ğ¶ , for several adjustment factors. note that for scenarios
with ğ‘ğ‘€ğ¶ â‰¤ 10, 000, the efficiency drops significantly for higher adjustment factors. however, for
scenarios where ğ‘ğ‘€ğ¶ â‰¥ 20, 000 samples the efficiency reaches values close to 80-90%. for scenarios where ğ‘ğ‘€ğ¶ â‰¥ 40, 000 we observe an efficiency close to 90% even for the higher adjustment
factor.
despite the lower accuracy presented in this scenario, the surrogate model could reach satisfactory predictions of the imaging condition, as we can see in fig. 16. in this figure, we show
the three randomly selected images from the test data set computed by the rtm model and the
surrogate model. note, however, that the image produced by the rtm model may not be the best
image we can compute for these conditions. the grid is adjusted only to satisfy the stability and
dispersion criteria for the 45 hz cutoff frequency. we do not optimize the domain size for a proper
r. s. m. freitas et al.

page 21 of 26

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

depth [m]

1000

1000

0

0

0

200

200

200

400 600
length [m]

400 600
length [m]

400 600
length [m]

800 1000

800 1000

800 1000

(a) rtm model

depth [m]

depth [m]

1000

depth [m]

0

depth [m]

depth [m]

an encoder-decoder deep surrogate for reverse time migration

0

0

0

200

200

200

400 600
length [m]

400 600
length [m]

400 600
length [m]

800 1000

800 1000

800 1000

0.0

0.0

0.0

(b) surrogate model

figure 16: randomly selected images from the test data set computed by the rtm model (a) and the
surrogate model (b) trained with 600 samples. the relative errors ğ‘’ğ¼ = are lower than 10%.

representation of the non-reflecting boundary conditions and source/receiver arrangement. furthermore, figure 17 shows a comparison between the standard deviation, ğœ(ğ«), confidence index,
ğ‘(ğ«), and coefficient of variation, ğ‘ğ‘£ (ğ«), computed by the rtm and surrogate models. we observe
that the surrogate model predicts the uq indexes with satisfactory accuracy. the relative errors
between the surrogate predictions to the rtm model for the uq indexes are lower than 6%.
we now investigate the probability density functions (pdfs) of the imaging condition at the
control points in fig. 7. we use again as reference solution pdfs obtained by the rtm model
r. s. m. freitas et al.

page 22 of 26

an encoder-decoder deep surrogate for reverse time migration
0.35

0.30

400

0.25

400

0.20
0.15

1000

depth [m]

200

800
0

200

400 600
length [m]

800 1000

0.05

0.25
0.20

600

0.15

800

0.10

standard deviation - rtm model

1000

0.10
0

200

400 600
length [m]

800 1000

0.05

standard deviation - surrogate model

0

1.0

0

1.0

200

0.8

200

0.8

400

0.6

400

0.6

600

0.4

600

0.4

800

0.2

800

0.2

0.0

1000

1000

0

200

400 600
length [m]

800 1000

confidence index - rtm model

depth [m]

depth [m]

0.35

0.30

600

depth [m]

0

200

0

200

400 600
length [m]

800 1000

0.0

confidence index - surrogate model

0

0.8

0

0.8

200

0.7

200

0.7

400

0.5

0.6

400

0.5
0.4

600

1000

0

200

400 600
length [m]

800 1000

0.6

600

0.4

0.2

800

0.2

0.1

1000

0.3

800

depth [m]

depth [m]

0

coefficient of variation - rtm model

0.3

0

200

400 600
length [m]

800 1000

0.1

coefficient of variation - surrogate model

figure 17: uq indexes - standard deviation, ğœ(ğ«), confidence index, ğ‘(ğ«), and coefficient of variation,
ğ‘ğ‘£ (ğ«) - predicted by the rtm model (left) and the surrogate model (right). the relative errors between
the surrogate predictions to the rtm model for the uq indexes are lower than 6%.

with 500 test samples to verify the accuracy of the surrogate models trained with different datasets
to estimate the pdfs at the control points. figure 18 depicts the imaging condition pdfs at the
control points estimated by the surrogate model trained with 200, 400, 600, 800 samples, together
with the reference pdfs. we observe that the pdfs obtained with the surrogate model capture well
the reference pdfs in all control points, particularly for large training datasets.
r. s. m. freitas et al.

page 23 of 26

an encoder-decoder deep surrogate for reverse time migration

10

rtm model
surrogate - 400 samples
surrogate - 600 samples
surrogate - 800 samples

6

4
3
pdf

pdf

8

2

4

1

2
0

0.0

0.2

0.4

0.6

0.8

0

1.0

0.0

0.2

0.4

ğ‘ƒ1

0.8

1.0

0.6

0.8

1.0

ğ‘ƒ2

4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0

pdf

pdf

6
5
4
3
2
1
0

0.6
i

i

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

i

ğ‘ƒ3

i

ğ‘ƒ4

figure 18: comparison between pdfs predicted by the rtm model and the surrogate model.

5. conclusions
we propose a deep learning model based on an encoder-decoder architecture to replace the
costly rtm technique on producing seismic images. this approach naturally fits the framework
of a computational workflow to produce seismic images with quantified uncertainty in [2]. this
surrogate model builds a scalable imageâ€“toâ€“image mapping, coping with the high dimensionality
of both the heterogeneous velocity fields that serve as inputs and images outputs. such surrogate
has revealed to be very efficient in the context of uq many-query tasks, as demonstrated by our
numerical examples. indeed, that was observed even in cases where we employ a non-optimal
neural network architecture.
we place our contribution in the emerging area of physics-informed machine learning, where
the final model, in many different ways, blends two main components: often expensive computational models relying on first principles and phenomenological closure equations, and machine
learning data-driven tools. such combination not only suits perfectly to the needs required by the
workflow mentioned earlier but also offers a broad spectrum of opportunities to improve performance, like employing more powerful training strategies and automatic hyperparameters optimization.

r. s. m. freitas et al.

page 24 of 26

an encoder-decoder deep surrogate for reverse time migration

acknowledgements
this study was financed in part by capes, brasil finance code 001. this work is also partially
supported by faperj (grant e-26/203.018/2017), cnpq (grant 302489/2016-9), and petrobras
(grant 2017/00148-9).

references
[1] yu chen, kai gao, eric s. davis, dipen n. sinha, cristian pantea, and lianjie huang. full-waveform inversion and least-squares reversetime migration imaging of collimated ultrasonic-beam data for high-resolution wellbore integrity monitoring. applied physics letters,
113(7):071903, 2018.
[2] carlos hs barbosa, liliane no kunstmann, rÃ´mulo m silva, charlan ds alves, bruno s silva, marta mattoso, fernando a rochinha,
alvaro lga coutinho, et al. a workflow for seismic imaging with quantified uncertainty. arxiv preprint arxiv:2001.06444, 2020.
[3] a. botero, a gesret, t. romary, m. noble, and c. maisons. stochastic seismic tomography by interacting markov chains. geophysical
journal international, 207:374â€“392, 2016.
[4] j. belhadj, t. romary, a. gesret, m. noble, , and b. figliuzzi. new parametrizations for bayesian seismic tomography. inverse problems,
34:33, 2018.
[5] n. brantut. time-resolved tomography using acoustic emissions in the laboratory, and application to sandstone compaction. geophysical
journal international, 213:2177â€“2192, 2018.
[6] james martin, lucas c wilcox, carsten burstedde, and omar ghattas. a stochastic newton mcmc method for large-scale statistical inverse
problems with application to seismic inversion. siam journal on scientific computing, 34(3):a1460â€“a1487, 2012.
[7] hejun zhu, siwei li, sergey fomel, georg stadler, and omar ghattas. a bayesian approach to estimate uncertainty for full-waveform
inversion using a priori information from depth migration. geophysics, 81(5):r307â€“r323, 2016.
[8] reetam biswas and mrinal sen. 2d full-waveform inversion and uncertainty estimation using the reversible jump hamiltonian monte carlo.
in seg technical program expanded abstracts 2017, pages 1280â€“1285. society of exploration geophysicists, 2017.
[9] lars gebraad, christian boehm, and andreas fichtner. bayesian elastic full-waveform inversion using hamiltonian monte carlo. eartharxiv,
page qftn5, 2019.
[10] zeyu zhao and mrinal k sen. a gradient based mcmc method for fwi and uncertainty analysis. in seg technical program expanded abstracts
2019, pages 1465â€“1469. society of exploration geophysicists, 2019.
[11] h-w. zhou, h. hu, z. zou, y. wo, and o. youn. reverse time migration: a prospect of seismic imaging methodology. earth-science
reviews, 179:207â€“227, 2018.
[12] c.e. rasmussen and c.k.i. williams. gaussian processes for machine learning. 2006.
[13] ilias bilionis and nicholas zabaras. multi-output local gaussian process regression: applications to uncertainty quantification. journal of
computational physics, 231(17):5718 â€“ 5746, 2012.
[14] ilias bilionis, nicholas zabaras, bledar a konomi, and guang lin. multi-output separable gaussian process: towards an efficient, fully
bayesian paradigm for uncertainty quantification. journal of computational physics, 241:212â€“239, 2013.
[15] l. parussini, d. venturi, p. perdikaris, and g.e. karniadakis. multi-fidelity gaussian process regression for prediction of random fields.
journal of computational physics, 337:36â€“50, 2017.
[16] p. perdikaris, d. venturi, and g. e. karniadakis. multi-fidelity information fusion algorithms for high-dimensional systems and massive data
sets. siam j. sci. comput., 38:b521 â€“ b538, 2016.
[17] g. s. h. pau, y. zhang, and s. finsterle. reduced order models for many-query subsurface flow applications. computational geosciences,
17:705â€“721, 2013.
[18] y. zhu and n. zabaras. bayesian deep convolutional encoder-decoder networks for surrogate modeling and uncertainty quantification. journal
of computational physics, 366:415â€“447, 2018.
[19] s. mo, y. zhu, n. zabaras, x. shi, and j. wu. deep convolutional encoderÃ¢ÄƒÅ™decoder networks for uncertainty quantification of dynamic
multiphase flow in heterogeneous media. water resources research, 55:703â€“728, 2019.
[20] y. zhu, n. zabaras, p.s. koutsourelakis, and p. perdikaris. physics-constrained deep learning for high-dimensional surrogate modeling and
uncertainty quantification without labeled data. journal of computational physics, 394:56â€“81, 2019.
[21] s. mo, n. zabaras, x. shi, and j. wu. integration of adversarial autoencoders with residual dense convolutional networks for estimation of
nonÃ¢ÄƒÅ™gaussian hydraulic conductivities. water resources research, 56:e2019wr026082, 2020.
[22] y. yang and p. perdikaris. conditional deep surrogate models for stochastic, high-dimensional, andmulti-fidelity systems. computational
mechanics, 64:417â€“434, 2019.
[23] m. tang, y. liu, and l. j.durlofsky. a deep-learning-based surrogate model for data assimilation in dynamic subsurface flow problems.
journal of computational physics, 413:109456, 2020.
[24] adar kahana, eli turkel, shai dekel, and dan givoli. obstacle segmentation based on the wave equation and deep learning. journal of
computational physics, 413:109458, 2020.
[25] jian sun, zhan niu, kristopher a. innanen, junxiao li, , and daniel o. trad. a theory-guided deep-learning formulation and optimization
of seismic waveform inversion. geophysics, 85(2):r87â€“r99, 2020.
[26] g. t. schuster. seismic inversion. society exploration geophysicists, 1 edition, 2017.
[27] charles cerjan, dan kosloff, ronnie kosloff, and moshe reshef. a nonreflecting boundary condition for discrete acoustic and elastic wave
equations. geophysics, 50(4):705â€“708, 1985.

r. s. m. freitas et al.

page 25 of 26

an encoder-decoder deep surrogate for reverse time migration
[28] wen-fong chang and george a mcmechan. reverse-time migration of offset vertical seismic profiling data using the excitation-time imaging
condition. geophysics, 51(1):67â€“84, 1986.
[29] wen-fong chang and george a mcmechan. elastic reverse-time migration. geophysics, 52(10):1365â€“1375, 1987.
[30] bao d nguyen and george a mcmechan. excitation amplitude imaging condition for prestack reverse-time migration. geophysics,
78(1):s37â€“s46, 2013.
[31] paul sava and sergey fomel. time-shift imaging condition in seismic migration. geophysics, 71(6):s209â€“s217, 2006.
[32] bin wang, chuck mason, manhong guo, kwangjin yoon, jun cai, jean ji, and zhiming li. subsalt velocity update and composite imaging
using reverse-time-migration based delayed-imaging-time scan. geophysics, 74(6):wca159â€“wca166, 2009.
[33] paul sava and ivan vasconcelos. extended imaging conditions for wave-equation migration. geophysical prospecting, 59(1):35â€“55, 2011.
[34] faqi liu, guanquan zhang, scott a morton, and jacques p leveille. an effective imaging condition for reverse-time migration using wavefield
decomposition. geophysics, 76(1):s29â€“s39, 2011.
[35] sandip chattopadhyay and george a mcmechan. imaging conditions for prestack reverse-time migration. geophysics, 73(3):s81â€“s89, 2008.
[36] john c. strikwerda. finite difference schemes and partial differential equations, second edition. siam: society for industrial and applied
mathematics.
[37] toktam zand, hamid r. siahkoohi, alison malcolm, ali gholami, and alan richardson. consensus optimization of total variationâ€“based
reverse time migration. computational geosciences, 2020.
[38] philip kearey, michael brooks, and ian hill. an introduction to geophysical exploration. john wiley & sons, 2013.
[39] Ã¶z yilmaz. seismic data analysis: processing, inversion, and interpretation of seismic data. society of exploration geophysicists, 2001.
[40] lanshu bai, huiyi lu, and yike liu. high-efficiency observations: compressive sensing and recovery of seismic waveform data. pure and
applied geophysics, 177(1):469â€“485, 2020.
[41] navjot kukreja, jan huckelheim, mathias louboutin, kaiyuan hou, fabio luporini, paul hovland, and gerard gorman. combining checkpointing and data compression for large scale seismic inversion. arxiv preprint arxiv:1810.05268, 2018.
[42] peter lindstrom, po chen, and en-jui lee. reducing disk storage of full-3d seismic waveform tomography (f3dt) through lossy online
compression. computers & geosciences, 93:45â€“54, 2016.
[43] philipp a witte, mathias louboutin, fabio luporini, gerard j gorman, and felix j herrmann. compressive least-squares migration with
on-the-fly fourier transforms. geophysics, 84(5):r655â€“r672, 2019.
[44] marco ballesio, joakim beck, anamika pandey, laura parisi, erik von schwerin, and raÃºl tempone. multilevel monte carlo acceleration of
seismic wave propagation under uncertainty. gem-international journal on geomathematics, 10(1):22, 2019.
[45] ian goodfellow, yoshua bengio, and aaron courville. deep learning. mit press, 2016. http://www.deeplearningbook.org.
[46] g. huang, z. liu, l. van der maaten, and k. q. weinberger. densely connected convolutional networks. proceedings of the ieee conference
on computer vision and pattern recognition, 2017.
[47] sergey ioffe and christian szegedy. batch normalization: accelerating deep network training by reducing internal covariate shift. in
proceedings of the 32nd international conference on international conference on machine learning - volume 37, icmlÃ¢ÄƒÅº15, page
448Ã¢ÄƒÅŸ456. jmlr.org, 2015.
[48] xavier glorot, antoine bordes, and yoshua bengio. deep sparse rectifier neural networks. in geoffrey gordon, david dunson, and miroslav
dudÃ£Å‹k, editors, proceedings of the fourteenth international conference on artificial intelligence and statistics, volume 15 of proceedings
of machine learning research, pages 315â€“323, fort lauderdale, fl, usa, 11â€“13 apr 2011. pmlr.
[49] f. chollet. deep learning with python. manning publications company, 2017.
[50] norman ricker. the form and laws of propagation of seismic wavelets. geophysics, 18(1):10â€“40, 01 1953.
[51] martÃ­n abadi, ashish agarwal, paul barham, eugene brevdo, zhifeng chen, craig citro, greg s. corrado, andy davis, jeffrey dean,
matthieu devin, sanjay ghemawat, ian goodfellow, andrew harp, geoffrey irving, michael isard, yangqing jia, rafal jozefowicz, lukasz
kaiser, manjunath kudlur, josh levenberg, dandelion manÃ©, rajat monga, sherry moore, derek murray, chris olah, mike schuster,
jonathon shlens, benoit steiner, ilya sutskever, kunal talwar, paul tucker, vincent vanhoucke, vijay vasudevan, fernanda viÃ©gas, oriol
vinyals, pete warden, martin wattenberg, martin wicke, yuan yu, and xiaoqiang zheng. tensorflow: large-scale machine learning on
heterogeneous systems, 2015. software available from tensorflow.org.
[52] p. kingma diederik and ba jimmy. adam: a method for stochastic optimization. preprint arxiv:1412.6980, 2014.
[53] sanford weisberg. applied linear regression. john wiley & sons, inc., 2005.
[54] y. li and j. sun. 3d magnetization inversion using fuzzy c-means clustering with application to geology differentiation. geophysics, 81:j61â€“
j78, 2016.
[55] t. huang, y. zhang, and h. zhang. the benefit of tti reverse time migration for subsalt imaging, gulf of mexico. 2009.

r. s. m. freitas et al.

page 26 of 26



sensing and computational frameworks for improving
drill-string dynamics estimation
jean auriol, nasser kazemi, silviu-iulian niculescu

to cite this version:
jean auriol, nasser kazemi, silviu-iulian niculescu. sensing and computational frameworks for
improving drill-string dynamics estimation. mechanical systems and signal processing, 2021,
ï¿½10.1016/j.ymssp.2021.107836ï¿½. ï¿½hal-03167464ï¿½

hal id: hal-03167464
https://hal.science/hal-03167464
submitted on 12 mar 2021

hal is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not. the documents may come from
teaching and research institutions in france or
abroad, or from public or private research centers.

lâ€™archive ouverte pluridisciplinaire hal, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.

sensing and computational frameworks for improving drill-string dynamics
estimation
jean auriolâˆ—,a , nasser kazemib , silviu-iulian niculescua
a

universiteÌ paris-saclay, cnrs, centralesupeÌlec, laboratoire des signaux et systeÌ€mes, gif-sur-yvette, france.
b
department of chemical and petroleum engineering, university of calgary, canada.

abstract
in this paper, we consider the axial motion of a directional multi-sectional drill-string. the drillstring dynamics are represented by a distributed dynamical model (wave equations) coupled with
an ordinary differential equation at the downhole boundary (bit-rock interaction). the interaction
between the drill-bit and rock can introduce severe vibrations in the drill-string and result in
safety and performance issues. consequently, the performance of drilling is interwoven with our
knowledge of the subsurface. to address these problems, we propose a sensing and computational
framework for estimating the drill-string dynamics and the specific intrinsic energy of the rocks
while drilling. by exploiting the derived modelsâ€™ particular structure, we combine the drill-string
dynamics modeling with top-drive hook-load (force) and hook-speed (velocity) measurements to
estimate the force-on-bit without requiring the knowledge of the sub-surface. then, we record and
model the seismic radiation patterns of drill-bit rock interactions near the surface (i.e., seismic
while drilling). such an idea allows deriving an appropriate estimation of the rocksâ€™ intrinsic
energy while drilling. we introduce two alternative rock property estimation algorithms based
on direct parameter estimation and machine learning concepts to complete the analysis. the
different approaches are tested and validated in simulations. we discuss their respective advantages
and drawbacks. finally, we show how to extend our methodologies in the presence of non-linear
coulomb friction terms and of coupled axial-torsional oscillations.
key words: dynamical system, distributed system, backstepping, seismic-while-drilling,
parameter estimation, neural networks

1. introduction

5

knowledge of the subsurface reservoirs, such as hydrocarbons, minerals, and geothermal, and an
efficient method for accessing them are crucial in exploring and exploiting the reserves. accessing
the reservoirs involves a drilling process. in drilling, the interaction with the rock can generate
significant vibrations and result in an inefficient rate of penetration (rop). it may also raise
safety issues. control mechanisms may reduce these vibrations in the system [? ? ]. however, such
control mechanisms often depend on the properties of the drilled rock. in this paper, we design
three algorithms that provide a near-real-time estimation of the rockâ€™s nature for a directional
multi-sectional well.
âˆ—

corresponding author
email address: jean.auriol@l2s.centralesupelec.fr (jean auriol)
preprint submitted to mechanical systems and signal processing

march 12, 2021

10

15

20

25

30

35

40

45

50

1.1. insights in modeling drill-string dynamics: complexity and related issues
drill-string dynamic behavior often manifests itself as drill string vibrations. these vibrations
have been extensively studied in the literature [? ? ? ? ]. among them, torsional oscillations,
known as stick-slip, are characterized by a series of stopping â€“ â€stickingâ€ â€“ and releasing â€“ â€slippingâ€
â€“ events of the bit. these oscillations can reduce the rate of penetration, damage the well by
causing fatigue on the equipment, and eventually lead to premature failure of the bit [? ]. such
oscillations are caused by coulomb friction-induced side forces [? ] and by non-linear frictional
force actuating at the bit by contact with the rock [? ? ]. indeed, numerous models consider those
stick-slip oscillations are related to the velocity-weakening of the frictional force at the bit (stribecklike effect) associated with typical dry friction profiles (static friction and dynamic friction) [? ?
]. however, the stick-slip phenomenon can also occur off-bottom and does not require a velocity
weakening in the bit-rock interaction [? ? ? ]. in this context, to reduce the harmful effects of such
vibrations and improve the drilling deviceâ€™s performance, it appears necessary to clearly understand
the dynamics of the drill-string. in [? ? ], the axial and torsional dynamics are described by a
set of hyperbolic partial differential equations (pdes). these equations are usually governed
by wave equations combined with gravitational terms and a non-linear coulomb side-force [? ].
in the case of multi-sectional wells, the change of the characteristic line impedance between each
section may reflect the traveling waves [? ? ]. these reflections are taken into account in the
model using interconnection between the pdes [? ]. such a class of distributed models has been
extensively analyzed in [? ? ]. a validation against field data for the corresponding torsional
model has been proposed in [? ]. regarding the boundary conditions, specific attention must be
paid to the bit-rock interaction law at the downhole boundary [? ? ? ]. different models have
been proposed in the literature [? ] to represent the contact between the cutting device and the
drilling surface. for instance, the approach developed in [? ] for drag bits and adjusted in [? ] for
roller-cone bit divides the toolâ€™s action into three independent processes: a pure cutting process, a
pure indentation process, and a frictional contact process. such a model is shown to be consistent
with laboratory results in [? ]. all in all, the drilling system can be modeled as interconnected
hyperbolic pdes coupled with ordinary differential equations (odes) at their boundaries. in
that sense, they are related to the larger class of flexible systems for which extensive literature
already exist [? ? ? ? ]. note that other models have been proposed in [? ? ] for percussive
drilling systems.or when trying to deal with vibration control for drilling systems, a problem that
naturally arises is that such drilling models depend on the nature of the drilled rock (which is a
priori unknown) the dependence of the drill-string dynamic response to the bit-rock interaction
and rock properties is verified in [? ]. accordingly, estimating the drilled rockâ€™s characteristics (such
as its seismic velocity or its intrinsic specific energy) appears necessary to improve drilling devicesâ€™
performance. estimating rocksâ€™ seismic velocities while drilling is not an easy task as downhole
sensors are expensive and may raise potential technical risks. thus, the disturbance observer
developed in [? ? ? ] cannot be straightforwardly adjusted (since the downhole measurements
are not available). to address this problem, an approach based on seismic while drilling (swd)
has been proposed in [? ]. seismic-while-drilling is a technique that records the seismic energy
of the drill-bit rock interactions in the nearby well locations or near the surface [? ]. swd has
several applications. for example, swd measurements can provide the drill-bit position through
check shots or sonic calibrations through reflectivity characterization [? ]. imaging swd data can
provide subsurface structure around and ahead of the drill-bit [? ? ]. moreover, full-waveform
inversion of swd data can result in the high-resolution estimation of rocksâ€™ seismic properties in
2

55

60

65

70

75

80

85

90

95

the sub-surface [? ]. here, we extend the formation seismic velocity estimation method of [?
], which is designed for uni-sectional vertical well, to a directional multi-sectional well without
considering coulomb friction or torsional motion. in drilling environments where recording the
swd data is not feasible or lacks quality, we need to explore alternative options. since the method
developed in [? ] provides an expression of the downhole force and velocity as functions of topdrive measurements, it is possible to use classical parameter estimations techniques [? ] on the
downhole boundary condition to estimate the rock properties. also, machine learning approaches
are efficient in estimating parameters based on available measurement signals [? ? ].
1.2. towards a unified methodology of modeling, sensing, analysis, and control
one of the practical problems arising in drilling consists of deriving an appropriate real-time
estimation of the nature of the drilled rock. to the best of the authorsâ€™ knowledge, such a problem
has not received a satisfactory answer in the open literature. in our opinion, the proposed results
and underlying ideas represent a new step in deriving an appropriate engineering-based solution.
more precisely, the main contributions of the paper are twofold: (i) first, improving the drill-string
dynamics parametersâ€™ estimations of the nature of the drilled rock in some unified dynamical model
based on a better understanding of the underlying transport phenomena by exploiting the dynamics
interconnection as well as the interconnection structure of the corresponding process; (ii) second,
explicitly presenting three algorithms to estimate the seismic velocity of the rock, the energy of
the rock interacting with the drill-bit.
to improve the parameter estimation, we will propose a recursive dynamics interconnection
framework allowing us to reduce the existing gap between modeling and estimation in the context
of complex dynamics as whose described by pdes to represent drilling dynamics. in our opinion,
such a recursive interconnection framework is better adapted to sensing and estimation since it
allows structuring the model on some simpler and more realistic interconnected dynamical subsystems (blocks) and better exploiting the structure of the interconnection. to the best of the authorsâ€™
knowledge, this framework represents a novelty in the open literature allowing a global estimation,
analysis, and control of the corresponding process with a reduced computational cost. it is worth
mentioning that the computational effort is essential when one of the focuses is on real-time estimations. finally, this framework is a natural environment to develop estimation techniques (as it
is done in this paper) or to solve control problems (for instance by adjusting the recent lyapunov
techniques developed for flexible systems in [? ? ? ? ] or the thrust force control methodology
developed in [? ]), to develop efficient state-observers [? ].
second, in the proposed framework, we present three algorithms to estimate the specific intrinsic
energy of the rock interacting with the drill-bit. we consider the case of the axial motion of a
directional multi-sectional well, for which we neglect the coulomb friction forces. however, we
show how to extend our procedures in the presence of coulomb non-linear friction force acting on
the well and to coupled axial-torsional oscillations. such a characterization of the formation is a new
step towards a subsurface-aware drilling system. the first algorithm uses swd measurements and
drill-string dynamics modeling. the algorithm matches the processed direct arrivals of primary
and secondary waveforms to the modeled radiation patterns to estimate rocksâ€™ seismic velocity
while drilling. to model the radiation patterns, we use drill-string dynamics modeling and topdrive measurements to simulate the force-on-bit. the second algorithm is based on parameter
estimation techniques applied to the model we have for the bit-rock interaction boundary condition.
such estimation techniques are classical, but we believe that their development in the proposed
framework is a novelty. finally, our last algorithm is based on a machine learning approach.
3

100

computing the top-drive measured signalsâ€™ spectra, we train a neural network that estimates the
rockâ€™s nature. in that sense, our approach is related to the results developed in [? ? ? ]
1.3. contributions of this work

105

110

115

120

125

130

135

the proposed paper presents a new theoretical framework (which is a novelty in terms of
systems theory) that is applied to a complex petroleum engineering problem (estimation of the
axial motion of a drilling system and the nature of the rock interacting with the drill-bit). more
precisely, the main contributions of this paper are as follows:
â€¢ from a mathematical point of view, we develop in this paper a new recursive dynamics interconnection framework. although this framework is developed here on a drilling application,
it can be extended to a large class of balance laws systems [? ]. this framework is a natural
environment to develop estimation techniques with a reduced computational cost. we believe
it is an important contribution from the system theory perspective.
â€¢ from a petroleum engineering point of view, we apply our recursive framework to estimate
the axial motion of the drill-string, which is a novelty and represents a step towards automated closed-loop geosteering. the distributed model we use to describe the axial dynamics
is based on recent contributions [? ? ] and captures a large part of the possible dynamics
for a significantly extended range of well lengths and surveys compared to the more standard lumped approximations. the development of estimation techniques for this class of
distributed model is a novelty as most of the contributions in the literature either deal with
finite-dimensional approximations of the drilling device (to apply already existing techniques
as kalmanâ€™s filter [? ]) or develop operational settings to measure the downhole state directly. the long latency (1 to 20 s) of downhole telemetry has made automatic feedback
control significantly slower than manual control [? ? ], which reduces its interest in terms of
control application.
â€¢ the third contribution consists of using the estimation of the drill-bit source signature to
estimate the nature of the rock interacting with the drill-bit. although the three proposed
algorithms are based on existing techniques (swd, machine-learning, and parameter estimation techniques), their application in this context constitutes an innovation.
â€¢ finally, the last contribution of this paper consists of two extensions on how to adjust the
proposed approach to coupled axial-torsional oscillations and the presence of coulomb friction
terms.
note that the different algorithms are tested in simulations against reliable models. these models
are accepted by a large part of the petroleum engineering community as reliable. extended simulations have been proposed in [? ? ] to test these models and all the trends predicted by the
models are supported by field measurements [? ]. however, complete validation of the proposed
approach would require several tests against field data. we believe that the development of an
experimental setting is outside this paperâ€™s scope and would be a contribution by itself.
1.4. paper structure and notations
the paper is organized as follows: in section 2, we present the mechanical and sensing systems
under consideration. in section 3, we derive a distributed model that describes the axial dynamics
4

140

145

of the drill-string. as we consider a multi-sectional drill-string, specific attention is paid to the
connections between the different pipes. in section 4, we express the drill-bit axial force and the
drill-bit velocity as functions of the top-drive force and velocity by using an iterative backstepping
approach. in section 5, we present three algorithms to compute the specific intrinsic energy of
the drilled rock. the first algorithm uses seismic-while-drilling technique, the second is based on
direct parameter estimation, and the third algorithm uses a neural network. simulation results are
given in section 6. also, we compare the advantages and drawbacks of each algorithm. finally,
we extend our approach to the case of non-linear coulomb side-forces and coupled axial-torsional
dynamics in section 7.
in what follows, given a set Ï‰ âŠ† r, its characteristic function will be denoted by

1 if Î¸ âˆˆ Ï‰
1Ï‰ (Î¸) =
0 otherwise.

2. mechanical and sensing systems under consideration
150

155

160

165

170

175

we consider in this paper the mechanical part of a drilling system. a drilling device comprises
three components: the surface drill rig, which includes a rotating mechanism (usually a rotary
table or a top drive suspended over the drill floor by the traveling block), the drill string, and
the bottom hole assembly (bha). the torsional and axial motions generated at the surface are
transferred to the drill string and the bha. the drill string is an interconnection of pipes that
are steel tubes with a length of typically 10 m. these pipes are usually run in tension to avoid
the effect of fatigue due to a potential helical buckling. they are hollow so that a mud pump
can inject a drilling fluid to clean, cool, and lubricate the bit to evacuate the rock cuttings. the
bha comprises the bit (a rock cutting device), a series of relatively heavy pipe sections, known
as drill collars (much thicker pipes that provide the necessary weight to perform the perforation),
stabilizers (at least two spaced apart) which prevent the drill string from unbalancing, and â€shock
subsâ€ that absorb vibrations between the bit and the drill-collars. while the bha length remains
constant, the drill pipesâ€™ total length may increase as the borehole depth does. the weight exerted
on the bit impacts the cutting processâ€™s performance, measured by the rate-of-penetration (rop).
the nature of the boundary conditions at the bitâ€“rock interface is a critical aspect of the model
and is discussed in detail below. this mechanical part is combined with a hydraulic system to
maintain the bottom-hole circulating pressure (bhcp) between pre-specified constraints. such
a drilling system is schematically pictured in figure 1.
moreover, interactions between the drill bit and rock generate significant elastic energy. understanding these interactions and modeling the seismic radiation patterns are crucial in improving
the drilling performances [? ]. drill bit-rock interactions generate compressional (p) and vertical
components of shear (sv) waves. these waves propagate through the sub-surface and can reach
the sensing devices (geophones or hydrophones). in onshore environments, geophones record the
particle velocity, and hydrophones, in offshore environments, measure the pressure component. the
drill bit-rock interaction wavefields recorded by the sensors near the surface are known as swd
data. drill bit-rock interactions can also generate reflected wavefields that travel through the drill
string. these wavefields can be recorded by accelerometers mounted on top-drive. the accelerometers provide hook-load and hook-speed measurements. the sensing tools give vital information
about the sub-surface, which helps us efficiently model the drill string dynamics.
5

table 1: nomenclature

parameters
e
Ï
cÎ¾
kÎ¾
a
l
ÏÌ„
mb
Ï‰bit
a
Î¶
wf

Ïf
vp
vs
Âµas
Âµak
vc
dependent variables
Î¾
w
v
u, z
w0
Ï†, Î³
ur
uÏ†
fa
independent variables
t
x

pipe youngâ€™s modulus (p a)
pipe mass density (kg/m3 )
axial wave velocity (m/s)
viscous shear stresses (1/s)
cross-sectional area of the drill-string (m2 )
drill-string length (m)
submerged weight (kg/m3 )
mass of the lumped bha (kg)
bit angular velocity (rad/s)
bit radius (m)
characterization of the cutting force
friction weight (n )
intrinsic specific energy of the rock (j/cm3 )
density of the rock (kg/m3 )
rock compressional velocity (m/s)
rock shear velocity (m/s)
static friction coefficient (1/kg)
kinetic friction coefficient (1/kg)
axial velocity threshold (m/s)
axial displacement (m)
axial force (n )
axial velocity (m/s)
riemann invariants (m/s)
weight on the drill-string (n )
inclination of the drill-string (deg)
p-wave radiation
s-wave radiation
normalized axial force (n )
time (s)
relative position (m)

6

figure 1: schematic representation of the drilling and sensing system
(figure modified from [? ]).

figure 2: schematic indicating the distributed drill string lying in deviate borehole.

3. drill-string model
180

in this section, we present a model that describes the mechanical dynamics of the drill-string.
this model is similar to the one presented in [? ] and is inspired by [? ? ]. a flowchart of the
mathematical representation of the system is given in figure 5 at the end of the section.
3.1. distributed axial dynamics of the drill-string
we model the dynamics of a directional drilling system of length l (figure 2). in this model,
x denotes the curvilinear abscissa, x = 0 is the top-drive position, and x = l is the position of the
drill bit. we denote Ï†(x) the wellâ€™s inclination at the position x. let us denote Î¾(t, x) the axial
displacement of the drill-string. it is a function of (t, x) evolving in {(t, x) | 0 < t < t, x âˆˆ [0, l]}
(where t is a positive time). the axial force associated to Î¾ can be found from the strain, given
as the local relative compression:
w(t, x) = ae

(Î¾(t, x) âˆ’ Î¾(t, x + dx))
,
dx

(1)

a being the cross-sectional area of the drill-string, e being its youngâ€™s modulus and dx â†’ 0 the
infinitesimal axial position increment. the axial velocity satisfies
v(t, x) =

âˆ‚Î¾(t, x)
.
âˆ‚t

these states are pictured in figure 2. the ratio diameter/length of a drillstring, typically less
than 10âˆ’4 , implies that the drillstring can be modeled using an eulerâ€“bernoulli beam model.
more precisely, using the distributed model given in [? ? ], we can derive the dynamics of interest

7

by assuming elastic deformations and using equations of continuity and state. the axial motion
satisfies the following wave partial differential equation
2
âˆ‚2Î¾
âˆ‚Î¾
ÏÌ„
2âˆ‚ Î¾
(t,
x)
âˆ’
c
(t, x) = âˆ’kÎ¾ (t, x) + g sin(Ï†(x)),
Î¾
2
2
âˆ‚t
âˆ‚x
âˆ‚t
Ï

where cÎ¾ =

q

e
Ï , Ï being the pipe mass density

(2)

and kÎ¾ is a damping coefficient representing

the viscous shear stresses acting on the pipe. the term h(x) = ÏÌ„Ï g sin(Ï†(x)) accounts for the
acceleration of gravity acting on the submerged weight ÏÌ„. it is a simple (but still realistic and
consistent) model for the gravitational force. we neglect for the moment the friction between
the drill-string and the borehole. from (2), we have that the axial force and velocity satisfy the
following set of pdes
âˆ‚w(t, x)
âˆ‚v(t, x)
+ ae
= 0,
âˆ‚t
âˆ‚x
1 âˆ‚w(t, x)
âˆ‚v(t, x)
+
= âˆ’kÎ¾ v(t, x) + h(x).
âˆ‚t
aÏ âˆ‚x

185

190

195

(3)
(4)

the topside weight on the drill-string, w(t, 0), corresponds to the system actuation. the downhole
boundary condition at x = l is obtained from a force balance on the lumped bottom-hole assembly
(bha). these two boundary conditions will be introduced below. model (3)-(4) does not consider
the coulomb friction between the drill string and the borehole, also known as the side force.
it has been shown in [? ] that for torsional oscillations, the normal component of this coulomb
friction term (usually modeled using a differential inclusion) can have significant effects on stability,
even when the bit is off-bottom since it can be a source of stick-slip. consequently, the axial
components of such coulomb non-linear friction terms may also have a non-negligible effect on the
dynamics (even if this has not been broadly studied in the literature). we choose not to include this
differential inclusion into our model to simplify the design of our different algorithms. however, we
will show in section 7.1 that our procedures can be extended in the presence of such a non-linear
term. note that the proposed model has been extensively analyzed in [? ? ]. all the trends
predicted by the models are supported by field measurements [? ]. moreover, validation against
field data (at least for the torsional part of the model) has been proposed in [? ].
3.2. discontinuities of a multiple sectioned drill-string
the lower part of the drill-string is usually made up of drill collars that may have a great impact
on the global dynamics due to their inertia [? ]. in particular, these pipes may have different
lengths, density, inertia or youngâ€™s modulus. this change of the characteristic line impedance may
cause reflections in the traveling waves. let us assume we have n different sections (n âˆˆ n), and
let us denote xi the spatial coordinate of the junction point between the (i âˆ’ 1)th -section and the
ith -section. let us denote x1 = 0, xn +1 = l and (wi (t, x), v i (t, x)) the force and velocity along the
ith section of the drill-string. the corresponding physical parameters will also be expressed using
the superscript i (for instance Ïi will be the density of the ith section). the boundary conditions
at the transition are given by the following continuity constraints
v i (t, xi+1 ) = v i+1 (t, xi+1 ),

wi (t, xi+1 ) = wi+1 (t, xi+1 ).

when there is no ambiguity, this superscript will be omitted to ease the notations.
8

(5)

figure 3: roller-cone bits used in softer and shallower formations (a); roller-cone bits used in deeper
and harder formations (b) (source: adapted from [?
]).

200

figure 4: forces acting on an inverted v shape tooth
(source: adapted from [? ]).

3.3. topside boundary condition
the drill-string is connected at the top to the top-drive suspended over the drill floor by
the traveling block. several drill lines connect this block, with one attached to the deadline anchor
and the other being spooled on a drum controlled by ac induction motors [? ]. thus, we can
consider that the operator controls the weight on the drill-string w0 (t) = w(t, 0). this yields

âˆ’ea
205

âˆ‚Î¾(t, x)
|x=0 = w0 (t),
âˆ‚x

(6)

where w0 is considered as a (known) input.
3.4. downhole boundary condition: bit-rock interaction
in this section, we compute the downhole boundary condition. as the length of the bha
(â‰ˆ 200m) is much smaller than the one of the drill-string (â‰ˆ 2000m), its effect can be lumped into
an ode coupled with the drill-string [? ? ]. thus, the downhole boundary condition at x = l
can be obtained from a force balance on the lumped bha. this yields
mb

âˆ‚v
ÏÌ„
(t, l) = âˆ’wb (v(t, l), w(t, l)) + w(t, l) + mb g,
âˆ‚t
Ï

(7)

where mb is the mass of the lumped bha and wb (Â·, Â·) the force acting from the rock on the bha
through the drilling bit, known as the weight on bit (wob).
this force balance requires the expression of the wob. we consider a roller-cone bit. to
express the bit-rock interface laws, we can follow the approach developed in [? ] for drag bits
(consistent with laboratory results). as suggested in [? ], the toolâ€™s action is decomposed into
three independent processes: a pure cutting process taking place ahead of the cutting face, a pure
indentation process, and a frictional contact process mobilized along with the interface between
9

w0 (t)
top ode (6)

pde system (3)-(4) for w and v

downhole ode (9)
rock

y : measurement

figure 5: flowchart of the mathematical model of the drilling device.

the wear flat and the rock. integrating the effects of all the individual cutters, the weight-on-bit
wb can be decomposed into a contribution associated with the forces transmitted by the cutting
face of each cutter (wc ), a contribution corresponding to the indentation process (wi ) and another
corresponding to the forces acting on the cutter wear flats (wf ). in what follows, we choose to
include the contribution wi into wf since both are constant. these two forces are pictured in
figure 4. following the approach elaborated within the context of pdc bits [? ? ], the cutting
process is split into three successive regimes. a frictional contact process dominates the first phase
(phase i) due to bit dullness. during the second phase (phase ii), contact forces are fully mobilized,
and additional forces contribute to cutting. finally, during the last phase (phase iii), the frictional
contact forces increase due to insufficient hole cleaning. ideally, drilling should be performed to the
foundering point (phase ii/iii transition). as shown in [? ], the expression of the wob depends
on the phase. we will assume that we are in phase ii (cutting phase) and that the depth of cut
is greater than the combined depth of cut at phase ii/iii transition. consequently, the wob is
expressed as
wb (v(t, l), w(t, l)) = wf + wc = wf + kd(t) = wf + aÎ¶d(t),

(8)

where wf is a friction weight independent of the bit velocity (and therefore constant) while k = aÎ¶,
with a being the bit radius, Î¶ a characterization of the cutting angle and  the intrinsic specific
energy of the rock [? ? ]. the function d(t) is the combined depth of cut per revolution [? ? ? ].
it satisfies the following equation,
v(t, l)
d(t) =
,
Ï‰bit (t)
210

215

where the bit angular velocity Ï‰bit is assumed known here. we have previously presented a fully
validated torsional model to estimate Ï‰bit which may be integrated into the present here through a
coupling at the drill-bit [? ]. more details about axial-torsional couplings are given in section 7.2.
experimental evidence for the proposed expression of the weight-on-bit (equation (8)) for drag bits
have been obtained with a small drilling machine in [? ]. for a roller-cone bit, such an expression
has been validated in [? ] with series of laboratory tests at atmospheric pressure conducted with
an in-house designed drilling rig together with published experimental data.
thus, the boundary condition (7) rewrites as
mb

âˆ‚2Î¾
aÎ¶ âˆ‚Î¾(t, l)
âˆ‚Î¾(t, l) ÏÌ„
âˆ’ wf âˆ’ ea
+ mb g.
(t, l) = âˆ’
2
âˆ‚t
Ï‰bit âˆ‚t
âˆ‚x
Ï

(9)

3.5. problem under consideration
the present paperâ€™s objective is to provide a near real-time estimation of the nature of the
drilled rock, i.e., intrinsic energy  or seismic velocity, to improve drill-string dynamics estimation.
10

220

225

230

235

240

to properly model the drill-string dynamics, we need to know the seismic velocity of the drilled
rocks. however, before drilling, the seismic velocities provided by surface seismic processing may
be inaccurate. here, we extend the results of [? ] to the case of a non-vertical multi-sectionals
drill-string. moreover, we present two new algorithms for estimating the seismic velocities of rocks.
the developed algorithms only require knowledge of the physical properties of the drilling system:
1. the first algorithm uses seismic-while-drilling measurements and extends the methodology
of [? ]. in this procedure, we first express the force exerted on the bit as a function of
the (measured) top-drive force and velocity without depending on the downhole boundary
condition or on the subsurface. we estimate the rockâ€™s seismic velocity by combining this
expression with the knowledge of far-field radial and angular displacements (obtained by
using seismic sensors).
2. the second algorithm directly uses the boundary condition (9). we express the force exerted
on the bit and the corresponding velocity as functions of the (measured) top-drive force and
velocity. then, we apply signal filtering and parameters estimation techniques to equation
(9) to obtain the intrinsic specific energy of the rock.
3. the last algorithm is based on a machine learning approach. applying a fast fourier transform (fft) to the measured outputs (top-drive force and velocity) and choosing relevant
attributes such as the dominant frequencies and their corresponding gains. then, by pairing
the relevant attributes to their corresponding , we run a neural network learning algorithm
to train the model. the training is done with a set of thousand simulations for which the
parameters of interest are modified between each run.
also, we discuss the advantages and drawbacks of each procedure. moreover, we show how the
proposed algorithms can be extended in the presence of non-linear coulomb friction terms (see
section 7.1) and to systems with coupled axial-torsional oscillations (see section 7.2).
4. expression of the drill-bit axial force and velocity

245

250

among the three algorithms we introduce in this paper, two of them require the knowledge of
the axial force and velocity exerted at the drill-bit. it has been shown in [? ] that it is possible (for
a vertical uni-sectional well) to express the drill-bit source signature as a function of top-drive force
and velocity measurements. the proposed method was based on the riemann invariants and the
backstepping methodology. here, we extend the results of [? ] to the case of our multi-sectional
non-vertical well.
4.1. derivation of riemann invariants
the riemann invariants of a hyperbolic pde are the states corresponding to a transformation
of the system which has a diagonalized transport matrix, i.e. the system can be written as a series
of transport equations only coupled through the source terms [? ]. on the set {(t, x)| 0 < t <
t, x âˆˆ [xi , xi+1 ]}, we define the riemann invariants as
i
kÎ¾

i
kÎ¾

x
âˆ’ ix
âˆ‚
âˆ‚
âˆ‚
âˆ‚
2ci
2c
u (t, x) = ( Î¾ i (t, x) âˆ’ ciÎ¾ Î¾ i (t, x))e Î¾ , z i (t, x) = ( Î¾ i (t, x) + ciÎ¾ Î¾ i (t, x))e Î¾ .
âˆ‚t
âˆ‚x
âˆ‚t
âˆ‚x
i

11

(10)

consequently, on each section, equation (2) rewrites
ki

kÎ¾i ciÎ¾ x
âˆ‚ i
âˆ‚
u (t, x) + ciÎ¾ ui (t, x) = âˆ’ e Î¾ z i (t, x) + hi1 (x),
âˆ‚t
âˆ‚x
2

(11)

ki

kÎ¾i âˆ’ ciÎ¾ x
âˆ‚ i
âˆ‚
z (t, x) âˆ’ ciÎ¾ z i (t, x) = âˆ’ e Î¾ ui (t, x) + hi2 (x),
âˆ‚t
âˆ‚x
2
i
kÎ¾
x
2ci
Î¾

âˆ’

(12)

i
kÎ¾
x
2ci
Î¾

and hi2 (x) = h(x)e
. in the riemann coordinates, the boundary
where hi1 (x) = h(x)e
conditions at the junctions (5) rewrite for i < n + 1
z i (t, xi+1 ) = ai1 ui (t, xi+1 ) + ai2 z i+1 (t, xi+1 ),

(13)

i+1

(14)

u

(t, xi+1 ) = ai3 ui (t, xi+1 ) + ai4 z i+1 (t, xi+1 ),

where
ki

ai1 =

Î¾
1 âˆ’ z i âˆ’ i xi+1

1 + zi

e

c
Î¾

, ai2 =

2z i

(

1 + zi

e

ki+1
ki
Î¾
âˆ’ Î¾i )xi+1
2c
2ci+1
Î¾
Î¾

, ai3 =

zi âˆ’ 1

1 i i
a , a =
e
zi 2 4 1 + zi

ki+1
Î¾
xi+1
ci+1
Î¾

,

(15)

where we have denoted the relative magnitude of the impedance as
i

z =

ci+1
Î¾

ciÎ¾

/
.
e i ai e i+1 ai+1

(16)

the boundary condition (6) remains unchanged
u1 (t, 0) = z 1 (t, 0) +

2c1Î¾
e 1 a1

w0 (t),

(17)

while the boundary condition (9) rewrites
k

âˆ’ cÎ¾ l

z(t, l) = âˆ’e
xÌ‡(t) = âˆ’

Î¾

k

u(t, l) + 2e

âˆ’ 2cÎ¾ l
Î¾

x(t),

wf
aÎ¶
ÏÌ„
eas
+ gâˆ’
x(t) âˆ’
(z(t, l)e
mb Ï‰bit
mb Ï
2cÎ¾ mb

(18)
kÎ¾
l
2cÎ¾

âˆ’ u(t, l)e

k
âˆ’ 2cÎ¾ l
Î¾

).

(19)

4.2. delayed equations
in this section, we adjust the ideas of [? ] to the case of our non-vertical well with multiple
connections. the objective is to express our pdes system as delayed equations. such a delay
approach is particularly suitable for this kind of systems due to the transport structure. it allows using the wide range of tools already developed for time-delay systems. let us consider the
extension for the backstepping volterra transformation [? ? ] that has been introduced in [? ]
z x
i
i
Î± (t, x) = u (t, x) +
(kiuu (x, y)ui (t, y) + kiuz (x, y)z i (t, y))dy,
(20)
xi
z x
Î² i (t, x) = z i (t, x) +
(kizu (x, y)ui (t, y) + kizz (x, y)z i (t, y))dy,
(21)
xi

12

255

where the kernels kiuu , kizu , kiuz and kizz are continuous functions defined on the domain ti =
{(x, y) âˆˆ [xi , xi+1 ]2 , y â‰¤ x}. they satisfy a set of pdes (that admits a unique solution) that is
given in [? ] (replacing 0 by xi ). note that theses kernels can be explicitly computed following the
approach given in [? ].
differentiating (20)-(21) with respect to time and space and integrating by parts, one can show
that the states Î±i and Î² i satisfy the following set of transport pdes
âˆ‚
âˆ‚ i
Î± (t, x) + ciÎ¾ Î±i (t, x) = âˆ’ciÎ¾ kiuz (x, xi )Î² i (t, xi ) + hi3 (x),
âˆ‚t
âˆ‚x
âˆ‚ i
âˆ‚
Î² (t, x) âˆ’ ciÎ¾ Î² i (t, x) = ciÎ¾ kizu (x, xi )Î±i (t, xi ) + hi4 (x).
âˆ‚t
âˆ‚x

(22)
(23)

where
hi3 (x) = hi1 (x) +

z x

kiuu (x, y)h1 (y) + kiuv (x, y)h2 (y)dy,

x

hi4 (x) = hi2 (x) +

z ix

kivu (x, y)h1 (y) + kivv (x, y)h2 (y)dy.

xi

applying the method of characteristics on (22)-(23), for any x âˆˆ [xi , xi+1 ] and for any t â‰¥ cxÎ¾ , we
get
! z xâˆ’xi
x âˆ’ xi
ci
i
i
Î¾
(24)
,
x
hi3 (x âˆ’ ciÎ¾ s) âˆ’ ciÎ¾ kiuz (x âˆ’ ciÎ¾ s, xi )Î² i (t âˆ’ s, xi )ds,
Î± (t, x) = Î± t âˆ’
+
i
i
cÎ¾
0
! z xâˆ’xi
x
âˆ’
x
ci
i
Î¾
Î² i (t, x) = Î² i t +
(25)
, xi âˆ’
hi4 (x âˆ’ ciÎ¾ s) âˆ’ ciÎ¾ kizu (x âˆ’ ciÎ¾ s, xi )Î±i (t + s, xi )ds.
i
cÎ¾
0
following the path of [? ], we obtain the following expressions (the complete computations are
given in appendix)
iâˆ†
kÎ¾
i

iâˆ†
kÎ¾
i

ki âˆ†

e i ai âˆ’ Î¾ i
âˆ†i
âˆ†i
1 âˆ’ 2ciÎ¾ i
âˆ†i
2ci
w (t âˆ’ i , xi )
wi (t, xi+1 ) = i (e 2cÎ¾ v i (t âˆ’ i , xi ) âˆ’ e Î¾ v i (t + i , xi )) + (e
2
2cÎ¾
cÎ¾
cÎ¾
cÎ¾
+e

iâˆ†
kÎ¾
i
2ci
Î¾

z âˆ†i
+

ci
Î¾

âˆ†i
wi (t + i , xi )) +
cÎ¾
ciÎ¾

âˆ’ ii ea
âˆ†

(e

i
kÎ¾
xi
2ci
Î¾

i
kÎ¾

z âˆ†i
ci
Î¾

âˆ†
âˆ’ ii
c
Î¾

âˆ’

fui (s) âˆ’ e

(e

xi
2ci
Î¾

i
kÎ¾
xi
2ci
Î¾

c
Î¾

13

fui (s) + e

âˆ’

i
kÎ¾
xi
2ci
Î¾

fzi (s))v i (t âˆ’ s, xi )ds

fzi (s))wi (t âˆ’ s, xi )ds + giw ,

(26)

and
ki âˆ†i

i

ki âˆ†i

kÎ¾ âˆ†i
Î¾
âˆ’ Î¾i
ciÎ¾
âˆ†i
1 âˆ’ 2c
âˆ†i
âˆ†i
2ci
2c
i
i
i
Î¾
Î¾
Î¾ w i (t âˆ’
v (t âˆ’ i , xi ) + e
, xi )
v (t, xi+1 ) = (e
v (t + i , xi )) +
(e
i
i
2
2e a
cÎ¾
cÎ¾
ciÎ¾
iâˆ†
kÎ¾
i

âˆ’e

+

2ci
Î¾

âˆ†i
wi (t + i , xi )) +
cÎ¾

z âˆ†i

ciÎ¾

âˆ†
âˆ’ ii
c
Î¾

ea

ci
Î¾

(gui (s)e

i
kÎ¾
xi
2ci
Î¾

z âˆ†i
ci
Î¾

âˆ†

âˆ’ ii

(gui (s)e

i
kÎ¾
xi
2ci
Î¾

+ gzi (s)e

âˆ’

i
kÎ¾
xi
2ci
Î¾

)v i (t âˆ’ s, xi )ds

c
Î¾

âˆ’ gzi (s)e

âˆ’

i
kÎ¾

xi
2ci
Î¾

)wi (t âˆ’ s, xi )ds + giz ,

(27)

where âˆ†i = xi+1 âˆ’ xi , where fui , fzi , gui and gzi are defined by equations (52)-(55) and where
i
kÎ¾

i
kÎ¾

xi+1
e i ai âˆ’ 2ciÎ¾ xi+1 i
2ci
Î¾
(e
h
âˆ’
e
hzi ),
giw =
u
2ciÎ¾

i
kÎ¾

i
kÎ¾

xi+1
1 âˆ’ 2ci xi+1 i
2ci
giz = (e Î¾
hu + e Î¾
hzi ),
2

(28)

the functions hui and hzi being defined by equations (49) and (51).
4.3. expression of the downhole velocity and force

260

equations (26) and (27) allow us to compute the velocity and the force at the point x = xi+1
simply knowing past and futures values of these states at the point x = xi . using the continuity of
the force and velocity at each junction, we can iterate the procedure and consequently express the
downhole velocity and force as
of past and futures values of the topside states (the total
p functions
âˆ†i pn âˆ†i
,
time-delay window being [âˆ’ n
i=1 ci ]). obviously, in the case of a uni-sectional vertical
i=1 ci
Î¾

265

Î¾

well, this corresponds to [? , equation (49)]. it is interesting to note that such an expression does
not directly depend on the downhole boundary condition. it is now possible to use the drill-bit
source signature to estimate the nature of the drilled rock.
5. estimation of the specific intrinsic energy 

270

275

the intrinsic specific energy refers to the amount of energy required to cut a unit volume of
rock. this value is dependent on cutter geometry, depth of cut, and rock. however, similar to
scratch test studies in rock mechanics, we assume that the specific intrinsic energy is a constant
quantity characterizing a particular combination of cutter geometry and rock. this section presents
three different algorithms to estimate the parameter  and discuss their respective advantages and
drawbacks. two of these procedures require the knowledge of the drill-bit source signature. we
use the drill-bit source signature estimation methodology in these algorithms, which is presented
in the previous section. the different algorithms are summarized by algorithms 1, 2 and 3 .
5.1. wavelet-based approaches
the two first algorithms we present are based on the knowledge of the drill-bit source signature.
the first one combines the drill-bit force and velocity estimation with seismic while drilling
techniques, while the second one estimates the parameter  directly from the model.
14

5.1.1. seismic while drilling estimation (algo. 1)
this section shows how to estimate intrinsic specific energy for different rocks using seismicwhile-drilling (swd) data. richard and dagrain [? ], by using different rock types, study the
relationship between the parameter  and the uniaxial compressive strength (ucs). they show
that by expressing the  as stress rather than energy, the parameter  is correlated with ucs
(m p a) â‰ˆ u cs(m p a).

(29)

several recent works aim to relate the rock strength to the seismic velocity of primary wave [?
]. for example, by comprehensive analysis of different rock types, ranging from sedimentary to
metamorphic, sharma and singh [? ] show that the seismic velocity of the primary wave and ucs
are linearly correlated, and the relationship is as follows
u cs = 0.0642vp âˆ’ 117.99,

(30)

where vp (m/s) is the seismic velocity of the primary wave. this relationship is derived based on
linear regression, and a strong correlation of r2 = 0.9022 is reported. by plugging equation (30)
into equation (29), we get
 â‰ˆ 0.0642vp âˆ’ 117.99.
(31)
280

285

290

hence, by knowing the rockâ€™s seismic velocity that the system is drilling into, the estimation of 
is possible. next, we show how to estimate the velocities of rocks while drilling.
swd measurements record the radiated elastic energy from the drill-bit rock interaction that
is traveled through the earthâ€™s structure. for example, variations in the amplitude of p-waves and
s-wavesâ€™ direct arrivals in the processed swd measurements, after removing the source signature,
reflect the changes in the rock properties near the drill-bit. in other words, changes in the energy
of p-waves and s-wavesâ€™ direct arrivals can imply that the bit is turning right and that the wellâ€™s
trajectory is modified. the direct arrivals can be used in a relative sense to infer the relative
changes in the rock properties such as their seismic velocities or unconstrained rock strengths [? ].
however, rector and hardage [? ] show that the radiation patterns of the direct arrivals depend
on the vp and vs velocities of the rock interacting with the drill-bit.
in the deviated well with an inclination angle of Î³, the radiation pattern for the primary wave
is as follows
a1 cos(Ï†j + Î³)
rj
urj (rj , Ï†j , t) =
wÌ‚(t âˆ’ , l),
(32)
2
Ïf vp rj
vp
where rj is the distance from drill-bit to the j th receiver near the surface, Ï†j is the opening angle
between the drill-bit and j th receiver, measured relative to z axis, Î³ is an inclination angle of
drill-string, which is measured as a deviation angle with respect to z axis, t is time, l is the
length of drill-string, Ïf is the density of the interacting rock with the drill-bit, wÌ‚ = w w is
the auto-correlation of drill-bit source signature, a1 is a constant, and urj is the far field radial
component of the primary wave radiation pattern recorded by the j th receiver. in most rocks we
have Ïf = 1.74vp0.25 , hence equation (30) simplifies to
urj (rj , Ï†j , t) =

rj
a1 cos(Ï†j + Î³)
wÌ‚(t âˆ’ , l).
1.74vp2.25 rj
vp

(33)

similarly, the far-field radiation pattern for secondary wave reads
uÏ†j (rj , Ï†j , t) =

a1 sin(Ï†j + Î³)
rj
wÌ‚(t âˆ’ , l).
0.25
2
1.74vp vs rj
vs
15

(34)

by matching the direct arrivals of the primary wave in the source compensated swd data to the
radiation pattern in equation (33), we can estimate the primary velocity of rock interacting with
the drill-bit. however, there are three unknowns in equation (33), i.e., vp , a1 , and w. hence, first,
we need to estimate the drill-bit source signature. using equation (26) and the iterative approach
described in section 4.3, we can consider as available the (delayed) values of the downhole force,
i.e., drill-bit source signature. now that we have the estimate of the drill-bit source signature w,
the only unknowns in equation (33) are the scaling factor a1 , and the primary velocity of rock vp .
the cross-correlation of the estimated source signature with the swd data gives the primary and
secondary direct arrivals. by least-squares matching of the primary direct arrivals in the source
compensated swd data, urobs
(rj , Ï†j , t), with the radiation pattern represented in equation (33)
j
and after plugging the auto-correlation of the estimated source signature, we estimate the primary
wave velocity of the rock by minimizing the following cost function
{aÌ‚1 , vË†p } = argmin
a1 ,vp

n x
m
x
(urobs
(rj , Ï†j , ti ) âˆ’ urcal
(rj , Ï†j , ti ))2 ,
j
j

(35)

i=1 j=1

where urcal
is the radiation pattern of primary wave estimated by using equation (33), n is the
j
number of time samples in the data, and m is the number of receivers or channels that are
recoding the swd data. equation (35) is solved by the least-squares matching method. finally,
after estimating the primary wave velocity of rock, we estimate  using (31).
after solving for vp , we can also apply the same procedure to estimate the secondary velocity
of rocks vs . to do so, we minimize
vË†s = argmin
vs
295

300

n x
m
x

(uÏ†obs
(rj , Ï†j , ti ) âˆ’ uÏ†cal
(rj , Ï†j , ti ))2 .
j
j

(36)

i=1 j=1

equation (36) is solved by the least-squares matching method, as well. note that, after solving
equation (35), the values of vp , and a1 are known. so, by plugging the estimates of vp , a1 , and
the auto-correlation of drill-bit source signature into the cost function of equation (36), the only
v
un-known is vs . by having the estimates of vp , and vs , the vps ratio can be further used to identify
reservoir fluids [? ]. the algorithm is described by algorithm 1. although this algorithm requires
several seismic sensors, the corresponding advantage is an increased robustness.
5.1.2. direct estimation from equation (9) (algo. 2)
the specific intrinsic energy of the rock naturally appears in equation (9). using equations (26)(27) and the iterative approach described in section 4.3, we can consider as available the (delayed)
2
values of the downhole force and velocity. regarding the term âˆ‚âˆ‚t2Î¾ (t, l) (that corresponds to
âˆ‚v
âˆ‚t (t, l)) that appears in equation (9), we can obtain it by differentiating the estimated downhole
velocity v(t, l). of course, the corresponding signals should be low-pass filtered to minimize the
negative effects of the boosted high frequency noise. equation (9) rewrites as
Ïˆ(t) = Ï†(t) + c,

(37)

where Ï† and Ïˆ are known functions defined by
Ïˆ(t) = mb

âˆ‚v
(t, l) âˆ’ w(t, l),
âˆ‚t
16

Ï†(t) =

aÎ¶
v(t),
Ï‰bit (t)

(38)

algorithm 1 estimation of  using an estimation of the seismic velocity of primary wave
require: urobs , w(t, 0), v(t, 0), rj , l, Ï†j , k = 0
1:
compute wÌ‚(t, l)p
using (26) and the iterative approach described in section 4.3
rj
m
1
initialize: vp0 = m
j=1 ( time delay[wÌ‚(t,l),u obs ] )
rj
while not converged
n p
m
p
a1 cos(Ï†j +Î³)
r
(urobs
(rj , Ï†j , ti ) âˆ’
2:
update amplitude : a1 k+1 = argmin
wÌ‚(ti âˆ’ v jk , l))
k 0.25
k 2
j
3:
4:

1.74(vp )
(vp ) rj
a1
i=1 j=1
n p
m
p
cos(Ï†)
ak+1
r
1
(urobs
(r
,
Ï†
,
t
)
âˆ’
update velocity :vpk+1 = argmin
wÌ‚(ti âˆ’ vjp , l))
j
j
i
j
1.74vp 0.25 vp 2 rj
vp
i=1 j=1

p

update k â† k + 1
if converged
vË†p â† vpk
aÌ‚1 â† ak1
output
Ë† â† 0.0642vË†p âˆ’ 117.99

and where c is a constant that is defined by
ÏÌ„
c = âˆ’wf + mb g.
Ï

(39)

note that the term c is potentially unknown (due to the term wf ). then, it becomes possible to
use parameter estimation techniques (such as recursive least squares) to estimate the unknown
parameters  and b. more precisely, these parameters are estimated by minimizing the following
cost function
n
x
{Ë†
, cÌ‚} = argmin
(ÏˆÌ‚(ti ) âˆ’ Ï†Ì‚(ti ) âˆ’ c)2 ,
(40)
,c

305

310

i=1

where Ï†Ì‚ and ÏˆÌ‚ are the estimations of the functions Ï† and Ïˆ obtained using the iterative approach
described in section 4.3. it is worth mentioning that with this procedure the gravitational forces
(i.e. the functions giw and giz that appear in equations (26) and (27)) do not actually need to be
estimated since these constant terms could be embedded into the term c. the main advantage
of this approach is that it is easy to implement and only requires top-drive measurements and
the knowledge of the different physical parameters of the well. however, such a procedure may
be sensitive to noise or to sensor defaults. to reduce the noise negative effects, we can filter the
measured signals with a low-pass filter (butterworth filter for instance). finally, this procedure
strongly depends on the accuracy of our model to describe the bit-rock interaction. the algorithm
is summarized by algorithm 2
5.2. machine learning estimation (algo. 3)
the last procedure we present in this paper is based on machine learning ([? ]). it does not
require any specific knowledge of the system (no model is needed). machine learning algorithms
have already be used in control applications in [? ] or in [? ], using adaptive neural networks.
since the measured outputs (top-drive force and velocity) somehow depend on the rockâ€™s intrinsic
17

algorithm 2 estimation of  using a parameter identification approach
require: w(t, 0), v(t, 0)
for each new measurement
1:
compute wÌ‚(t, l) and vÌ‚(t, l) using (26)-(27) and the approach described in section 4.3
2:
low-pass filter the signals (butterworth)
3:
differentiate the vÌ‚(t, l) using using an euler approximation
4:
compute Ï†Ì‚k (t) and ÏˆÌ‚ k (t) using (38)
pn
k
k
k 2
5:
recursive least squares algorithm: {Ë†
k , cÌ‚ k } = argmin
i=1 (ÏˆÌ‚ (ti ) âˆ’ Ï†Ì‚ (ti ) âˆ’ cÌ‚ )
,c

output
Ë† â† Ë†k
specific energy, their spectrum should contain sufficient information to distinguish the drilled rockâ€™s
nature. by applying a fast fourier transform (fft) to the measured outputs (top-drive force
and velocity), we can obtain these signalsâ€™ spectra. more precisely, let us denote y(t) the measured
continuous output. in practice, this signal is not continuous but sampled (with a sampling rate
that depends on our sensorsâ€™ quality). let us denote y, the vector obtained by concatenation of
all our measurements. let us assume y has n components (i.e., we have n measurements). its
discrete fourier transform yÌ‚ is a vector with n components defined by
yÌ‚ (k) =

n
x

y(k)eâˆ’2Ï€iâˆ—

(kâˆ’1)(nâˆ’1)
n

.

(41)

n=1

315

320

325

330

the vector yÌ‚ is a complex function that corresponds to the spectrum of the signal y. several
attributes characterize it. among them, we can cite the dominant peak (highest value of the
modulus of yÌ‚ ), the number of peaks, and the corresponding frequencies. the values of these
attributes are somehow related to the value of . for a set of known physical parameters that
characterize the well (including its geometry), we can run thousands of simulations, only modifying
the unknown parameters (i.e.,  but also the potentially unknown friction weight) between each
simulation. from this data set, for which the correct values of  are known (since these data
are simulated), similar to what is done by the human brain (experience), the machine learning
algorithm will learn and find the suitable correlations between the previously defined attributes
and the unknown parameter . once adequately trained, the machine learning algorithm can then
be applied to make predictions for new data sets (for which the parameter  is unknown).
to solve this regression problem and correctly predict the parameter , we choose to use a neural
network algorithm. this kind of machine learning algorithm is efficient and easy to implement [?
]. such a network consists of different connected nodes (that model the neurons of a real brain)
called artificial neurons. each neuron receives different inputs and produces a single output that
can be sent to other neurons. the output of each neuron is obtained by applying an activation
function to a weighted sum of the inputs. this activation function is usually non-linear. among the
most commonly used activation functions, we can cite the sigmoid function or the tanh function.
more details on how to choose the activation function or the weight can be found in [? ? ]. a
neural network can have several layers. the input of the first layers corresponds to the attributes
we have chosen (in our case, the attributes that characterize the spectrum of the top-drive force and
velocity), while the inputs for the next layers correspond to the neuronsâ€™ outputs in the previous
18

335

340

345

350

355

layer. the global output of the neural network is the predicted value of .
to train the network, we first simulate thousands of test points for which we know the correct
output and measure the corresponding top-drive velocity. we generate the spectrum of the topdrive velocity signal for each simulation and choose a set of relevant features (such as the dominant
peak, the corresponding frequency) that will constitute our dataset. while training our neural
network, it appeared that the relevant attributes for learning are the two dominant gains (and
their corresponding frequencies) of the top-drive velocity spectrum. this dataset is divided into
two sets: the training set and the testing set. we first construct the network (i.e., we choose the
number of neurons, layers, weights) and train it on the training set. we then test its performance on
the testing set by computing the mean squared error between the real values of  and the predicted
ones. we repeat the procedure, changing the networkâ€™s parameters and mixing the training and
testing set (cross-validation). finally, we choose the best network (i.e., the one for which the error
is minimum) and train the algorithm with the data available. our algorithm can then be used on
unknown datasets.
such a neural network algorithm is fast, reliable, and easy to implement. however, the main
drawback is that it requires many data (i.e., number of simulations) to train the algorithm. moreover, changing the physical parameters or the wellâ€™s geometry implies training the algorithm again
with a new set of data. the algorithm may also depend on the systemâ€™s initial condition and the
input (although this is not a real problem since we can always consider a rest state and the same
input to estimate ). a solution to increase the robustness could be to include these parameters
into the neural network at the cost of producing a higher number of training points. another
approach to improving the trained modelâ€™s generalizability is to use transfer learning concepts [?
? ? ]. the details of the algorithm is summarized in algorithm 3.
algorithm 3 estimation of  using a ml algorithm
require: wtest (t, 0), v test (t, 0), training dataset (wk (t, 0), v k (t, 0), k , wfk )
training of the network
1:
compute the fourier transform of wk (t, 0), v k (t, 0) using (41)
2:
compute the two dominant gains and the corresponding frequencies
3:
using cross-validation, design a neural network with sklearn.neural network.mlpregressor
estimation
4:
compute the fourier transform of wtest (t, 0), v test (t, 0)
5:
compute the two dominant gains and the corresponding frequencies
6:
run the neural network with this new point â†’ Ë†, wÌ‚f
output
Ë† â† Ë†

6. simulation results

360

in this section, we test the performance of our different algorithms against simulated data. first,
we evaluate the quality of the weight-on-bit estimations and the bit velocity provided by equations
(26) and (27) in the presence of noisy measurements. then, we will compare the efficiency of our
three algorithms in estimating the nature of the rock interacting with the drill-bit.
the axial dynamics simulated adjusting the model of [? ]. the control input w0 is chosen as
a constant to which is added a sinusoidal function. in the modelâ€™s numerical implementation, the
19

dls = 3o/30m
1500 m
inc = 60o
2000 m
figure 6: wellbore survey of the well. the length of the drill-string is 1750m.
table 2: numerical values of the parameters for the drill string model in figure 6.

param.
e1
c1Î¾
a1
e2
c2Î¾
a2
mb
a
wf

365

370

375

value
2 Ã— 1011 pa
5000 m/s
3.5 Ã— 10âˆ’3 m2
2.5 Ã— 1011 pa
5423 m/s
5 Ã— 10âˆ’2 m2
30000 kg
0.1m
71280 n

param.
Ï1
kÎ¾1
l
Ï2
kÎ¾2
x1
Ï‰bit
Î¶

value
8000 kg/m3
0.23/s
2000 m
8500 kg/m3
0.3/s
1750 m
5 rad/s
0.6

wave equation is transformed into transport equations discretized using a first-order upwind scheme.
this choice is made to ensure numerical robustness and avoid spurious oscillations, as higherorder schemes perform poorly due to the temporal discontinuities introduced by the distributed
differential inclusions used to represent the coulomb friction. to guarantee numerical accuracy, we
choose a sufficiently fine spatial grid. this is an amenable approach since the simulation speed is
not of critical importance for the present analysis (only the computational efforts of the estimation
algorithms matter). in all simulations, a spatial grid of 500 cells is used for the drill string. we
choose the time-step such that the courant-friedrichs-lewy (cfl) condition [? ] is satisfied. all
the simulations are done using matlab except the neural network algorithm that is performed
using python.
6.1. estimation of the drill-bit source
in this section we evaluate the quality of the estimations of the weight-on-bit and of the bit
velocity. the drill-bit is interacting with unconsolidated sands (i.e. water saturated) for which
real and v real while
 = 11 jcmâˆ’3 . the corresponding force-on-bit and bit velocity are denoted wbit
bit
20

est and v rest . these estimations are obtained using equations (26)
their estimations denoted wbit
bit
Â·Â·
and (27). the kernels k are computed using the method of characteristics and a fixed point
algorithm (see [? ] for details). the different integrals are computed using a trapezoidal method
with adjustable precision. we consider the case of noisy measurements. we model the noise by a
white gaussian noise that is characterized by its signal-to-noise ratio (snr). as it is done in [? ],
the snr is defined as

sn r =

380

390

Î¾

Î¾

400

Î¾

terms present in equations (26) and (27) out of this time interval (we recall that x1 corresponds to
the length of the pipe-section). to reduce the effect of the noise we use a second order butterworth
real and w est are plotted in figure 7,
low-pass filter on the noisy measurements. the functions wbit
bit
est
est
while the functions vbit and vbit are plotted in figure 8. we notice that despite the presence
of noise, the estimations are comparable to the real states. for a space grid of 500 cells, our
est and v est . this computational time is directly related
algorithm needs 90 seconds to compute vbit
bit
to the space-step (and consequently to the time step, with the cfl condition). however, most
of the computations can be done off-line (computation of the backstepping kernels for instance),
and the real computational time is around 20s to estimate on a 20s time-window. this makes the
current code amenable for real implementation, although in that case complexity of the algorithm
should also be considered. note that the computational burden can be reduced by using a less
time-consuming algorithm to compute the different integral terms or by decreasing the space-step
(at the cost of a lower accuracy).
as is done in [? ? ] we introduce the quality of the reconstruction metric for the estimated forceon-bit. this will help us to examine the performance of the proposed method. a similar verification
real ) and y
is done for the bit velocity. let us denote y0 as true generic signal (in our case y0 = wbit
est
its estimation (in our case y = wbit ). we define the quality of the reconstruction q as follows
q = 10 log

395

(42)

where arms is the root-mean-square amplitude of the noise-free signal, and Ïƒn2 is the variance of
the noise. choosing a value for the snr gives the corresponding amplitude for the white gaussian
noise. note that the value of the snr may be different for the velocity and force measurements.
we compare the real force-on-bit with our estimation in figure 7. the comparison between the
real axial bit velocity and the estimated one is done in figure 8. the snr is equal to 10. the
1
1
estimation is only performed for xc11 + lâˆ’x
< t < tf âˆ’ ( xc11 + lâˆ’x
), since we cannot compute all the
c2
c2
Î¾

385

a2rms
,
Ïƒn2

ky0 k22
,
ky0 âˆ’ yk22

(43)

where k Â· k2 is the `2 norm. we have plotted in figure 9 and figure 10 the variations of qf
and qv (reconstruction for the downhole force and downhole velocity) for different values of the
snr of top-drive velocity and the top-drive force (the snr being chosen between 1 and 15). we
have also plotted a dashed line that corresponds to the limit value of q = 10. we can observe
a symmetrical effect between the snr of top-drive velocity and the snr of top-drive force. the
critical value of q = 10 is not reached for low snr values (sn r < 2). in our simulations, we
find that when the quality factor is larger than 10, the rock intrinsic energy estimation algorithms
result in satisfactory performances.
21

10 5

12

3.5

10

3

8

2.5

6

2

4

1.5

2

1

0

0.5

-2

0

-4

-0.5
0

2

4

6

8

10

12

14

16

18

20

0

figure 7: comparison of the simulated force-onbit and of the estimated one using noisy top-drive
measurements in the case of unconsolidated sands
(snr=10).

2

4

6

8

10

12

2

4

6

8

10

12

14

16

18

20

figure 8: comparison of the simulated bit velocity and of the estimated one using noisy top-drive
measurements in the case of unconsolidated sands
(snr=10).

2

14

4

6

8

10

12

14
16

2

2

16

14

4

4
14

12

6

6
12

10

8

8
10

8

10

10
8

6

12

12

4

6

14

14
2

figure 9: noise sensitivity analysis of the drill-bit
force estimation. we use monte carlo simulations
with 75 realizations for each noise level pairs and
report the average value of the quality of the reconstruction q. the dashed line corresponds to q = 10
contour, which is used as the success limit for the
drill-bit source estimation.

figure 10: noise sensitivity analysis of the drill-bit
velocity estimation. we use monte carlo simulations
with 75 realizations for each noise level pairs and report the average value of the quality of the reconstruction q. the dashed line corresponds to q = 10
contour, which is used as the success limit for the
drill-bit source estimation.

22

405

6.2. estimation of the rock interacting with the drill-bit
we now use our three algorithms to estimate the nature of the rock interacting with the drillbit. the efficiency of the different algorithms are compared for three different types of rocks that
have different hardness (i.e., different compressional and shear velocities)
â€¢ unconsolidated sands (i.e. water saturated) for which vp = 2000 m/s, vs = 1150 m/s, Ïf = 2
g/cm3 and  = 11 j/cm3 .

410

â€¢ sedimentary rocks (e.g. sandstone) for which vp = 2710 m/s, vs = 1500 m/s, Ïf = 2.2
g/cm3 and  = 57 j/cm3 .
â€¢ igneous or metamorphic rocks (e.g. granite) for which vp = 4610 m/s vs = 2700 m/s,
Ïf = 2.7 g/cm3 and  = 175 j/cm3 .

415

420

425

430

435

440

6.2.1. seismic while drilling algorithm
we use our first algorithm (swd algorithm) to estimate the rocksâ€™ velocity that are interacting
with the drill-bit. we consider that 100 seismic sensors are available (50 on each side of the drillstring). the distance between each receiver is equal to 25 meters. note that this distribution may
not be optimal due to the bitâ€™s final orientation (which is equal to 60 degrees according to figure 6).
using the simulated weight-on-bit and the (known) value of vp , we compute the radiation patterns
for each sensor to which we add noise with snr equal to 5. moreover, by using the top-drive force
and velocity measurements, which are given by the pde simulation, we estimate the weight-on-bit,
and consequently, it becomes possible to apply algorithm 1. for the three different rocks, we have
performed 75 simulations and compute the corresponding  using such a procedure. we have given
in table 3 the value of the mean and the standard deviation of our estimations. one can notice
that the estimations are almost equal to the real values even in the presence of significant noise.
moreover, the standard deviation remains extremely low. this underlines the robustness of the
proposed approach that uses a large number of sensors. for one testing point, algorithm 1 runs
in 0.06s.
6.2.2. direct estimation algorithm
let us now estimate  using algorithm 2. the results are shown in table 3. we have assumed
that the constant c that appears in (39) is not known. one can notice that the estimations are
close to the real data (even if, due to the important noise (snr equal to 5), the standard deviation
may be important. however, one must keep in mind that such an approach is hugely dependent on
the model we have for the downhole boundary condition. the computational time to run algorithm
2 on one testing point is of 0.01s.
6.2.3. machine learning algorithm
we now design our machine learning algorithm. as explained in section 5.2 and algorithm 3,
we first train our algorithm on a set of data made of 10000 simulations for which only the values of
 and wf change between each simulation. an example of a power spectrum of top-drive velocity,
for the case  = 11, is given in figure 11. note that the spectrum has been normalized by its
maximum gain and that a saturation has been set at âˆ’20db.
to run our neural network, we have used pythonâ€™s algorithm sklearn.neural network.mlpregressor
with 50 hidden layers, a bfgs solver for weight optimization, and an adaptive learning rate. again,
the results are shown in table 3. one can notice that these predictions are close to the real values
23

0
-2
-4
-6
-8
-10
-12
-14
-16
-18
-20
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

figure 11: normalized power spectrum of noise-free top-drive velocity. saturation is set at âˆ’20 db.

table 3: mean and standard deviation of the intrinsic specific energy of the rock  using swd estimation (100
sensors), direct estimation and machine learning estimation. 75 simulations and estimations have been performed
for each rock. the snr for all the signals is equal to 5.

rock
unconsolidated
sands
sedimentary
rocks
metamorphic
rocks

445

450

455

460

real 
11
57
175

mean
stand. dev.
mean
stand. dev.
mean
stand. dev.

swd
11.1
0.4
57.2
0.32
175.38
0.5

direct estimation
10.2
1.1
53.5
2.19
186
24

machine learning
9.7
2
60.9
3.3
184
17.8

(at least for low to medium ). the prediction is more accurate for large values of  (metamorphic
rocks) than the one we obtained using the two other algorithms. the computational time to train
the network is 140s while the time required to run the algorithm on one testing point is 0.01s.
6.2.4. comparison between the different algorithms
the three different algorithms provide satisfying and reliable estimation for the rockâ€™s intrinsic
specific energy in the presence of important noises. however, the swd algorithm presents an
impressively high accuracy with a low standard deviation. this is related to its inherent robustness
properties due to the use of a large number of sensors. moreover, the estimation procedure is simple
and can be easily adjusted for new constant physical parameters. the second algorithm also gives
reliable estimates (at least for low values of ). however, one must be aware that this algorithm
requires a reliable model for the downhole condition (bit rock-interaction). finally, even if the
machine learning algorithm shows satisfactory performance, it requires important datasets to be
run, which can be time-consuming. note that another drawback of our ml procedure is that the
available data currently comes from simulations, which implies a model dependency (although it
is not as crucial as for the second procedure). however, such a machine learning algorithm is
easy to implement and fast to run once adequately trained. moreover, it is not sensitive to the
discretization step since it only uses top-drive measurements.
24

465

470

475

480

485

490

495

500

we now compare the robustness properties of the different algorithms concerning uncertainties
on the different physical parameters. more precisely, we address the effects of mismatch between
the physical and mathematically modeled drill string dynamics (especially, errors in downhole
boundary condition: bit-rock interaction modeling) on the three algorithmsâ€™ performances. adding
discrepancies to the modeling will introduce errors in estimating the force-on-bit but not the
measured hook-load, hook-speed, and swd measurements. hence, in the swd algorithm, an
incorrect source will be correlated with the data. in the direct estimation algorithm, the effect
of incorrect bit-rock interaction coefficients will be added to a wrong estimation of the force-onbit. finally, the machine learning algorithm will be trained with the wrong model. the proposed
analysis is the first step towards a complete sensitivity analysis (that would be outside the scope
of this paper). in what follows, we consider that all the physical parameters used to simulate
the drilling system (e.g., inertia, inclination, length, mass) are subject to a random uncertainty
(bounded by a maximum percentage d). we have pictured in figure 12 the comparison between
the simulated force-on bit and the estimated one in the absence of noise but in the presence of a
discrepancy of 10% between the physical parameters used for the simulation and those used for
the simulation. note that for this simulation, all the uncertainties have been set to the maximum
bound of 10%. as expected, there is a mismatch between the estimated force and the simulated one
(around 15 %). note that the phase of the function is barely changed. we now compare our three
algorithmsâ€™ performance in the presence of uncertainties on the different parameters. we consider
the case of sedimentary rock for which  = 57. we have performed 75 simulations for which there
is a discrepancy up to d% between the parameters used for the estimation (and for the training of
the last algorithm) and the real ones. we have given in table 4 the values of the mean and the
standard deviation of our estimations for different maximal bound d (10%, 20% and 30%). we have
used an snr of 10 for all the measured signals. all three algorithms provide relevant estimations
even in the presence of important uncertainties. the machine learning algorithm seems to be
the most accurate but suffers from an important standard deviation. a similar dispersion of the
estimations can be observed for the direct estimation algorithm. this is not surprising since these
two algorithms are trained on a false model. conversely, the swd algorithm gives an accurate
estimation with a relatively low standard deviation. we can explain these good performances by
the fact that the uncertainties barely affect the phase of the estimated force-on-bit. as the phase is
the most important attribute when correlating the estimated force with swd measurements, this
results in better estimation and a lower standard deviation. this emphasizes the robustness of such
an approach even if this is done at the cost of an important number of sensors. that said, the swd
method (i.e., algorithm 1) is an alternating minimization method, which requires proper initial
estimation of the formation velocity. in algorithm 1, we use the time delay between the recorded
data near the surface and the calculated force-on-bit (source signature) at the bit location to provide
an initial estimation of formation velocity, i.e., vp0 . this method may not always provide a proper
initial estimation of formation velocity. accordingly, we can take advantage of the direct estimation
and machine learning methods in providing vp0 necessary in the swd algorithm. moreover, in the
swd algorithm, having access to a proper vp0 value will improve the robustness and convergence
rate of the swd algorithm, as well.
7. extensions
the three algorithms we design in this paper provide an efficient and reliable estimation of
the rockâ€™s nature that is interacting with the drill-bit, enabling a more precise characterization of
25

10 5

12

10

8

6

4

2

0
0

2

4

6

8

10

12

14

16

18

20

figure 12: comparison of the simulated force-on bit and of the estimated one in the presence of a discrepancy of
10% between the physical parameters used for the simulation and those used for the simulation (no noise)

table 4: influence of a parameter mismatch on the estimation of the intrinsic specific energy of a sedimentary
rock  (real value: 57) for each algorithm (swd estimation (100 sensors), direct estimation and machine learning
estimation). 75 simulations and estimations have been performed for each level of disturbance. the snr for all the
signals is equal to 5.

disturbance
10%
20%
30%

505

510

mean
stand. dev.
mean
stand. dev.
mean
stand. dev.

swd
54.7
2.47
53.7
10.7
48.9
11

direct estimation
51.6
6
50
12.5
46.1
18

machine learning
57.5
7.6
57.5
17
58.4
21

the subsurface. however, the proposed axial dynamics given by equations (3)-(4) neglect several
aspects that are crucial to envision real implementation. first, we have neglected the coulomb
friction between the drill string and the borehole. we have then neglected the torsional dynamics
of the drilling device by assuming that the bit angular velocity was perfectly known. the main
reason behind this choice of an (over)-simplistic model was to facilitate the design of our different
procedures. this section shows how our algorithms can be extended to these more sophisticated
(and realistic) models.
7.1. non-linear coulomb friction terms
the proposed (simplified) axial dynamics given by equations (3)-(4) neglect the coulomb friction between the drill string and the borehole (also known as side force). it has been shown in
[? ], that for torsional oscillations, this coulomb friction term may have a non-negligible impact
on the dynamics. more precisely, it has a velocity weakening effect that may generate stick-slip
oscillations. as far as we know, the side force effect on the axial motion of the drill-string has
not been studied in the literature. however, it seems reasonable to assume that this force can
be expressed using a similar model as the one used for torsional oscillations, namely a differential
26

inclusion. more precisely, inspired by [? ] we add the term f(t, x) to equation (4) that is a differential inclusion that represents the coulomb friction between the drill-string and the borehole.
the model we propose uses the following inclusion
ï£±
a
ï£´
v(t) > vc ,
ï£²f(t, x) = Âµk fa (x),
a
(44)
f(t, x) âˆˆ Â±Âµs fa (x),
|v(t)| < vc ,
ï£´
ï£³
a
f(t, x) = âˆ’Âµk fa (x),
v(t) < âˆ’vc ,
where Âµas is the static friction coefficient (i.e. the friction between two or more solid objects that
are not moving relative to each other) and Âµak kinetic friction coefficient (also known as dynamic
friction or sliding friction, which occurs when two objects are moving relative to each other and
rub together), vc is the threshold on the axial velocity where the coulomb friction transits from
static to dynamic. the function fa is the normalized axial force acting between the drill-string
and the borehole wall. the function f(t, x) âˆˆ Â±Âµas fa (x) denotes the inclusion where
f(t, x) = âˆ’

1 âˆ‚
w(t, x) âˆ’ kÎ¾ v(t, x) âˆˆ [âˆ’Âµas fa (x), Âµas fa (x)],
aÏ âˆ‚x

(45)

and takes the boundary values Â±Âµas fa (x) if this relationship does not hold. let us assume that the
coefficients Âµas , Âµak and vc are known as well as the function fa . how to estimate such coefficients
is outside the scope of this paper, but when the bit is off-bottom, adaptive observers can be
designed following the procedure detailed in [? ]. when all the points of the drill-string reach the
kinematic mode, then the function f(t, x) does not (directly) depend on time anymore since we
have f(t, x) = Â±Âµak fa (x). thus, this term can be added to the function h in (4) and the previous
computations can be still be applied to compute the downhole force and velocity as functions of
past and future values of the top-drive states. these estimations only hold if we have |v| > vc
for all drill-string points. thus, we need to apply our formula to all points (in practice, to a
sufficiently large number of points) of the drill-string to verify that the condition is not violated.
among all the estimations of the drill-bit source signature, only the ones for which this condition
is fulfilled can be used to estimate the rockâ€™s nature if using algorithms 1 and 2. note that using
adequate control laws may enforce the system to remain in a steady-state for which this condition
is always verified. regarding the machine learning algorithm, it can straightforwardly be adjusted
to deal with this coulomb friction term. moreover, by slightly modifying the algorithm, it is also
possible to estimate the friction coefficients Âµas and Âµak . we compare the real force-on-bit with our
estimation in figure 13. the comparison between the real axial bit velocity and the estimated one
is made in figure 13. the different signals are noise-free. the coulomb friction parameters are
chosen as follows:
Âµas = 0.45, Âµak = 0.28, vc = 0.25.

515

they are assumed to remain the same for the pipe and the collar. one can notice that the downhole
force and velocity estimations remain extremely satisfying except for some time intervals for which
the velocity is lower than vc . this is consistent with our analysis since, in that case, the system
has not broken the static friction limit.
7.2. torsional motion of the drill-string
this paper focused on the drill-bitâ€™s axial motion and assumed that the bit angular velocity
Ï‰bit was a known positive function. however, the proposed axial dynamics have to be coupled with
27

10 5

9

1.5

8
7

1

6
5

0.5

4
3

0

2
1

-0.5

0
-1

-1
0

2

4

6

8

10

12

14

16

18

20

figure 13: comparison of the simulated force-onbit and of the estimated one in presence of coulomb
friction terms (no noise).

520

525

530

535

0

2

4

6

8

10

12

14

16

18

20

figure 14: comparison of the simulated bit velocity
and of the estimated one in presence of coulomb
friction terms (no noise).

a torsional model to obtain an axial-torsional drill-string model. such a torsional model has been
proposed in [? ] in the case of an off-bottom bit. the equations have the same structure as (3)-(4),
including a non-linear coulomb friction term similar to (44). nevertheless, for non-vertical wells,
in the presence of angular and axial movements, the coulomb friction term couples the angular
and axial dynamics, and equation (44) has to be modified accordingly. regarding the downhole
boundary conditions (equation (9) and the analogous equation for torque as given in [? ]), as
1
suggested in [? ], the term Ï‰bit
has to replaced by an inclusion (g non-linearity) to deal with small
axial and angular velocities. for torsional oscillations, such a term can be the source of stick-slip
oscillations. the derivation of a complete and validated axial-torsional model is out of the scope of
this paper. however, even if the downhole boundary conditions become more complex, adjusting
the approach presented in section 4, it becomes possible (eventually including the new coulomb
side forces) to express the bit torque and angular velocity as delayed and future values of the topdrive states. indeed, our approach does not require the use of the downhole boundary condition.
consequently, our first algorithm can still be applied. however, the second algorithm still requires
a good model for the downhole boundary condition. finally, the machine learning procedure can
still be implemented by including the top-drive torque and angular velocity spectrum.
8. concluding remarks

540

545

we have developed three different algorithms for a near-real-time estimation of the rockâ€™s intrinsic specific energy interacting with the drill-bit. we have considered a multi-sectional directional
well. the algorithms only use surface measurements and do not depend on any explicit knowledge about the subsurface properties. the algorithms were based on seismic-while-drilling, direct
parameter estimation, and machine learning. they provided an efficient and reliable framework
for estimating the nature of rocks interacting with the drill-bit, enabling a subsurfaceâ€™s practical
characterization. we have discussed the advantages and drawbacks of each approach. among
our three algorithms, the model-based estimation algorithm (algorithm 2) provides accurate estimations. however, this algorithm requires a reliable model for the downhole condition and, in
that sense, is extremely model-dependent. it is easy to implement and present satisfying robust28

550

555

560

ness properties regarding the swd procedure due to the available multiple seismic sensors. the
machine learning algorithm is also easy to implement and fast to run once properly trained but
may not be robust to changes in the system parameters (as a new long training phase would be
necessary). we have shown that the proposed methodologies can be applied to drilling systems in
the presence of non-linear coulomb friction terms and coupled axial-torsional oscillations of the
drill string. theoretically, the proposed estimations can be coupled with existing observers and
controllers to improve the drilling performances by reducing non-productive time, increasing the
rop, and reducing the effect of undesired torsional oscillations. this is an important step towards a
subsurface-aware drilling system. even though the proposed approach is a necessary and significant
step in incorporating formation information and sensing in drill string dynamics estimation, our
future interest entails adding more complexities into the modeling system (by combining torsional
and axial dynamics), generalizing the machine learning mapping operator, designing formationaware observers for optimal controlling of drill string dynamics, and validating the algorithms in
field trail.
appendix
derivation of the delayed equation (26)
in this appendix, we detail the computations to obtain equation (26). we follow the steps given
in [? ], the only differences being that we are between xi and xi+1 (instead of 0 and l) and the
presence of the functions h3 and h4 (since the well is not vertical). the transformation (20)-(21)
being a volterra transformation, it is invertible [? ]. the inverse transformation is defined by
z x
Î±Î²
i
i
(lÎ±Î±
(46)
ui (t, x) = Î±i (t, x) +
i (x, y)Î± (t, y) + li (x, y)Î² (t, y))dy,
xi
z xi
Î²Î²
i
i
i
i
(lÎ²Î±
(47)
z (t, x) = Î² (t, x) +
i (x, y)Î± (t, y) + li (x, y)Î² (t, y))dy,
0

where the kernels li can be expressed as functions of the kernels k i (see [? ? ]). injecting (24)
and (25) into (46), using fubiniâ€™s theorem and simple changes of variables, we obtain (see [? ] for
details)
! z âˆ†i
âˆ†
ci
i
Î¾
ui (t, xi+1 ) =Î±i t âˆ’ i , xi +
ci (f1 )ui (s)Î±i (t âˆ’ s, xi ) + ciÎ¾ (f2 )ui (s)Î² i (t âˆ’ s, xi )ds + hui , (48)
âˆ†i Î¾
cÎ¾
âˆ’ i
c
Î¾

i âˆ†i
where (f1 )ui and (f2 )ui are lâˆ functions defined on the interval [âˆ’ âˆ†
cÎ¾ , cÎ¾ ] by

i
(f1 )ui (s) =1[0, âˆ†i ] (s)lÎ±Î±
i (xi+1 , xi + cÎ¾ s) âˆ’ 1[âˆ’ âˆ†i ,0] (s)
ci
Î¾

z xi+1

cÎ¾

âˆ’ciÎ¾ s+xi

zu
i
lÎ±Î²
i (xi+1 , y)ki (y + cÎ¾ s, xi )dy,

i
uz
i
(f2 )ui (s) =1[âˆ’ âˆ†i ,0] (s)(lÎ±Î²
i (xi+1 , xi âˆ’ cÎ¾ s)) âˆ’ 1[0, âˆ†i ] (s)(ki (xi+1 âˆ’ cÎ¾ s, xi )
ci

z Î¾xi+1
+
ciÎ¾ s+xi

ci
Î¾

uz
i
lÎ±Î±
i (xi+1 , y)ki (y âˆ’ cÎ¾ s, xi )dy),

29

and where hui is defined by
hui =

z âˆ†i
ci
Î¾

hi3 (xi+1 âˆ’ ciÎ¾ y)dy +

z xi+1 z yâˆ’xi
ci
Î¾

i
i
(lÎ±Î±
i (xi+1 , y)h3 (y âˆ’ cÎ¾ Î½)

xi
0
Î±Î²
i
i
âˆ’ li (xi+1 , y)h4 (y âˆ’ cÎ¾ Î½))dÎ½dy.
0

(49)

note that the function hui was not present in [? ] since the well was considered as vertical which
yields h â‰¡ 0. similarly, the state z i (t, xi+1 ) verifies
! z âˆ†i
âˆ†
ci
i
Î¾
z i (t, xx+1 ) =Î² i t + i , xi +
ciÎ¾ (f1 )zi (s)Î±i (t âˆ’ s, xi ) + ciÎ¾ (f2 )zi (s)Î² i (t âˆ’ s, xi )ds + hzi , (50)
âˆ†
cÎ¾
âˆ’ ii
c
Î¾

i âˆ†i
where (f1 )zi and (f2 )zi are lâˆ functions defined on the interval [âˆ’ âˆ†
cÎ¾ , cÎ¾ ] by
z xi+1
zu
i
i
(f1 )zi (s) =1[0, âˆ†i ] (s)(lÎ²Î±
lÎ²Î²
(s)(
(x
,
x
+
c
s))
âˆ’
1
âˆ†
i+1 i
Î¾
i (xi+1 , y) Â· ki (y + cÎ¾ s, xi )dy
i
[âˆ’ i ,0]
ci
Î¾

âˆ’ciÎ¾ s+xi

cÎ¾

+ kizu (xi+1 + cÎ¾ s, 0)),
i
(f2 )zi (s) =1[âˆ’ âˆ†i ,0] (s)(lÎ²Î²
i (xi+1 , xi âˆ’ cÎ¾ s)) âˆ’ 1[0, âˆ†i ] (s)
ci
Î¾

!

z xi+1
ciÎ¾ s+xi

ci
Î¾

uz
i
lÎ²Î±
i (xi+1 , y)ki (y âˆ’ cÎ¾ s, xi )dy ,

and where hzi is defined by
hzi = âˆ’

z âˆ†i
ci
Î¾

hi4 (xi+1 âˆ’ ciÎ¾ y)dy +

z xi+1 z yâˆ’xi
ci
Î¾

xi
0
Î²Î²
i
i
âˆ’ li (xi+1 , y)h4 (y âˆ’ cÎ¾ Î½))dÎ½dy.

0

i
i
(lÎ²Î±
i (xi+1 , y)h3 (y âˆ’ cÎ¾ Î½)

(51)

since we have Î±i (t, xi ) = ui (t, xi ) and Î² i (t, xi ) = z i (t, xi ) and since
e i ai âˆ’
w(t, xi+1 ) =
(e
2ciÎ¾
1 âˆ’
v(t, xi+1 ) = (e
2

ix
kÎ¾
i+1
2ci
Î¾

ix
kÎ¾
i+1
2ci
Î¾

ix
kÎ¾
i+1
2ci
Î¾

i

u (t, xi+1 ) âˆ’ e

i

u (t, xi+1 ) + e

ix
kÎ¾
i+1
2ci
Î¾

z i (t, xi+1 )),

z i (t, xi+1 )),

injecting (48) and (50) into these equations, we immediately obtain (26) and (27) where we have
ki x

e i ai âˆ’ Î¾2ci+1
Î¾
fui (s) =
(e
(f1 )ui (s) âˆ’ e
2

ix
kÎ¾
i+1
2ci
Î¾

(f1 )zi (s)),

(52)

Î¾ i+1
Î¾ i+1
e i ai âˆ’ 2ciÎ¾
2ci
fzi (s) =
(e
(f2 )ui (s) âˆ’ e Î¾ (f2 )zi (s)),

(53)

ki x

ki x

2

gui (s) =
gzi (s) =

ciÎ¾
2
ciÎ¾
2

ix
kÎ¾
i+1

ki xi+1

(e

âˆ’ Î¾2c

Î¾

(f1 )ui (s) + e

ki xi+1

âˆ’ Î¾

(e

2ci
Î¾

2ci
Î¾

(f1 )zi (s)),

(54)

(f2 )zi (s)).

(55)

ix
kÎ¾
i+1

(f2 )ui (s) + e
30

2ci
Î¾

acknowledgments

565

this work was partially supported by the university of calgaryâ€™s canada first research excellence fund program, the global research initiative in sustainable low carbon unconventional
resources.
references

570

575

580

585

590

595

600

605

610

[] u. j. f. aarsnes, f. di meglio, r. j. shor, avoiding stick slip vibrations in drilling through startup trajectory
design, journal of process control 70 (2018) 24â€“35. url: https://linkinghub.elsevier.com/retrieve/pii/
s0959152418301859. doi:10.1016/j.jprocont.2018.07.019.
[] u. j. f. aarsnes, f. di meglio, r. shor, benchmarking of industrial stick-slip mitigation controllers, ifacpapersonline 51 (2018) 233â€“238.
[] v. a. dunayevsky, f. abbassian, application of stability approach to bit dynamics, spe drilling & completion
13 (1998) 22â€“25. url: https://www.onepetro.org/journal-paper/spe-30478-pa. doi:10.2118/30478-pa.
[] j. d. jansen, nonlinear dynamics of oilwell drillstrings, ph.d. thesis, delft university of technology, 1993. url: http://repository.tudelft.nl/assets/uuid:d205b287-bcdd-4b49-b12a-963d9b906dea/
3me{_}jansen{_}19930616.pdf.
[] b. saldivar, s. mondieÌ, j.-j. loiseau, v. rasvan, stick-slip oscillations in oillwell drilstrings: distributed
parameter and neutral type retarded model approaches, ifac proceedings volumes (ifac-papersonline) 18
(2011) 284â€“289. doi:10.3182/20110828-6-it-1002.00084.
[] b. saldivar, i. boussaada, h. mounier, s.-i. niculescu, analysis and control of oilwell drilling vibrations, in: a
time-delay systems approach, ser. advances in industrial control, springer, 2015.
[] p. c. kriesels, w. . j. g. keultjes, p. dumont, i. huneidi, o. owoeye, r. a. hartmann, et al., cost savings
through an integrated approach to drillstring vibration control, in: spe/iadc middle east drilling technology
conference, society of petroleum engineers, 1999.
[] u. j. aarsnes, r. j. shor, torsional vibrations with bit off bottom: modeling, characterization and field data
validation, journal of petroleum science and engineering 163 (2018) 712â€“721. url: http://linkinghub.
elsevier.com/retrieve/pii/s0920410517309075. doi:10.1016/j.petrol.2017.11.024.
[] r. i. leine, d. h. van campen, w. j. g. keultjes, stick-slip whirl interaction in drillstring dynamics, journal
of vibration and acoustics 124 (2002) 209. url: http://vibrationacoustics.asmedigitalcollection.asme.
org/article.aspx?articleid=1470408. doi:10.1115/1.1452745.
[] k. nandakumar, m. wiercigroch, stability analysis of a state dependent delayed, coupled two dof model
of drill-string vibration, journal of sound and vibration 332 (2013) 2575â€“2592. url: http://linkinghub.
elsevier.com/retrieve/pii/s0022460x1200987x. doi:10.1016/j.jsv.2012.12.020.
[] j. f. brett, the genesis of bit-induced torsional drillstring vibrations, spe drilling engineering
7 (1992) 168â€“174. url: http://www.onepetro.org/mslib/servlet/onepetropreview?id=00021943{&}soc=
spehttp://www.onepetro.org/doi/10.2118/21943-pa. doi:10.2118/21943-pa.
[] m. kapitaniak, v. v. hamaneh, j. p. chaÌvez, k. nandakumar, m. wiercigroch, unveiling complexity of
drillâ€“string vibrations: experiments and modelling, international journal of mechanical sciences 101 (2015)
324â€“337.
[] j. f. brett, a. d. beckett, c. a. holt, d. l. smith, uses and limitations of drillstring tension and
torque models for monitoring hole conditions, spe drilling engineering 4 (1989) 223â€“229. url: http:
//www.onepetro.org/doi/10.2118/16664-pa. doi:10.2118/16664-pa.
[] g. w. halsey, a. kyllingstad, t. v. aarrestad, d. lysne, drillstring vibrations: comparison between theory
and experiments on a full-scale research drilling rig, in: spe/iadc drilling conference, iadc/spe 14760,
society of petroleum engineers, 1986, pp. 311â€“321. url: http://www.onepetro.org/doi/10.2118/14760-ms.
doi:10.2118/14760-ms.
[] d. zhao, s. hovda, s. sangesland, abnormal down hole pressure variation by axial stick-slip of drillstring, journal of petroleum science and engineering 145 (2016) 194â€“204. url: http://dx.doi.org/10.1016/
j.petrol.2016.04.004. doi:10.1016/j.petrol.2016.04.004.
[] f. di meglio, u. j. f. aarsnes, a distributed parameter systems view of control problems in drilling, ifacpapersonline 48 (2015) 272â€“278.
[] b. saldivar, s. mondieÌ, s.-i. niculescu, h. mounier, i. boussaada, a control oriented guided tour in oilwell
drilling vibration modeling, annual reviews in control 42 (2016) 100â€“113.

31

615

620

625

630

635

640

645

650

655

660

665

670

[] u. j. f. aarsnes, o. m. aamo, linear stability analysis of self-excited vibrations in drilling using an infinite
dimensional model, journal of sound and vibration 360 (2016) 239â€“259. url: http://linkinghub.elsevier.
com/retrieve/pii/s0022460x15007385. doi:10.1016/j.jsv.2015.09.017.
[] u. j. f. aarsnes, n. van de wouw, axial and torsional self-excited vibrations of a distributed drill-string,
journal of sound and vibration 444 (2019) 127â€“151.
[] c. germay, v. denoeÌˆl, e. detournay, multiple mode analysis of the self-excited vibrations of rotary drilling systems, journal of sound and vibration 325 (2009) 362â€“381. url: http://linkinghub.elsevier.com/retrieve/
pii/s0022460x09002478. doi:10.1016/j.jsv.2009.03.017.
[] u. j. f. aarsnes, r. j. shor, torsional vibrations with bit off bottom: modeling, characterization and field data
validation, journal of petroleum science and engineering 163 (2018) 712â€“721.
[] i. boussaada, h. mounier, s.-i. niculescu, a. cela, analysis of drilling vibrations: a time-delay system approach,
in: 2012 20th mediterranean conference on control & automation (med), ieee, 2012, pp. 610â€“614.
[] t. richard, c. germay, e. detournay, self-excited stick-slip oscillations of drill bits, comptes rendus
meÌcanique 332 (2004) 619â€“626. url: http://linkinghub.elsevier.com/retrieve/pii/s1631072104001135.
doi:10.1016/j.crme.2004.01.016.
[] t. richard, c. germay, e. detournay, a simplified model to explore the root cause of stickâ€“slip vibrations in
drilling systems with drag bits, journal of sound and vibration 305 (2007) 432â€“456. url: http://linkinghub.
elsevier.com/retrieve/pii/s0022460x07002908. doi:10.1016/j.jsv.2007.04.015.
[] l. franca, drilling action of roller-cone bits: modeling and experimental validation, journal of energy resources
technology 132 (2010).
[] z. zhao, x. he, z. ren, g. wen, boundary adaptive robust control of a flexible riser system with input
nonlinearities, ieee transactions on systems, man, and cybernetics: systems 49 (2018) 1971â€“1980.
[] y. liu, y. fu, w. he, q. hui, modeling and observer-based vibration control of a flexible spacecraft with
external disturbances, ieee transactions on industrial electronics 66 (2018) 8648â€“8658.
[] x. he, w. he, j. shi, c. sun, boundary vibration control of variable length crane systems in two-dimensional
space with output constraints, ieee/asme transactions on mechatronics 22 (2017) 1952â€“1962.
[] y. liu, f. guo, x. he, q. hui, boundary control for an axially moving system with input restriction based on
disturbance observers, ieee transactions on systems, man, and cybernetics: systems 49 (2018) 2242â€“2253.
[] m. liao, j. ing, m. sayah, m. wiercigroch, dynamic method of stiffness identification in impacting systems for
percussive drilling applications, mechanical systems and signal processing 80 (2016) 224â€“244.
[] m. liao, m. wiercigroch, m. sayah, j. ing, experimental verification of the percussive drilling model, mechanical
systems and signal processing 146 (2021) 107067.
[] r. j. shor, m. w. dykstra, o. j. hoffmann, m. coming, for better or worse: applications of the transfer matrix
approach for analyzing axial and torsional vibration, in: spe/iadc drilling conference and exhibition, society
of petroleum engineers, 2015.
[] z. zhao, x. he, c.-k. ahn, boundary disturbance observer-based control of a vibrating single-link flexible
manipulator, ieee transactions on systems, man, and cybernetics: systems (2019).
[] z. zhao, c.-k. ahn, h.-x. li, boundary antidisturbance control of a spatially nonlinear flexible string system,
ieee transactions on industrial electronics 67 (2019) 4846â€“4856.
[] j. auriol, n. kazemi, r. j. shor, k. a. innanen, i. d. gates, a sensing and computational framework for
estimating the seismic velocities of rocks interacting with the drill bit, ieee transactions on geoscience and
remote sensing 58 (2019) 3178â€“3189.
[] f. poletto, f. miranda, seismic while drilling: fundamentals of drill-bit seismic for exploration, volume 35,
elsevier, 2004.
[] n. kazemi, r. shor, k. innanen, illumination compensation with seismic-while-drilling plus surface seismic
imaging, in: 80th eage conference and exhibition 2018, volume 2018, european association of geoscientists
& engineers, 2018, pp. 1â€“5.
[] s. nejadi, n. kazemi, j. a. curkan, j. auriol, p. r. durkin, s. m. hubbard, k. a. innanen, r. j. shor, i. d.
gates, et al., look ahead of the bit while drilling: potential impacts and challenges of acoustic seismic while
drilling in the mcmurray formation, spe journal (2020).
[] n. kazemi, j. auriol, k. innanen, r. shor, i. gates, successive full-waveform inversion of surface seismic
and seismic-while-drilling datasets without low frequencies, in: 82nd eage annual conference & exhibition,
volume 2020, european association of geoscientists & engineers, 2020, pp. 1â€“5.
[] j. v. beck, k. j. arnold, parameter estimation in engineering and science, james beck, 1977.
[] s. chen, s. a. billings, p. m. grant, non-linear system identification using neural networks, international
journal of control 51 (1990) 1191â€“1214.

32

675

680

685

690

695

700

705

710

715

720

725

[] i. goodfellow, y. bengio, a. courville, deep learning, mit press, 2016.
[] y. liu, w. zhan, m. xing, y. wu, r. xu, x. wu, boundary control of a rotating and length-varying flexible
robotic manipulator system, ieee transactions on systems, man, and cybernetics: systems (2020).
[] j.-b. kim, s.-j. lee, y.-p. park, stable and efficient drilling process by active control of the thrust force,
mechanical systems and signal processing 8 (1994) 585â€“595.
[] y. liu, x. chen, y. mei, y. wu, observer-based boundary control for an asymmetric output-constrained
flexible robotic manipulator, science china information sciences (2020).
[] j. yu, p. shi, j. liu, c. lin, neuroadaptive finite-time control for nonlinear mimo systems with input constraint,
ieee transactions on cybernetics (2020).
[] c. fu, q.-g. wang, j. yu, c. lin, neural network-based finite-time command filtering control for switched
nonlinear systems with backlash-like hysteresis, ieee transactions on neural networks and learning systems
(2020).
[] g. cui, j. yu, q.-g. wang, finite-time adaptive fuzzy control for mimo nonlinear systems with input saturation
via improved command-filtered backstepping, ieee transactions on systems, man, and cybernetics: systems
(2020).
[] j.-m. bastin, g.and coron, stability and boundary stabilization of 1-d hyperbolic systems, volume 88, springer,
2016.
[] l. hong, i. girsang, j. s. dhupia, identification and control of stickâ€“slip vibrations using kalman estimator in
oil-well drill strings, journal of petroleum science and engineering 140 (2016) 119â€“127.
[] l. w. ledgerwood, r. spencer, o. matthews, j. a. bomidi, j. mendoza, j. hanson, the effect of bit type on
reactive torque and consequent tool-face-control anomalies, spe drilling & completion 31 (2016) 095â€“105.
[] i. j. inyang, j. f. whidborne, m. t. bayliss, directional drilling attitude control with input disturbances and
feedback delay, ifac-papersonline 50 (2017) 1409â€“1414.
[] n. kazemi, s. nejadi, j. auriol, j. curkan, r. j. shor, k. a. innanen, s. m. hubbard, i. d. gates, advanced
sensing and imaging for efficient energy exploration in complex reservoirs, energy reports 6 (2020) 3104â€“3118.
[] k.-d. chen, j.-q. chen, d. hong, x.-y. zhong, z.-b. cheng, q.-h. lu, j.-p. liu, z.-h. zhao, g.-x. ren,
efficient and high-fidelity steering ability prediction of a slender drilling assembly, acta mechanica 230 (2019)
3963â€“3988.
[] c. germay, n. van de wouw, h. nijmeijer, r. sepulchre, nonlinear drillstring dynamics analysis, siam journal
on applied dynamical systems 8 (2009) 527â€“553. url: http://epubs.siam.org/doi/abs/10.1137/060675848.
doi:10.1137/060675848.
[] e. cayeux, on the importance of boundary conditions for real-time transient drill-string mechanical estimations, in: iadc/spe drilling conference and exhibition, society of petroleum engineers, 2018. url:
http://www.onepetro.org/doi/10.2118/189642-ms. doi:10.2118/189642-ms.
[] u. j. f. aarsnes, o. m. aamo, m. krstic, extremum seeking for real-time optimal drilling control, in: 2019
american control conference (acc), ieee, 2019, pp. 5222â€“5227.
[] e. detournay, t. richard, m. shepherd, drilling response of drag bits: theory and experiment, international
journal of rock mechanics and mining sciences 45 (2008) 1347â€“1360.
[] e. detournay, p. defourny, a phenomenological model for the drilling action of drag bits, international
journal of rock mechanics and mining sciences & geomechanics abstracts 29 (1992) 13â€“23. url: http:
//linkinghub.elsevier.com/retrieve/pii/0148906292910413. doi:10.1016/0148-9062(92)91041-3.
[] j. auriol, u. j. f. aarsnes, r. shor, self-tuning torsional drilling model for real-time applications, in: american
control conference, 2020.
[] r. j. leveque, finite volume methods for hyperbolic problems, cambridge university press, 2002.
[] m. krstic, a. smyshlyaev, boundary control of pdes: a course on backstepping designs, siam, 2008.
[] j.-m. coron, r. vazquez, m. krstic, g. bastin, local exponential h 2 stabilization of a 2 x 2 quasilinear hyperbolic system using backstepping, siam journal on control and optimization 51 (2013) 2005â€“2035. url:
http://ieeexplore.ieee.org/document/6161075/http://epubs.siam.org/doi/10.1137/120875739. doi:10.
1137/120875739. arxiv:1208.6475.
[] t. richard, f. dagrain, e. poyol, e. detournay, rock strength determination from scratch tests, engineering
geology 147 (2012) 91â€“100.
[] p. sharma, t. singh, a correlation between p-wave velocity, impact strength index, slake durability index and
uniaxial compressive strength, bulletin of engineering geology and the environment 67 (2008) 17â€“22.
[] j. auriol, n. kazemi, k. innanen, r. j. shor, combining formation seismic velocities while drilling and a pdeode observer to improve the drill-string dynamics estimation, in: 2020 american control conference (acc),
ieee, 2020, pp. 3120â€“3125.

33

730

735

740

745

[] j. rector iii, b. hardage, radiation pattern and seismic waves generated by a working roller-cone drill bit,
geophysics 57 (1992) 1319â€“1333.
[] b. goodway, t. chen, j. downton, improved avo fluid detection and lithology discrimination using lameÌ
petrophysical parameters;â€œÎ»Ïâ€,â€œÂµÏâ€, & â€œÎ»/Âµ fluid stackâ€, from p and s inversions, in: seg technical program
expanded abstracts 1997, society of exploration geophysicists, 1997, pp. 183â€“186.
[] c. nwankpa, w. ijomah, a. gachagan, s. marshall, activation functions: comparison of trends in practice
and research for deep learning, arxiv preprint arxiv:1811.03378 (2018).
[] j. yosinski, j. clune, y. bengio, h. lipson, how transferable are features in deep neural networks?, in:
advances in neural information processing systems, 2014, pp. 3320â€“3328.
[] e. tzeng, j. hoffman, t. darrell, k. saenko, simultaneous deep transfer across domains and tasks, in:
proceedings of the ieee international conference on computer vision, 2015, pp. 4068â€“4076.
[] n. kazemi, across-domains transferability of deep-red in de-noising and compressive sensing recovery of seismic
data, arxiv preprint arxiv:2007.10250 (2020).
[] r. courant, k. friedrichs, h. lewy, on the partial difference equations of mathematical physics, ibm journal
of research and development 11 (1967) 215â€“234.
[] j. auriol, robust design of backstepping controllers for systems of linear hyperbolic pdes, ph.d. thesis, psl
research university, 2018.
[] n. kazemi, e. bongajum, m. d. sacchi, surface-consistent sparse multichannel blind deconvolution of seismic
signals, ieee transactions on geoscience and remote sensing 54 (2016) 3200â€“3207.
[] u. j. f. aarsnes, j. auriol, f. di meglio, r. j. shor, estimating friction factors while drilling, journal of
petroleum science and engineering 179 (2019) 80â€“91.

34



journal of geophysics and engineering
journal of geophysics and engineering (2022) 19, 269â€“282

https://doi.org/10.1093/jge/gxac016

high-frequency wavefield extrapolation using the
fourier neural operator
and yanghua wang

centre for reservoir geophysics, resource geophysics academy, imperial college london, south
kensingtion, london sw7 2bp, uk
corresponding author: yanghua wang. e-mail: yanghua.wang@imperial.ac.uk
received 15 october 2021, revised 11 january 2022
accepted for publication 12 march 2022

abstract
in seismic wave simulation, solving the wave equation in the frequency domain requires
calculating the inverse of the impedance matrix. the total cost strictly depends on the number of
frequency components that are considered, if using a finite-difference method. for the
applications such as seismic imaging and inversion, high-frequency information is always
required and thus the wave simulation is always a challenging task as it demands tremendous
computational cost for obtaining dispersion-free high-frequency wavefields for large subsurface
models. this paper demonstrates that a data-driven machine learning method, called the fourier
neural operator (fno), is capable of predicting high-frequency wavefields, based on a limited
number of low-frequency components. as the fno method is for the first time applied to
seismic wavefield extrapolation, the experiment reveals three attractive features with fno: high
efficiency, high accuracy and, importantly, the predicted high-frequency wavefields are dispersion
free.
keywords: dispersion free, fourier neural operator, high-frequency wavefield, machine
learning, wavefield extrapolation

1. introduction
the finite-difference method is often used in seismic wave
simulation because of its straightforward concept and easy
implementation (dablain 1986; virieux et al. 2011). in the
frequency domain, the wave equation can be written in a
compact form of a helmholtz equation:
l(x, f )u(x, xs , f ) = s(xs , f ),

(1)

where x is the spatial coordinate, xs denotes the source coordinates, f is the frequency, s(xs , f ) is the source function,
u(x, xs , f ) is the frequency-domain wavefield and l(x, f ) is
the impedance operator. after the finite-differencing discretisation, the impedance matrix l(x, f ) is a sparse matrix. solving equation (1) requires computation of the matrix inverse
of l(x, f ), and an lower-upper (lu) decomposition method

which is a lu factorisation is often used factorising the matrix and to derive the solution recursively (wang 2011).
the total time cost and total required memory are both
linearly proportional to the number of frequencies that are
considered.
in seismic full-waveform inversion and reverse-time migration, the same wave simulation engine is used many times,
and thus the time cost and memory requirement become
a significant issue in seismic application (virieux & operto 2009; wang & rao 2009; wang 2016). moreover, for
high-frequency wavefield solutions, a finer spatial sampling is
required to mitigate the numerical dispersion in the finitedifference method. such a fine sampling might exceed the capabilities of the computational resources (wu & alkhalifah
2018). to address the numerical dispersion issue, many
methods have been proposed, such as high-order finite

Â© the author(s) 2022. published by oxford university press on behalf of the sinopec geophysical research institute. this is an open access article distributed under the terms of
the creative commons attribution license (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium,
provided the original work is properly cited.

269

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

chao song

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

trained for every different velocity model and frequency.
frequency-domain seismic imaging and inversion methods
need multiple frequencies to provide sufficient wavenumber
components to refine the resolution of the model. however, it is difficult for pinn to generate high-frequency
components due to the spectra-bias issue, which indicates
the learning priority of low-frequency components over
high-frequency ones (rahaman et al. 2019).
regarding the data-driven perspective, machine learning
extracts key features from data and simulates the dynamic
evolution of underlying physics disciplines. for example, the
cnn framework is used to calculate fluid flow solutions corresponding to inviscid euler equations efficiently (tompson
et al. 2017), and to resolve reynolds-averaged navierâ€“stokes
(rans) equations (thuerey et al. 2020). a cnn-based autoencoder is used as a surrogate modelling engine for fast
seismic data simulation and several dynamical pde systems
(geneva & zabaras 2020; moseley et al. 2020). a neural operator, which is a different concept from cnn, is effective in
learning the mapping between function spaces with limited
training samples (li et al. 2020a) and can be implemented in
the fourier space domain to accelerate the training process
(li et al. 2020b).
in this paper, we propose to use a data-driven machine
learning method to achieve fast high-frequency wavefield extrapolation free of numerical dispersion errors. the wavefields can be treated as solutions from pdes parameterised by
the frequency and can be solved by a machine learning-based
pde solver. this pde solver is the fourier neural operator
(fno) (li et al. 2020b), which will we use in this paper.
the rest of this paper will be organised as follows: first, we
introduce how to implement fno for the wavefield extrapolation. next, we explain the used data for training, validating
and testing. then, we compare the high-frequency wavefield
result from fno and that from the conventional finitedifference method. finally, we draw the conclusion that
fno is effective and efficient in generating high-frequency
wavefields.
2. fno for wavefield extrapolation
frequency-domain seismic inversion and imaging processes
need wavefields of multiple frequencies to provide sufficient
wavenumber components to reconstruct the model. we attempt to exploit a machine learning method from the datadriven perspective using the fno. when we experiment the
fno usage in wavefield extrapolation, the implementation
consists of four steps.
(i) we generate synthetic wavefields within a wide frequency range using the finite-difference method and
split these wavefields into small wavefield pieces.
(ii) we divide the resulting wavefield pieces into a lowfrequency wavefield group and a high-frequency group,
270

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

difference, compact finite difference and low-rank approximation (dablain 1986; yang et al. 2006; fomel et al. 2013;
wu & alkhalifah 2014). in the current paper, we propose
to improve the efficiency of high-frequency wave simulation
using a machine learning method rather than solving the
wave equation (1) directly for all frequency components.
machine learning methods are attractive because they can
deal with large-scale data and images, and still show excellent
performance. one of the popular machine learning methods
is the neural network-based method. for example, the deep
neural network method is effective in picking the fresnel
zone (sun et al. 2019), eliminating surface waves (kaur et al.
2020a), improving the qualities of the migration images
(kaur et al. 2020b) and enhancing the resolution of velocity models reconstructed by waveform inversion (li et al.
2021). the convolutional neural network (cnn) method
is capable of extracting feature maps from images by using a
convolution filter and is applicable to detecting salt bodies
and faults from migration images (li et al. 2019; shi et al.
2019). cnn has also been used to predict low-frequency
components of the data that are needed for the waveform
inversion and for detecting microseismic events
(ovcharenko et al. 2019).
the machine learning methods are also effective in solving
partial differential equations (pdes). for seismic wave equations, either in the time or frequency domain, which are typical pdes, the machine learning methods can be used from
two perspectives. one is to use the underlying physics disciplines as loss functions. for example, a framework called
the physics-informed neural network (pinn) can use automatic differentiation to calculate the partial derivatives of
target output functions with respect to spatial and temporal coordinates (baydin et al. 2017; raissi et al. 2019). the
other perspective is data-driven machine learning. the pdes
of frequency-domain wave equations are parameterised by
frequency and subsurface property models. by using a large
amount of training data, data-driven machine learning methods are popularly used to solve pdes by transforming the
question into an optimisation problem.
regarding the physics-informed perspective, the enormous training cost for multiple pinns corresponding to
different pdes parameters will adversely affect their performances in real-life applications (waheed et al. 2021). in
addition, many trial-and-error tests must be performed to
tune the hyperparameters in the networks to get reasonable
results. while song et al. (2021) showed the flexibility and
versatility of pinns for isotropic and anisotropic acoustic
media, and for models with irregular shapes, such as topography, in solving the scattered form of the helmholtz equation,
they also stated the limitations of pinn-based helmholtz
equation solvers, including the high computational cost of
training for small models and low resolution of resulting
wavefield solutions. in addition, a new network needs to be

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

by doing so, the trained network becomes a highfrequency wavefield generator that can produce highfrequency wavefields for large models. the proposed method
is purely data-driven. even though large-scale models have
many details and complex structures, we can still divide them
into small sections and generate low and high wavefields for
training. if these complicated structures and corresponding
wavefields are included in the training, we will be able to
make accurate wavefield predictions in the test data for whole
large-scale models.
3. data generation for training, validation and testing
we demonstrate the performance of fno using three test
models. all three models are based on the marmousi model
(figure 1), but each test model has a different level of complexity. therefore, the generated wavefields will have different complexities and of course will have different implications in the fno training and prediction. test model i
(figure 1a) is a strongly smoothed model, smoothed by a 2d
gaussian filter with a smoothing window of 40 grid points.
test model ii (figure 1b) is smoothed with a smoothing window of 20 grid points. finally, test model iii (figure 1c) is the
original marmousi model.
the size of the marmousi model is 300 Ã— 1500 grids with
a 10-m spatial sampling interval in both vertical and horizontal directions. we randomly select five velocity portions
from the model. one of them is shown in the green frame
in figure 1a. the size of each selected velocity portion is
300 Ã— 800 grids. we set 40 source points in each velocity
portion, and the source points are uniformly distributed on
the surface at the depth of 20 m. the total source number is
40 Ã— 5 = 200. we use a nine-point finite-differencing scheme
to discretise the wave equation and generate the training
data ( jo et al. 1996).
for these 200 source points, we generate the frequencydomain wavefields from 5 to 30 hz with a frequency interval
of 1 hz. these frequencies are in a practical frequency range
for waveform inversion, in which the extreme low-frequency
components are often missing from the recorded data or
contaminated by noise. for each wavefield, we divide it into
24 wavefield pieces with the size of 50 Ã— 50. in total, there

4. network training
in the training process, we extend data set a to five dimensions arranged as follows:
271

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

are 200 Ã— 24 = 4800 wavefield pieces for each frequency.
among these 4800 pieces, we use 4700 wavefield pieces as
the training data and the remaining 100 pieces as validation
data. figure 2 displays 24 wavefield pieces arbitrarily selected
from training data, and all the wavefield pieces are plotted
along the same scale. we divide the training data into two
data sets: the wavefields ranging from 5 to 12 hz (data set
a) and the wavefields ranging from 13 to 30 hz (data set
b). as a result, the size of data set a is 4700 Ã— 50 Ã— 50 Ã— 8,
and the size of data set b is 4700 Ã— 50 Ã— 50 Ã— 18. we target
predicting data set b, which has 18 frequencies, using data
set a, which has eight frequencies. we consider this target
quite challenging as we try to predict 18 high-frequency
wavefields, based on only eight input low-frequency ones.
our ultimate goal is to generate high-frequency wavefields
for the whole marmousi model using test data. as the input wavefields are at low frequencies (5â€“12 hz), we can generate test data using a coarse spatial grid sampling interval.
we downsample the original marmousi model by half in
both vertical and horizontal directions and double the spatial grid sampling interval to 20 m. thus, the survey area remains the same as 3 Ã— 15 km2 . now the size of the model
becomes 150 Ã— 750 grids. we set a source located at (0.02,
7.5 km) and divide the resulting wavefield into small pieces
with the size of 50 Ã— 50 as test data. as a result, there are
(150/50)Ã—(750/50) = 45 sets of wavefield pieces. considering the frequency number, the size of the test data set is
45 Ã— 50 Ã— 50 Ã— 8.
as we know, for finite-differencing approximation to the
wave equation, the grid size depends on the maximum frequency considered to avoid numerical dispersion errors.
if the highest frequency is doubled, the grid size should
be half smaller. this would require significantly more storage and computation time. thus, we effectively cut the
considered highest frequency by at least half in the finitedifferencing wave simulation when using a machine learning method to predict the high-frequency wavefields. this
procedure significantly reduces the complexity of numerical
computation.
we generate the training, validation and test data using a
delta function as the source signature, so trained networks
will act as greenâ€™s function for generating high-frequency
wavefields. because of the delta source signature, the amplitude and phase of predicted wavefields are the same for all
frequencies. thus, in the realistic seismic surveys, we shall
recover the true wavefields by modification based on the
corresponding frequency components of the actual source
wavelet.

and train the networks to build the mapping relationship between the low-frequency and high-frequency
groups.
(iii) once we complete the training of the networks, we
import a limited number of low-frequency wavefields
to the trained networks and predict multiple highfrequency wavefields.
(iv) finally, we stitch the predicted wavefield pieces together to produce the final high-frequency wavefields
for the entire large velocity model.

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

(i) the first dimension corresponds to the number of
wavefield piece sets used in training; it is 4700 in this
case.
(ii) the second dimension corresponds to the index of vertical grids within the wavefield piece; it is 50 in this case.
(iii) the third dimension corresponds to the index of horizontal grids within the wavefield piece; it is also 50 in
this case.

(iv) the fourth dimension corresponds to the desired number of high-frequency wavefield frequencies; it is 18 in
this case.
(v) the fifth dimension corresponds to the number of
channels within the first block; it is equal to the sum of
the number of frequencies within data set a, the vertical index and the horizontal index, and the frequency
index; it is 11 (= 8 + 1 + 1 + 1) in this case. the three

272

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

figure 1. the marmousi velocity model. (a) test model i: a strongly smoothed model, using a 2d gaussian smoothing filter with a window of 40 grid
points. (b) test model ii: a moderately smoothed model, using a 2d gaussian smoothing filter with a window of 20 grid points. (c) test model iii: the
original marmousi model. three models have different complexity in wavefields that will lead to different accuracies in fno prediction.

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

indexes (1 + 1 + 1) here are normalised between zero
and one.
for the internal output (data set bâ€™) from fno, the dimension is reduced to four since the channel number in the
fifth dimension is one and can be compressed and ignored.
the rest four dimensions of data set bâ€™ are the same as the
size of the training data set b, and this data set bâ€™ shall match
the data set b. after 250 epochs, the mismatch between bâ€™
and b is sufficiently small, we complete the network training
successfully.
we adopt the adam optimiser in the network training. the adam optimiser is a gradient-based optimisation
method of stochastic objective functions, which is suitable
for optimisation problems with large-scale data (kingma &
ba 2014).
there are two key parameters in the adam optimiser. the
first one is the learning rate, which is equivalent to a step
length in the optimisation problem. we determine the learning rate based on trial-and-error. we start the learning rate
with 0.0025 and reduce it by half after every 50 epochs. the
total epoch number is 250.
the second parameter is the size of a mini batch, which
is the number of wavefield piece sets taken in each training
step. after sequentially training the steps when all wavefield
piece sets are used, it completes an epoch in the machine
learning method. in this example, we set the size of the mini
batch to 50.
figure 3 displays the training loss history curves for the
three test models using the unit of decibels (db). the displayed training loss curves are normalised to db. it is obvious that the training loss decreases faster and reaches a lower
loss value for a smoother velocity model, as in the curves for
test model i (solid black curve) and test model ii (dashed
red curve). a sharper velocity model produces more scattering details in the wavefield solutions. compare the smoothed
wavefields generated from test model i and from test model

figure 3. the normalised training loss history curves (in db) for the three
test models. the solid black curve corresponds to the test model i (figure 1a); the dashed red curve corresponds to the test model ii (figure 1b)
and the dotted blue curve corresponds to the test model iii (figure 1c).

ii; the scattering wavefields generated from test model iii
are more difficult to learn for the machine learning method.
however, we can still achieve the training convergence with
a slightly higher training loss, as shown by the dotted blue
curve in figure 3.
5. performance of fno for the validation data
after the networks are trained, we input the low-frequency
(5â€“12 hz) wavefield pieces from the validation data into the
trained networks to produce the high-frequency (13â€“30 hz)
wavefield ones. we arbitrarily select two wavefield piece sets
from validation data. we show one validation wavefield piece
comparison in figure 4a and another comparison in figure 4b.
from the left column to the right column, the displayed
wavefield frequencies are 15, 18, 21, 24, 27 and 30 hz. in
figure 4a and b, the validation wavefield pieces generated
273

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

figure 2. wavefield pieces (24) arbitrarily selected from the training data set. the entire traning data set consists of 4800 of such sample pieces.

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

5 Ã— 20 = 100 m. according to the dependence of the lowest
velocity (v) and the longest wavelength (ğœ†), the minimum
frequency of wavefields that are free of dispersion should be
15 hz.
we compare the wavefields generated from the finitedifference method and from fno using three frequencies,
13, 22 and 30 hz, for the three test models used. for 13 hz,
both finite-difference and fno wavefields will not suffer
from dispersion errors; for 22 hz, dispersion errors will appear in the wavefield from the finite-difference method; while
for 30 hz, the finite-difference method will suffer from dispersion severely.
to quantitatively measure the similarity of the wavefields
from the finite-difference and the fno methods, we calculate the correlation coefficients between them for each grid
point using the following formulation (gao & wang 2020):

from the finite-difference method are displayed in the top
row, and the fno-predicted wavefield pieces are displayed in
the bottom row. plotting the wavefield pieces from the finitedifference and the fno methods on the same scale, we can
barely observe any difference between them for both comparisons in figure 4a and 4b, and this confirms the accuracy
of the fno.

6. performance of fno for the test data
after confirming the accuracy of the fno training using the validation data, we input the test data into the
trained fno to get the predicted high-frequency wavefield
pieces with the size of 45 Ã— 50 Ã— 50 Ã— 18. we stitch the
predicted wavefield pieces (13â€“30 hz) to get the final highfrequency wavefields for the whole marmousi model. to
compare the fno-predicted wavefields with the numerical
solutions, we calculate 13â€“30 hz wavefields using the finitedifference method from the downsampled marmousi model
(150 Ã— 750) with a coarse spatial grid sampling interval
(20 m). the reason for downsampling the marmousi model
is that the memory requirement for the original marmousi
model is unaffordable for our hardware equipment.
the minimal velocity value in the original marmousi
model is 1500 m s-1 . to mitigate the dispersion errors for the
finite-difference method, there have to be at least five grid
points in each wavelength (hall & wang 2009). considering the coarse spatial grid sampling interval of 20 m, the minimal wavelength that is immune to the dispersion error is

|
|
âˆ‘ âˆ‘ l,k l,k
|
|
u
u
|
|
l
k fd fno
|,
|
cor(i, j) = | âˆš
|
| âˆ‘ âˆ‘ l,k 2 âˆ‘ âˆ‘ l,k 2 |
|
|
(u
)
(u
)
l
k
l
k
fd
fno |
|

(2)

where ufd and ufno represent the wavefields from the finitedifference and the fno methods, respectively; i and j denote the indexes for the grid point in the vertical and horizontal directions, respectively, and l and k represent the
integral indexes within the ranges of [i âˆ’ nw, i + nw] and
[j âˆ’ nw, j + nw], respectively. nw is the half size of the window in which we calculate the correlation coefficients. in this
paper, we assume nw equal to 10 for all the cases. the resulting correlation coefficient values range from 0 to 1. a
274

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

figure 4. (a, b) two comparisons of the acoustic wavefield piece. the wavefield displays six frequency components: 15, 18, 21, 24, 27, 30 hz. there is
no obvious difference between wavefield pieces generated from the finite-difference method (top row) and from fno (the bottom row).

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

large value close to 1 indicates high similarity between the
two wavefields, while a small value close to 0 indicates low
similarity.
in this section, we plot all the wavefields for the marmousi
model in the same scale. first, we show the wavefields of
13 hz from the finite-difference and the fno methods for
the test model i (figure 1a) in figure 5a and b, respectively. we
observe that fno can generate a reasonable wavefield prediction result (figure 5b) that is close to the finite-difference
solution (figure 5a). figure 5c displays the correlation coefficients between the wavefields generated from the finitedifference and the fno methods. in the whole model domain, the correlation coefficients are close to 1, which shows
high similarity between the 13 hz wavefields from the finitedifference and the fno methods. for the 13 hz wavefield,
the finite-difference method should be accurate according to
our analysis, thus the high similarity in figure 5c indicates that
fno is equally accurate to the finite-difference method.
next, we show the 22 hz wavefields from the finitedifference and the fno methods in figure 6a and b, respectively. as discussed previously, the lowest frequency that is
free of dispersion error for the marmousi model is 15 hz,
so the accuracy of the resulting wavefield at 22 hz from the
finite-difference method is compromised by the dispersion.

although we do not observe obvious dispersion effects in
the wavefield from the finite-difference method in figure 6a,
these errors are revealed by the low correlation coefficients between the finite-difference wavefield (figure 6a)
and the fno-predicted wavefield (figure 6b) shown in
figure 6c. these low correlation coefficients are located in
the shallow part of the velocity model, which is consistent with the low-velocity region that may cause dispersion
errors.
next, figure 7a and b display the 30-hz wavefields from
the finite-difference and the fno methods, respectively. for
this high frequency, we observe obvious dispersion errors
(reflected in both phase shift and amplitude distortion in
figure 7a) in the wavefield solution from the finite-difference
method, as indicate by the arrows in figure 7a. by comparison, the fno-predicted wavefield (figure 7b) is immune to
the dispersion effect, which shows fnoâ€™s accuracy superiority over the finite-difference method. figure 7c displays the
wavefield correlation coefficient between figure 7a and b. the
dispersion errors in figure 7a are reflected by low correlation
coefficients that are widely distributed in the low-velocity
areas in the shallow part.
to explore how fno will perform for more complicated
wavefields, we consider a less smoothed marmousi model
275

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

figure 5. wavefields at 13 hz from (a) the finite-difference method, (b) from fno and (c) the correlation coefficients between (a) and (b) corresponding to test model i (figure 1a).

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

figure 6. the same plotting configuration as figure 5 for 22 hz.

figure 7. the same plotting configuration as figure 5 for 30 hz. the arrows in (a) point to dispersion errors.
276

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

figure 9. the same plotting configuration as figure 8 for 22 hz.
277

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

figure 8. wavefields at 13 hz from (a) the finite-difference method, (b) from fno and (c) the correlation coefficients between (a) and (b) corresponding to test model ii (figure 1b).

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

figure 11. wavefields at 13 hz from (a) the finite-difference method, (b) from fno and (c) the correlation coefficients between (a) and (b) corresponding to test model iii (figure 1c).
278

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

figure 10. the same plotting configuration as figure 9 for 30 hz. the arrows in (a) point to dispersion errors, while the arrows in (b) point to discontinuities between wavefield piece boundaries.

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

method become obvious in the shallow part of the model,
as shown in figure 10a (pointed by the arrows). figure 10b
displays the dispersion-free fno-predicted wavefield. although we observe some discontinuities on wavefield piece
boundaries in figure 10b (indicated by the arrows), these
mild discontinuities will not affect the applications of the
fno-predicted wavefield in the waveform inversion as they
can be smeared by smoothing filters focusing on the horizontal direction in the velocity gradient calculation. we show the
wavefield correlation coefficient between figure 10a and b in
figure 10c, and small correlation coefficients are distributed
in large areas that correspond to low velocity.
finally, we directly show the applications of the finitedifference and fno methods for the test model iii. using
the same training setup, we obtain the 13-hz wavefields from
the finite-difference and the fno methods and display them
in figure 11 parts a and b, respectively. it is obvious that the
wavefields get more complicated as more detailed structures
are considered in the velocity model. the fno-predicted
wavefield (figure 11b) is very close to the finite-difference solution (figure 11a), and this is confirmed by the large correlation coefficient shown in figure 11c.
figure 12 displays the 22-hz wavefields from the finitedifference and the fno methods for the test model iii. although the wavefield becomes more complex due to the increase in the model complexity, fno is still able to generate a

(figure 1b). repeating the training, validation, and test data
generation and the training of the network using the test
model ii, we directly show the wavefield results for test data.
figure 8 parts a and b display the 13 hz wavefields from the
finite-difference and the fno methods for the test model ii,
respectively. fno can generate equally accurate wavefields
with the finite-difference method for relatively low frequencies. the wavefield correlation coefficient between figure 8a
and b is shown in figure 8c, which is equal to one in most
areas despite some low correlation coefficients on the edge
of the model. with the increase of test model complexity,
the wavefield complexity also increases and the amplitude
of the wavefield becomes unbalanced in the model domain.
the wavefield in the regions with very weak amplitudes is
difficult to recover, which is indicated by the low correlation
coefficient on the edges.
figure 9 displays the 22-hz wavefields from the finitedifference and fno methods for test model ii. the general
wave shapes of figure 9a and b are similar, but we can capture
the dispersion errors in figure 9a by the low correlation coefficients shown in figure 9c. comparing figure 9c with figure 7c,
we observe that low correlation coefficients are located in the
same area in the shallow part of the model caused by the low
velocity.
when we increase the frequency to 30 hz, the dispersion
effects in the wavefield calculated by the finite-difference
279

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

figure 12. the same plotting configuration as figure 11 for 22 hz.

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

reasonably good wavefield solution. in the original marmousi
model (figure 1c), the low-velocity layers are not smeared but
distributed in the whole model domain. as a result, the dispersion errors caused by low-velocity layers become more severe, as indicated by the low correlation coefficient between
the wavefields in figure 12a and b, shown in figure 12c.
for 30 hz, the finite-difference method suffers notably
from the dispersion error, especially in the shallow area of low
velocity (figure 13a). by comparison, fno is immune to dispersion errors in spite of some discontinuities in the fnopredicted wavefield (figure 13b). the correlation coefficient
between wavefields from the finite-difference and the fno
methods is shown in figure 13c, which is small spanning the
whole model space. when the wavefield gets complicated,
it gets more difficult to recover the details of the wavefield.
we shall increase the number of fno blocks to improve the
wavefield prediction accuracy. when the number of the fno
blocks is increased, the training computational cost will be increased linearly, whereas the efficiency for the wavefield generation is kept the same.
from the computational efficiency aspect, fno is almost two orders of magnitude faster than the finite-difference
method after the training process. for future applications,
we can start the training from a pre-trained model based on
the theory of transfer learning (pan & yang 2009; song et al.

2022). consequently, we can further reduce the training cost
and retain the efficiency feature of the fno.
7. conclusions
we have experimented for the first time to extrapolate
high-frequency wavefield using the fourier neural operator
(fno). the aim was to build a mapping relationship between low- and high-frequency wavefields. considering the
limit of computer capacity, we have divided the model into
small pieces, trained the fno and conducted predictions using small pieces. we have demonstrated that only a limited
number of low frequencies are needed to complete the task
of extrapolation from low- to high-frequency wavefields. by
stitching all the small pieces of predictions together, we have
obtained the high-frequency wavefields for large models that
a computer using the conventional finite-difference method
cannot cope with.
acknowledgements
the authors are grateful to the sponsors of the centre for reservoir
geophysics, imperial college london, for supporting this research.

conflict of interest statement: authors declare that there is
no conflict of interest.
280

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

figure 13. the same plotting configuration as figure 12 for 30 hz.

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang

this series rğœ™ can be parameterised directly as a complexvalued tensor, to replace the fourier transform of ğœ…ğœ™ . in the
parameterisation of rğœ™ , we treat rğœ™ as a low-pass filter, to suppress unnecessary high-frequency components in f[pj (x)],
and thus truncate the series. in this way, we effectively reduce
the number of trainable parameters and effectively improve
the efficiency of training. we define the process of equation
(a.1) as one fno block (figure a.1).

appendix. fourier neural operator (fno)

references
baydin, a.g., pearlmutter, b.a., radul, a.a. & siskind, j.m., 2017. automatic differentiation in machine learning: a survey, journal of machine
learning research, 18, 5595â€“5637.
dablain, m.a., 1986. the application of high-order differencing to the
scalar wave equation. geophysics, 51, 54â€“66.
fomel, s., ying, l. & song, x., 2013. seismic wave extrapolation using lowrank symbol approximation, geophysical prospecting, 61, 526â€“536.
gao, f. & wang, y., 2020. radiation pattern analyses for seismic multiparameter inversion of hti anisotropic media, journal of geophysics and
engineering, 17, 65â€“75.
geneva, n. & zabaras, n., 2020. modelling the dynamics of pde systems with physics-constrained deep auto-regressive networks, journal of
computational physics, 403, 109056.
hall, f. & wang, y., 2009. elastic wave modelling by an integrated finite
difference method, geophysical journal international, 177, 104â€“114.
jo, c.h., shin, c. & suh, j.h., 1996. an optimal 9-point, finite-difference,
frequency-space, 2-d scalar wave extrapolator, geophysics, 61,
529â€“537.
kaur, h., fomel, s. & pham, n., 2020a. seismic ground-roll noise attenuation using deep learning, geophysical prospecting, 68, 2064â€“2077.
kaur, h., pham, n. & fomel, s., 2020b. improving resolution of migrated
images by approximating the inverse hessian using deep learning, geophysics, 85, wa173â€“wa18.
kingma, d.p. & ba, j., 2014. adam: a method for stochastic optimization.
arxiv preprint arxiv:1412.6980.
li, s., yang, c., sun, h. & zhang, h., 2019. seismic fault detection using
an encoderâ€“decoder convolutional neural network with a small training
set, journal of geophysics and engineering, 16, 175â€“189.
li, y., alkhalifah, t. & zhang, z., 2021. deep-learning assisted regularized
elastic full waveform inversion using the velocity distribution information from wells, geophysical journal international, 226, 1322â€“1335.
li, z., kovachki, n., azizzadenesheli, k., liu, b., bhattacharya, k., stuart,
a. & anandkumar, a., 2020a. neural operator: graph kernel network
for partial differential equations, arxiv preprint, arxiv:2003.03485.
li, z., kovachki, n., azizzadenesheli, k., liu, b., bhattacharya, k., stuart,
a. & anandkumar, a., 2020b. fourier neural operator for parametric
partial differential equations, arxiv preprint, arxiv:2010.08895.
moseley, b., nissen-meyer, t. & markham, a., 2020. deep learning for fast
simulation of seismic waves in complex media, solid earth, 11, 1527â€“
1549.
ovcharenko, o., kazei, v., kalita, m., peter, d. & alkhalifah, t., 2019. deep
learning for low-frequency extrapolation from multi-offset seismic data,
geophysics, 84, r989â€“r1001.
pan, s.j. & yang, q., 2009. a survey on transfer learning. ieee transactions
on knowledge and data engineering, 22, 1345â€“1359.
rahaman, n., baratin, a., arpit, d., draxler, f., lin, m., hamprecht,
f., bengio, y. & courville, a., 2019. on the spectral bias of neural
networks, proceedings of the 36th international conference on machine
learning, arxiv:1806.08734.

with
(k(ğœ™)pj (x) =

âˆ«

ğœ…ğœ™ (x, y)pj (y)dy,

(a.2)

where ğœ…ğœ™ is a neural network parameterised by weights in the
network ğœ™, and w is a linear transformation. both w and ğœ™
can be learned from the training data. the activation function
ğœ(x) is the non-linear element-wise activation function.
to accelerate the integral process of equation (a.2), one
could impose the condition ğœ…ğœ™ (x, y) = ğœ…ğœ™ (x âˆ’ y) and turn
the integral into a convolution
(k(ğœ™)pj )(x) = âˆ« ğœ…ğœ™[(x âˆ’ y)pj (y)dy ]
= f âˆ’1 f[ğœ…ğœ™ (x)]f[pj (x)] ,

(a.3)

where f denotes the fourier transform, and f âˆ’1 denotes the
inverse fourier transform. because of the fourier transform
implementation, this neural operator is referred to as fno
(li et al. 2020b).
while ğœ…ğœ™ is defined in the fourier transform space,
rğœ™ = f[ğœ…ğœ™ (x)],

(a.4)

figure a.1. block of fno. f denotes the forward fast fourier transform;
f âˆ’1 denotes the inverse fourier transform; rğœ™ denotes a linear transform
filters out the high-frequency components; w denotes a local linear transform; ğœ denotes a non-linear activation function and pj and pj+1 are the
input and output of the current block, respectively.
281

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

fno is one of the neural operator methods. the concept of
the neural operator was developed for the mapping between
function spaces, and especially is applicable to solve pdes.
for instance, if we want to find a non-linear mapping operator
g to connect two functions a = a(a) and b = b(b), the procedure consists of two local transformations. in the first local
transformation, the input a âˆˆ a is lifted to a higher dimension by a shallow fully connected neural network p0 = p(a)
as the initial state and, then, an iterative architecture p0 â†’
p1 ... â†’ pj ... â†’ pt is used to evolve their state until the connection with the output. in the second local transformation,
using another shallow fully connected neural network q , the
last state output pt is projected to the dimension of output
using b = q (pt ).
the iterative update from state pj to pj+1 is defined by an
activation function ğœ(x) as
(
)
pj+1 (x) = ğœ wpj (x)+(k(ğœ™)pj )(x) ,
(a.1)

journal of geophysics and engineering (2022) 19, 269â€“282

song and wang
virieux, j. & operto, s., 2009. an overview of full-waveform inversion in
exploration geophysics, geophysics, 74, wcc1â€“wcc26.
virieux, j., calandra, h. & plessix, r.Ã©., 2011. a review of the spectral, pseudo-spectral, finite-difference and finite-element modelling
techniques for geophysical imaging, geophysical prospecting, 59,
794â€“813.
waheed bin, u., haghighat, e., alkhalifah, t., song, c. & hao, q., 2021.
pinneik: eikonal solution using physics-informed neural networks,
computers & geosciences, 155, 104833.
wang, y. & rao, y., 2009. reflection seismic waveform tomography, journal of geophysical research, 114, b03304.
wang, y., 2011, seismic, waveform modelling and tomography, in encyclopedia
of solid earth geophysics, springer verlag, 1290â€“1301.
wang, y., 2016. seismic inversion: theory and applications, john wiley &
sons.
wu, z. & alkhalifah, t., 2014. the optimized expansion based low-rank
method for wavefield extrapolation, geophysics, 79, t51â€“t60.
wu, z. & alkhalifah, t., 2018. a highly accurate finite-difference method
with minimum dispersion error for solving the helmholtz equation,
journal of computational physics, 365, 350â€“361.
yang, d., peng, j., lu, m. & terlaky, t., 2006. optimal nearly analytic discrete approximation to the scalar wave equation, bulletin of the seismological society of america, 96, 1114â€“1130.

282

downloaded from https://academic.oup.com/jge/article/19/2/269/6576250 by universidad eafit user on 28 august 2024

raissi, m., perdikaris, p. & karniadakis, g.e., 2019. physics-informed neural networks: a deep learning framework for solving forward and inverse
problems involving nonlinear partial differential equations, journal of
computational physics, 378, 686â€“707.
shi, y., wu, x. & fomel, s., 2019. saltseg: automatic 3d salt segmentation
using a deep convolutional neural network, interpretation, 7, se113â€“
se122.
song, c., alkhalifah, t. & waheed, u.b., 2021. solving the frequencydomain acoustic vti wave equation using physics-informed neural networks, geophysical journal international, 225, 846â€“859.
song, c., alkhalifah, t. & waheed, u.b., 2022. a versatile framework to
solve the helmholtz equation using physics-informed neural networks,
geophysical journal international, 228, 1750â€“1762.
sun, h., zhang, h., song, m., li, s. & lu, y., 2019. automatic fresnel zone
picking in the dip-angle domain using deep neural networks, journal of
geophysics and engineering, 16, 136â€“145.
thuerey, n., weiÃŸenow, k., prantl, l. & hu, x., 2020. deep learning methods for reynolds-averaged navierâ€“stokes simulations of airfoil flows,
aiaa journal, 58, 25â€“36.
tompson, j., schlachter, k., sprechmann, p. & perlin, k., 2017. accelerating eulerian fluid simulation with convolutional networks, proceedings of the 34th international conference on machine learning,
arxiv:1607.03597.



pinnslope: seismic data interpolation and local slope

arxiv:2305.15990v1 [physics.geo-ph] 25 may 2023

estimation with physics informed neural networks

francesco brandolin, matteo ravasi, tariq alkhalifah
king abdullah university of science and technology (kaust)
thuwal, kingdom of saudi arabia
{francesco.brandolin, matteo.ravasi, tariq.alkhalifah}@kaust.edu.sa

a bstract
interpolation of aliased seismic data constitutes a key step in a seismic processing workflow to obtain
high quality velocity models and seismic images. leveraging on the idea of describing seismic
wavefields as a superposition of local plane waves, we propose to interpolate seismic data by utilizing
a physics informed neural network (pinn). in the proposed framework, two feed-forward neural
networks are jointly trained using the local plane wave differential equation as well as the available
data as two terms in the objective function: a primary network assisted by positional encoding
is tasked with reconstructing the seismic data, whilst an auxiliary, smaller network estimates the
associated local slopes. results on synthetic and field data validate the effectiveness of the proposed
method in handling aliased (sampled coarsely) data and data with large gaps. our method compares
favorably against a classic least-squares inversion approach regularized by the local plane-wave
equation as well as a pinn-based approach with a single network and pre-computed local slopes.
we find that by introducing a second network to estimate the local slopes whilst at the same time
interpolating the aliased data, the overall reconstruction capabilities and convergence behavior of
the primary network is enhanced. an additional positional encoding, embedded as a network layer,
confers to the network the ability to converge faster improving the accuracy of the data term.

1

introduction

the idea of describing seismic data as a superposition of local plane-waves was introduced by jon claerbout back in
1992. this elementary type of waves can be modelled by the plane-wave partial differential equation (plane-wave pde),
which is only parameterized by the local slope factor (also referred to as slowness, or ray parameter). [1] demonstrated
how a plane-wave defined by its local slope can be annihilated within a given wavefield by the mean of a plane-wave
partial differential operator. leveraging on this simple concept, he developed a linear approach for local-slope
estimation using small moving windows across data, where a single slope at the center of the window is computed by
linear least-squares. later, [2] proposed the plane-wave destruction filters (pwd), a global approach for local slope
estimation that requires the solution of a non-linear system to estimate the dip, but removes the need for windowing
the data. several techniques have been developed during the years that utilizes the plane-wave approximation in
seismic processing algorithms with applications ranging from denoising ([3]), trace interpolation, detection of local
discontinuities ([2]), velocity-independent imaging ([4]) and as a regularization in seismic estimation problems ([5]).
in this work we build upon the concept introduced by claerbout, utilizing neural networks informed by the local
plane-wave equation to simultaneously interpolate seismic data and estimate the local slopes of the events. in the
machine learning literature the idea of integrating the governing laws of physics into the learning process of a neural
network is commonly referred to as: physics informed neural networks (pinns - [6]). pinns have recently emerged as
a novel computational paradigm in the field of scientific machine learning and have been shown to be very effective in
representing solutions of partial differential equations (pdes). the pinns framework can be utilized to solve both
forward and inverse problems and has been successfully applied in various domains of computational physics. in the
context of exploration seismology, pinns have been utilized with different pdes, for modelling wave propagation in
time domain using the wave equation ([7], [8]) and to model wavefields in frequency domain leveraging the helmholtz

equation and the vertical transversely isotropic wave equation ([9], [10] and [11]). pinns have also been applied to the
eikonal equation to help overcome some limitations of conventional techniques in seismic tomography problems ([12],
[13], [14], [15]).
pinns are a special class of feed-forward neural networks that predicts a solution that satisfies the pde, which governs
the physics of the problem, restricting the space of possible solutions in favor of physically reliable ones. in the
standard scenario, where the loss function includes a data fitting term (as in our implementation), the pde term acts as a
soft constrain of the network optimization problem. specifically, pinns are generic function approximators promoting
reconstructions that are naturally similar to the available data. with this advantage we can obtain an algorithm that
leverages on the interpolation capabilities of feed-forward neural networks, but whose solution is heavily dependent on
the physics of the problem. moreover, compared to traditional numerical approaches that rely on the discretization of
derivatives involved in a pde, pinns learn a direct mapping from spatial coordinates to wavefield amplitudes (or any
other physical quantity), removing the need for finite difference approximations, and relying on functional derivatives,
which are more stable.
a recent deep learning framework named coordinate based learning has emerged in the literature ([16], [17]).
coordinate based learning aims to solve imaging inverse problems by utilizing a feed-forward network to learn a
continuous mapping from the measurement coordinates to the corresponding sensor responses. the approach bear
resemblance to pinns except that it does not utilize any physics laws to constrain the network solution. [16] found that
having the feed-forward network acting directly on the input coordinates performs poorly at representing high-frequency
component of the signals in accord to the low-frequency bias of neural networks as demonstrated by [18]. to
overcome this problem he proposed to map the input coordinate to a higher dimensional space through positional
encoding before feeding them to the network. successful implementations of positional encoding in seismic
applications can be found in the works of [19] and [20]. in our implementation, we also leverage positional encoding
to ensure that the network is capable to reconstruct multi-scale, oscillating signals like those encountered in seismic data.
to support our claims, we present numerical examples focused on two of the most challenging tasks in seismic
interpolation: namely, interpolation of regularly subsampled data (beyond aliasing) and data with large-gaps. we first
evaluate the performance of the proposed approach on two simple synthetic data examples, comparing the results
obtained by the pinns approach with a simple plane-wave regularized least-square inversion (pwls) and against a
previous version of our framework called pwd-pinn ([21]). we then consider a field data example where given the
challenge of the higher frequency content and higher complexity of the recorded signals, we decided to leverage on
positional encoding to overcome the low frequency bias of these types of architectures ([18]). the proposed framework,
that we refer to as pinnslope, not only allows to interpolate seismic data, but it can also be used to estimate slopes
from fully sampled data of quality comparable to those of the pwd filters. in the context of sparsely sampled data,
the procedure is advantageous because we can directly estimate the slopes during the interpolation of the recorded
wavefield (i.e. using the aliased data), without the need of low-pass filtering the data to obtain an alias-free version on
which to perform a reliable slope estimation as in our previous work. the inversion result of the slope network turn
outs to be smooth, but a more accurate version than the one estimated from the low frequency data by means of pwd
filters. thanks to that the physical constrain the network is very efficient in steering the interpolation process towards
an accurate physically driven solution.
to summarize, our main contributions comprise:
1. a novel machine learning framework for slope assisted seismic data interpolation.

2. an innovative procedure for local slope attribute estimation by the mean of physics informed neural networks.

3. successful application of the framework on field data.
the paper is organized as follows. first, we present the theoretical background of our methodology. we then describe
the network architecture, focusing on some of the key implementation details needed to achieve a stable training process.
finally, our method is applied to a range of synthetic and field data. its reconstruction capabilities are compared to
those of a classic least-squares inversion regularized by the discretised local plane-wave equation (pwls), as well as to
our previous published pwd-pinn approach which uses pre-computed local slopes.
2

2

theory

2.1

problem statement

the objective of this paper is to formulate the problem of seismic data interpolation within the framework of physics
informed neural networks.
to begin with letâ€™s define the basic mathematical model used to obtain a decimated version of the original seismic data:
a restriction operator r, is defined such that it samples the columns of the data matrix u = [ut1 , ut2 , ..., utn ] at desired
locations (where n is the number of total traces in the dataset), removing the missing traces that we wish to interpolate.
in matrix notation the operation of subsampling a gather of traces can be written as:
d = ru

(1)

where d is the subsampled data with either missing traces at a regular interval to simulate spatial aliasing, or missing a
large number of consecutive traces to simulate a gap in the acquisition geometry.
2.2

slope estimation with plane-wave destructors

the physical model used to express seismic data as local plane-waves is represented by the local plane-wave differential
equation:
âˆ‚u(t, x)
âˆ‚u(t, x)
+ Ïƒ(t, x)
= r(t, x) â‰ˆ 0,
âˆ‚x
âˆ‚t

(2)

where u(t, x) is the pressure wavefield, r(t, x) is the pde residual, and the parameter Ïƒ(t, x) is the local slope (or
wavenumber) with units equal to the inverse of the velocity of propagation. an analytical expression exists for the
solution of equation 2 in case of a constant slope, which is simply represented by a plane-wave
u(t, x) = f (t âˆ’ Ïƒx),

(3)

where f (t) is an arbitrary waveform at x = 0. we can see that the left hand side of equation 2 decreases as the
observation u(t, x) matches the wave displacement u(t âˆ’ Ïƒx) ([22]).
in our work, we are interested in computing a slope varying both in time and space, but no analytical solution exists for
such a case. hence, claerbout (1992) casts the dip estimation as a linear least-squares problem, through an operation
named plane-wave destruction. in this approach, the curvature of the events is linearly approximated by computing the
slope in a small window of the entire data. the slope is estimated through equation 2, minimizing the quadratic residual:
q(Ïƒ) = (ux + Ïƒut ) Â· (ux + Ïƒut ),

(4)

where ux and ut are respectively defined as the spatial and temporal derivatives of the wavefield u. setting the derivative
of q(Ïƒ) to zero, we can find its minimum as:
Ïƒ=âˆ’

ux Â· ut
.
ut Â· ut

(5)

fomel (2002), on the other hand, frames the slope estimation as a non-linear least-squares problem computing the
slope globally on the entire data by the means of a plane-wave destruction filters. given a wavefield u as a gather of
seismic traces u = [u1 , u2 , ..., un ]t , the destruction operator predicts each trace from the previous one by shifting the
observed trace along the dominant local slopes of the seismic data and subtracts the prediction from the original one.
the phase-shift operation on the traces is approximated by an all-pass digital filter (or prediction filter in 2d). the filter
coefficients are determined by fitting the filter frequency response (at low frequencies) to the response of the phase-shift
operator. in this implementation, the slope (Ïƒ) enters in the filters coefficients in a non linear way. to characterize
and describe the entire gather, the prediction of several plane waves is needed (and not only one). this is achieved by
cascading various filters of the above mentioned form. the filter is applied to the data u as a convolutional operator
d(Ïƒ). in matrix notation, the slope estimation problem can be written as
d(Ïƒ)u = r,
3

(6)

where r is the residual. this non-linear least-squares problem is solved via gauss-newton iterations, which implies
solving
dâ€² (Ïƒ0 )âˆ†Ïƒu + d(Ïƒ0 )u = r,

(7)

where dâ€² (Ïƒ0 ) is the derivative of the filter coefficients d(Ïƒ) with respect to Ïƒ. the minimization problem is solved for
the update âˆ†Ïƒ, which is repeatedly added (at every iteration) for an initial guess Ïƒ0 . the problem can be regularized by
adding an appropriate penalty term that avoids oscillatory solutions of the slope attribute.
2.3

plane-wave regularized least-squares interpolation

in this section, we describe a conventional approach to take into account pre-computed slopes whilst interpolating
seismic data (i.e., restoring missing traces). this method will be later used as a benchmark for our pinns approach.
the inverse problem is cast as follows: finding the shot-gather u (i.e., the full gather of traces) that minimizes the
euclidean distance between the subsampled data d and the estimated subsampled data ru, whilst at the same time
satisfying the plane-wave differential equation with pre-computed slopes. the objective function is formally defined as
f (u) = âˆ¥d âˆ’ ruâˆ¥22 + Ïµr âˆ¥ux + Ïƒut âˆ¥22

(8)

where ux and ut are respectively defined as the spatial and temporal derivatives of the data u, Ïƒ is a diagonal matrix
that applies element-wise multiplication of the pre-computed local slope and Ïµr is a weight to control the contribution of
the pde in the solution. the data term of the objective function aims at accurately reproducing the available traces from
the estimated full shot-gather u subsampled by the restriction operator r. this means that all the interpolation operation
between the traces is performed by the local plane-wave regularization term. in other words, the regularization term has
the function of filling the gaps between the subsampled traces, spraying the information available from two neighboring
traces along the curvature of the provided local slope field.
2.4

physics informed neural networks

in this section, we aim to show that starting from a knowledge of a slope field, estimated via pwd filters or any other
algorithm, the problem of seismic interpolation can be formulated within the pinns framework.
pinns have been designed to blend the universal function approximator capabilities of neural networks ([23]) with a
physical constraint given by a pde, which describes the physical system under study. in our specific case, the pde that
we seek to satisfy is the local plane-wave differential equation (equation 2).
a neural network Ï•Î¸ (t, x) is designed to approximate the function u(t, x), where Î¸ refers to the weights (and biases) to
be optimized and the pair (t, x) represents the input to the network. the network predicts the recorded wavefield u(t, x)
at the corresponding location in the time-space domain of interest. a remarkable convenience of pinns is that in
contrast to traditional numerical methods, they do not require a discretization of the computational domain. the partial
derivatives of the underlying pdes are computed by the means of automatic differentiation (ad), which is a general and
efficient way to compute derivatives based on the chain rule. ad is usually implemented in neural networks training to
compute the derivatives of the loss function with respect to the parameters of the network. however, ad can be more
broadly applied to every computational program that performs simple arithmetic operations and calculates elementary
functions (linear transformations and non-linear activation functions in the case of neural networks) by keeping track
of the operations dependencies via a computational graph and successively computing their derivatives using the
chain rule. the pinn framework is trained in an unsupervised manner, using a loss function which includes both the
local plane differential equation and a set of nt (number of traces in the subsampled gather) boundary conditions
corresponding to the available traces
n

u
1 x
l=
nu i=0



âˆ‚Ï•Î¸ (ti , xi )
âˆ‚Ï•Î¸ (ti , xi )
+ Ïƒ(ti , xi )
âˆ‚x
âˆ‚t

2

ï£«

ï£¶
nt
x
1
+ Î»ï£­
|u(tj , xj ) âˆ’ Ï•Î¸ (tj , xj )|ï£¸ ,
nt j=0

(9)

where (ti , xi ) are points randomly sampled from the input space with nu as number of total grid points, u(tj , xj ) is the
known solution regions at points indexed by j (available traces), and Î» is a scalar weight for the second term.
in this first approach named pwd-pinn (1), the slope Ïƒ(ti , xi ) is pre-computed and it is estimated by means of pwd
filters, outside the training process of the network. the slope array remains fixed during training and it is not updated.
4

figure 1: a diagram explaining the pwd-pinn algorithm. the network is trained while maintaining the slope array
fixed during training.

2.5

simultaneous data interpolation and slope estimation

in this section, we introduce the slope estimation framework using physics informed neural networks, named pinnslope.
we propose to estimate the local slopes while at the same time interpolating the aliased data (or any other type of
interpolation task). specifically, we simultaneously train two neural networks to predict the data and the local slopes
that satisfy the plane-wave pde. this approach bears similarity with previous works by [12] and [24], in the context of
traveltime tomography.

figure 2: double network scheme for joint wavefield and slope estimation in the same training procedure.
as shown in the diagram of fig.2, both networks have fully-connected architectures and utilize t anh(Â·) activation
functions; moreover, a positional encoding is added to the wavefield network to tackle multi-scale signals (i.e., signal
exhibiting both low and high frequency components) such as our seismic traces. the two networks also differ in the
number and size of the layers: the wavefield network aims at reconstructing the shot gather data and requires a much
larger number of degrees of freedom to fit the complexity of seismic signals (i.e., the seismic traces u(tj , xj )); rather
for the slope, as we aim to obtain a smooth solution, this can be achieved by using more compact architecture. after
computing the loss function, both networks are simultaneously updated. two separate adam optimizers are utilized to
allow two different learning rate values if necessary.
2.6

positional encoding

during the numerical experiments both frameworks struggled in fitting signals with high frequency content. in our
previous experiments, the low frequency bias of neural networks ([18]) was addressed with frequency upscaling by the
5

mean of neuron splitting ([25]) and with locally adaptive activation function ([26]). here, the low frequency bias of
multi-layer perceptrons (mlps) is tackled by including positional encoding of spatial coordinates ([16]).
differently from the classical transformer approach to positional encoding, where it is used to track the token positions,
in our application we use it to map the input coordinate grid into a higher dimensional space, which allows for a better
fit of high frequency signals. the approach implemented in this work resembles the one previously presented in [17]
and referred to as fourier feature mapping, where the authors utilized a linear sampling in the fourier space that enables
for a large amount of frequency components in the low-frequency regions. this modification is fundamental also for our
implementation as other forms of encodings were introducing noise into the reconstruction, as the higher frequencies
present in the encodings were able to fit the noise in the traces. additionally, [20] proposed an anisotropic version
of positional encoding justified by the idea that seismic data components present different features and should not be
equally encoded. the formulation utilized here can be summarized as follows:
Î³x (x) = [cos(k0 x), sin(k0 x), ..., cos(kxâˆ’1 x), sin(kxâˆ’1 x)],

(10)

where kx = Ï€x
2 with x = 0, ..., x âˆ’ 1, âˆˆ n, is a simple linear sampling, and x represents the number of encoded
frequencies in the x coordinate. the t coordinate is encoded in the same way with the number of frequencies equal to t
âˆˆ n, and the encoded coordinates are subsequently concatenated together as
Î³x,t (x, t) = [Î³x (x), Î³t (t)].

(11)

the positional encoding operation has been embedded as a network layer inside the architecture of the data network,
and the number of frequencies corresponding to each coordinate is decided through trial and errors.

3

numerical experiments

in this section, the proposed methodology is tested on synthetic and field data. for both the pwdpinn and pinnslope
approaches, a feed-forward neural network architecture with 4 layers and a t anh(Â·) activation function was utilized.
we set the number of neurons to be the same for all layers in all our experiments but this number can vary in different
experiments as we specify it in the subsections. in both frameworks, the networks are trained in an unsupervised
manner, passing as input an ensemble of (x, t) points. the ensemble is passed to networks in batches of 1000 randomly
sampled points. for every batch, the ensemble of the collocation points is concatenated to an array containing half of
the points (xj , tj ) associated with the available traces to be fitted. all networks in every experiments are trained using
adam optimizer, with learning rate fixed at 10âˆ’3 . these parameters are chosen based on some initial tests and kept
fixed throughout the study.

3.1

synthetic data examples

3.1.1

local slope estimation

in this first example, we estimate the slope with the pwd algorithm and with the pinnslope framework, to compare
their performance. the synthetic seismic image (sigmoid model, [1]) is assumed to be fully sampled and all the traces
have been utilized in the training process.

200

0.50

300

0.25

400

0.00

500

âˆ’0.25

600

âˆ’0.50

700

âˆ’0.75
50

100
150
offset (m)

200

âˆ’1.00

0

1.0

1.0

0.5

0.5

0.0

50

100
150
offset (m)

200

0.0

âˆ’0.5

âˆ’0.5

âˆ’1.0

âˆ’1.0

âˆ’1.5

âˆ’1.5

0

50

100
150
offset (m)

d) difference

1.5

200

1.00
0.75
0.50
0.25

twt (ms)

0.75

c) pinn estimated sl pe

1.5

twt (ms)

twt (ms)

100

0

b) pwd estimated sl pe

1.00

twt (ms)

a) sigm id m del

0

0.00
âˆ’0.25
âˆ’0.50
âˆ’0.75

0

50

100
150
offset (m)

200

âˆ’1.00

figure 3: slope comparison between pwd algorithm. a) seismic image, b) local slope estimate from plane-wave
destruction filters [2], c) local slope estimate obtained with the pinnslope approach, d) difference between the pwd
and pinnslope estimated slopes.
6

as shown in fig.3, the pinnslope framework can accurately estimate the local slope of complex subsurface geometries,
and it results in a slightly smoother version with less artefacts near the major fault compared to the local slope estimated
via the pwd algorithm.
3.1.2

interpolation beyond aliasing with local slope estimation

the goal of this second example is to reduce the spatial aliasing present in the recorded data by interpolating the missing
traces. the synthetic data in fig.4a have a trace spacing of 10 meters and have been subsampled by a factor of 5 through
the operator r, to obtain the aliased version in fig.4b.

a) da(a

200

c) low-pass fi (ered da(a

0.020

0.75

0.015

0.50

0.010

500

0.25

0.005

600

0.00

0.000

700

âˆ’0.25

âˆ’0.005

800

âˆ’0.50

âˆ’0.010

900

âˆ’0.75

âˆ’0.015

300
400
twt (ms)

b) s)bsamp ed da(a

1.00

1000
0

0

200 400 600 800 1000
offset (m)

d) fft of da(a

âˆ’1.00
300

10

0

200 400 600 800 1000
offse( (m)
e) fft of s)bsamp ed data

0

300

150

40
50

100

60

50

70
âˆ’0.04 âˆ’0.02 0.00 0.02
wavenumber (mâˆ’1)

0.04

0

0.04

0.6

0.4

500

0.3

600
700

0.2

800

0.1

900
âˆ’0.04 âˆ’0.02 0.00 0.02
wavenumber (mâˆ’1)

âˆ’0.020

0.5

400

200
twt (ms)

freq)ency (hz)

20
30

f) pwd es(ima(ed s ope

200

250

200 400 600 800 1000
offse( (m)

1000

0.0
0

200

400 600 800 1000
offset (m)

figure 4: a) original seismic data, b) seismic data with missing traces, c) low frequency data from which the pwd
slope has been computed, d) f âˆ’ k spectrum of the data, e) f âˆ’ k spectrum of the subsampled data, f) pwd estimated
slope from the low frequency data in fig.4c.
it is not possible to apply the pwd filter directly to estimate the slope from the subsampled data, since it can lead to
erroneous estimates by picking the aliased dips instead of the true ones of the fully sampled data [2].
to avoid this issue, the following pre-processing steps have been performed:
1. apply f âˆ’ k filter to the spectrum of the aliased data.
2. inverse transform the filtered spectrum to get a low frequency alias-free version of the data.
3. apply the pwd filters algorithm to the low frequency data and estimate the slope.
4. utilize the pwd estimated slope from low frequency data inside the pinn loss function.
the network capacity corresponds to 4 layers with 512 neurons each, with the number of encoding frequencies set to
x = 8 and t = 32 for the x and t coordinates respectively. the network is trained using the loss function in eq.9 with
the parameter Î» set to 1000 and Ïƒ correspond to the pwd estimated slope displayed in fig. 4c.
fig.5 compares the results obtained with the different approaches. the output of the regularized least-squares inversion
in fig.5a demonstrates the importance of the plane-wave penalty term, which helps in filling the gap between the
7

) pwls (n3ers(on, snr=11.137

b) pwd-pinn, snr=6.471

c) pinn0),-e, snr=20.681

200
300

twt (ms)

400
500
600
700
800
900
1000

0

200

400

600
offse1 (m)

800

1000

d) d(fference, mse=0.004

0

200

400

600
800
off0e1 (m)

1000

e) d(fference, mse=0.012

0

200

400

600
800
off0e1 (m)

1000

f) d(fference, mse=0.00044

200
300

twt (m0)

400
500
600
700
800
900
1000

0

h) fft of pwd

g) fft of pwls (n3er0(on

() fft of pinns)oâˆ’e

-pinn

10

freq2enc5 (h6)

20
30
40
50
60
70

âˆ’0.04

âˆ’0.02
0.00
0.02
4aven2mber (mâˆ’1)

0.04

âˆ’0.04

âˆ’0.02
0.00
0.02
4aven2mber (mâˆ’1)

0.04

âˆ’0.04

âˆ’0.02
0.00
0.02
4aven2mber (mâˆ’1)

0.04

figure 5: a) plane-wave regularized least-squares inversion interpolation result, b) pwd-pinn interpolation result,
c) pinnslope interpolation result, d) difference between fig.4a and fig.5a , e) difference between fig.4a and fig.5b, f)
difference between fig.4a and fig.5c, g) f âˆ’ k spectrum of fig.5a, h) f âˆ’ k spectrum of fig.5b, i) f âˆ’ k spectrum of
fig.5c.

available traces following the correct overall geometry of the arrivals. unfortunately, as soon as the reflections start
bending their resolution decrease, worsening towards the far-offset. in this interpolation attempt the sharp and definite
seismic response that characterize this simple synthetic data is slightly spread in a fuzzy pattern, a sign that the algorithm
cannot properly restore the energy in the correct position. the difference with the original data in fig.5d shows the
amount of energy lost, as well as some artifacts. the achieved result is almost perfect where the arrivals are generally
linear. anyhow, ls inversion is almost instantaneous compared to the neural networks approach. the result displayed
in fig.5b requires a runtime of approximately 26 minutes for 2000 epochs as shown in the plot of the loss curves in
fig.6a.
as in the previous result, the quality of the interpolation of the pwd-pinn algorithm decreases in the far-offset,
although only for the first few reflections. this is a limitation of the algorithms that leverage on the pwd estimated
slope, which is inaccurate at the far-offsets where the events are steeper; the slope estimated via pwd inherently
contains errors because of the procedure through which it has been computed, but even more it has been estimated from
a low frequency version of the original data. despite the poor interpolation of the above mentioned arrivals, all the
others look adequately restored. most of the energy is in the correct position as we can see from its spectrum in fig.5h.
8

a) losses pwd-pinn

b) losses pinnslope

160

log scale losses

log scale losses

1400

102

140

101

1200

100

120

10âˆ’1

1000

10âˆ’2

100

800

epochs

loss

loss

1600

total loss
data te m loss
physical te m loss

600

10âˆ’3

80

epochs

60

400

40

200

20

0

total loss
data te m loss
physical te m loss

0
0

250

500

750

1000
epochs

1250

1500

1750

2000

0

250

500

750

1000
epochs

1250

1500

1750

2000

figure 6: a) loss of the pwd-pinn training. b) loss resulting from the pinnslope training. in the small box inside
the two plots it is shown the contribution of each term of the loss function in log-scale, that is: data-term (in green),
physical-term (in blue) and total loss (in orange).

anyhow, in this result the resolution is lower, in fact the traces interpolated at the far-offsets include gaps. moreover, in
the first two events the amplitude is not properly reproduced.
the best reconstruction is clearly given by the pinnslope framework. the architecture of the network has the same
capacity then the one of the pwd-pinn algorithm and the loss function in fig.6b shows that it has been trained for the
same amount of epochs as pwd-pinn. the key difference in the result is made by the second smaller network that
approximates the local slope function. the slope estimate is carried on simultaneously with the interpolation performed
by the bigger data network on the original shot gather (fig. 4a), no filtering required. this simultaneous updating
process of data and slope allows for larger search space to speed up convergence to the accurate data. as can see from
fig.7, the pinn estimated slope closely matches the accurate pwd slope computed from the data of figure 4a (ignoring
the right-upper part where there are no arrivals and the two approaches clearly extrapolate the values).
a) pwd e()ima)ed (lope ( rom ull da)a) 1.6

b) pinn(lope e()ima)ed (lope

200

500

1.4

0.75

1.2

1.2

0.50

1.0

1.0

0.25

0.8

600

0.8

twt (m()

twt (m()

400

1.00

1.4

twt (m()

300

c) di erence, mse=0.030

1.6

0.00

700

0.6

0.6

âˆ’0.25

800

0.4

0.4

âˆ’0.50

900

0.2

0.2

âˆ’0.75

1000

0

200

400

600 800
offset (m)

1000

0.0

0

200

400

600 800
o (e) (m)

1000

0.0

0

200

400

600 800
o (e) (m)

1000

âˆ’1.00

figure 7: a) accurate slope estimated with pwd algorithm from the data shown in 4a. b)slope estimate through the
pinnslope framework while simultaneously interpolating the data in fig.5c, c) difference between fig.7a and fig.7b.

3.2
3.2.1

field data examples
interpolation beyond aliasing with local slope estimation

the numerical examples below are performed on a field dataset from the gulf of mexico. here, only the results for
the pinnslope approach and pwd-pinn will be compared as we want to focus on the pinn approaches. the trace
spacing in the original shot-gather of fig.8a is 26.7 meters and we subsampled it by a factor of 5, increasing the spacing
between the traces to 133.5 meters (fig.8b).
9

a) data
2000

c) l (-pass filtered data
0.004

0.15
0.10

2500
twt (ms)

b) subsampled data

0.20

0.002

0.05
3000

0.00

0.000

âˆ’0.05

3500

âˆ’0.002

âˆ’0.10
4000

âˆ’0.15

0

500 1000 1500 2000 2500
offset (m)

d) fft f data

10

300

0

500 1000 1500 2000 2500
offset (m)

0

e) fft f subsampled data

500 1000 1500 2000 2500
offset (m)

f) pwd estimated sl pe
2000

0.4

2500

0.3

3000

0.2

100

3500

0.1

50

4000

0.0

250

20
frequenc) (hz)

âˆ’0.20

200

30

twt (ms)

0

âˆ’0.004

150

40
50
60
âˆ’0.01
0.00
0.01
wavenumber (mâˆ’1)

0

âˆ’0.01
0.00
0.01
(avenumber (mâˆ’1)

0

500 1000 1500 2000 2500
offset (m)

figure 8: a) the original seismic data, b) subsampled seismic data, c) low frequency data from which the pwd slope
has been computed, d) f âˆ’ k spectrum of the original data, e) f âˆ’ k spectrum of the subsampled data, f) pwd estimated
slope from the low frequency data in fig.8c.

as mentioned earlier, computing directly the slope from the subsampled gather is not feasible and some pre-processing
steps are required. filtering out the aliased part of the field data in fig.8e is far more challenging then for the on synthetic
ones; the part of the signal that is not aliased is very small and does not contain significant energy. the retrieved low
frequency data (fig.8c) are fed into the pwd algorithm and, due to the low frequency nature of the data, the resulting
local slope is a low resolution rough estimate of the slope of the high-resolution data (fig.8f). the network has an
architecture of 4 layers and 512 neurons in each layer. it is equal to the one used for synthetic data. that is because
even if the traces are way more complex in the field data, the synthetic traces have an amplitude that does not decrease
as much in time. from our initial tests the network requires the same capacity to easily fit the strong oscillations of the
synthetic signals. the slope network (as in the synthetic case) has 2 layers with 2 neurons to estimate a smooth version
of the slope field.
the pdw-pinn algorithm (fig.9a) does not achieve a good result. only the interpolation in the near-offset could be
consider reasonable. in the far-offsets the reconstruction is worse as we already observed in the synthetic data. in this
part of the dataset, the pwd estimated slope is prone to errors and it does not allow a good interpolation. the loss
curves (fig.10a) show that we had to increase the Î» parameter to very high values (Î» = 10000) to make the network
properly fit the traces. the network struggles to accurately fit the traces if the accuracy of the local slope is poor, as this
will negatively effect the pde term of the loss function. as a result, increasing the weight on the data fitting term
rendered the pwd-pinn algorithm to be a data fitting algorithm.
in contrast, the pinnslope approach admits a good performance. it reproduces the original dataset; most of the energy
has been restored and the aliasing has been suppressed. the extra degrees of freedom provided by the small slope
network helped the convergence.
10

a) pwd-pinn, snr=1.458

b) difference, mse=0.008

0

c) fft of pwd

-pinn

2000
10
20
fre)âˆ’enc0 (h1)

twt (ms)

2500
3000
3500

30
40
50

4000

60
0

500

1000 1500
offset (m)

2000

2500

0

, snr=6.888

500

1000 1500
offse, (m)

2000

2500

âˆ’0.01
0.00
0.01
wa.enâˆ’mber (mâˆ’1)

e) difference, mse=0.002

) pinnslo(e

0

f) fft of pinnslo(e

2000

10
20
fre)âˆ’enc0 (h1)

twt (ms)

2500

3000

30
40

3500

50
4000

60
0

500

1000

1500

2000

offset (m)

2500

0

500

1000 1500
offse, (m)

2000

2500

âˆ’0.01
0.00
0.01
wa.enâˆ’mber (mâˆ’1)

figure 9: a) pwd-pinn interpolation result, b) difference between the original full data (fig.8a) and pwd-pinn result,
c) f âˆ’ k spectrum of pwd-pinn result, d) pinnslope interpolation result, e) difference between the original full data
(fig.8a) and pinnslope result, f) f âˆ’ k spectrum of pinnslope result.
in fig.11, the pinn estimated slope is compared to the pwd slope computed from the full data (fig.8a). the pinn
slope is smoother than the pwd one and again admits generally lower values, probably because it has been estimate on
less dense data. however, the overall trend of the pinnslope slope is correct and its smoothness serves to its purpose in
the plane-wave regularization term. if compared to the pwd slope in fig.8f, which is the realistically achievable slope
when we try to solve an interpolation problem of this kind, the pinn slope is a way better and more precise estimate.
the residuals shown in fig.9f and fig.9e, differently from the synthetic data, are partially due to the field data containing
secondary events with conflicting dips that cannot be recovered by our method. we note that this is a general weakness
of interpolation methods relying on the plane-wave pde.
3.2.2

performance assessment

in this section, the pinnslope framework is tasked to a harder interpolation where fewer traces are available. the aim
is to assess its performance on the current dataset and evaluate its limits. moreover, as subsequent shot gathers in the
dataset will have only minor changes between them, we test the converge behaviour of the pre-trained network when
applied to the next gathers is the data. we first apply pinnslope on the shot-gather subsampled by various factors:
6, 7 and 8 (respectively, 160.2 meters, 186.9 meters, 213.6 meters intervals between each traces). finally, we also
test the ability of our framework to interpolate a dataset with a large gap of traces (i.e. 15 traces, for a total of 400.5
11

a) losses pwd-pinn
70000

7

102
100

50000

10âˆ’1

6

40000

epochs

loss

10âˆ’2
total loss
data te m loss
physical te m loss

30000

log scale losses

101

104

60000

loss

b) losses pinnslope

8

log scale losses

10âˆ’3

5

epochs

total loss
data te m loss
physical te m loss

4

20000
3

10000

2

0
0

250

500

750

1000
epochs

1250

1500

1750

2000

0

250

500

750

1000
epochs

1250

1500

1750

2000

figure 10: a) loss curves of the pwd-pinn training on real data. b) loss curves of the pinnslope training on real
data. in the small box inside the two plots we show the contribution of each term of the loss function in log-scale, that
is: data-term (in green), physical-term (in blue) and total loss (in orange).
a) pwd esti ated slope (fro full data) 0.5

b) pinnslope esti ated slope

c) difference, mse=0.009

0.5

0.3

2000
0.4

0.4

0.3

0.3

0.2

2500

0.2

3500

0.2

twt ( s)

3000

twt ( s)

twt ( s)

0.1
0.0
âˆ’0.1
0.1

4000

0

500

1000 1500
offset ( )

2000

2500

0.0

0.1

0

500

1000 1500
offset ( )

2000

2500

0.0

âˆ’0.2

0

500

1000 1500
offset ( )

2000

2500

âˆ’0.3

figure 11: a) the slope estimated with pwd algorithm from the data shown in 8a. b) the slope estimate through
the pinnslope framework while simultaneously interpolating the data in fig.9e, c) the difference between fig.11a and
fig.11b.

meters gap) placed in the middle of the gather. this a very hard task for the network, which has to rely solely on the
information obtainable from the left and right side of the gather, as in the gap region, there is no knowledge of the shape
of the arrivals nor no that of their slope field. so far, we are not aware of any interpolation algorithm that can solve this
category of interpolation tasks in an automatic and physically driven manner.
the performance results are shown in fig.12. for this shot gather, a higher subsampling of 6 and 7 (respectively fig.12a
and fig.12b) does not impact the network performance. the signal to noise ratio is maintained almost constant from the
result described in the previous section and the arrivals are perfectly interpolated. the framework start to face some
challenge when the gather is subsampled by a factor of 8 (213.6 meters interval between the traces). in the near-offset
the interpolation is still accurate but as the dip of the arrival starts to increase, pinnslope is unable to retrieve the
correct slopes and struggles in interpolating the arrivals. we consider this sampling to be the threshold limit for the
framework for this gather (and the freqeuncy range involved). the presented result are obtained with an increasing the
number of epochs as the subsampling increases (1500 epochs for a subsampling of 6, and 2500 for a subsampling
of 8). in the case of the higher subsampling factor, it was additionally needed a Î» value of 10000 instead of the usual 100.
by enlarging fig.12 at early times (fig.13) we can see the main difference between the original data and the pinnslope
interpolation. in fact, most of the events are well reproduced, as what is missing is the energy corresponding to the
often weaker events with conflicting; as such the interpolated data looks cleaner than the original one. as mentioned
before most algorithms that rely on slope estimation to perform their tasks cannot leverage on the energy of the second
12

a) pinnslope - sub=6

b) pinnslope - sub=7

c) pinnslope - sub=8

d) pinnslope - gap=15 traces

snr=6.218

snr=5.603

snr=3.096

snr=11.597

2000

s)

2500

twt (

3000

3500

4000

0

500

1000

1500

offset (

2000

2500

0

500

)

1000

1500

offset (

2000

2500

0

500

)

1000

1500

offset (

2000

2500

)

0

500

1000

1500

offset (

2000

2500

)

figure 12: the performance of pinnslope with high subsampling rates and a particular case where an entire part
of the gather is missing: a) the result obtained starting from data subsampled by 6, b) result obtained starting from
data subsampled by 7, c) result obtained starting from data subsampled by 8, d) result obtained starting from data that
contains a large gap of 15 traces (white hatched lines delimit the gap area).

b) pinnslope - sub=7
snr=5.603

a) data

2000

c) pinnslope - gap=15 traces
snr=11.597

2200

twt ( s)

2400
2600
2800
3000
3200
3400

600

800 1000 1200 1400 1600 1800 2000
offset ( )

600

800 1000 1200 1400 1600 1800 2000
offset ( )

600

800 1000 1200 1400 1600 1800 2000
offset ( )

figure 13: zoom in of some of the results in fig.12. a) zoom in of the original data, b) zoom in of the result obtained
starting from a data subsampled by a factor of 7, c) zoom in of the result starting from a data that contains a large gap
of 15 traces (white hatched lines delimit the gap area).

order dips for its reconstruction unless some additional slopes are included in the process.
the result of the gap interpolation (fig.12d and fig.13c) is remarkable. the pinnslope framework is able to extrapolate
the main reflection from the left and right and connect them together. of course we are aware of the high errors in
the middle of the gap compared it to the same part in the original data. the algorithm cannot completely restore the
missing part, as mentioned above it lacks information on the conflicting dips. is it worth mentioning that this experiment
has been done to show the interpolation capabilities of neural networks and especially of pinns. fig.14 shows the
convergence capabilities of the pinnslope network (pre-trained on the gather of fig.9a with a subsampling factor of 5)
on the subsequent gathers of the dataset (that have been subsampled by a factor of 5).
specifically, the gathers of fig.14e and fig.14f are consecutive to the one used as benchmark until now. instead, the ones
in fig.14g and fig.14h are way more distant. the idea is to use only one pre-training step (on the gather of fig.9a) and a
small fine-tuning step to make the network fit the subsequent gather. as we can see, the number of epochs needed for
fine-tuning the network depend on how different the gathers are from the one used for the pre-training. although, all
the gathers has been well reproduced with a small number of epochs.
13

a) csg #2
ep chs=50, snr=5.833

b) csg #3
ep chs=50, snr=6.65

c) csg #100
ep chs=100, snr=8.043

d) csg #150
ep chs=200, snr=5.265

e) csg #2

f) csg #3

g) csg #100

h) csg #150

2000

twt (ms)

2500
3000
3500
4000

2000

twt (ms)

2500
3000
3500
4000
0

500

1000 1500
offse# (m)

2000

2500

0

500

1000 1500
offse# (m)

2000

2500

0

500

1000 1500
offse# (m)

2000

2500

0

500

1000 1500
offse# (m)

2000

2500

figure 14: the results of fine-tuning pinnslope on different gathers from the one used to pre-train the network
(fig.9a). a) interpolated gather subsequent to the one in fig.9a, b) interpolated gather subsequent to the one in fig.14b, c)
interpolated gather at position 100 in the dataset, d) interpolated gather at position 150 in the dataset, e) original gather
at position 2, f) original gather at position 3, g) original gather at position 100, h) original gather at position 150.

4

discussion

out of all the implementations we tested, pinnslope achieves the best performance. it can interpolate the missing
data eliminating the alias present in the data whether they are related to coarse recordings or the presence of obstacles
during the acquisition. parameterising the plane-wave pde in the loss function with a small neural network does
not significantly affect the runtime with respect to pwd-pinn. for pwd-pinn and pwls inversion a number of
pre-processing steps should be performed before being able to compute the local slope to use in the pde regularization
term. this is also a time consuming part of these approaches. although, the pwls inversion is almost instantaneous
compared to the pinns.
all of the three methodologies present similar drawbacks due to the of the plane-wave approximation. in fact, the
estimated slopes with the pwd filters (or any other algorithm that can estimate slopes) and pinnslope are computed
with respect to the main events and they are not able to retrieve information for the conflicting slopes (slopes that have
opposite or different direction) in the data. the pinnslope framework cannot reproduce the events that have a reverse
slope with respect to the main trend of the arrivals. moreover, the estimate of the slope on a data point where two
events are crossing each other will always induce errors in the data fit. currently, it is not possible to compute the
value of two slopes on one single point of the dataset unless we include two slopes in the framework, which will be
investigated in the future.
pinnslope has the potential to fill a large gap of traces via a physically driven approach not achievable by any other
type of algorithm, furthermore estimating the full slope field. this example illustrates the interpolation capabilities of
this type of implementation. a substantial help to the network fitting ability is derived from the positional encoding
layer. in the past experiments and implementations various methodologies have been tested to make the network able to
fit complex signals as field seismic traces. locally-adaptive activation functions ([26]) has been implemented along with
a sin() activation function ([21]), to allow the network to be more expressive without the need to extend its capacity,
but still it was not enough. in addition, a technique named frequency upscaling by the mean of neuron splitting ([25])
have been tested achieving good results in fitting complex high frequency signals. one of its drawbacks is that it was
injecting low level noise in the reconstructed result and required to train various time the network to adequately fit the
entire frequency content of the dataset. on the other hand, positional encoding solved this issue allowing the network to
14

accurately reproduce the field data and achieve a faster convergence, without the need of training several times on the
same dataset (as frequency upscaling with neuron splitting).

5

conclusions

we introduced a novel pinn framework in the seismic signal processing field for simultaneous seismic data interpolation
and local slope estimation. it is discernible also as an innovative procedure for local slope attribute estimation of
complex subsurface images. the obtained results are compared on synthetics and field datasets against pwd-pinn,
which showed good results previously in the synthetic data experiments, and the plane-wave regularized least-square
inversion (for the synthetic example) to better examine the pinns performances against an approach that does not rely
on neural networks to optimize the same objective function.
we found that introducing a second network to estimate the local slope attribute while at the same time interpolating
the aliased data achieves better results in terms of signal to noise ratio, while also improving the overall network
convergence. the positional encoding layer was a fundamental addition to the architecture and helped overcome
previous difficulties such as high frequency fitting and noise introduction during the interpolation process. the pinn
estimated slopes look accurate and consistent with the interpolated data, their accuracy is comparable to the one
obtainable with the plane-wave destruction filters estimate.

acknowledgments
this publication is based on work supported by the king abdullah university of science and technology (kaust).
the authors thank the deepwave sponsors for supporting this research.

references
[1] j f claerbout. earth soundings analysis: processing versus inversion. blackwell scientific publications, 1992.
[2] s fomel. applications of plane-wave destruction filters. geophysics, 67:1946â€“1960, 2002.
[3] luis l. canales. random noise reduction, pages 525â€“527. 2005.
[4] sergey fomel. velocity-independent time-domain seismic imaging using local event slopes. geophysics, 72, 05
2007.
[5] sergey fomel and antoine guitton. regularizing seismic inverse problems by model reparameterization using
plane-wave construction. geophysics, 71, 09 2006.
[6] maziar raissi, paris perdikaris, and george em karniadakis. physics-informed neural networks: a deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equations. j. comput.
phys., 378:686â€“707, 2019.
[7] pu ren, chengping rao, hao sun, and yang liu. seismicnet: physics-informed neural networks for seismic wave
modeling in semi-infinite domain. 10 2022.
[8] ben moseley, a. markham, and tarje nissen-meyer. solving the wave equation with physics-informed deep
learning. arxiv: computational physics, 2020.
[9] tariq alkhalifah, chao song, umair bin waheed, and qi hao. wavefield solutions from machine learned functions
constrained by the helmholtz equation. artificial intelligence in geosciences, 2:11â€“19, 2021.
[10] chao song, tariq alkhalifah, and umair bin waheed. solving the frequency-domain acoustic vti wave equation
using physics-informed neural networks. geophysical journal international, 01 2021.
[11] t. konuk and j. shragge. physics-guided deep learning using fourier neural operators for solving the acoustic vti
wave equation. 2021(1):1â€“5, 2021.
[12] umair bin waheed, tariq alkhalifah, ehsan haghighat, chao song, and jean virieux. pinntomo: seismic
tomography using physics-informed neural networks, 2021.
[13] jonathan d. smith, kamyar azizzadenesheli, and zachary e. ross. eikonet: solving the eikonal equation with
deep neural networks. ieee transactions on geoscience and remote sensing, 59(12):10685â€“10696, 2021.
[14] yunpeng chen, sjoerd a. l. de ridder, sebastian rost, zhen guo, xiaoyang wu, and yongshun chen. eikonal tomography with physics-informed neural networks: rayleigh wave phase velocity in the northeastern margin of the
tibetan plateau. geophysical research letters, 49(21):e2022gl099053, 2022. e2022gl099053 2022gl099053.
15

[15] serafim grubas, anton duchkov, and georgy loginov. neural eikonal solver: improving accuracy of physicsinformed neural networks for solving eikonal equation in case of caustics. journal of computational physics,
474:111789, 2023.
[16] ben mildenhall, pratul p. srinivasan, matthew tancik, jonathan t. barron, ravi ramamoorthi, and ren ng. nerf:
representing scenes as neural radiance fields for view synthesis. commun. acm, 65(1):99â€“106, dec 2021.
[17] yu sun, jiaming liu, mingyang xie, brendt wohlberg, and ulugbek s. kamilov. coil: coordinate-based internal
learning for tomographic imaging. ieee transactions on computational imaging, 7:1400â€“1412, 2021.
[18] nasim rahaman, aristide baratin, devansh arpit, felix draxler, min lin, fred hamprecht, yoshua bengio, and
aaron courville. on the spectral bias of neural networks. in kamalika chaudhuri and ruslan salakhutdinov,
editors, proceedings of the 36th international conference on machine learning, volume 97 of proceedings of
machine learning research, pages 5301â€“5310. pmlr, 09â€“15 jun 2019.
[19] xinquan huang, tariq alkhalifah, and chao song. a modified physics-informed neural network with positional
encoding, pages 2480â€“2484. 2021.
[20] paul goyes, edwin vargas, claudia correa, yu sun, ulugbek kamilov, brendt wohlberg, and henry arguello.
coordinate-based seismic interpolation in irregular land survey: a deep internal learning approach, 2022.
[21] francesco brandolin, matteo ravasi, and tariq alkhalifah. pwd-pinn: slope-assisted seismic interpolation with
physics-informed neural networks. pages 2646â€“2650, 08 2022.
[22] j f claerbout and s fomel. geophysical image estimation by example. lulu.com, 2014.
[23] kurt hornik, maxwell stinchcombe, and halbert white. multilayer feedforward networks are universal approximators. neural networks, 2(5):359â€“366, 1989.
[24] mh taufik and t alkhalifah. a neural network travel time function for direct travel time tomography. in seismic
tomography: what comes next?, volume 2022. seismological society of america, 2022.
[25] xinquan huang and tariq alkhalifah. pinnup: robust neural network wavefield solutions using frequency
upscaling and neuron splitting. journal of geophysical research: solid earth, 127(6):e2021jb023703, 2022.
e2021jb023703 2021jb023703.
[26] ameya d. jagtap, kenji kawaguchi, and george em karniadakis. locally adaptive activation functions with
slope recovery for deep and physics-informed neural networks. proceedings of the royal society a: mathematical,
physical and engineering sciences, 476, 2020.

16



acta geophysica (2022) 70:593â€“607
https://doi.org/10.1007/s11600-022-00740-8

research article - applied geophysics

deepâ€‘neuralâ€‘networksâ€‘based approaches for biotâ€“squirt model
in rock physics
fansheng xiong1,2 Â· jiawei liu2,3,4

Â· zhenwei guo2,3,5 Â· jianxin liu2,3,5

received: 17 june 2021 / accepted: 22 january 2022 / published online: 10 february 2022
Â© the author(s) under exclusive licence to institute of geophysics, polish academy of sciences & polish academy of sciences 2022

abstract
a new cost-effective surrogate model using deep neural network (dnn) for seismic wave propagation in rocks saturated
with fluid is presented. in this field, the dispersion/attenuation analysis and wave-field simulation are two key measurements
which can be carried out by solving wave equations. the biotâ€“squirt (bisq) equation is a classical wave propagation model
in geophysical forward modeling and has been widely used. the solution of such equation, especially by numerical method,
is often complex and time-consuming. in this work, a dnn model is trained with the dataset of velocity and inverse quality
factor generated from bisq model. the results show that the relative mean square error between the predictions of dnn
model and that of bisq model on the test sets are all less than 3%. it indicates that the dnn model has learned the highdimensional space well and then can realize the dispersion/attenuation analysis for any given rock physical parameters.
besides, the other well-trained dnn model is used to obtain the simulation results with second-order accuracy according to
results by finite difference scheme with first-order accuracy. it reveals that the fast wave-field simulation can be implemented
once the results with lower accuracy are obtained.
keywords wave propagation model Â· deep neural network Â· training and test Â· dispersion and attenuation Â· numerical
simulation

edited by dr. liang xiao (associate editor).
* jiawei liu
jw-liu@csu.edu.cn
fansheng xiong
hxw1334@126.com
zhenwei guo
guozhenwei@csu.edu.cn
jianxin liu
ljx6666@126.com
1

institute of applied physics and computational
mathematics, beijing, china

2

key laboratory of metallogenic prediction of nonferrous
metals and geological environment monitoring (central
south university), ministry of education, changsha 410083,
china

3

school of geosciences and infoâ€‘physics, central south
university, changsha 410083, china

4

advanced institute for materials research, tohoku
university, sendai 980â€‘8577, japan

5

hunan key laboratory of nonferrous resources
and geological hazard exploration, changsha 410083,
china

introduction
in recent years, the demand for energy resources, such as oil and
gas, in various industries is increasing. in oil and gas deposit
research, we heavily rely on how to determine the distribution
locations and storage capacity of the underground reservoirs
(e.g., mÃ¼ller et al. 2010; mavko et al. 2020). it is well-known in
the seismic exploration that the dynamics of wave propagation
in porous media saturated with fluids is important to predict
the underground reservoir and attracts the interests of many
geoscientists (mÃ¼ller et al. 2010). furthermore, it is commonly
accepted that the biot and squirt-flow theories can be seen as
the two most important mechanisms to characterize the energy
dissipation which is caused by the wave-induced fluid flows
in fluid-saturated rocks (carcione 2007; mÃ¼ller et al. 2010).
biotâ€™s theory consists of the macroscopic rock properties and is
used to describe the macroscopic flow. the squirt-flow theory
is related to the squirting mechanism of the pore fluid out of
the cracks and based on the microscopic rock properties. obviously, these two mechanisms occur simultaneously and affect

13

vol.:(0123456789)

594

each other, as well as the energy dissipation during seismic
wave propagation.
dvorkin and nur (1993), dvorkin et al. (1995) developed a consistent model dealing the biot and squirt theories as a couple process. it is the widely used biot/squirt
(bisq) model. many investigators agree that this theory
is adequate to relate the dynamic poroelastic performance
of porous media containing fluids to the geophysical constants such as porosity, bulk modulus and fluid saturation
(yang and zhang 2002). it has served as rigorous and formal
foundations to interpret various wave phenomena. numerous efforts were made to study the solutions of the bisq
model. these theories mainly focus on the plane wave solutions which can analyze the wave dispersion and attenuation,
and the numerical results by numerical methods which can
simulate the wave propagation in rocks saturated with fluids.
the finite difference (fd) method is widely used to solve
wave equations in numerical simulation (e.g., carcione and
quiroga-goode 1995; carcione 2007; he et al. 2013). in
traditional, the numerical methods concern the accuracy of
the constructed schemes. more recently, it has been indicated that the stability analysis of wave propagation models
and the corresponding fd methods are also necessary to
the numerical simulations (liu and yong 2016; liu et al.
2020; xiong et al. 2020a, b). for this issue, liu et al. (2020)
constructed two finite difference schemes which are able
to offer the stable numerical simulations. on the one hand,
the plane wave solutions for bisq model can be degenerate
to find the roots of equations as long as the parameters of
porous media are determined. it also means that such solutions need to recalculate the roots once any change occurs
in the constant parameters, and thus it is lack of flexibility
in some way. on the other hand, the second-order scheme
can provide more accurate solutions but requires much more
cost in computation which is almost three times than that of
the first-order scheme. the motivation behind this work is to
overcome such inconvenience and costliness in calculation.
in view of this, it can be considered by using the deep learning technology which is very popular currently.
deep learning is a branch of machine learning and has
been widely explored in many different applications (apolinario-arzube et al. 2020; nosratabadi et al. 2020; pilz
et al. 2020). in the field of geophysics, machine learning
technology has been used to deal with problems extensively
and only a small part can be cited here. maiti et al. (2007)
employed neural network for lithological classification
and modeling according to well log data. pilz et al. (2020)
studied data-driven and machine learning identification of
seismic reference stations in europe. agarwal et al. (2020)
proposed a surrogate model based on machine learning to
study marsâ€™ thermal evolution. yang and ma (2019) applied
deep learning algorithm to geophysical inversion work, and
they built a fully convolutional neural network to seismic

13

acta geophysica (2022) 70:593â€“607

wave imaging, which established a new seismic velocity
model building method.
deep neural network (dnn) is a data-driven technology in deep learning and has been used extensively. dnn
has some outstanding advantages in the practical problems
of establishing surrogate model (mehta et al. 2019): it has
strong expression ability and outstanding ability to establish
(nonlinear) mapping relationship between data sets. in addition, its application in image processing shows its powerful
ability to process high-dimensional data sets. on the work in
rock physics, moseley et al. (2020) proposed the â€œwavenet
networkâ€ based on deep neural network to realize fast simulation of wave propagation. the classical acoustic equation is
used to generate data, and the results showed that the network
can achieve rapid simulation even in complex media through
the test of some special geological models. some researchers
had also tried to solve the classical wave equations by dnn
model (siahkoohi et al. 2019). in addition, a representative
work is that you et al. (2020) employed the dnns to study
shale anisotropy modeling. they constantly tune the hyperparameters to determine the optimal network construction.
the generation of datasets depends on hudson-cheng model
(cheng 1993). they formalized an inversion workflow to
estimate anisotropy from well logs and also compared the
results obtained by dnn and conventional inversion methods
(li et al. 2019). it has been showed that anisotropy parameters predicted by dnn model agree well with that of inversion results and laboratory measurements. however, as far as
we know, there is not much work in applying dnn to wave
propagation model with partial differential equations form.
this is partially caused by the solution of the equations is
difficult and the difficulty in obtaining actual data.
as a new model establishing method, this study uses
dnn to link the seismic attributes with basic rock physics parameters, which is a data-driven approach instead of
relying on mathematical and physical knowledge. relationships between different datasets are captured by dnn in
this study. also we aim to model the numerical results from
bisq model by the dispersion/attenuation analysis and finite
difference method accurately in a cost-effective manner
using dnns. we focus on the flexibility of the dnn model
applied in the high-dimensional space and the high efficiency when used in the wave-field simulation. similar work
can be applied to other classical wave propagation models.
in sect. 2, the classical bisq model and the corresponding
two solution methods are reviewed. with these, two kinds
of theoretical datasets can be produced based on the plane
wave analysis and numerical results from fd methods. furthermore, the process of the data preparation and the design
of dnn model based on tuning the related hyper-parameters
are also introduced with the detailed description. in sect. 3,
the qualitative and quantitative comparisons between the
dnn surrogate models and the original wave propagation

acta geophysica (2022) 70:593â€“607

model are given and discussed. firstly, an extensive dataset
that covers the physically reasonable range of each parameter is produced by the original bisq model based on plane
wave analysis. the accuracy of dnn model can be guaranteed from the error distribution of the network in test
set. further, several special test sets are used to show that
the dnn model can learn the high-dimensional parameter
space well and then obtain the great interpolation effect.
thus, it is convenient to get reasonable prediction by dnn
model according to any input data. moreover, two kinds of
finite difference schemes with different accuracy are used
to solve bisq model to get the datasets. the results show
that the wave-field simulation results with second-order
accuracy can be obtained by inputting the results with firstorder accuracy into the trained network, which can improve
efficiency and save time. it is demonstrated that the trained
dnn model can be a surrogate model of the original bisq
model. last comes the conclusions in sect. 4.

595
2 f

2 s

2 s

ğœ• u
ğœ• u
ğœ•p
ğœ• u
â§ (1 âˆ’ ğœ™)ğœŒs ğœ•t2 + ğœ™ğœŒf ğœ•t2 = m ğœ•h2 âˆ’ ğ›¼ ğœ•h ,
s âˆ’uf
2 us âˆ’uf
2
2
f
âª
ğœ•
ğœ•
u
)
(
) ğœ‚ğœ™ (
ğœ• u
= âˆ’ğœ™ ğœ•p
,
â¨ ğœ™ğœŒf ğœ•t2 âˆ’ï¿½ ğœŒa ğœ•t2 âˆ’ï¿½ï¿½ğœ…
ğœ•t
ï¿½ ğœ•h
f
2j1 (Î»r)
âª ğœ•p
ğœ•u
ğ›¼âˆ’ğœ™ ğœ•us
â© ğœ•t = âˆ’f 1 âˆ’ Î»rj0 (Î»r) ğœ•hğœ•t + ğœ™ ğœ•hğœ•t ,

(1)

where us and uf denote the displacements of the solid and
fluid, respectively, p is the fluid pressure, Ïs, Ïf and Ïa are the
respective solidâ€™s, fluidâ€™s and additional coupling densities, Î·
is the viscosity, Îº is the permeability, Ï• is the porosity, m is
the uniaxial modulus of the solid skeleton, Î± is a poroelastic
coefficient, and f is the biot-flow coefficient. in addition, h
denotes space, and t denotes time. the calculation of each
physical quantity is shown in the reference (dvorkin and nur
1993; dvorkin et al. 1995). j0 and j1 are the zero- and firstorder bessel functions, respectively, r is the characteristic
squirt-flow length, and Ï‰ is the frequency. moreover, the
intermediate quantity Î» in eq. (1) is taken as follows:

ğœŒf ğœ”2

ï¿½

âˆš
ğœŒa
ğœ‚ğœ™
+ âˆ’1
ğœ™ğœŒf
ğœ”ğœ…ğœŒf

ï¿½
(2)

method

Î»2 =

dispersion and attenuation analysis of the fast p wave characterized by velocity vp and inverse quality factor qâˆ’1 can be
carried out based on wave propagation models. the variation
of the two quantities with the basic parameters such as rock
porosity and fluid saturation can also be investigated. meanwhile, the process of wave propagation can be simulated by
solving the wave equation numerically. generally, the scheme
with high accuracy is more complex. many solutions for
wave equations have been constructed specifically to consider
the accuracy, while the stability of these schemes may not
be guaranteed. thus, it may provide unstable and expensive
simulation results when applied in the realistic cases.
among the well-established wave equations, bisq
model developed by dvorkin and nur [3,4] is widely used
and attracts many interests. the objective of this work is to
provide the dnn model to surrogate such a classical model
in two aspects. firstly, dispersion and attenuation prediction
can be directly realized by dnn model. then high precision simulation results can be obtained as the original model
solved with stable fd schemes to reduce the instability.

where Ïf1, Ïf2 and Î·1, Î·2 are the density and viscosity of
fluid 1 and 2, respectively. it should be noted that the basic
parameters are Ï‰, Ï•, ks, Î¼s, Ïs, sw and Îº once the type of
fluids are fixed in this model. specific calculation formula
and details can be seen in references (dvorkin and nur 1993;
dvorkin et al. 1995).
as mentioned above, a workflow is established to analyze
the wave dispersion and attenuation in geophysics, and the
fd schemes selected to solve eq. (1) in numerical. first, the
fourier transformation with respect to h and t is applied to get
the velocity vp and inverse quality factor qâˆ’1. according to
the principle of plane wave analysis, one can look for solutions
of wave equation in the following form:

bisq model and the solutions
the bisq model incorporated the biot and squirt-flow
mechanisms. firstly, based on the elasto-dynamic relationships derived by biot (1956a, b), rice and cleary (1976),
the motion equations which describe the deformation of the
solid and fluid are:

f

1+

ï¿½
ï¿½âˆš
â§ s
s
âˆ’1(kh âˆ’ ğœ”t)
âª u (h, t) = u0 exp
ï¿½âˆš
ï¿½
â¨ f
f
u
exp
t)
=
u
âˆ’1(kh
âˆ’
ğœ”t)
(h,
âª
0
â©

,

(3)

with the initial data us0 and u0 , and then the equation of the
complex wave number k can be derived from the original
equation system and in form represented as:
f

b 0 k 2 + b 1 k + b2 = 0

(4)

with the corresponding coefficients consisted of solidâ€™s and
fluidâ€™s parameters:

13

596

b0 =

b1 =

acta geophysica (2022) 70:593â€“607

ğœ™fsm
,
ğœ™ğœŒf
âˆš
ï¿½
ï¿½ ï¿½
ï¿½ï¿½
ï¿½
2
ğœŒa
(1âˆ’ğœ™)ğœŒ
âˆ’1ğœ‚ğœ™
fs 2ğ›¼ âˆ’ ğœ™ âˆ’ ğœŒ s âˆ’ m + fsğ›¼
+
1
+
ğœ™
ğœ™ğœŒ
ğœ”ğœ…ğœŒ
f

f

ğœ™ğœŒf
ï¿½
âˆš
ï¿½
ï¿½ï¿½
ğœŒa
(1 âˆ’ ğœ™)ğœŒs
(1 âˆ’ ğœ™)ğœŒs
âˆ’1ğœ‚ğœ™
+ 1+
+
.
b2 =
ğœ™ğœŒf
ğœ™ğœŒf
ğœ™ğœŒf
ğœ”ğœ…ğœŒf

f

,

ğ•t (d) = ğœ†d ğ•h (d),

the roots of eq. (4) provide the quasi-compressional
wave number k. in this way, vp and qâˆ’1 can be directly
obtained by:

vp =

ğœ”
,
re(k)

qâˆ’1 =

2im(k)
,
re(k)

(5)

where re(k) and im(k) denote the real and imaginary parts
of k, respectively.
in addition to the plane wave solutions, the numerical
simulation results from the fd methods are also the key
issue to solve the wave equations. the first- and secondorder fd schemes are constructed in our previous work
(liu et al. 2020). for this issue, liu et al. (2020), which
can minimize the instability in calculations, are employed
to numerically simulate the wave propagation in an isotropic porous medium.
by means of the operator splitting theory and the
upwind difference scheme, the first-order scheme for bisq
model can be represented as:
[
] (
)
(
)
Î´t ğš²+ ğ
n
n
Ì‚
ğ‹ ğ”j âˆ’ ğ”j =
ğ‹ ğ”nj+1 âˆ’ ğ”nj
Î´h ğ ğ
[
] (
)
Î´t ğ ğ
(6)
+
ğ‹ ğ”nj âˆ’ ğ”njâˆ’1 ,
Î´h ğ ğš²âˆ’
(
)âˆ’1
ğ”nj+1 = ğˆ âˆ’ Î´tğ€2 ğ”Ì‚ nj ,
ï¿½
ğ‹ğ€1 ğ‹âˆ’1 =

ï¿½
ğš²+ ğ
,
ğ ğš²âˆ’

0
0
m + ğ›¼fs(ğ›¼ âˆ’ ğœ™)âˆ•ğœ™ ğ›¼fs â¤
â¡
â¢
0
0
âˆ’fs(ğ›¼ âˆ’ ğœ™)âˆ•ğœ™ âˆ’fs â¥
ï¿½
ğ€1 = â¢ ï¿½
h ğœ™ğœŒf + ğœŒa
0
0 â¥
hğœ™2 ğœŒf
â¥
â¢
â£
hğœŒa
âˆ’h(1 âˆ’ ğœ™)ğœ™ğœŒs
0
0 â¦
0
0
â¡0 0
â¤
â¥
hğœ‚ğœ™2 â¢ 0 0
0
0
ğ€2 =
â¢
â¥,
0
0
âˆ’ğœ™ğœŒ
ğœ™ğœŒ
ğœ… â¢
fï¿½
f ï¿½ â¥
â£ 0 0 (1 âˆ’ ğœ™) ğœŒs âˆ’(1 âˆ’ ğœ™) ğœŒs â¦

where vs is the solidâ€™s velocity, vf is the fluidâ€™s
velocity. Ï„ is the total
stress
on )the bulk mate[
(
]âˆ’1
r i a l , (
,
h=
ğœ™ğœŒ
+
âˆ’
ğœ™)
(1
f ) ğœŒa ğœŒs + ğœ™ğœŒa ğœŒf
)
(
ğš²+ = diag ğœ†1 , ğœ†2 ğš²âˆ’ = diag ğœ†3 , ğœ†4 and ğœ†d (d = 1, 2, 3, 4)

13

d = 1, 2, 3, 4,

(7)

where ğ•(d) is the d-th element of the vector ğ• = ğ‹ğ” . for
any chosen d and the corresponding Î»d, ğ®nj = ğ•nj (d) is set. in
this way, the scheme for Î»d > 0 is:

)(
)]
(
j âˆ’ 1 âˆ’ j[ n
Î´t
n+1âˆ•2
n+1âˆ•2
ğ®j+1 âˆ’ 1 âˆ’
ğœ†d ğ®j+1 âˆ’ ğ®j
jâˆ’1
Î´h
(
)(
)]
j [ n
Î´t
n+1âˆ•2
n+1âˆ•2
+
ğœ†d ğ®j
âˆ’ ğ®jâˆ’1
ğ®jâˆ’1 + 1 +
,
jâˆ’1
Î´h

ğ®Ì‚ nj =

and the scheme for Î»d < 0 is:
)(
)]
(
jâˆ’j[ n
Î´t
n+1âˆ•2
n+1âˆ•2
ğ®j+1 âˆ’ 1 âˆ’
ğœ†d ğ®j+1 âˆ’ ğ®j
ğ®Ì‚ nj =
jâˆ’1
Î´h
(
)(
)]
jâˆ’1[ n
Î´t
n+1âˆ•2
n+1âˆ•2
+
ğ®jâˆ’1 + 1 +
ğœ†d ğ®j
âˆ’ ğ®jâˆ’1
,
jâˆ’1
Î´h
1
is a positive integer. with the calculated ğ•Ì‚ nj
where j= Î´h
consisted of ğ®Ì‚ nj , we take:
(
)
Ì‚ ğ•n+1 + ğ•n + Î´t ğ
Ì‚ n,
Ì‚ n + Î´t ğ
ğ„ğ•n+1
=
ğ„
ğ•
(8)
j
j
j
j
2
2 j

in which
[
Ì‚ n = Î´t
ğ
j
Î´h

] (
(
(
)
))
Ì‚ j âˆ’ 1 âˆ’ j ğ•n âˆ’ ğ•n + j
ğ•nj âˆ’ ğ•njâˆ’1
ğ
j
j+1
jâˆ’1
jâˆ’1
ğ ğ

ğš²+ ğ

[
Î´t
+
Î´h

] (
)
))
(
(
Ì‚ j âˆ’ j ğ•n âˆ’ ğ•n + j âˆ’ 1 ğ•n âˆ’ ğ•n
,
ğ
j
j
j+1
jâˆ’1
jâˆ’1
jâˆ’1
ğ ğš²âˆ’
ğ ğ

Ì‚ =ğ‹âˆ’t ğ€0 ğ€2 ğ‹âˆ’1 ,
ğ

with
ï¿½
ï¿½
ğ”= ğœ, p, vs , vf ,

are the eigenvalues of the matrix ğ€1, ğ is a constant 2 Ã— 2
matrix with the value of zero.
for simplicity, on the second-order scheme, the homogeneous partial differential equations derived from the operator
splitting theory can be rewritten as:

with

ğ„=ğ‹âˆ’t ğ€0 ğ‹âˆ’1 ,

ğœŒ = (1 âˆ’ ğœ™)ğœŒs + ğœ™ğœŒf ,

ğ›¼
1
â¡ mğœŒ
mğœŒ
â¢ ğ›¼ fsğ›¼2 +mğœ™
â¢ mğœŒ mfsğœŒ

ğ€0 = â¢
â¢ 0
â¢ 0
â£

0
0

â¤
â¥
0
0 â¥
âˆ’ğœŒa â¥.
(1âˆ’ğœ™)ğœŒs +ğœŒa
â¥
ğœŒ
ğœŒ
ğœ™ğœŒf +ğœŒa â¥
âˆ’ğœŒa
â¦
ğœŒ
ğœŒ
0

0

the boundary condition is taken as follows to maintain
the overall stability which is consistent with the scheme at
the interior points:
[
]
[
]
ğ•+ (t, 1)
ğ• (t, 1)
=ğŠ âˆ’
,
(9)
ğ•âˆ’ (t, 0)
ğ•+ (t, 0)

acta geophysica (2022) 70:593â€“607

597

where ğ•+ and ğ•âˆ’ are the elements of the vector ğ• with
respect to Î»d>0 and Î»d<0, ğŠ is a constant 4 Ã— 4 diagonal
matrix as long as the condition in theorem 3 provided by
jiawei liu et al. (2020) is satisfied.

â¡ ğœ”1 ğœ™1 â‹¯ ğœ…1 â¤
ï¿½
ï¿½ â¢ğœ” ğœ™ â‹¯ ğœ… â¥
2
âƒ— s , ğœ‡âƒ—s , ğœŒâƒ—s , sâƒ—w , ğœ…âƒ— = â¢ 2 2
âƒ— k
ğ— = ğœ”,
âƒ— ğœ™,
â¥.
â¢ â‹® â‹® â‹± â‹® â¥
â£ ğœ” m ğœ™ m â‹¯ ğœ…m â¦

data preparation and the dnn design

then, substitute each row of x into eqs. (1)â€“(6) to get the
velocity and the inverse quality factor, which forms the output
and also the label of neural network as follows:

for a physical model, the corresponding output(s) can be
obtained for a given set of parameters, during which the
influence of some parameters on the results can be investigated. in this case, the physical model acts as the bridge
between the inputs (parameters) and outputs (results). while
in some cases, although a large number of data can be collected, the model characterizing the relationship between
them is unknown. note x and y as two matrices composed
of data points with a certain amount. further, for such an
issue suppose there are m variables included in x and each
of them has n sample points,
namely ğ— âˆˆ] â„mÃ—n and
[
t
[
]
ğ— = ğ±1 , ğ±2 , â€¦ , ğ±n , where ğ±i = xi(1) , xi(2) , â€¦ , xi(m) is the i-th
component
of x, and]i = 1, 2, â€¦, n. similarly, ğ˜ âˆˆ â„mÃ—l and
[
t
ğ²j = y(1)
, y(2)
, â€¦ , yj(m) is the j-th component of y, where
j
j
j = 1, 2, â€¦, l. in practice, it is often needed to explore the
relationship between two datasets, and thus x and y can be
regarded as independent and dependent variable,
respectively.
generally, deep neural networks are data-driven technique and frequently used to solve problems in the black
box. therefore, instead of establishing an explicit function
relation like y = f (x) in the traditional way, it can be considered to establish relationship between the two datasets by
dnn as â€˜ ğ˜Ì‚ = net(x)â€™, where x and ğ˜Ì‚ are the input and output of a dnn model, respectively, and ğ˜Ì‚ is equal to or very
close to y. according to this kind of thoughts, the dnn
model is used to realize two functions of bisq model in this
work. the designed dnn can not only calculate the dispersion and attenuation of the fast p wave, but also realize the
numerical simulation of wave propagation by fd scheme
with second-order accuracy. thus, two dnn models will
be trained below. although it takes some time to train the
neural network, similar results as the original bisq model
can be obtained quickly if the network is trained well. once
a large number of repeated calculations are needed, it is
easy to complete and will save time based on a trained dnn
model.
for the calculation of dispersion and attenuation, seven rock
physical parameters are sampled m times randomly within
their value range, and this forms the input dataset ğ— âˆˆ â„mÃ—7
as follows:

â¡ v (1) qâˆ’1,(1) â¤
â¢ p(2) âˆ’1,(2) â¥
v q
â¥,
ğ˜=â¢ p
â¢ â‹®
â¥
â‹®
â¢ v (m) qâˆ’1,(m) â¥
â£ p
â¦
where ğ˜ âˆˆ â„mÃ—2.
note that random sampling is used in parameter space since
it is difficult to do grid search in high-dimensional parameter
space, and random search is often more effective than grid
search actually (bergstra and bengio 2012). it should be noted
that the variation of vp caused by different parameters differs, which may make it difficult to train the network. thus,
each component of y will be normalized to a same range after
obtaining the raw outputs. once the calculation of y according to x and the related preprocessing work completed, the
whole dataset are divided into training and test sets one by one
randomly. in addition, a validation set composed of part of the
whole dataset is needed when designing the network.
meanwhile, as introduced above, an accurate numerical
method for solving the bisq equations is important for the
full waveform inversion, but normally expensive in calculation. to improve this issue, the effective numerical simulations
for one-dimensional case via dnn model is carried out based
on the intrinsic one-to-one correspondence between the timestepping algorithms with first- and second-order accuracy as
mentioned in sect. 2.2. in this case, the input and output of
dnn model are the simulation results by the first- and secondorder scheme, respectively. a set of wave-field snapshots can
be obtained for a given set of rock physical parameters and
calculation time. specifically, suppose that the spatial range
can be divided into nx grid points and m sets of rock physical parameters are sampled randomly, the calculation time is
fixed at t, the dataset of simulation results ğ— âˆˆ â„mÃ—nx with
first-order accuracy can be written as follows:

ï¿½ ï¿½

space

ï¿½ âŸ¶
ï¿½
ï¿½ ï¿½
x1 ï¿½h2 ï¿½ â‹¯ x1 ï¿½hnx ï¿½ â¤
x2 h2 â‹¯ x2 hnx â¥
â¢ â‹®
â¥,
â¢ ï¿½ ï¿½ ï¿½â‹® ï¿½ â‹± ï¿½â‹® ï¿½ â¥
â£ xm h1 xm h2 â‹¯ xm hnx â¦

t
â¡ ğ±1 â¤ â¡ x1 ï¿½h1 ï¿½
â¢ ğ±t â¥ â¢ x h
ğ—= 2 = 2 1

â¢ â‹® â¥
â¢ tâ¥
â£ ğ±m â¦

where xi(hj) denotes the amplitude on the grid numbered hj
under the parameter of i-th rock sample, here i = 1, 2, â€¦, m

13

598

and j = 1, 2, â€¦, nx. the elements of x varies at different
times. the simulation results ğ˜ âˆˆ â„mÃ—nx with second-order
accuracy can be written in the same way. the partition of the
dataset is the same as above.
the simulation by the scheme with second-order accuracy is much more time-consuming than that of the firstorder, and thus the results with higher accuracy according
to the results with lower accuracy can be obtained by dnn
model in a shorter time once the network is trained well.
the structure of multi-layer perceptron (mlp) is adopted
here to design dnn model. as is well-known, many hyperparameters need to be determined to construct a dnn
model. it can be done by trial and error to select the optimal
one or according to empiricism or both. for simplicity in
principle, different hyper-parameters are selected to calculate the training and validation loss. the test set is randomly
selected from 50% of the given data set and used to investigate dnnâ€™s performance after training. the validation set
is a subset of the test set in this study, and its number of data
points is 10% of the given data set. the dnn models used
below are the optimal one through multiple attempts. here,
the optimal dnn model means three aspects. first of all,
the constructed neural network should ensure the accuracy
of training, and the lowest minimal validation loss can be
obtained. secondly, the depth and width of the dnn should
be as small as possible to reduce the computational resources
required. lastly, there will be no overfitting problem. in this

fig. 1â€‚â€‰schematic diagram of
dnn representation of the relationship between two datasets.
the structure of the optimal
dnn is shown in the dashed
line box. the two datasets need
to be normalized before training
the network. after training, the
output of the network needs to
be scale back to their original
ranges

13

acta geophysica (2022) 70:593â€“607

work, pytorch is used to realize the construction and calculation of neural network.
as shown in fig. 1, the optimal construction of dnn
is a fully connected network with three hidden layers, and
each hidden layer has 150 nodes. after each, middle hidden layer is the activation function. the activation function
is selected as tanh instead of relu since its derivative is
discontinuous (jagtap et al. 2020; kharazmi et al. 2021;
oostwal et al. 2021). note that the elements of x need to
be normalized to a special range (e.g., [âˆ’1, 1]) to avoid the
problem of gradient vanishing. training a dnn is a process
of updating network parameters (weight and bias) based on
continuously optimizing the loss function, which is taken
as the mean square error (mse) between the prediction of
dnn ğ˜Ì‚ and the label y. the adam optimizer (kingma and
ba 2014) is used here since its convergence speed is faster
compared with other optimizers such as stochastic gradient
descent (sgd) with momentum. the initial learning rate is
set to 0.001, and a technique of learning rate decay is used
here based on the function â€œlr_scheduler,â€ a built-in function for adjusting the learning rate along with the number
of epochs in pytorch (paszke et al. 2017). it makes the loss
function decrease more stable and avoids the large oscillation of loss curve.
in short, the general workflow of the dnn model to
replace the bisq model proposed in this work is as follows:
first, the preprocessed data are taken as the input of the dnn

acta geophysica (2022) 70:593â€“607

model and then training the neural network. the criterion
to identify the success of network training is whether the
training error and the test error decrease and converge to a
similar relatively low value after some epochs. the outputs
are scaled back to normal ranges of each quantity by label
denormalization. finally, different dataset are used to test the
trained dnn model, and the calculation process of obtaining the output from input is very fast since the network has
been trained.

599
table 1â€‚â€‰sampling ranges for the seven rock physical parameters
parameter

minimum value

maximum value

Ï‰ (hz)
Ï•
ks (gpa)
Î¼s (gpa)
Ïs (kg/m3)
sw
Îº Â­(m2)

101
0.001
20
8
1500
10â€“5
10â€“16

108
0.4
60
50
3000
1â€“10âˆ’5
10â€“9

results

table 2â€‚â€‰deep neural network architecture used for the surrogate
model of bisq model

in this part, the first examples to show are the trained dnns
which can get the similar results as the original bisq model
for dispersion and attenuation. furthermore, it will be shown
that the accurate wave-fields, e.g., with second-order accuracy, can be directly obtained through the inaccurate approximation to the wave equations.

hyper-parameter

value

layers
neurons
adaptive activation function
loss function

3
150
tanh
mean
squared
error
0.001
adam

wave dispersion and attenuation
the dispersion and attenuation of the fast p wave are always
seen as a key point to reveal the fluid flow mechanism in the
rocks. in this way, the performance of wave velocity (vp) and
inverse attenuation quality factor (qâˆ’1) on different parameters were tested and a surrogate model for dispersion and
attenuation prediction was established.
firstly, 1,400,000 data points were produced by sampling the basic parameters of the bisq model randomly,
they are frequency (Ï‰), porosity (Ï•), the bulk modulus of
solid (ks), the shear modulus of solid (Î¼s), the density of
solid (Ïs), water saturation (sw) and permeability (Îº). the
sampling range on these parameters referred to the reasonable range of each parameter in physics for different rock
samples (mavko et al. 2020) as shown in table 1, which
aims to ensure the reality and consider more possibilities.
based on these parameter combinations, the bisq model
was employed to generate 1,400,000 paired data of features
(i.e., the inputs of the dnn model) and labels (i.e., ground
truth for the outputs of the dnn model). the time discussed
in this paper is under the condition of the same computer
resources. on personal computer, it takes less than 15 min
to produce such a data set. the generated dataset was split
into training and test subsets with a ratio of 1:1. normally,
the training subset was chosen to train the network while
the test subset was used to evaluate the trained dnn model.
as required by the procedure from eqs. (1) to (6), the
seven parameters Ï‰, Ï•, ks, Î¼s, Ïs, sw and Îº were set as the
inputs and vp, qâˆ’1 as the outputs, respectively. thus, two
high-dimensional function spaces were produced as:

initial learning rate
optimizer

(
)
vp =vp ğœ”, ğœ™, ks , ğœ‡s , ğœŒs , sw , ğœ… ,
(
)
qâˆ’1 = qâˆ’1 ğœ”, ğœ™, ks , ğœ‡s , ğœŒs , sw , ğœ… .
with the above formulations, it will be convenient to
precisely describe the dependence of vp and qâˆ’1 on the
arbitrary multiple combinations of such seven parameters.
the dnn model was trained with the hyper-parameters as
mentioned in sect. 2. for convenience, they are summarized
in table 2. among them, the loss function is defined as the
mse between vp and qâˆ’1 calculated
model and
[ by bisq
]
that of dnnâ€™s prediction, i.e., ğ˜ = vp , qâˆ’1
.
it
takes
38 h to
p
complete the training on personal computer since the amount
of data is large. once the network is trained, the predicted vp
and qâˆ’1 can be obtained instantly by feeding x into the dnn
model. figure 2 shows the loss curve for the training process.
on a whole, the training loss shown in fig. 2 decreases
and converges to relatively low values after 10,000 epochs.
the training and validation loss converge at the same level.
the curve declines in a relatively smooth trend. the optimal
dnn model was obtained at epoch 50,000, where the training loss reaches its minimum, and the test loss of this dnn
model is almost as low as the training loss. this fact reveals
that the training of the dnn model is successful.
after the training process, 700,000 data points were chosen as test set to compare the prediction of dnn model with
the ground truth by the bisq model. the corresponding frequency distribution map of the variations between the truth
and predictions of the trained dnn model Î´vp is shown

13

600

in fig. 3. the frequency of Î´vp is obtained by dividing
the number of data points with the same value range by the
total number of data points. note that 68 abnormal points for
velocity prediction and 26 points for inverse quality factor
prediction were deleted since this will make the variation
range too large. then, the variation ranges of vp and qâˆ’1
are [âˆ’19.738, 28.556] and [âˆ’0.0519, 0.0384], respectively.
obviously, both the variation ranges of vp and qâˆ’1 focus on
0 and comparing the mean values of these two quantities,

fig. 2â€‚â€‰loss curve of the dnn model for dispersion and attenuation
analysis. the loss values were taken the square root based on mse
error. training loss and validation loss are denoted with orange and
green solid lines, respectively. the validation set is a subset of the test
set. the dnn model is tested on test set at epoch 50, 000, and the
value of the test loss is 3.19e-2, which is almost the same as the convergence value of training loss

fig. 3â€‚â€‰frequency distribution of the variations between the prediction
by dnn model and the bisq model. among them, 700, 000 test data
points are counted in each distribution. the mean value of vp and

13

acta geophysica (2022) 70:593â€“607

it is obvious that the relative error is also small enough. it
indicates that the dnn model has learned the two highdimensional function spaces well and confirms the wellperformance of this algorithm.
among the chosen seven parameters, wave frequency
(Ï‰) contributes to observe the dissipation amplitude of
energy, porosity (Ï•) represents the ability of solid to contain fluids, water saturation (sw) determines the composition of fluids, which can be seen as three key factors in
describing the interested reservoirs with special significance in geophysics. to further illustrate the level of accuracy with dnn model, three special test sets related to
the three parameters were constructed to display the misfit between the two models. only one parameter varied
in each test set within the range as shown in table 1, and
the other six parameters were taken as their mean value,
respectively. obviously, this newly constructed test set is
not concluded in and more complex than the original datasets produced by random sampling as mentioned above.
each subplot in fig. 4 can be seen
( as one cross section
) of
and
the (high-dimensional space
v
ğœ”,
ğœ™,
k
,
ğœ‡
,
ğœŒ
,
s
,
ğœ…
p
s s s w
)
qâˆ’1 ğœ”, ğœ™, ks , ğœ‡s , ğœŒs , sw , ğœ… .
as shown in fig. 4, the velocity and attenuation curves
calculated by the original bisq model (black line) and dnn
model (red line) on three representative test sets show the
high consistency in the whole calculation range. this further manifests the fact that the proposed dnn model has
already got the bisq model in the two high-dimensional
spaces. besides, it should be noted that the points at one end
in fig. 4e and f occur certain misfits compared with others
plots. this is because the fluid flow mechanism in rocks will
be greatly influenced when the containing fluid changed from

qâˆ’1 are 4211.030 and âˆ’4.186, respectively. the variation range of vp
(left) and qâˆ’1 (right) are [âˆ’19.738, 28.556] and [âˆ’0.0519, 0.0384],
respectively

acta geophysica (2022) 70:593â€“607

fig. 4â€‚â€‰comparison of the dispersion and attenuation by the trained
dnn model and the bisq model on special test sets. left column
from top to bottom: vp versus Ï‰, Ï• and sw, in order; right column

601

from top to bottom: the logarithm of qâˆ’1 versus Ï‰, Ï• and sw, in order.
ytruth: the ground truth by the original bisq model, yout: the predictions by the trained dnn model

13

602

acta geophysica (2022) 70:593â€“607

one single fluid into two immiscible fluids, e.g., the points in
which sw = 0 and 1.
in addition, the mean absolute error (mae) and rootmean-square error (rmse) corresponding to each plots in
fig. 4 are introduced and listed in table 3 to evaluate the
performance of the dnn model. the mae and rmse which
is used to reveal the overall misfit between the ground truth
(ytruth) and the output (yout) of dnn model can be expressed
as:

1 âˆ‘| i
|
âˆ’ yiout |,
mae =
|y
|
n i=1 | truth
âˆš
âˆš n
âˆš1 âˆ‘ (
)2
yitruth âˆ’ yiout ,
rmse = âˆš
n i=1
where rmse offers the larger errors higher weights and
then can prevent the unexpected large errors, and mae
weights the errors for any components equally.
as listed in table 3, the mae and rmse values for the
same case are similar. the maximum values are no more
than 2.162, which means that all the simulations performed
well.
at the end of this subsection, 28 laboratory measurements of rock samples collected from the published paper
(wang 2017) were introduced to test and verify the ability
of dnn model as a surrogate model for any arbitrary input.
the experimental data provided by wang (2017) contained
the density of fluid-saturated medium Ï, the velocity of compression wave vp, the velocity of shear wave vs instead of
ks, Î¼s, Ïs and Îº. in such a case, it is needed to supplement the
inputs of dnn model by the following formulations (wang
2017):
n

âˆš

yout (e.g., the predictions of dnn model) of vp is shown in
fig. 5. the great agreement provides the robust proof that
the dnn model trained well-enough to offer an accurate
approximation to the bisq model for different geological
dataset.
to sum up, the dnn algorithm was successfully
trained and tested on the wave attenuation and dispersion calculations. unlike the prediction by bisq model,
the well-trained dnn model can quickly give the simulation result at any point from the multidimensional
domain consisted of Ï‰, Ï•, k s, Î¼ s, Ï s, s w and Îº, without
the recalculation of roots for different input parameters.
fitting
(
) of
( in other words, )the high-dimensional
vp ğœ”, ğœ™, ks , ğœ‡s , ğœŒs , sw , ğœ… and qâˆ’1 ğœ”, ğœ™, ks , ğœ‡s , ğœŒs , sw , ğœ… was
carried out.

application to the wave propagation simulation
in order to clarify the effectiveness of the low-to-high
accuracy implemented by dnn model precisely, it is motivated to compare the simulation results from the secondorder scheme mentioned above with the trained dnn, in
which the calculation domain was fixed at h âˆˆ [âˆ’2000 m,
2000 m], the spatial increment Î´h = 10 m, and the timestep size Î´t = 0.001 s, while the calculation time and the

âˆš

kf
kb
ğœ‡
k
+ (
,
=
) , ğœ‡ = ğœ‡b ,
ğœŒ ks âˆ’ k
ks âˆ’ kb ğœ™ ks âˆ’ kf
/
kb =ks (1 âˆ’ ğœ™)4âˆ•(1âˆ’ğœ™) , ğœ‡b = kb ğœ‡s ks , ğœ‚ = 10s1 lg ğœ‚1 +(1âˆ’s1 ) lg ğœ‚2 ,
vp =

k+4ğœ‡âˆ•3
, vs =
ğœŒ

ğœ… = 0.0227 exp (0.3672ğœ™).

in this example, the loss function is defined as the mse
between vp by bisq model and that of dnnâ€™s prediction,
i.e., y = [vp]. it only takes a few minutes to complete the
training since the amount of data is small (about 2000).
a wave propagation model that can describe these data is
obtained by training the dnn model. the bivariate distribution map of ytruth (e.g., the laboratory measurements) and

table 3â€‚â€‰mae and rmse
between the ground truth and
the output of dnn model

13

fig. 5â€‚â€‰comparison of dnn predictions with experimental data. the
mean value of experimental data vp is 4640.7. the rmse and mae
between dnn predictions and the truth are 12.205 and 2.956, respectively

evaluations

(a)

(b)

(c)

(d)

(e)

(f)

mae
rmse

0.794
0.774

0.108
0.0182

1.213
0.994

0.0527
0.00318

1.133
2.162

0.0625
0.00520

acta geophysica (2022) 70:593â€“607

experimental samples used were varied. the source term
is a vertical point force with a ricker wavelet:
(
)
(
)
s(t) = 1 âˆ’ 2ğœ‹ 2 ğœ”20 t2 exp âˆ’ğœ‹ 2 ğœ”20 t2 ,
in which the peak frequency Ï‰0 is 35 hz, Ï€ = 3.14. the
source is located at the center of the space domain, e.g.,
h = 0 m. under the same conditions, the calculation time of
the second order is at least three times that of the first order
(liu et al. 2020).
in the following, the snapshot of wave-fields differ in the
time period of the wave propagation which was compared.
the observation time points were set as 0.1 s, 0.2 s, 0.3 s and
0.4 s, while keeping the space domain fixed. the training
and testing subsets were generated based on the experimental samples (xiong et al. 2020a, b; liu et al. 2020). among
them, 40,128 paired data of features (results by first-order
scheme) and labels (results by second-order scheme) were
produced. the generated dataset was split into training and
test subsets with a ratio of 8:2. the loss function is defined
as the mse between the wave-field by the second-order
scheme and that of dnnâ€™s prediction. it takes about 19 h to
generate the dataset and no more than half an hour to train
the network. due to the calculation time required by the
first-order and second-order schemes differ much, the trained
network can produce the second-order results instantly as
long as the results by the first-order scheme are given.
the loss curve and the statistics of dnnâ€™s prediction
accuracy on the test set are shown in fig. 6. from fig. 6a,
the loss curve for the dnn model of wave-field snapshot
also decreases and converged to relatively low values which
are almost same as the test loss.

fig. 6â€‚â€‰(left) loss curve of the dnn model for wave-field simulation with high accuracy; (right) the frequency distribution of the
variations between prediction by dnn model and those generated by

603

meanwhile, we investigate the effect of the trained network on the test set. as shown in fig. 6b, the rmse of the
normalized amplitude of snapshot Î´a0 focus on 5%, indicating that the dnn model was trained well and could be
generalized to other dataset. in this way, two rock samples
listed in table 4 were selected to validate the trained dnn
model here.
figures 7 shows the comparison between the trained
dnn model (black solid lines) and the bisq model solved
with the second-order finite difference scheme (red dashed
lines). the amplitudes of each plot were normalized by the
maximum value. the desired outputs of the trained dnn
are the second-order wave-field snapshots simulated on
space domain during the wave propagation. as it can be
seen, the dnn model holds the quality of simulating the
wave propagation for the whole space domain with different
time points and experimental data. note that the numerical simulation results from the two methods were all stable
during the wave propagation which is due to the stability of
the selected first- and second-order schemes. it means that
table 4â€‚â€‰parameters of the rock samples 1 and 2
parameter

rock sample 1

rock sample 2

Ï‰ (hz)
Ï•
ks(gpa)
Î¼s(gpa)
Ïs(kg/m3)
sw
Îº Â­(m2)

101
0.0498
31.44
16.25
2696.3
0
2.31 Ã—â€‰Â­10â€“14

103
0.097
47.16
17.61
2580.8
0.5
2.35 Ã—â€‰Â­10â€“14

fd method. among them, Î´a0 means the rmse of the normalized
amplitude of snapshot, 8025 data points are counted in each distribution. the mean value of Î´a0 is 0.0608

13

604

13

acta geophysica (2022) 70:593â€“607

acta geophysica (2022) 70:593â€“607
â—‚fig. 7â€‚â€‰wave-field simulation by dnn model and fd method at dif-

ferent times. left: rock sample 1; right: rock sample 2; from top to
bottom: the time points 0.1, 0.2, 0.3 and 0.4 s, in order

the proposed dnn model can provide stable and real-time
numerical simulations.
to prove the robustness of our method, we add 1,
2 and 5% random noises to the 1d wave-field simulations by the second-order scheme. after adding noises
to the labels, we retrain the network and investigate the

605

prediction accuracy on the same parameters as fig. 7c.
as shown in fig. 8, adding 1 and 2% noise has little
effect on the results. although adding 5% noise causes
some small oscillations in the result, the overall results
are still acceptable. the amplitude of the wave signal
in each plot is normalized, and the rmse is 0.32, 0.51
and 1.12% respectively. compared with the prediction
accuracy without noise, i.e., 0.26%, the error increases
relatively smoothly. it shows the algorithm is robust to
small noise.

fig. 8â€‚â€‰wave-field simulation by fd method and the retrained dnn model in the presence of noises. a, b and c correspond to the results with the
presence of 1, 2 and 5% noises

13

606

conclusions
this study presented the application of deep neural network for the wave propagation models in geoscience. by
means of different synthetic data and experimental data, the
comparisons of numerical simulation result demonstrate
that the well-trained dnn model is capable of approximating the attenuation and dispersion curves calculated by the
original wave equations, as well as the snapshot of wavefield simulated by high-accuracy scheme. on the other
hand, this method estimates the plane wave and numerical
solutions from geophysical data in less time. it makes the
solutions of wave equations considerably faster than previous schemes, which is particularly useful for large-scale
measurements. the use of dnn model for wave equations
also opens up a wide-arsenal of the solutions that assist to
carry out the real-time forward simulations.
future work intends to apply the dnn for more
complex equation systems. such application may significantly reduce the calculated time need for the wave
equation simulations and may give the chance to carry
out the optimal option of various rock physics models
and unknown parameters to interpret any interested seismic dataset. among them, data augmentations (grill
et al. 2020) can be applied to strengthen the quality of
training.
acknowledgements this research is supported by basic science
center project for national natural science foundation of china (no.
72088101, the theory and application of resource and environment
management in the digital economy era). this research is funded by
hunan provincial natural science foundation of china under grant
2021jj40723 and supported by changsha municipal natural science
foundation (kq2014124). this work has also been supported by the
natural science foundation of china (nsfc) (42074169; 41804073).

declarations
conflict of interest the authors declare no conflict of interest.

references
agarwal s, tosi n, breuer d, padovan s, kessel p, montavon g
(2020) a machine-learning-based surrogate model of marsâ€™
thermal evolution. geophys j int 222(3):1656â€“1670
apolinario-arzube Ã³, garcÃ­a-dÃ­az ja, medina-moreira j, lunaaveiga h, valencia-garcÃ­a r (2020) comparing deep-learning
architectures and traditional machine-learning approaches for
satire identification in spanish tweets. mathematics 8(11):2075
bergstra j, bengio yj (2012) random search for hyper-parameter
optimization. j mach learn res 13(10):281â€“305
biot ma (1956a) theory of elastic waves in a fluid-saturated porous
solid. 1. low frequency range. j acoust soc am 28:168â€“178
biot ma (1956b) theory of propagation of elastic waves in a fluidsaturated porous solid. ii. higher frequency range. j acoust
soc am 28:179â€“191

13

acta geophysica (2022) 70:593â€“607
carcione jm, quiroga-goode g (1995) some aspects of the physics
and numerical modelling of biot compressional waves. j comput acoust 3(4):261â€“280
carcione jm (2007) wave fields in real media: wave propagation
in anisotropic, anelastic, porous and electromagnetic media.
elsevie, amsterdam.
cheng c (1993) crack models for a transversely isotropic medium.
j geophys res-sol ea 98(b1):675â€“684
dvorkin j, nur a (1993) dynamic poroelasticity: a unified
model with the squirt and the biot mechanisms. geophys
58(4):524â€“533
dvorkin j, mavko g, nur a (1995) squirt flow in fully saturated
rocks. geophys 60(1):97â€“107
grill jean-bastien, florian strub, florent altchÂ´e, corentin tallec,
pierre h richemond, elena buchatskaya, carl doersch, bernardo
avila pires, zhaohan daniel guo, mohammad gheshlaghi azar
et al (2020) bootstrap your own latent: a new approach to selfsupervised learning. arxiv preprint arxiv:â€‹2006.â€‹07733.
he x, hu h, wang x (2013) finite difference modelling of dipole
acoustic logs in a poroelastic formation with anisotropic permeability. geophys j int 192(1):359â€“374
jagtap ad, kawaguchi k, karniadakis gem (2020) adaptive activation functions accelerate convergence in deep and physicsinformed neural networks. j comput phys 404:109136.
kharazmi e, zhang z, karniadakis gem (2021) hp-vpinns: variational physics-informed neural networks with domain decomposition. comput method appl m 374:113547
kingma dp, ba j (2014) adam: a method for stochastic optimization.
arxiv preprint arxiv:â€‹1412.â€‹6980.
li ye, cheng ac, you n (2019) shale anisotropy estimation from
logs in vertical wells. j geophys res- sol ea 124(7):6602â€“6611
liu j, yong w (2016) stability analysis of the biot/squirt models
for wave propagation in saturated porous media. geophys j int
204(1):535â€“543
liu j, yong w, liu j, guo z (2020) stable finite-difference methods
for elastic wave modeling with characteristic boundary conditions. mathematics 8(6):1039
maiti s, krishna tiwari r, kÃ¼mpel h (2007) neural network modelling and classification of lithofacies using well log data: a case
study from ktb borehole site. geophys j int 169(2):733â€“746
mavko g, mukerji t, dvorkin j (2020) the rock physics handbook.
cambridge university press, cambridge
mehta p, wang ch, day agr, richardson c, bukov m, fisher ck,
schwab dj (2019) a high-bias, low-variance introduction to
machine learning for physicists. phys rep 810:1â€“124
moseley b, nissen-meyer t, markham a (2020) deep learning for
fast simulation of seismic waves in complex media. solid earth
11:1527â€“1549
mÃ¼ller tm, gurevich b, lebedev m (2010) seismic wave attenuation and dispersion resulting from wave-induced flow in porous
rocksâ€”a review. geophys 75(5):75a147â€“175a164.
nosratabadi s, mosavi a, duan p, ghamisi p, filip f, band ss, reuter u, gama j, gandomi ah (2020) data science in economics:
comprehensive review of advanced machine learning and deep
learning methods. mathematics 8(10):1799
oostwal e, straa m, biehl m (2021) hidden unit specialization in
layered neural networks: relu vs. sigmoidal activation. physica
a 564:125517.
paszke a, gross s, chintala s, chanan g, yang e, devito z, lin
z, desmaison a, antiga l, lerer l (2017) automatic differentiation in pytorch. paper presented at the 31st conference
on neural information processing systems (nips 2017), long
beach, ca.
pilz m, cotton f, kotha sr (2020) data-driven and machine learning
identification of seismic reference stations in europe. geophys
j int 222(2):861â€“873

acta geophysica (2022) 70:593â€“607
rice jr, cleary mp (1976) some basic stress diffusion solutions for
fluid-saturated elastic porous media with compressible constituents. rev geophys 14(2):227â€“241
siahkoohi a, louboutin m, herrmann f (2019) neural network augmented wave-equation simulation. arxiv preprint arxiv:â€‹00925.
wang d (2017) a study on the rock physics model of gas reservoir
in tight sandstone. chin j geophys 60:64â€“83
xiong f, sun w, liu j (2020b) the stability of poro-elastic wave
equations in saturated porous media. acta geophys 69:65â€“75.
https://â€‹doi.â€‹org/â€‹10.â€‹1007/â€‹s11600-â€‹020-â€‹00508-y
xiong f, sun w, ba j carcione jm (2020a) effects of fluid rheology
and pore connectivity on rock permeability based on a network
model. j geophys res sol ea 125(3):2019jb018857.

607
yang f, ma j (2019) deep-learning inversion: a next-generation seismic velocity model building method. geophys
84(4):r583â€“r599
yang d, zhang z (2002) poroelastic wave equation including the biot/
squirt mechanism and the solid/fluid coupling anisotropy. wave
motion 35:223â€“245
you n, li ye, cheng a (2020) shale anisotropy model building based on deep neural networks. j geophys res- sol ea
125(2):e2019jb019042.

13



applied
sciences
article

seismic velocity inversion via physical embedding recurrent
neural networks (rnn)
cai lu 1 and chunlong zhang 2, *
1

2

*

citation: lu, c.; zhang, c. seismic
velocity inversion via physical
embedding recurrent neural

school of information and communication engineering, university of electronic science and technology of
china, chengdu 611731, china; lucai@uestc.edu.cn
school of resources and environment, university of electronic science and technology of china,
chengdu 611731, china
correspondence: zcl.aaron@gmail.com

abstract: seismic velocity inversion is one of the most critical issues in the field of seismic exploration
and has long been the focus of numerous experts and scholars. in recent years, the advancement
of machine learning technologies has infused new vitality into the research of seismic velocity
inversion and yielded a wealth of research outcomes. typically, seismic velocity inversion based on
machine learning lacks control over physical processes and interpretability. starting from wave theory
and the physical processes of seismic data acquisition, this paper proposes a method for seismic
velocity model inversion based on physical embedding recurrent neural networks. firstly, the wave
equation is a mathematical representation of the physical process of acoustic waves propagating
through a medium, and the finite difference method is an effective approach to solving the wave
equation. with this in mind, we introduce the architecture of recurrent neural networks to describe
the finite difference solution of the wave equation, realizing the embedding of physical processes
into machine learning. secondly, in seismic data acquisition, the propagation of acoustic waves from
multiple sources through the medium represents a high-dimensional causal time series (wavefield
snapshots), where the influential variable is the velocity model, and the received signals are the
observations of the wavefield. this forms a forward modeling process as the forward simulation of
the wavefield equation, and the use of error back-propagation between observations and calculations
as the velocity inversion process. through time-lapse inversion and by incorporating the causal
information of wavefield propagation, the non-uniqueness issue in velocity inversion is mitigated.
through mathematical derivations and theoretical model analyses, the effectiveness and rationality
of the method are demonstrated. in conjunction with simulation results for complex models, the
method proposed in this paper can achieve velocity inversion in complex geological structures.

networks (rnn). appl. sci. 2023, 13,
13312. https://doi.org/10.3390/
app132413312

keywords: velocity modeling; seismic waveform inversion; physical information neural network;
causal sequence

academic editor: roberto scarpa
received: 17 november 2023
revised: 14 december 2023
accepted: 15 december 2023
published: 16 december 2023

copyright: Â© 2023 by the authors.
licensee mdpi, basel, switzerland.
this article is an open access article
distributed under the terms and
conditions of the creative commons
attribution (cc by) license (https://
creativecommons.org/licenses/by/
4.0/).

1. introduction
seismic waveform inversion is an essential geophysical method that enables the
inversion of physical parameters of the subsurface medium, such as velocity, density, and
elastic moduli, from seismic data [1]. these parameters are of great significance in fields
such as oil and gas exploration, earthquake hazard assessment, and geological structure
analysis [2]. however, seismic waveform inversion is also a non-linear, multi-modal, nonconvex optimization problem and faces various challenges, such as dependency on initial
models, susceptibility to local minima traps, and high computational complexity [3].
to overcome these challenges, deep learning techniques have been introduced to
seismic inversion. rÃ¶th and tarantola were the first to propose the use of neural networks
to transform time-domain seismic data into acoustic velocity models in 1994 [4]. nath
et al., in 1999, trained neural networks with synthetic data, resulting in networks capable of

appl. sci. 2023, 13, 13312. https://doi.org/10.3390/app132413312

https://www.mdpi.com/journal/applsci

appl. sci. 2023, 13, 13312

2 of 16

estimating formation velocities using inter-well data [5,6]. in 2019, yang and ma developed
a fully convolutional neural network for inverting p-wave velocity models from raw seismic
data [7]. a commonality among these networks is the need for extensive labeled data for
training, and once trained, the neural networks can be used as inversion solvers. however,
in the field of seismic exploration, acquiring a large labeled dataset is challenging.
physics-embedded neural networks (pinns) offer a novel approach to seismic velocity modeling [8]. the fundamental idea behind pinns is to use neural networks to
approximate unknown functions or parameters and incorporate the residuals of physical
equations as part of the loss function. the neural networks are trained by minimizing
this loss function, resulting in solutions that satisfy physical constraints [9]. moseley et al.
(2020) utilized pinns as solvers for acoustic wave forward propagation [10,11], while
smith et al. [12] and waheed et al. [13] applied pinns to the eikonal equation for firstarrival prediction and travel-time tomography, respectively. song et al. employed pinns to
solve the frequency-domain anisotropic acoustic wave equation [14]. additionally, pinns
have been successfully used for forward and inverse analyses of various complex systems
in cases with limited sensor measurements [15â€“20] or without labeled data [21â€“25].
the recurrent neural network, as a special kind of neural network, introduces a time
dependence, which is more compatible with the wave field propagation process. in this
paper, a recurrent neural network is used to simulate the two-dimensional time-domain
finite-difference acoustic wave equation as the forward process using a similar idea of
full-waveform inverse forward simulation combined with backpropagation correction,
and the velocity is used as a network parameter [26]. the velocity model is updated
during the backward propagation of the network. this approach enables the integration of
information from the acoustic wave equation into the network, guiding the optimization
direction of the network, and avoiding explicit solutions for wavefields and gradients,
thereby reducing computational costs. furthermore, it leverages the non-linear fitting
capabilities of neural networks to enhance inversion accuracy and robustness. additionally,
given the temporal causality in wavefields, where the current computed wavefield is
related to previous wavefields, this paper adopts a time-lapse inversion approach. this
ensures that the residuals of the wavefields are minimized at each timestep, guaranteeing
the accuracy of subsequent wavefield computations and alleviating the non-uniqueness
issue in velocity inversion. the effectiveness and superiority of the method proposed in
this paper are verified through numerical experiments.
2. materials and methods
2.1. finite-difference acoustic wave equation
the numerical simulation of wave equation forward modeling is fundamental to
inversion. the time-space domain finite difference algorithm is widely used in geophysical forward modeling due to its simplicity and high computational efficiency, and it is
employed to simulate the propagation of seismic waves. in the process of solving partial
differential wave equations using the finite difference method, the computation area is
first discretized into a grid, then grid differences are utilized to approximate the derivative
operators, and the wave equation is converted into a difference equation.
in a 2d isotropic acoustic medium, the time-domain wave equation is

âˆ‡2 u(r, t) =

1
v2 ( r )

Â·

âˆ‚u2 (r, t)
+ s(r, t)Î´(r âˆ’ rs )
âˆ‚t2

(1)

where âˆ‡u2 (r, t) represents the pressure or displacement at coordinate r at time t, v2 (r)
represents the speed of sound at coordinate r, s(r, t) represents the source function at
coordinate r at time t, and Î´(r âˆ’ rs ) is a dirac (delta) function, indicating that energy is
released only at the source position.
for the 2d medium, spatial discretization is undertaken by converting coordinates
(x, y) into grid points (i, j), where i represents the index in the x direction and j represents

appl.
sci.
2023,
13,
13312
appl.
sci.
2023,
13,
13312
appl.
sci.
2023,
13,
13312

3 3of
16
3 of1616

the index
in2d
the
ymedium,
direction.
uniform
grid division
is usually adopted,
dividing
the space
for
spatial
discretization
isisundertaken
bybyconverting
coordinates
forthe
the
2dmedium,
spatial
discretization
undertaken
converting
coordinates
into
a
series
of
equal-sized
cells.
time
t
is
discretized
into
time
steps
âˆ†t,
allowing
time
(x,
y)
into
grid
points
(i,
j),
where
i
represents
the
index
in
the
x
direction
and
j
represents
(x, y) into grid points (i, j), where i represents the index in the x direction and j represents
to
progress
in
fixed
intervals.
based
on
discrete
spatial
grids
and
time
steps,
difference
the
index
in
the
y
direction.
uniform
grid
division
is
usually
adopted,
dividing
the
space
the index in the y direction. uniform grid division is usually adopted, dividing the space
approximations
are used tocells.
replace
derivative
terms. into
ainto
centered
difference
approximation
into
t is
time
Î´t,
time
intoa aseries
seriesofofequal-sized
equal-sized
cells.time
time
t isdiscretized
discretized
time2steps
steps
Î´t,allowing
allowing
timetoto
is
used
to
approximate
the
second-order
spatial
derivative
âˆ‡
u
r,
t
:
(
)
progress
progressininfixed
fixedintervals.
intervals.based
basedonondiscrete
discretespatial
spatialgrids
gridsand
andtime
timesteps,
steps,diï¬€erence
differenceapapproximations
are
used
to
replace
derivative
terms.
a
centered
diï¬€erence
approximation
to(ireplace
a (centered
u(i, j + terms.
1, t) âˆ’ 2u
i, j, t) + udifference
u(proximations
i + 1, j, t) âˆ’ 2uare
t) + u
âˆ’ 1, j, t)derivative
(i, j âˆ’ 1, t) approximation
(i, j,used
2 (r,
(r,
+ spatial
(2)
âˆ‡u2 (i, j, t) â‰ˆ isisused
the
second-order
derivative
âˆ‡u
t):t):
usedtotoapproximate
approximate
the
second-order
spatial
derivative
âˆ‡u
2
2
âˆ†x
âˆ†y

u(i
1, t)t)âˆ’âˆ’2u(i,
1,1,t)t)
u(i++1,1,j, j,t)t)âˆ’âˆ’2u(i,
2u(i,j, j,t)t)++u(i
u(iâˆ’âˆ’1,1,j, j,t)t) u(i,
u(i,j +
j+
2u(i,j,yt)
j,directions,
t)++u(i,
u(i,j âˆ’
jâˆ’
2 (i,
in1,the
x and
respectively.
(2)
âˆ‡u
j, j,t)t) â‰ˆ where âˆ†x and âˆ†y represent the grid+spacing
(2)
âˆ‡u(i,
+
2
2
Î´x
Î´y
Î´x centered difference approximation is
Î´yused to approximate the seconda second-order
2:
order
time
derivative
âˆ‚u2 (r,the
tthe
)/âˆ‚t
where
Î´y
grid
spacing
whereÎ´x
Î´xand
and
Î´yrepresent
represent
grid
spacingininthe
thex xand
andy ydirections,
directions,respectively.
respectively.
aasecond-order
centered
diï¬€erence
approximation
is
used
to
approximate
second-order centered
difference approximation is used to approximatethe
thesecsecâˆ‚u2 (r, âˆ‚u
t) (r,
ut)/
(i, j,âˆ‚tt +
2 (r,
2 âˆ†t) âˆ’ 2u(i, j, t) + u(i, j, t âˆ’ âˆ†t)
ond-order
time
derivative
:
ond-order time derivative âˆ‚u
t)/ âˆ‚t :
â‰ˆ
(3)
âˆ‚t2
âˆ†t2
2
âˆ‚uâˆ‚u(r,(r,t)t) u(i,
u(i,j, j,t +
t +Î´t)
Î´t)âˆ’âˆ’2u(i,
2u(i,j, j,t)t)++u(i,
u(i,j, j,t âˆ’
t âˆ’Î´t)
Î´t)
(3)
substituting the difference
original wave equation yields(3)
the
2
âˆ‚tâˆ‚t 2 â‰ˆapproximations into
Î´tÎ´tthe
2d time-domain finite difference acoustic wave equation:
substituting
the diï¬€erence approximations into
the original wave equation yields the
substituting
h the difference approximations
i into the original wave equation yields the
2d time-domain
finite
diï¬€erence acoustic
wave equation:
2
2
time-domain
equation:
u(i, j, t + âˆ†t)2d
=v
t) âˆ’ s(i, j, t)acoustic
Î´(i, j âˆ’ rswave
(4)
(i, j)âˆ†t2 âˆ‡ufinite
(i, j, difference
) + 2u
(i, j, t) âˆ’ u(i, j, t âˆ’ âˆ†t)
2 (i,
2 (i,
(4)
(i,
(i,
u(i,
j, j,t +
j)Î´t
j, j,t)t)âˆ’âˆ’s(i,
u(i,
tj, âˆ’
(4)
[âˆ‡u
u(i,
t +Î´t)
Î´t)=
=vv
j)Î´t 2âˆ‡u
s(i,j,equation,
j,t)Î´(i,
t)Î´(i,j âˆ’
j âˆ’rthe
r)s )]+
+2u(i,
2u(i,j, j,t)t)âˆ’
u(i,j,current
t âˆ’Î´t)
Î´t)
through
the
difference
acoustic
wave
wavefield
atâˆ’the
time can
through
acoustic
equation,
the
atatthe
can
be calculated
using
the wavefields
ofwave
the past
two time
steps,
the source,
and
thetime
velocity
throughthe
thediï¬€erence
difference
acoustic
wave
equation,
thewavefield
wavefield
thecurrent
current
time
can
be
calculated
using
the
wavefields
of
the
past
two
time
steps,
the
source,
and
the
velocity
model.
this
relationship
can
be
seen
as
a
function
g:
be calculated using the wavefields of the past two time steps, the source, and the velocity
model.
asasa afunction

model.this
thisrelationship
relationshipcan
canbebeseen
seen
functiong:g:
u(r, t + âˆ†t) = g u(r, t), u(r, t âˆ’ âˆ†t), s(r, t), v2 (2r)
(5)
u(r,
u(r, t),t),u(r,
(5)
(r))
u(r,t +
t +Î´t)
Î´t)==g g(u(r,
u(r,t âˆ’
t âˆ’Î´t),
Î´t),s(r,
s(r,t),t),v v(r)
(5)

the
wavefield
each
moment
can
be
considered
part
the
time
series
data.
the
the
thewavefield
wavefieldatat
ateach
eachmoment
momentcan
canbe
beconsidered
consideredasas
aspart
partofof
ofthe
thetime
timeseries
seriesdata.
data.the
the
calculation
process
of
the
difference
acoustic
wave
equation
is
shown
in
figures
1
and
calculation
process
of
the
diï¬€erence
acoustic
wave
equation
is
shown
in
figures
1
and
calculation process of the difference acoustic wave equation is shown in figures 1 and2.2.
2.
more
in-depth
study
the
propagation
process
the
acoustic
wave
equation
more
aa
amore
morein-depth
in-depthstudy
studyofof
ofthe
thepropagation
propagationprocess
processofof
ofthe
theacoustic
acousticwave
waveequation
equationinin
inmore
more
complex
media
was
carried
out
by
abolghasmifar
al.
[27].
complex
complexmedia
mediawas
wascarried
carriedout
outby
byabolghasmifar
abolghasmifaretet
etal.
al.[27].
[27].

figure
figure
schematic
wave
field
propagation.
figure1.1.
1.schematic
schematicofof
ofwave
wavefield
fieldpropagation.
propagation.

figure2.2.schematic
schematic diagramofofdiï¬€erential
differential acousticwave
wave equation.
figure
figure 2. schematicdiagram
diagram of differentialacoustic
acoustic waveequation.
equation.

appl. sci.
sci. 2023,
2023, 13,
13, 13312
13312
appl.
appl.
sci. 2023,
13, 13312

of 16
16
444 of
of 16

2.2. recurrent
recurrent neural
neuralnetwork
networkwith
withacoustic
acousticwave
wave equation
equation embedding
embedding
2.2.
2.2.
recurrent
neural
network
with
acoustic
wave
equation
embedding
thestructure
structureof
ofthe
thernn
rnn (recurrent
(recurrentneural
neural network)
network) is
is shown
shown in
in figure
figure 3.
3. by
by comcomthe
the
structure
of
the
rnn
(recurrent
neural
network)
is
shown
in
figure
3.
parison,
it
can
be
observed
that
the
propagation
structure
of
the
diï¬€erence
wave
equation
parison,
parison,ititcan
can be
be observed
observed that
that the
the propagation
propagation structure
structureof
ofthe
thedifference
diï¬€erencewave
wave equation
equation
has high
high similarity
similarity with
with the
the network
network structure
structure of
of the
the rnn,
rnn, and
and existing
existing experiments
experiments ininhas
has
high
similarity
with
the
network
structure
of
the
rnn,
and
existing
experiments
dicate that
that the
the dynamics
dynamics of
of the
the wave
wave equation
equation are
are conceptually
conceptually equivalent
equivalent to the rnn.
rnn.
dicate
dicate
that
the
dynamics
of
the
wave
equation
are
conceptually
equivalent
therefore,the
theacoustic
acoustic
diï¬€erence
equation
can
be embedded
embedded
into
the structure,
rnn structure,
structure,
therefore,
difference
equation
can can
be embedded
into the
rnn
which
therefore,
the
acoustic
diï¬€erence
equation
be
into
the
rnn
which enables
enables
better understanding
understanding
of the
the propagation
propagation
characteristics
of maps
wavesthem
and
enables
a better understanding
of the propagation
characteristics
of waves and
which
aa better
of
characteristics
of
waves
and
maps
them
into
the
neural
network
structure
[28].
into
the
neural
network
structure
[28].
maps them into the neural network structure [28].

figure 3.
3. schematic
schematic diagram
diagram of
of rnn
rnn network.
network.
figure
figure
3. schematic
diagram of
rnn network.

consideringthe
thedependence
dependenceof
ofthe
theacoustic
acousticdifference
diï¬€erenceequation
equationon
onthe
thewavefields
wavefieldsof
of
considering
considering
the
dependence
of
the
acoustic
diï¬€erence
equation
on
the
wavefields
of
the two
two previous
previous time
time steps,
steps, these
these wavefields
wavefields can
can be
be used
used as
as hidden
hidden states
states in
in the
the rnn.
rnn.
the
the
two
previous
time
steps,
these
wavefields
the source
source function
function at
at
the
current
time
step
can
bebeused
used
asas
the
input,
and
thethe
signal
at the
the
the
at the
the current
currenttime
timestep
stepcan
canbe
used
the
input,
and
signal
at
the
source
as
the
input,
and
the
signal
at
detector
position
for
each
time
step
â€˜tâ€™
can
be
used
as
the
shot
gather
at
time
â€˜tâ€™.
the
velocthe
detector
position
for each
â€˜tâ€™ be
canused
be used
the gather
shot gather
at â€˜tâ€™.
time
â€˜tâ€™.velocthe
detector
position
for each
timetime
stepstep
â€˜tâ€™ can
as theasshot
at time
the
ity model
model
in the
the
diï¬€erence
equation
can can
be treated
treated
as aaastrainable
trainable
parameter
within
the
velocity
model
in diï¬€erence
the difference
equation
be treated
a trainable
parameter
within
ity
in
equation
can
be
as
parameter
within
the
network.
by using
using
the the
error
between
the observed
observed
andand
predicted
values
for backward
backward
corthe
network.
by using
error
between
the
observed
predicted
values
for backward
network.
by
the
error
between
the
and
predicted
values
for
correction, this
this
process
cancan
bebe
viewed
as as
thethe
inversion
of of
thethe
velocity
model.
correction,
this
process
viewed
inversion
velocity
model.
rection,
process
can
be
viewed
as
the
inversion
of
the
velocity
model.
thisapproach
approachallows
allowsthe
theinformation
informationfrom
fromthe
the
acoustic
wave
equation
to
be
incorthis
acoustic
wave
equation
to to
be be
incorpothis
approach
allows
the
information
from
the
acoustic
wave
equation
incorporated
into
the
network,
guiding
the
optimization
direction
of
the
network
and
avoiding
rated
into
the
network,
guiding
the
optimization
direction
of
the
network
and
avoiding
porated into the network, guiding the optimization direction of the network and avoiding
theexplicit
explicitcalculation
calculationof
ofgradients,
gradients,which
which reduces
reducescomputational
computationalcosts.
costs. additionally,
additionally,the
the
the
the
explicit
calculation
of
gradients,
which
reduces
computational
costs.
additionally,
the
non-linear
non-linear fitting
fitting capabilities
capabilities of
of neural
neural networks
networks can
can be
beleveraged
leveraged to
toimprove
improveinversion
inversion
non-linear
fitting
capabilities
of
neural
networks
can
be
leveraged
to
improve
inversion
accuracy
accuracyand
androbustness.
robustness.
accuracy
and
robustness.
the
thestructure
structureof
ofthe
thernn
rnnwith
withembedded
embeddeddifference
diï¬€erenceacoustic
acousticwave
waveequation
equationand
andthe
the
the
structure
of
the
rnn
with
embedded
diï¬€erence
acoustic
wave
equation
and
the
structure
of
operators
are
shown
in
figures
4
and
5,
respectively.
structure
of
operators
are
shown
in
figures
4
and
5,
respectively.
structure of operators are shown in figures 4 and 5, respectively.

figure 4.
4. schematic
schematic
diagram
ofof
thethe
structure
of the
the
rnn
network
embedded
in the
the
acoustic
wave
figure
schematicdiagram
diagramof
structure
of the
rnn
network
embedded
inacoustic
the acoustic
figure
the
structure
of
rnn
network
embedded
in
wave
equation.
equation.
wave equation.

appl.
sci.
2023,
appl.
sci.
2023,13,
13,13312
13312

5 of 16

5 of 1

figure5.5.embedding
embedding
of the
acoustic
equation
into
theoperator.
rnn operator.
figure
of the
acoustic
wavewave
equation
into the
rnn

2.3. time-by-time inversion algorithm

2.3. time-by-time inversion algorithm

in rnns, two common network updating algorithms are generally employed:
in rnns,
twoand
common
network
updating
algorithms
are time
generally
employed:
(1
(1) compute
the loss
update the
network
at each time
step: at each
step, the
loss
and
updatethe
thenetwork
network
at each time
at each time
step, the loss i
iscompute
computedthe
andloss
used
to update
parameters.
thisstep:
is a traditional
time-based
computed
and
used
to
update
the
network
parameters.
this
is
a
traditional
time-based
loss update method. (2) compute the loss at each time step, sum them, and then update the
loss update
method.
(2) the
compute
at each and
timeaccumulated
step, sum with
them,
andfrom
then updat
network:
at each
time step,
current the
loss loss
is computed
losses
previous
time steps.
thetime
network
parameters
areloss
thenis
updated
usingand
the accumulated
accumulated sum
the network:
at each
step,
the current
computed
with losse
of
losses.
this
method
integrates
information
from
previous
time
steps
into
the
current
from previous time steps. the network parameters are then updated using the accumu
time
loss
provide
global
information.
latedstep
sum
oftolosses.
this
method
integrates information from previous time steps into th
in the rnn embedded with the acoustic wave equation, updating the network by
current time step loss to provide global information.
computing the loss at each time step would result in changing the velocity model at different
in theaccording
rnn embedded
with theformula
acoustic
wave
equation,
updating
by
time steps.
to the calculation
of the
difference
wave
equationthe
(5), network
the
computing
at each
time throughout
step wouldthe
result
in changing
model at dif
velocity
needsthe
to loss
remain
consistent
process
of solving the
the velocity
acoustic wave
ferent
time
steps.
according
to
the
calculation
formula
of
the
difference
wave
equation. therefore, computing the loss and updating the parameters separately at eachequation
(5), the
needs to
consistent
theequation,
process of
solving
time
stepvelocity
would violate
theremain
constraints
of the throughout
acoustic wave
leading
to the
the acousti
inversion
not converging.
thiscomputing
article introduces
a new
network
loss the
update
method called
wave equation.
therefore,
the loss
and
updating
parameters
separately a
the
step-by-step
algorithm.
consideringof
the
causality
of wave
wavefield
propagation,
each
time step inversion
would violate
the constraints
the
acoustic
equation,
leading to th
the
calculation
of
the
wavefield
at
the
current
time
step
is
related
to
the
previous
wavefields.
inversion not converging. this article introduces a new network loss
update method
if errors are present in the calculation of the preceding wavefields, this will introduce
called the step-by-step inversion algorithm. considering the causality of wavefield prop
additional errors in the subsequent wavefield calculations. therefore, it is necessary to
agation, the calculation of the wavefield at the current time step is related to the previou
ensure the correctness of the preceding wavefield calculations before calculating subsequent
wavefields.the
if errors
are present
in adopts
the calculation
the preceding
wavefields,
wavefields.
step-by-step
inversion
the idea ofofensuring
the correctness
of the this wil
introducewavefields
additional
in the
subsequent
wavefield
calculations.
therefore,
preceding
forerrors
inversion.
specifically,
starting
from the
beginning of
wavefield it is nec

essary to ensure the correctness of the preceding wavefield calculations before calculating
subsequent wavefields. the step-by-step inversion adopts the idea of ensuring the cor
rectness of the preceding wavefields for inversion. specifically, starting from the begin
ning of wavefield propagation, the sum of the losses of the wavefield at each time step
and its preceding time steps is computed to calculate the total loss. the model parameter

appl. sci. 2023, 13, 13312

6 of 16

propagation, the sum of the losses of the wavefield at each time step and its preceding
time steps is computed to calculate the total loss. the model parameters are updated
according to the total loss to ensure the correctness of the wavefield at the current time step.
compared to the traditional method of independently calculating the loss and updating
the network at each time step or calculating the sum of the losses for all time steps, this
method has significant advantages. firstly, step-by-step inversion not only minimizes the
overall residual but also ensures that the residual of the wavefield at each time step is as
small as possible. in addition, by ensuring that the residual of the wavefield at the previous
time step is small before propagating to the next time step, this method incorporates causal
information of wavefield propagation, satisfying the propagation conditions of the acoustic
wave equation. this avoids errors at the current time step that could be caused by errors
from the previous time step, improving the accuracy of the inversion. by using a velocity
model that has been partially corrected through inversion as the network parameters for
the next time step, the initial model for the next time step inversion is altered, which can
alleviate the dependency on the initial model in velocity inversion.
the mean squared error loss is calculated at each time step by comparing the output
with the label:
2
Î´dt = (d t âˆ’ dâ€² t
(6)
where dt is the output of the network at time t, and dâ€² t is the label sequence at time t. the
residual at time t used for computing the gradient through backpropagation is:
t

losst = âˆ‘ âˆ‘ Î´dt
rs

(7)

0

where rs represents different shot gathers. the corrections to the model parameters can be
derived from the derivatives of the total losses to the parameters:
âˆ‚losst
âˆ‚v(r, t)

(8)

losst
âˆ‚u(r, t)
Â·
âˆ‚u
r,
t
( ) âˆ‚v(r)
0

(9)

g(r) =
t

g(r) = âˆ‘

where the partial derivatives of the loss with respect to the wave field can be computed by
equation (4) and the backpropagation of the network is accomplished by passing the computation to the initial moment through the chain rule. after obtaining the parameter update
quantity, the parameters are updated by equation (10), which show the learning rate.
vn + 1 ( r ) = vn ( r ) âˆ’ a Â· g ( r )

(10)

the gradient is propagated through the chain rule, so the velocity correction is related
to dt , which causes the residual. through the two-dimensional finite difference acoustic
wave equation in the time domain (5), it is known that dt is related to the velocity model
and the wavefields of the previous two time steps. the spatial domain variations at
different time steps are caused by the difference in the wavefields of the previous time
step. therefore, the range of velocity correction gradually expands with the range of the
wavefield. through the step-by-step correction algorithm in the time domain, the velocity
model can be gradually corrected in the spatial domain, starting from the source and
progressively correcting the velocity model in all directions. the gradual correction in
the spatial domain makes the inversion parameters smaller, thereby reducing the nonuniqueness of the inversion problem. moreover, a more accurate shallow velocity can
improve the accuracy and stability of deep velocity corrections. the schematic diagram of
the step-by-step inversion process is shown in figure 6.

appl.sci.
sci.2023,
2023,13,
13,13312
13312
appl.

77 of
16
of 16

figure6.
6. schematic
schematic diagram
diagram of
of the
thetime-by-time
time-by-timeinversion
inversionalgorithm.
algorithm.
figure

3.
3. results
results
to
thethe
step-by-step
inversion
algorithm
andand
its advantovalidate
validatethe
theeffectiveness
effectivenessofof
step-by-step
inversion
algorithm
its adtages
over
the
sum-of-losses
update
method,
numerical
experiments
were
conducted
vantages over the sum-of-losses update method, numerical experiments were conducted
using
using both
both aa homogeneous
homogeneous layer
layer model
model and
and aa sloping
sloping layer
layer model
model for
for comparison
comparison and
and
analysis.
ultimately,
the
step-by-step
inversion
algorithm
was
applied
to
analysis. ultimately, the step-by-step inversion algorithm was applied to synthetic
synthetic data
data
computed
computed using
usingthe
themarmousi
marmousimodel
modelto
toverify
verifythe
thefeasibility
feasibilityof
ofthis
thisalgorithm.
algorithm.
3.1. homogeneous layer model
3.1. homogeneous layer model
in order to validate the efficacy of the step-by-step inversion algorithm, a homoin order to validate the efficacy of the step-by-step inversion algorithm, a homogegeneous layer model was constructed. in this model, the velocities from the shallow
neous layer model was constructed. in this model, the velocities from the shallow to deep
to deep regions are set as [3, 3.5, 4] km/s. synthetic datasets were computed through
regions are set as [3, 3.5, 4] km/s. synthetic datasets were computed through forward
forward propagation using a vertical seismic profile (vsp) observation system, orthogonal
propagation using a vertical seismic profile (vsp) observation system, orthogonal paparameter points are 100 Ã— 100, with a spatial step of 10 m and a time step of 0.5 ms. the
rameter
points
100 Ã— 100,
with
a spatial
stepfrom
of 10shallow
m andto
a deep
time step
of 0.5
ms. the
initial
model
hasare
velocities
set as
[3, 3.4,
3.9] km/s
regions,
simulating
model
has velocities
as [3, 3.4,
3.9] km/saccurate.
from shallow
deep regions,
simulating
ainitial
scenario
where
the initialset
model
is relatively
the to
inversion
was performed
a
scenario
where
the
initial
model
is
relatively
accurate.
the
inversion
was
performed
ususing the adam optimization algorithm, with identical parameter settings for all tests. the
ing
the
adam
optimization
algorithm,
with
identical
parameter
settings
for
all
tests.
the
homogeneous layer model and its inversion results are depicted in figure 7.
homogeneous
layer
model
and7,itsthe
inversion
results
are depicted
in figure
7. smoother
as can be seen
from
figure
step-by-step
inversion
algorithm
yields
results both in shallow and deep areas, especially near the source points close to the surface
and the lateral regions. slices were taken at depths of 500 m and 800 m for velocity profile
comparisons, with the results shown in figures 8 and 9, respectively.
it can be deduced from figures 8 and 9 that, in terms of inversion accuracy, the
step-by-step inversion generally achieves higher accuracy compared to the sum-of-losses
inversion. the inversion results are closer to the true model. moreover, by employing a
time-stepped inversion, which takes into account the propagation of waves, the accuracy
of the preceding wavefield is ensured. this, in turn, leads to more precise calculations in
subsequent wavefield propagations, manifesting in the inversion results as more accurate
representations on the lateral sides and deeper regions of the model.

appl. sci. 2023, 13, 13312
appl. sci. 2023, 13, 13312

8 of 16

figure 7. homogeneous layer model and inversion results. (a) true model. (b) total loss inversion.
(c) time by time inversion.

as can be seen from figure 7, the step-by-step inversion algorithm yields smoother
results both in shallow and deep areas, especially near the source points close to the surface
and
the lateral regions.
slices
were taken
at depths
of 500(b)
mtotal
andloss
800inversion.
m for velocity
figure
7. homogeneous
layer model
and inversion
results.
(a) true model.
figure
7. homogeneous
layer
inversion
results.
(a) true
model. (b) total loss inv
profile
with
themodel
resultsand
shown
in figures
8 and
9, respectively.
(c) time comparisons,
by time inversion.
(c) time by time inversion.

as can be seen from figure 7, the step-by-step inversion algorithm yields sm
results both in shallow and deep areas, especially near the source points close to th
face and the lateral regions. slices were taken at depths of 500 m and 800 m for v
profile comparisons, with the results shown in figures 8 and 9, respectively.

figure 8.
500
mm
deep
velocity
profiles
of homogeneous
layer model.
figure
8. comparison
comparisonofof
500
deep
velocity
profiles
of homogeneous
layer model.

appl. sci. 2023, 13, 13312

9 of 16

appl. sci. 2023, 13, 13312

9 of 16

figure
800mm
deep
velocity
profiles
of homogeneous
figure 9.
9. comparison
comparison ofof800
deep
velocity
profiles
of homogeneous
layer layer
model.model.

3.2. sloping layer model

it can be deduced from figures 8 and 9 that, in terms of inversion accuracy, the stepto inversion
evaluate the
performance
of the
step-by-step
inversion
algorithm
in inclined
by-step
generally
achieves
higher
accuracy
compared
to the sum-of-losses
ingeological structures, as well as its weak dependence on the initial model, a sloping layer
version. the inversion results are closer to the true model. moreover, by employing a timemodel was constructed. in this second model, the velocities increase from the shallow
stepped
inversion,
which takes
into4.5]
account
propagation
waves, the
accuracy
to deeper
layers, following
[2.5, 3.5,
km/s,the
respectively.
theofsynthetic
dataset
was of the
preceding
wavefield
is
ensured.
this,
in
turn,
leads
to
more
precise
calculations
in subsecomputed using a vsp (vertical seismic profile) observation system via forward modeling,
quent
wavefield
propagations,
in model
the inversion
results
as had
more
accurate repand the
orthogonal
parameters aremanifesting
consistent with
i. the initial
model
velocities
[2, 3, 4] km/s,
significant
discrepancies
initial
and true models. this
resentations
onwith
the lateral
sides
and deeperbetween
regions the
of the
model.
was to validate the weak dependency of the step-by-step inversion algorithm on the
initial
model,
demonstrating
that accurate inversion is still achievable despite large errors
3.2.
sloping
layer
model
in the initial model. the inversion employed the adam optimization algorithm with
to evaluate the performance of the step-by-step inversion algorithm in inclined geidentical parameter settings. the results of the second model and its inversion are shown
ological
as well as its weak dependence on the initial model, a sloping layer
in figurestructures,
10.
modelaswas
in figure
this second
model,
velocities
increase
shallow to
canconstructed.
be observed in
10, despite
thethe
substantial
error
in thefrom
initialthe
model
deeper
layers,of
following
4.5] the
km/s,
respectively.
the synthetic
was comin the context
a sloping [2.5,
layer3.5,
model,
step-by-step
inversion
algorithmdataset
exhibited
superior
performance
overall. this
suggests
that the
algorithm has
a lower
on
puted
using
a vsp (vertical
seismic
profile)
observation
system
viadependency
forward modeling,
the initial
model and parameters
is capable of achieving
reasonably
even
whenhad
the velociand
the orthogonal
are consistent
with accurate
model i.inversions
the initial
model
initial
has large
in contrast, thebetween
traditional
inversion
ties
[2, model
3, 4] km/s,
withdiscrepancies.
significant discrepancies
thesum-of-losses
initial and true
models. this
performed poorly, indicating a higher dependency on the initial model. velocity profiles
was to validate the weak dependency of the step-by-step inversion algorithm on the iniat depths of 150 m, 500 m, and 800 m were selected for comparison and are displayed in
tial model, demonstrating that accurate inversion is still achievable despite large errors in
figures 11â€“13, respectively.
the initial
the inversionvelocity
employed
the adam
optimization
algorithm
basedmodel.
on the cross-sectional
comparison
plots,
it can be observed
that, with
even identical
parameter
theinitial
results
of the
model
and itsalgorithm
inversionwas
areable
shown in
with
significantsettings.
errors in the
model,
the second
step-by-step
inversion
figure
10.
to more accurately invert velocities at various depths, including 150 m, 500 m, and 800 m,
demonstrating
precision
in the
as can be superior
observed
in figure
10,results.
despite the substantial error in the initial model in

the context of a sloping layer model, the step-by-step inversion algorithm exhibited superior performance overall. this suggests that the algorithm has a lower dependency on
the initial model and is capable of achieving reasonably accurate inversions even when
the initial model has large discrepancies. in contrast, the traditional sum-of-losses inversion performed poorly, indicating a higher dependency on the initial model. velocity profiles at depths of 150 m, 500 m, and 800 m were selected for comparison and are displayed
in figures 11, 12 and 13, respectively.

appl. sci. 2023, 13, 13312

1

appl. sci. 2023, 13, 13312

10 of 16

figure 10. sloping layer model and inversion results. (a) true model. (b) total loss inversion. (c)
time by time inversion.

based on the cross-sectional velocity comparison plots, it can be observed that, even
with significant errors in the initial model, the step-by-step inversion algorithm was able
to
more accurately invert velocities at various depths, including 150 m, 500 m, and 800 m,
figure 10. sloping layer model and inversion results. (a) true model. (b) total loss inversion. (c) time
figure
sloping
layer model
and inversion
results. (a) true model. (b) total loss inversion
demonstrating
superior
precision
in the results.
by time10.
inversion.
time by time inversion.

based on the cross-sectional velocity comparison plots, it can be observed that
with significant errors in the initial model, the step-by-step inversion algorithm wa
to more accurately invert velocities at various depths, including 150 m, 500 m, and
demonstrating superior precision in the results.

figure 11.
150
mm
deep
velocity
profiles
of sloping
layer layer
model.model.
figure
11. comparison
comparisonofof
150
deep
velocity
profiles
of sloping

appl.
appl. sci.
sci. 2023,
2023, 13,
13, 13312
13312

11
11 of
of 16
16

appl. sci. 2023, 13, 13312

11 of 16

figure
12
500
deep
velocity
profiles
of
figure 12.
comparison
mm
deep
velocity
profiles
of sloping
layerlayer
model.model.
figure
12 comparison
comparisonofof
of500
500
m
deep
velocity
profiles
of sloping
sloping
layer
model.

figure 13.
ofof
800
mm
deep
velocity
profiles
of sloping
layer layer
model.
figure
13.
comparison
800
velocity
profiles
of
figure
13.comparison
comparison
of
800
m deep
deep
velocity
profiles
of sloping
sloping
layer model.
model.

3.3. marmousi model

3.3.
3.3. marmousi
marmousi model
model
to verify the effectiveness of the algorithm under more complex real-world conditions,
to
verify
eï¬€ectiveness
of
algorithm
under
more
complex
real-world
to verify the
the
eï¬€ectiveness
of the
the the
algorithm
under
more
complex
real-world condicondithe experiments
used
a marmousi model.
model was
created
through
a collaboration
tions,
tions, the
the experiments
experiments used
used aa marmousi
marmousi model.
model. the
the model
model was
was created
created through
through aa collabcollaboration
oration of
of numerous
numerous geologists,
geologists, geophysicists,
geophysicists, and
and computer
computer scientists
scientists and
and is
is capable
capable of
of
simulating
actual
complex
geological
conditions
and
is
geologically
consistent
[29].
a
relsimulating actual complex geological conditions and is geologically consistent [29]. a relatively
atively accurate
accurate and
and smooth
smooth velocity
velocity model
model was
was used
used as
as the
the initial
initial model
model for
for the
the inversion.
inversion.
the
real
and
initial
models
are
shown
in
figure
14.
the
vsp
observation
system
the real and initial models are shown in figure 14. the vsp observation system was
was used
used

appl. sci. 2023, 13, 13312

appl. sci. 2023, 13, 13312
appl. sci. 2023, 13, 13312

12 of 16

of numerous geologists, geophysicists, and computer scientists and is capable of simulating
actual complex geological conditions and is geologically consistent [29].
12 ofa16relatively
12 of 16
accurate and smooth velocity model was used as the initial model for the inversion. the
real and initial models are shown in figure 14. the vsp observation system was used to
generate
thereal
realmodel
modelwith
with
parameters
of 100
Ã—a200,
a spatial
to
generatesynthetic
syntheticdata
data from
from the
parameters
of 100
Ã— 200,
spatial
step step of
to generate synthetic data from the real model with parameters of 100 Ã— 200, a spatial step
10
m,m,
and
ms.
of 10
anda atime
timestep
step of
of 0.5 ms.
of 10 m, and a time step of 0.5 ms.

figure14.
14.marmousi
marmousi
model
initial
model.
(a) model.
true model.
(b)model.
initial model.
figure
model
andand
initial
model.
(a) true
(b) initial
figure 14. marmousi model and initial model. (a) true model. (b) initial model.

the
model
andand
inversion
results
are shown
in figure
15. as can
themarmousi
marmousi
model
inversion
results
are shown
in figure
15. be
asseen
can be seen
the marmousi model and inversion results are shown in figure 15. as can be seen
from
the
inversion
results,
both
the
overall
inversion
andand
time-by-time
inversion
results
from
the
inversion
results,
both
the
overall
inversion
time-by-time
inversion
from the inversion results, both the overall inversion and time-by-time inversion resultsresults are
are
relatively
close to thereal
realmodel.
model. aacomparison
of of
horizontal
velocity
values at depths
relatively
close
comparison
horizontal
velocity
at depths of
are
relatively
closetotothe
the real model.
a comparison
of horizontal
velocity
valuesvalues
at depths
of
500
m
and
850
m
isshown
shownininfigures
figures16
16 and
and 17,
17, and
and aa comparison
comparison ofofdepth
velocity
500
m
and
850
m
is
depth
velocity
of 500 m and 850 m is shown in figures 16 and 17, and a comparison of depth velocity values
values
at a horizontal
distance
of 1000
m is shown in
in figure
18.
from
the velocity
comparat a horizontal
distance
of of
1000
figure18.
18.
from
velocity
comparison
values
at a horizontal
distance
1000mmisisshown
shown in figure
from
the the
velocity
comparison curve, it can be seen that both the time-by-time inversion and the overall inversion
ison
curve,
it can
seenthat
thatboth
both the
the time-by-time
time-by-time inversion
andand
the the
overall
inversion
curve,
it can
bebe
seen
inversion
overall
inversion are
are relatively close to the real model. moreover, the velocity of the time-by-time inversion
are
relativelyclose
closeto
to the
the real
moreover,
the velocity
of the of
time-by-time
inversioninversion
relatively
realmodel.
model.
moreover,
the velocity
the time-by-time
results is smoother, with fewer velocity anomalies, making the inversion more stable.
results
with
fewer
velocity
anomalies,
making
the inversion
more stable.
resultsisissmoother,
smoother,
with
fewer
velocity
anomalies,
making
the inversion
more stable.

figure 15. marmousi model and inversion results. (a) true model. (b) total loss inversion. (c) time
figure15.
15.marmousi
marmousi
model
inversion
results.
(a)model.
true model.
loss inversion.
figure
model
and and
inversion
results.
(a) true
(b) total(b)
losstotal
inversion.
(c) time (c) time
by time inversion.
by
time
inversion.
by time inversion.

appl. sci.
sci. 2023,
2023, 13,
13, 13312
13312
appl.

appl. sci. 2023, 13, 13312

13 of 16

figure
16.comparison
comparison
of
500
m deep
deep
velocity
profiles
of marmousi
marmousi
model.
figure
16.
comparison
500
m
velocity
profiles
of
figure 16.
ofof
500
m deep
velocity
profiles
of marmousi
model.model.

figure 17.
comparison
850
mm
deep
velocity
profiles
of marmousi
model.
figure
17 comparison
comparisonofof
of
850
m
deep
velocity
profiles
of marmousi
marmousi
model.
figure
17
850
deep
velocity
profiles
of
model.

13 of
of 16
16
13

appl. sci. 2023, 13, 13312

appl. sci. 2023, 13, 13312

14 of 16

14 of 16

figure
18.comparison
comparison
1000
m distance
velocity
profiles
of marmousi
figure 18.
of of
1000
m distance
velocity
profiles
of marmousi
model. model.

4. discussion

4. discussion

we believe that maximizing the use of known information in the velocity inversion
wecan
believe
that
theinversion.
use of known
information
in thepropagation
velocity inversion
process
improve
themaximizing
accuracy of the
the causality
of wavefield
process
can improve
the accuracy
of recurrent
the inversion.
causality
of recurrent
wavefieldneural
propagation
is introduced
in physically
embedded
neuralthe
networks.
the
is
introduced
in physically
embedded
recurrent
neural
networks.
the
network
can characterize
the forward
propagation
of the
wave field
well, and
therecurrent
time-by- neural
time inversion
can strengthenthe
theforward
causality propagation
of the wave field
propagation.
in time-by-time
network
can characterize
of the
wave field
well, and the timeinversion,inversion
the inversion
result of the the
previous
moment
is used
the initial
model of in
thetime-byby-time
can strengthen
causality
of the
waveasfield
propagation.
inversion
in
the
subsequent
moment,
which
can
make
the
wave
field
propagation
not
only
time inversion, the inversion result of the previous moment is used as the initial model of
causal but also correct. through the experiments and comparisons of the three different
the
inversion in the subsequent moment, which can make the wave field propagation not
models, it can be seen that when the initial model is better, the time-by-time inversion can
only causal but also correct. through the experiments and comparisons of the three difachieve more accurate results than the traditional overall inversion, which shows that it
ferent
models,
it canthe
beaccuracy
seen thatofwhen
the initial
is better,
thepoor,
time-by-time
is effective
to ensure
causality.
when model
the initial
model is
the time- inversion
caninversion
achieve more
resultswhich
than the
traditional
overall
inversion,
which shows
by-time
is stillaccurate
more effective,
indicates
that the
time-by-time
inversion
that
it
is
eï¬€ective
to
ensure
the
accuracy
of
causality.
when
the
initial
model
is poor, the
is less dependent on the initial model. finally, in the marmousi model, the time-by-time
inversion achieved
more is
accurate
results
on bothwhich
sides indicates
and in thethat
deeper
it is invertime-by-time
inversion
still more
eï¬€ective,
the regions.
time-by-time
worthismentioning
that since
inversion
ensure
the correctness
thetime-bysion
less dependent
on the
the time-by-time
initial model.
finally,has
intothe
marmousi
model,ofthe
cause
and
effect,
when
the
error
of
the
initial
model
is
large,
the
time-by-time
inversion
time inversion achieved more accurate results on both sides and in the deeper regions. it
needs more time to correct the wavefield at each moment. the study by rointan et al.
is
worth mentioning that since the time-by-time inversion has to ensure the correctness of
provides guidance on initial model selection [30].

the cause and eï¬€ect, when the error of the initial model is large, the time-by-time inversion
needs
more time to correct the wavefield at each moment. the study by rointan et al.
5. conclusions
provides
guidance
on initial
modelembedded
selection [30].
this paper
presents
a physically
recurrent neural network (rnn)-based
time-by-time inversion algorithm. using the time-by-time inversion algorithm, the causal

5.
conclusions
information
in the time evolution of the wave equation is mapped into spatial information

for velocity
modelpresents
correction.a the
velocity model
is corrected
in theneural
spatial region
according
this paper
physically
embedded
recurrent
network
(rnn)-based
to the wave field propagation range. the corrected model will be used as the initial model
time-by-time inversion algorithm. using the time-by-time inversion algorithm, the causal
for subsequent time steps covering a larger time range, thus reducing the dependence
information
in the time evolution of the wave equation is mapped into spatial information
of the inversion on the initial model. in addition, controlling the correction region can
for
velocity
model correction.
theeffectively
velocity model
is corrected
in the of
spatial
region accordreduce
the inversion
parameters and
reduce the
non-uniqueness
the inversion

ing to the wave field propagation range. the corrected model will be used as the initial
model for subsequent time steps covering a larger time range, thus reducing the dependence of the inversion on the initial model. in addition, controlling the correction region
can reduce the inversion parameters and eï¬€ectively reduce the non-uniqueness of the in-

appl. sci. 2023, 13, 13312

15 of 16

solution within a single time step, and eventually reduce the overall non-uniqueness, while
improving the accuracy and stability of the inversion. overall, the time-by-time inversion
can achieve more accurate results than the overall inversion, but it may require longer
computation time in the case of large initial model errors. the effectiveness and superiority
of the method proposed in this paper are verified by numerical experiments.
author contributions: conceptualization, c.l. and c.z.; methodology, c.l.; software, c.z.; validation, c.z. and c.l.; formal analysis, c.l.; investigation, c.l.; resources, c.z.; data curation, c.z.;
writingâ€”original draft preparation, c.z.; writingâ€”review and editing, c.l.; visualization, c.l.;
supervision, c.l.; project administration, c.l.; funding acquisition, c.l. all authors have read and
agreed to the published version of the manuscript.
funding: this research was funded by research on key technology of intelligent geological structure
modeling based on tectonic analysis, grant number 41974147.
institutional review board statement: not applicable.
informed consent statement: not applicable.
data availability statement: data are contained within the article.
acknowledgments: we are grateful to the reviewers for their valuable comments.
conflicts of interest: the authors declare no conflict of interest.

references
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.

liu, y.; he, b.; zheng, y. controlled-order multiple waveform inversion. geophysics 2020, 85, r243â€“r250. [crossref]
kuang, w.; yuan, c.; zhang, j. real-time determination of earthquake focal mechanism via deep learning. nat. commun. 2021,
12, 1432. [crossref]
zhang, j.; zhang, h.; chen, e.; zheng, y.; kuang, w.; zhang, x. real-time earthquake monitoring using a search engine method.
nat. commun. 2014, 5, 5664. [crossref] [pubmed]
roth, g.; tarantola, a. neural networks and inversion of seismic data. j. geophys. res. solid earth 1994, 99, 6753â€“6768. [crossref]
nath, s.k.; vasu, r.; pandit, m. wavelet based compression and denoising of optical tomography data. opt. commun. 1999, 167,
37â€“46. [crossref]
kumar, n.s.; subrata, c.; kumar, s.s.; nilanjan, g. velocity inversion in cross-hole seismic tomography by counter-propagation
neural network, genetic algorithm and evolutionary programming techniques. geophys. j. int. 1999, 138, 108â€“124.
yang, f.; ma, j. deep-learning inversion: a next-generation seismic velocity model building method. geophysics 2019, 84,
r583â€“r599. [crossref]
ren, p.; rao, c.; sun, h.; liu, y. physics-informed neural network for seismic wave inversion in layered semi-infinite domain.
comput. methods appl. math. 2022, 22, 1â€“24.
raissi, m.; perdikaris, p.; karniadakis, g.e. physics-informed neural networks: a deep learning framework for solving forward
and inverse problems involving nonlinear partial differential equations. j. comput. phys. 2019, 378, 686â€“707. [crossref]
moseley, b.; nissen-meyer, t.; markham, a. deep learning for fast simulation of seismic waves in complex media. solid earth
2020, 11, 1527â€“1549. [crossref]
moseley, b.; markham, a.; nissen-meyer, t. solving the wave equation with physics-informed deep learning. arxiv 2020,
arxiv:2006.11894.
smith, j.d.; azizzadenesheli, k.; ross, z.e. eikonet: solving the eikonal equation with deep neural networks. ieee trans. geosci.
remote sens. 2020, 59, 10685â€“10696. [crossref]
waheed, u.b.; alkhalifah, t.; haghighat, e.; song, c.; virieux, j. pinntomo: seismic tomography using physics-informed neural
networks. arxiv 2021, arxiv:2104.01588.
song, c.; alkhalifah, t.; waheed, u.b. solving the frequency-domain acoustic vti wave equation using physics-informed neural
networks. geophys. j. int. 2021, 225, 846â€“859. [crossref]
karniadakis, g.e.; kevrekidis, i.g.; lu, l.; perdikaris, p.; wang, s.; yang, l. physics-informed machine learning. nat. rev. phys.
2021, 3, 422â€“440. [crossref]
rao, c.; sun, h.; liu, y. physics-informed deep learning for incompressible laminar flows. theor. appl. mech. lett. 2020, 10,
207â€“212. [crossref]
raissi, m.; yazdani, a.; karniadakis, g.e. hidden fluid mechanics: learning velocity and pressure fields from flow visualizations.
science 2020, 367, 1026â€“1030. [crossref]
lu, l.; meng, x.; mao, z.; karniadakis, g.e. deepxde: a deep learning library for solving differential equations. siam rev. 2021,
63, 208â€“228. [crossref]

appl. sci. 2023, 13, 13312

19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.

16 of 16

haghighat, e.; raissi, m.; moure, a.; gomez, h.; juanes, r. a physics-informed deep learning framework for inversion and
surrogate modeling in solid mechanics. comput. methods appl. mech. eng. 2021, 379, 1137411. [crossref]
ren, p.; rao, c.; liu, y.; ma, z.; wang, q.; wang, j.-x.; sun, h. physics-informed deep super-resolution for spatiotemporal data.
arxiv 2022, arxiv:2208.01462. [crossref]
sun, l.; gao, h.; pan, s.; wang, j.-x. surrogate modeling for fluid flows based on physics-constrained deep learning without
simulation data. comput. methods appl. mech. eng. 2019, 361, 112732. [crossref]
rao, c.; sun, h.; liu, y. physics-informed deep learning for computational elastodynamics without labeled data. j. eng. mech.
2021, 147, 04021043. [crossref]
gao, h.; sun, l.; wang, j.-x. phygeonet: physics-informed geometry-adaptive convolutional neural networks for solving
parameterized steady-state pdes on irregular domain. j. comput. phys. 2021, 428, 110079. [crossref]
ren, p.; rao, c.; liu, y.; wang, j.-x.; sun, h. phycrnet: physics-informed convolutional-recurrent network for solving spatiotemporal pdes. comput. methods appl. mech. eng. 2022, 389, 114399. [crossref]
gao, h.; zahr, m.j.; wang, j.-x. physics-informed graph neural galerkin networks: a unified framework for solving pdegoverned forward and inverse problems. comput. methods appl. mech. eng. 2022, 390, 114502. [crossref]
alaei, n.; soleimani monfared, m.; roshandel kahoo, a.; bohlen, t. seismic imaging of complex velocity structures by 2d
pseudo-viscoelastic time-domain full-waveform inversion. appl. sci. 2022, 12, 7741. [crossref]
abolghasemifar, s.a.; monfared, m.s.; aghajani, h. acoustic wave propagation in strongly heterogeneous media: a lattice
boltzmann approach. int. multidiscip. sci. geoconference sgem 2009, 1, 643â€“648.
sun, j.; niu, z.; innanen, k.a.; li, j.; trad, d.o. a theory-guided deep-learning formulation and optimization of seismic waveform
inversion. geophysics 2020, 85, r87â€“r99. [crossref]
brougois, a.; bourget, m.; lailly, p.; poulet, m. marmousi, model and data. in eaeg workshop-practical aspects of seismic data
inversion; european association of geoscientists & engineers: utrecht, the netherlands, 1990; cp-108-00002.
rointan, a.; soleimani monfared, m.; aghajani, h. improvement of seismic velocity model by selective removal of irrelevant
velocity variations. acta geod. et geophys. 2021, 56, 145â€“176. [crossref]

disclaimer/publisherâ€™s note: the statements, opinions and data contained in all publications are solely those of the individual
author(s) and contributor(s) and not of mdpi and/or the editor(s). mdpi and/or the editor(s) disclaim responsibility for any injury to
people or property resulting from any ideas, methods, instructions or products referred to in the content.



arxiv:2311.09608v1 [physics.geo-ph] 16 nov 2023

deep neural helmholtz operators for 3d elastic wave
propagation and inversion
1

caifeng zou1 , kamyar azizzadenesheli2 , zachary e. ross1 , and robert w. clayton1
seismological laboratory, california institute of technology, pasadena, ca 91125, usa
2
nvidia corporation, santa clara, ca 95051, usa

abstract
numerical simulations of seismic wave propagation in heterogeneous 3d media are central to
investigating subsurface structures and understanding earthquake processes, yet are computationally expensive for large problems. this is particularly problematic for full waveform inversion,
which typically involves numerous runs of the forward process. in machine learning there has been
considerable recent work in the area of operator learning, with a new class of models called neural
operators allowing for data-driven solutions to partial differential equations. recent works in
seismology have shown that when neural operators are adequately trained, they can significantly
shorten the compute time for wave propagation. however, the memory required for the 3d
time domain equations may be prohibitive. in this study, we show that these limitations can be
overcome by solving the wave equations in the frequency domain, also known as the helmholtz
equations, since the solutions for a set of frequencies can be determined in parallel. the 3d
helmholtz neural operator is 40 times more memory-efficient than an equivalent time-domain
version. we employ a u-shaped neural operator for 2d and 3d elastic wave modeling, achieving
two orders of magnitude acceleration compared to a baseline spectral element method. the
neural operator accurately generalizes to variable velocity structures and can be evaluated on
denser input meshes than used in the training simulations. we also show that when solving
for wavefields strictly on the surface, the accuracy can be significantly improved via a graph
neural operator layer. in leveraging automatic differentiation, the proposed method can serve
as an alternative to the adjoint-state approach for 3d full-waveform inversion, reducing the
computation time by a factor of 350.

keywords: 3d elastic wave propagation; helmholtz equations; machine learning; neural operators;
full-waveform inversion; automatic differentiation.

1

introduction

numerical simulations of seismic wave propagation serve as the foundation for a wide array of
seismological investigations, including subsurface imaging, ground motion simulation for seismic
hazard assessment, and deriving earthquake source properties. however, computational cost and
memory requirements become major concerns for large 3d problems, as well as inverse problems,
which require numerous evaluations of the forward model. in addition, the availability of denser
seismic networks spanning wider regions offers an unprecedented wealth of data to be used. therefore,

1

accelerating seismic wave modeling has become a pressing need for modern seismology to align with
the rapid advancements in big data and computer capabilities.
traditional methods for solving partial differential equations (pdes) are based on brute-force
numerical schemes that discretize a physical domain and solve a governing equation on a grid.
this discretization introduces a trade-off between the speed and accuracy: coarser grids provide
faster results but with reduced accuracy, while finer grids offer higher accuracy at the cost of slower
computations. in seismology, the finite difference method (fdm) and finite element method (fem)
are generally the most common approaches for numerical simulation of wave propagation (kelly
et al., 1976; olsen, 2000; fichtner et al., 2009; de basabe & sen, 2009). as a version of fem, the
spectral element method (sem) developed by komatitsch et al. (2002) combines the higher-order
accuracy from spectral methods with the flexibility of fems. for any of the above-mentioned
solvers, a certain number of elements or grid points per seismic wavelength are required to achieve a
specific level of accuracy. the most demanding calculations are at high frequencies and the slowest
parts require the densest grids. the computational expense remains a bottleneck for conventional
numerical methods, especially in 3d.
the recent emergence of machine learning approaches has the potential to overcome the limitations
of conventional pde solvers in computing speed and cost, especially for large inverse problems
(raissi et al., 2019; li et al., 2020a,b; khoo et al., 2021; lu et al., 2021). to solve pdes with
neural networks, one can simply discretize the input (e.g., the elastic properties of the continuum)
and output (e.g., the displacement wavefield) function spaces into finite-dimensional grids, which
naturally fits into a standard neural network framework such as the convolutional neural network
(guo et al., 2016; zhu & zabaras, 2018; winovich et al., 2019; moseley et al., 2020b). however,
this approach is limited to a specific discretization and cannot provide solutions for new, abitrary
meshes within the same physical domain. the so-called physics-informed neural networks (pinns)
(raissi et al., 2019) allow for the querying of solutions at new points by directly parameterizing the
solution function as a neural network. pinns penalize the residual of the governing equation in the
loss function, which can be trained on their own and do not necessarily need data from a separate
numerical solver.
in the last few years, there have been a number of studies applying pinns to seismological problems.
they have been used successfully to solve eikonal equations and derive earthquake hypocenters
(smith et al., 2020, 2022). pinns have been used to solve the 2d and 3d scattered forms of the
frequency-domain acoustic wave equations (song & alkhalifah, 2021; alkhalifah et al., 2021; song
et al., 2021; huang & alkhalifah, 2022a,b). other applications lie in acoustic wave propagation
(moseley et al., 2020a; song et al., 2022), inversion (rasht-behesht et al., 2022; zhang et al., 2023b),
and wavefield-modeling-based source imaging (huang & alkhalifah, 2023). compared to acoustic
wave equations, the use of pinns for the more computationally expensive solutions to elastic wave
equations is sparsely documented (ren et al., 2022; song et al., 2023).
however, in most realistic cases these models only learn a specific instance of the pde and generally
require training a new network for every new instance of the pde parameters (e.g., elastic media and
source properties). to address the shortcomings of the above-mentioned methods, li et al. (2020b)
proposed the discretization-invariant neural operator that learns an entire family of pdes, removing
the need for retraining with varied pde coefficients. the universal approximation theorem (hornik
et al., 1989) serves as the theoretical foundation for the neural operators composed of linear operators
2

and nonlinear activations to approximate any given nonlinear continuous operator (kovachki et al.,
2023).
neural operators have seen increasing usage in seismology in recent years. the appeal of neural
operators is that one can train a single deep learning model and apply it to arbitrary pde parameter
functions. neural operators have been explored for 2d acoustic (yang et al., 2021; li et al., 2022),
elastic (yang et al., 2023; zhang et al., 2023a), and viscoelastic wave modeling (wei & fu, 2022),
high-frequency wavefield extrapolation (song & wang, 2022), and earthquake locating (sun et al.,
2022). the only documented application by far in 3d seismic wave propagation was conducted by
lehmann et al. (2023), who simulated the ground motions by mapping the depth of the geological
model to the time of the velocity wavefields.
most of the applications have thus-far been limited to 2d, as the amount of gpu memory required
for 3d wave propagation in the time domain can quickly end up in the range of hundreds of
gb. this is because the time domain problem requires a 4d neural operator, and so the memory
requirements grow extremely quickly. one way to address this issue is to solve the wave equations in
the frequency domain, which are also referred to as the helmholtz equations. in this way, the 4d
machine learning model can be simplified to a 3d model. moreover, individual frequencies can be
handled independently, benefitting further from parallelized computing. the time-domain solutions
can be obtained by taking the inverse fourier transform of the frequency-domain solutions.
in this study, our contributions are as follows. we propose a neural operator for 3d elastic wave
propagation and inversion. we train a general solution operator, parameterized as a u-shaped
deep neural operator, to solve the helmholtz equations for variable velocity structures, source
locations, mesh discretizations, and multiple frequencies. we provide options to model either
complete wavefields or ground motions only, with the latter being more accurately predicted via a
graph neural operator (gno) layer (li et al., 2020b). the size of the helmholtz neural operator
is only one-fortieth of that of an equivalent 3d time domain version. the 3d helmholtz neural
operator is roughly 100 times faster than the baseline numerical solvers for forward propagation
and 350 times faster for inverse problems. the trained neural operator combined with automatic
differentiation facilitates rapid full-waveform inversion (fwi) with accuracy measured by relative l2
misfit of 0.03.

2

methods

2.1

elastic wave equations

the time-domain elastic wave equation for inhomogeneous, isotropic media is given by
h
i
âˆ‚2u
Ï 2 =âˆ‡Î» (âˆ‡ Â· u) + âˆ‡Âµ Â· âˆ‡u + (âˆ‡u)t +
âˆ‚t
(Î» + 2Âµ) âˆ‡ (âˆ‡ Â· u) âˆ’ Âµâˆ‡ Ã— âˆ‡ Ã— u + f,

(1)

where Ï is the density, u is the displacement vector, Î» and Âµ are the lamÃ© parameters, and f is
the body force (or source term). equation 1 can be expressed in terms of p-wave velocity vp and
s-wave velocity vs through
s
r
Î» + 2Âµ
Âµ
vp =
, vs =
.
(2)
Ï
Ï
3

the time-dependent wave equation is an initial value problem, which can be solved by explicit or
implicit time-stepping schemes. the fourier transform of the time-domain equations are referred to
as the helmholtz equations:
 


âˆ‚
âˆ‚ux âˆ‚uy
âˆ‚ux
âˆ‚uz
2
Ï‰ Ïux +
Î»
+ 2Âµ
+
+
âˆ‚x
âˆ‚x
âˆ‚y
âˆ‚z
âˆ‚x
 

âˆ‚ux âˆ‚uy
âˆ‚
Âµ
+
+
âˆ‚y
âˆ‚y
âˆ‚x
 

âˆ‚
âˆ‚ux âˆ‚uz
+
Âµ
= âˆ’fx ,
+
âˆ‚z
âˆ‚z
âˆ‚x
 

âˆ‚
âˆ‚ux âˆ‚uy
2
Ï‰ Ïuy +
Âµ
+
âˆ‚x
âˆ‚y
âˆ‚x
 


âˆ‚uy
âˆ‚ux âˆ‚uy
âˆ‚uz
âˆ‚
Î»
+ 2Âµ
+
+
(3)
+
âˆ‚y
âˆ‚x
âˆ‚y
âˆ‚z
âˆ‚y
 

âˆ‚uy
âˆ‚
âˆ‚uz
+
Âµ
+
= âˆ’fy ,
âˆ‚z
âˆ‚z
âˆ‚y
 

âˆ‚ux âˆ‚uz
âˆ‚
Âµ
+
Ï‰ 2 Ïuz +
âˆ‚x
âˆ‚z
âˆ‚x
 

âˆ‚uy
âˆ‚
âˆ‚uz
+
Âµ
+
âˆ‚y
âˆ‚z
âˆ‚y
 


âˆ‚
âˆ‚ux âˆ‚uy
âˆ‚uz
âˆ‚uz
+
Î»
+
+
+ 2Âµ
= âˆ’fz ,
âˆ‚z
âˆ‚x
âˆ‚y
âˆ‚z
âˆ‚z
where Ï‰ is the angular frequency, u = (ux , uy , uz ) is the displacement vector in the frequency
domain, and f = (fx , fy , fz ) is the frequency-domain body force. the wave propagation in the
frequency domain becomes a boundary value problem, which can be expressed in a more compact,
discretized formulation (pratt, 1990):
l (Ï‰) [u (Ï‰)] = âˆ’f (Ï‰) ,

(4)

where l (Ï‰) is an n Ã— n matrix and n is the number of discretization points. in this linear
system, the frequency-domain wavefield u (Ï‰) is related to the source term f (Ï‰) via a sparse matrix
l (Ï‰). the solution to equation 4 can be obtained using either direct or iterative solvers. direct
solvers, such as lower-upper (lu) decomposition or cholesky factorization, suffer from intensive
memory requirements and prohibitive computational time for large linear systems. iterative solvers
heavily rely on a well-designed preconditioner to prevent divergence or slow convergence (huang &
greenhalgh, 2021).

2.2

background on neural operators

neural operators (li et al., 2020b) are a class of models composed of linear integral operators and
nonlinear activations. such models are universal approximators of arbitrary nonlinear continuous
operators (kovachki et al., 2023). let la be a linear differential operator determined by parameter
a and consider the following pde:
(la u) (x) = f (x) , x âˆˆ d,
4

(5)

where u is the pde solution, f is a fixed function such as the source term in a wave equation, and
d âŠ‚ rd is a bounded, open set. the greenâ€™s function g is defined as the unique solution to
la g (x, Â·) = Î´x ,

(6)

where Î´x is the dirac delta function centered at x. note that g depends on a, so the solution to
equation 5 is given by
z
u (x) =

ga (x, y) f (y) dy.

(7)

d

in seismology, equation 7 remains a linear system when the velocity structure, a(x) is fixed, and
describes the mapping from the input source function f (x) to the output wavefield solution u (x).
this study considers a more complicated problem, however, in which we aim to obtain a solution
operator that maps from a(x) to u(x). this operator becomes nonlinear and is not known in closed
form; here we instead aim to approximate it with a learnable parametric model, i.e. a neural operator.
these neural operators incorporate a nonlinear point-wise activation function following each linear
integral operator. a neural operator containing l layers is formulated with an iterative architecture
in the following manner:
v0 (x) = (p a) (x) ,


z
Îºl (x, y) vl (y) dy ,
vl+1 (x) = Ïƒ wl vl (x) +

(8)

d

u (x) = (q vl ) (x) , l = 0, .., l âˆ’ 1,
where a (x) is the input function(s) (e.g., velocity and source functions), u (x) is the output function(s)
(e.g., displacement wavefields), vl is the hidden representation of the lth layer and is input to the
next layer, p is a point-wise operator lifting the input to a higher dimensional representation, q is a
point-wise operator projecting the last hidden representation to the output dimensionality, wl is
a point-wise operator included to learn nonperiodic behavior on the boundaries of the domain, Îºl
is a parametric kernel function, and Ïƒ is a point-wise nonlinear activation. in order to speed up
computations, li et al. (2020a) proposed the fourier neural operator (fno) which replaces the
kernel integral operator with a convolution operator defined in the fourier space:
z
Îºl (x, y) vl (y) dy = f âˆ’1 (f (Îºl ) Â· f (vl )) ,
(9)
d

where f and f âˆ’1 denote the fourier transform and inverse fourier transform, respectively.
since the development of fno, other neural operator models have been developed to improve upon
it further. inspired by the u-net (ronneberger et al., 2015), rahman et al. (2022) designed a
u-shaped neural operator (u-no) that allows for much deeper models by progressive contraction and
expansion of the physical domain and co-domain. these deeper models achieve better performance
and memory usage, and are the choice used in this study. specifically, we use a 6-layer u-no taking
the fno as the inner integral operator, as illustrated in fig. 1.
note that in this study the u-no learns the solution in the frequency domain, mainly to break
the memory bottleneck. for a 3d problem, the solution to the time-domain wave equation has a
form of u(x, y, z, t). this requires a neural operator with four dimensions, which will be significantly
5

memory demanding. however, the solution to the frequency-domain wave equation (helmholtz
equation) can be divided by frequencies. each individual frequency has its independent solution in
the form of uÏ‰ (x, y, z), so that a 3d neural operator is qualified. for the 3d problems in this study,
an equivalent time-domain neural operator can take up 215 gb only for model parameters. but
with the proposed helmholtz neural operator, the model size can be reduced by a factor of 40. in
addition to saving model memory, this formulation enables parallelization at the frequency level to
accelerate model training and reduce data memory.

2.3

automatic differentiation

automatic differentiation (ad) exploits the fact that all numerical computations can be seen as a
sequence of elementary operations with known derivatives, and by applying the chain rule to the
derivatives of the constituent operations we can obtain the derivative of the overall composition
(griewank & walther, 2008). ad falls between numerical differentiation and symbolic differentiation.
it differentiates common functions and expressions using a symbolic differentiation approach and
populates the obtained derivatives with numerical values, saving the intermediate results. these
saved results are then combined to obtain the final desired value. ad is compatible not only with
closed-form expressions but also with control flow structures such as loops, branching, recursion,
and procedure calls, because they will all be translated into numeric evaluation traces which form
the basis of ad (baydin et al., 2018).
there are two modes of automatic differentiation, the forward mode and the reverse mode. in the
forward mode, derivatives of all dependent variables with respect to a certain independent variable
are computed simultaneously with the forward propagation of the computational graph. hence,
the forward accumulation is more efficient in scenarios where the number of independent variables
is significantly smaller than the number of dependent variables. however, in machine learning
practice, the primary focus revolves around computing the gradients of a scalar-valued objective
function with respect to a large number of parameters. this establishes the reverse-mode automatic
differentiation as the cornerstone of the backpropagation algorithm. as opposed to the forward mode,
the reverse-mode ad is a two-phase process. firstly, a forward pass of the code is implemented
to populate the intermediate variables and record the dependencies in the computational graph.
secondly, gradients are computed by propagating the derivatives back from the objective function to
the parameters of interest using the chain rule. these gradients are later employed by a gradient
descent method to iteratively update the parameters of interest, aiming to minimize the objective
function.

3

data

in this study, we train neural operators with supervised learning, i.e. a set of previously computed
numerical simulations are used to learn from. we use a sem software package named salvus
(afanasiev et al., 2019) to generate 31000 simulations for both 2d and 3d data sets, hereafter
referred to as the ground truth. within each simulation data set, 27000 simulations are used for
neural operator training, 3000 simulations are used to validate the model hyperparameters, and
1000 simulations are used to test the generalizability of the trained models. to further demonstrate
the generalization ability, a 3d overthrust velocity model depicting complex thrusting overlying an
6

extensional and rift sequence (aminzadeh et al., 1994) is used for u-no-based waveform modeling
and full-waveform inversion.
for the sem solver, the computational domain is 5 km Ã— 5 km (Ã— 5 km). we take one element per
wavelength at the maximum frequency and a polynomial degree of 4, as recommended in the salvus
documentation. this combination should provide sufficient grid points to sample the domain. the
time step for numerical simulation is set to 0.002 s to satisfy the courantâ€“friedrichsâ€“lewy (cfl)
(courant et al., 1967) condition. the displacement wavefield is excited with a ricker wavelet with a
central frequency of 3 hz. the spatial component of this source is a delta function, which in practice
is approximated by a narrow gaussian for differentiability. the ricker source is configured as an
isotropic explosive source randomly placed within the computational domain. the s-wave velocity
(vs ) models are generated from a background of 3 km/s perturbed by von kÃ¡rmÃ¡n-type random
fields (von karman, 1948) with a hurst exponent of 0.5, correlation length of 8 grid cells, and a
fractional magnitude of the fluctuation set to 10%. several authors have shown that the von kÃ¡rmÃ¡n
correlation function can represent the earthâ€™s inhomogeneities(chemingui, 2001; mai & beroza,
2002; nakata & beroza, 2015). the p- to s-wave velocity ratio (vp /vs ) models are generated from
a background of 1.732 perturbed by gaussian random fields with a correlation length of 32 grid cells
and standard deviation of 2%. the p-wave velocity (vp ) models are obtained by multiplying vs
by vp /vs and the density is set to a constant 2.7 g/cmÂ³. the free-surface condition is set for the
top boundary, while absorbing boundary conditions (clayton & engquist, 1977) are configured for
the other five sides in 3d or three edges in 2d. displacement wavefields with a total duration of 2.5
s are simulated in both 2d and 3d. each 2d simulation takes about 1.6 s using one gpu with a
memory usage of 0.3 gb and each 3d simulation takes about 30 s using one gpu with a memory
usage of 0.8 gb.
the source-time functions and solutions to the elastic wave equations from salvus are fourier
transformed for use with u-no. as our model computes a solution for a single frequency component,
the number of training samples available for learning is the number of simulations in time multiplied
by the number of frequencies of interest. alternatively, if a helmholtz solver is available, it would be
possible to directly generate solutions for a desired frequency and avoid this extra computational
overhead. the input to u-no consists of only spatial functions, comprising vp , vs , the complexvalued source term indicating the source location, and the frequency desired given as a spatiallyconstant function. the output of u-no is either a 2d or 3d complex-valued displacement wavefield
that corresponds to the given frequency input. by evaluating the forward model for multiple
frequencies, we can return to the time domain by taking the inverse fourier transform. in this
formulation, frequencies of interest can be tackled by u-no in parallel, making the problem highly
efficient for use with gpus. both the input and output data are standardized using the statistics of
training data for basic pre-processing.
note that neural operators are not required to adhere to the spatial and temporal discretization
schemes of the numerical solver. for computational efficiency, the time step for u-no is set to 0.05
s, with the nyquist-shannon sampling theorem taken into account. the spatial discretization adopts
a regular mesh of 64 Ã— 64 (Ã— 64) covering the same physical domain as the sem solver. to improve
computational efficiency further, frequencies with little energy in the power density spectrum of the
wavefield are eliminated. the significant frequencies range from 1.6 hz to 6 hz with an interval of
0.4 hz determined by the 2.5-s duration, which are able to reconstruct the time-domain waveform.

7

thus by evaluating the solution in the fourier domain, we can gain performance advantages by not
computing solutions for unnecessary frequencies.

4

results

4.1

2d elastic wave modeling with u-no

the 2d u-no has 65,652,068 trainable parameters taking up 0.24 gb. the model architecture,
depicted in the methods section, is determined through trial and error based on the validation
performance. the other hyperparameters are also optimized with the validation set. to strike a
balance between accuracy and robustness, we adopt a loss function comprising the relative l1 norm
with a weight of 0.9 plus the relative l2 norm with a weight of 0.1. a batch size of 32 is used for
neural operator training. we employ an adam (adaptive moment estimation) (kingma & ba, 2014)
optimizer with a learning rate of 0.001 and a learning rate scheduler that decays the learning rate
by half every 30 epochs. the machine learning model is trained for 100 epochs until the validation
set converges well, as shown in fig. 2. each epoch takes around 15 minutes using eight nvidia
rtx a6000 gpus, each with an average memory usage of 1.3 gb. note that the model can be
trained with just a single gpu, but here we want to take advantage of pytorchâ€™s parallel computing
capability, especially for the many frequencies that can be handled independently. once the u-no is
trained, one run of the 2d elastic wave modeling with new velocity parameters and source locations
for all the frequencies of interest only takes 0.03 s using one gpu with a memory usage of 0.7 gb,
achieving a 53-fold acceleration in comparison to the sem solver.
fig. 2 shows that the u-no is able to fit the training data well and has an impressive predictive
performance on the validation set after hyperparameter optimization. however, the generalization
ability of the trained u-no should be evaluated on a separate test set excluded from either model
parameter training or hyperparameter optimization. the test set consists of 1000 simulations in the
time domain, which after the fourier transform becomes 12000 input-output pairs in the frequency
domain. the trained u-no achieves a relative test loss of 0.116 in the frequency domain. because
the relative loss tends to be dominated by small amplitudes, frequencies with less energy can be
overweighted. for a fairer evaluation, the predicted results are also assessed in the time domain
through cross-correlation with the ground truth simulated by the sem. the correlation coefficient is
more robust to small errors in fourier phase. fig. 3 shows the distribution of correlation coefficients
between the 2d u-no predictions and ground truth in the time domain for the test set. the 2d
predictive performance is excellent with a mean correlation coefficient of 0.994, and all of the 1000
simulations have correlation coefficients over 0.98.
we can also evaluate the performance of u-no at a frequency level. fig. 4 shows 2d helmholtz
solutions with u-no for displacement fields of 2 hz, 4 hz, and 6 hz for an instance from the test
set. the relative losses for these cases are 0.060, 0.077, and 0.126, respectively. we only show the
real parts of the wavefields here for brevity, and the imaginary parts are given in supplementary
materials (fig. s1). the u-no predictive accuracy depends on the given frequency and intuitively,
higher frequencies are harder to learn. this will be discussed in detail in the following section. for
the time-domain evaluation, this example demonstrates the excellent predictive capability of neural
operators for seismic wave modeling with a correlation coefficient of 0.997 (fig. 5). the success
in 2d serves as a stepping-stone towards the exciting application of neural operators for 3d elastic
8

wave modeling.

4.2

3d elastic wave modeling with u-no

through hyperparameter optimization, the 3d model has 1,453,400,214 trainable parameters taking
up 5.4 gb. each epoch of model training takes about 3.7 hours using eight nvidia rtx a6000
gpus, using an average of 16 gb memory during the training process after including the data and
other overhead. the 3d loss curves during training are given in fig. 6, which demonstrate similar
training behavior to the 2d model. it is necessary to mention that the learning rate scheduler has
an appreciable influence on the training performance and can be considered a critical component.
although the training stage is time-consuming, it is only done once. with u-no, one 3d forward
simulation for all the frequencies of interest (explained in the data section) takes 0.3 s with one
gpu, while using 20 gb of gpu memory. this is about 100 times faster than the classic sem
and allows for more computationally efficient 3d fwi. the relatively heavy gpu memory usage
arises from the fact that we evaluate the helmholtz solutions for the full set of frequencies desired in
parallel, which contrasts with the time stepping scheme of the sem that only processes one time
step at a time. one can conserve memory by solving for the helmholtz solutions sequentially, but
this comes at the expense of increased processing time.
in addition to the random velocity fields, we incorporate an overthrust model commonly used in
exploration studies (aminzadeh et al., 1994) into the generalization test for the 3d u-no. 1000
random subpanels are extracted from the 3d overthrust model and a perturbation range of 30% is
imposed on an average vs of 3 km/s. the vp models are calculated from the vs models multiplied
by 1.732. the density is set to 2.7 g/cmÂ³ and the source is randomly placed in the computational
domain for both test sets. the overall performance of the 3d u-no on the test set of random
velocity fields is 0.176 measured by the relative loss in the frequency domain, and 0.238 on the test
set of overthrust models. fig. 7 shows the 3d predicted results for the real parts of 2-, 4-, and 6-hz
wavefields for a subpanel from the overthrust model. the imaginary parts are given in supplementary
materials (fig. s2). fig. 8 illustrates the reconstructed wavefields for several snapshots in time, with
the corresponding ground truth. it is hard to distinguish between the u-no-predicted waveforms
and sem-simulated waveforms by eye. importantly, the neural operator learns the diffraction effects
at the sharp discontinuities. figs s3, s4 and s5 in supplementary materials show the predicted
results for an instance of random velocity fields in the frequency and time domains, respectively.
for the 1000 simulations in each test data set, the 3d u-no achieves a mean correlation coefficient
of 0.986 for random velocity fields and 0.971 for random subpanels from the overthrust model, as
shown in fig. 9. the higher standard deviation observed in the overthrust case could be in part
attributed to the modelâ€™s complex heterogeneities. in general, the evaluation metrics demonstrate
the promising generalization power of u-no for real-world applications.

4.3

full-waveform inversion with automatic differentiation

full-waveform inversion (fwi) is a prime beneficiary of the accelerated seismic wave simulations,
as it uses the complete information of recorded waveforms to infer subsurface physical parameters
sampled by seismic waves. the adjoint-state method is conventionally used for fwi, which requires
derivation of the gradient of the objective function with respect to each parameter of interest
(plessix, 2006). a counterpart to the adjoint-state method has been discovered in the deep learning
9

community, known as automatic differentiation (ad). it has been shown that these two methods are
mathematically equivalent (lecun et al., 1988; zhu et al., 2021). the advantage of ad over the
adjoint-state method is that it automatically computes gradients for any desired parameter based
on the computational graph using the chain rule, integrating a wide range of inverse problems in
a unified framework. neural operators are inherently designed to be compatible with automatic
differentiation. in performing fwi, we freeze the u-no model parameters and instead treat the
velocity parameters as the objects of training. the objective function is the same as used for training
the u-no model, except we add a regularization term that encourages model smoothness using the
laplacian operator. we take the overthrust velocity models used in fig. 8 for the fwi experiment.
the â€œobservationsâ€ are wavefields simulated with salvus for 30 events evenly distributed on the
surface of a sphere with a radius of 1.5 km (fig. 10). receivers are placed at each grid point of the
64Ã—64Ã—64 mesh. the vp and vs models are simultaneously updated, starting from homogeneous
initial values of 5 km/s and 3 km/s, respectively. after conducting 10 epochs of training using an
adam optimizer with a learning rate of 0.03 and a batch size of 16, we achieve vp and vs models
with relative l2 misfit of 0.03, as depicted in fig. 10.
this fwi experiment validates the effectiveness of automatic differentiation as a substitute for the
adjoint-state method, and further showcases the accuracy of 3d elastic wave modeling with u-no.
we also conduct a similar experiment with receivers only on the surface (fig. 11). in plotting the
results, we mask out the regions with poor ray coverage as they are difficult to infer. the regions
with sufficient ray density still obtain satisfactory inversion accuracy, with relative l2 misfit of 0.03.
for one event and one tomographic iteration, the u-no-based fwi takes 0.4 s (including the forward
propagation) using one gpu, while the sem with the adjoint method takes 140 s under the same
settings. this translates to a speed-up of 350 times.

4.4

generalization to denser input/output meshes

while classic neural networks are mesh-dependent, neural operators are discretization-invariant in
physical space and can be evaluated on denser input and output meshes after training has concluded
(li et al., 2020a). one of the most valuable aspects about this is that the training simulations do not
need to be performed at the target resolution, which may be too computationally expensive. here,
we test this critical property of neural operators by applying the u-no trained on a 64Ã—64Ã—64 mesh
to a 128Ã—128Ã—64 mesh with finer spacing in the horizontal plane. both the input velocity models
and the output displacement wavefields are evaluated on denser grids. we conduct 100 experiments
using random fields to get an average correlation coefficient of 0.971. fig. 12 shows one of the
generalization experiments, with the ground motion plotted as an example. it can be seen that the
u-no can effectively generalize to denser discretization without additional training. for the 3d
super-resolution test, the neural operator has a further speed-up of 144 times over the sem solver in
forward propagation.

4.5

predicting wavefields at only the free surface

thus far, we have employed the machine learning model to simulate wavefields over the full spatial
domain. however, in most real-world cases we will only have access to waveform data at/near the
surface. in the framework of supervised learning, it is technically feasible to directly map from
elastic properties to surface ground motions directly at (irregular) receiver points. under many
10

circumstances, this could be more memory- and computationally-efficient. to achieve this, the model
architecture needs to be slightly modified for generalizability. we add a gno (li et al., 2020b)
layer to the top of the u-no architecture to query the ground motions at the desired points in
3d space. gno layers are evalulated using message passing graph neural networks (gilmer et al.,
2017). they incorporate positional information into edge features, allowing for the querying of
solutions at arbitrary points on arbitrary discretizations, not limited to a planar free surface. fig.
13(a) gives the model architecture for predicting wavefields on the free surface, where the gno
employs a kernel function Îºl parameterized as a three-layer neural network. Îºl takes the positional
information and the output of the u-no as input. for the gno, we let messages pass across the
whole computational domain to the free surface through a linear integral. the remaining part of
the model adheres to the u-no architecture in fig. 1. we compare the predictive accuracy for
3-component surface wavefields using the plain u-no and the gno-embedded model. for the plain
u-no, the ground motions are output over the whole domain but evaluated strictly at the surface.
we calculate the correlation coefficients for 1000 random velocity models unseen in training, as
shown in fig. 13(b). incorporating the gno systematically improves surface wavefield predictions
while also being more data-efficient. we re-emphasize that a gno layer can query arbitrary points
on arbitrary discretizations, not just the free surface. this is crucial for future applications having
an irregular geometry of seismic stations at variable elevation.

5

discussion

excluding the dataset preparation, the most computationally expensive part of the proposed method
is the model training process. optimizing the model architecture and hyperparameters is quite time
consuming and tedious. rather than documenting every detail of this trial-and-error process, we
summarize the key findings here:
1. we find that skip connections play an important role in the predictive accuracy of u-no. they
also contribute to alleviating vanishing gradient issues.
2. expanding the size of the co-domain can improve the u-no performance, but at the cost of
higher memory usage.
3. the number of fourier modes in each layer is a sensitive hyperparameter and should be tuned
carefully. in seismic wave propagation, the elastic media exhibit complex heterogeneities that
amplify high-frequency components in the resulting wavefields. therefore, we retain all the
fourier modes in respective dimensions.
4. as with many deep learning studies, the batch size has an appreciable effect on the performance.
5. learning rate scheduling is very effective, which reduces the learning rate as training progresses.
6. lastly, in the specific setting described in this paper, neural operators are very data hungry. we
use 27000 simulations (which make âˆ¼300000 frequency-domain instances) to train each u-no.
the belief is that the increased data volume will enhance accuracy, but this also translates
into additional time taken up by numerical simulations and u-no training.
the frequencies in this study are discrete values determined by the fast fourier transform, as we
use a time-domain solver to generate data. if a helmholtz solver is available, the frequencies can
11

be randomly picked from the real number line r for training and not restricted to a discrete set.
within a predefined frequency band [fmin , fmax ], a well-trained neural operator can be queried at
arbitrary frequencies for helmholtz solutions, because it learns mappings between function spaces.
this adapts well to the band-limited characteristics of real seismic data. an additional advantage of
the proposed method over a classical time-domain solver emerges when extending the simulation
duration. the time consumed by a conventional solver following the time-stepping scheme increases
linearly with the simulation duration, theoretically. however, because extending duration translates
to querying intermediate frequencies, the increased run time for the helmholtz neural operator can
be alleviated by parallelization.
in nearly all numerical methods, higher frequencies correspond to shorter wavelengths, necessitating
finer gridding. therefore, for any given same mesh, lower accuracy is expected for higher frequencies.
this holds true for the machine learning method as well. we compute the relative loss with respect
to each frequency of interest for the 2d test set, as shown in fig. 14. it is evident that the loss
monotonously goes up with increasing frequencies. similar behavior is observed in the training set
and 3d cases. while this arises naturally in part from the physics, it can also be partially attributed
to the tendency of neural networks to generate smooth output to enhance generalizability (neal
et al., 2018). to reconcile the biased learning of neural operators towards low frequencies, future
work can focus on exploring sampling methods, improved positional encoding, and weighted training
(li et al., 2021a; zhao et al., 2021), among other potential approaches.
this study serves as a toolbox for applying neural operators to real data applications. in a realistic
scenario, the physical domain could be larger, which places greater demands on computing efficiency.
one way to reduce memory requirements and accelerate model training is to employ mixed precision
training for neural operators (white et al., 2023). for now, our machine learning model still depends
on a separate numerical solver, but a physics-informed version of neural operators (li et al., 2021b)
has the potential to free the machine learning model from any external solver. in this study, we have
given particular emphasis to the discretization invariance of neural operators and demonstrated it
with an example of denser grids, but neural operators can also deal with irregular grids. through a
gno layer, local predictions at arbitrary points (e.g., where there are seismic stations) can be queried
and enhanced. for now, we have still kept the source as isotropic, but for earthquake scenarios,
future work will involve parameterizing the source as different types of moment tensors.
ultimately, we aim to provide a few models pretrained to high accuracy with very large datasets that
can be shared as community resources. these models would be applicable to any regions on earth
as long as the study area fits within the constraints of the physical domain for which the model was
trained. the training cost would therefore be a one time upfront expense, as future downstream
users would not need to repeat this process.

6

conclusions

we show that elastic wave modeling in 2d and 3d can be accelerated by two orders of magnitude
with u-shaped neural operators. the prohibitive expense of memory-intensive models required
in 3d is alleviated by modeling the seismic wave propagation in the frequency domain, due to the
fact that multiple frequencies can be handled separately by parallel computing. we demonstrate
the generalizability of the once-trained neural operator to variable velocity structures with random
12

source locations, which is based on the theory that random fields can approximate most physical
functions with arbitrary accuracy. the overall generalization accuracy is above 0.97 evaluated by
cross-correlation coefficients, taking simulations with a spectral element method as ground truth.
this sheds some light on the potential of the proposed method in real-world applications. we also
show that neural operators can generalize to denser discretization without additional training. an
additional gno layer greatly improves the waveform predictions on the free surface. moreover,
we incorporate the trained model with automatic differentiation to facilitate rapid full-waveform
inversion for velocity structures. the 3d inversion process can be accelerated by a factor of 350 in
comparison to the sem with the adjoint-state method, with accuracy measured by relative l2 misfit
of 0.03. while being mathematically equivalent to the adjoint method, automatic differentiation can
integrate a wide range of inverse problems in a unified framework without manual derivation.

acknowledgements
zer thanks the david and lucile packard foundation for supporting this study through a packard
fellowship.

references
afanasiev, m., boehm, c., van driel, m., krischer, l., rietmann, m., may, d. a., knepley, m. g.,
& fichtner, a., 2019. modular and flexible spectral-element waveform modelling in two and three
dimensions, geophysical journal international , 216(3), 1675â€“1692.
alkhalifah, t., song, c., bin waheed, u., & hao, q., 2021. wavefield solutions from machine learned
functions constrained by the helmholtz equation, artificial intelligence in geosciences, 2, 11â€“19.
aminzadeh, f., burkhard, n., nicoletis, l., rocca, f., & wyatt, k., 1994. seg/eaeg 3-d modeling
project: 2nd update, the leading edge, 13(9), 949â€“952.
baydin, a. g., pearlmutter, b. a., radul, a. a., & siskind, j. m., 2018. automatic differentiation
in machine learning: a survey, journal of marchine learning research, 18, 1â€“43.
chemingui, n., 2001. modeling 3-d anisotropic fractal media, stanford exploration project, report,
80, 1â€“586.
clayton, r. & engquist, b., 1977. absorbing boundary conditions for acoustic and elastic wave
equations, bulletin of the seismological society of america, 67(6), 1529â€“1540.
courant, r., friedrichs, k., & lewy, h., 1967. on the partial difference equations of mathematical
physics, ibm journal of research and development, 11(2), 215â€“234.
de basabe, j. d. & sen, m. k., 2009. new developments in the finite-element method for seismic
modeling, the leading edge, 28(5), 562â€“567.
fichtner, a., igel, h., bunge, h.-p., & kennett, b. l., 2009. simulation and inversion of seismic
wave propagation on continental scales based on a spectral-element method, journal of numerical
analysis, industrial and applied mathematics, 4(1-2), 11â€“22.
13

gilmer, j., schoenholz, s. s., riley, p. f., vinyals, o., & dahl, g. e., 2017. neural message passing
for quantum chemistry, in international conference on machine learning, pp. 1263â€“1272, pmlr.
griewank, a. & walther, a., 2008. evaluating derivatives: principles and techniques of algorithmic
differentiation, siam.
guo, x., li, w., & iorio, f., 2016. convolutional neural networks for steady flow approximation, in
proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data
mining, pp. 481â€“490.
hornik, k., stinchcombe, m., & white, h., 1989. multilayer feedforward networks are universal
approximators, neural networks, 2(5), 359â€“366.
huang, x. & alkhalifah, t., 2022a. pinnup: robust neural network wavefield solutions using
frequency upscaling and neuron splitting, journal of geophysical research: solid earth, 127(6),
e2021jb023703.
huang, x. & alkhalifah, t., 2022b. single reference frequency loss for multifrequency wavefield
representation using physics-informed neural networks, ieee geoscience and remote sensing
letters, 19, 1â€“5.
huang, x. & alkhalifah, t., 2023. microseismic source imaging using physics-informed neural
networks with hard constraints, arxiv preprint arxiv:2304.04315 .
huang, x. & greenhalgh, s., 2021. a finite-difference iterative solver of the helmholtz equation
for frequency-domain seismic wave modeling and full-waveform inversion, geophysics, 86(2),
t107â€“t116.
kelly, k. r., ward, r. w., treitel, s., & alford, r. m., 1976. synthetic seismograms: a finitedifference approach, geophysics, 41(1), 2â€“27.
khoo, y., lu, j., & ying, l., 2021. solving parametric pde problems with artificial neural networks,
european journal of applied mathematics, 32(3), 421â€“435.
kingma, d. p. & ba, j., 2014. adam: a method for stochastic optimization, arxiv preprint
arxiv:1412.6980 .
komatitsch, d., ritsema, j., & tromp, j., 2002. the spectral-element method, beowulf computing,
and global seismology, science, 298(5599), 1737â€“1742.
kovachki, n. b., li, z., liu, b., azizzadenesheli, k., bhattacharya, k., stuart, a. m., & anandkumar,
a., 2023. neural operator: learning maps between function spaces with applications to pdes., j.
mach. learn. res., 24(89), 1â€“97.
lecun, y., touresky, d., hinton, g., & sejnowski, t., 1988. a theoretical framework for backpropagation, in proceedings of the 1988 connectionist models summer school , vol. 1, pp. 21â€“28,
san mateo, ca, usa.
lehmann, f., gatti, f., bertin, m., & clouteau, d., 2023. fourier neural operator surrogate model
to predict 3d seismic waves propagation, arxiv preprint arxiv:2304.10242 .
14

li, b., wang, h., feng, s., yang, x., & lin, y., 2022. solving seismic wave equations on variable
velocity models with fourier neural operator, arxiv preprint arxiv:2209.12340 .
li, z., kovachki, n., azizzadenesheli, k., liu, b., bhattacharya, k., stuart, a., & anandkumar,
a., 2020a. fourier neural operator for parametric partial differential equations, arxiv preprint
arxiv:2010.08895 .
li, z., kovachki, n., azizzadenesheli, k., liu, b., bhattacharya, k., stuart, a., & anandkumar, a.,
2020b. neural operator: graph kernel network for partial differential equations, arxiv preprint
arxiv:2003.03485 .
li, z., liu-schiaffini, m., kovachki, n., liu, b., azizzadenesheli, k., bhattacharya, k., stuart, a.,
& anandkumar, a., 2021a. learning dissipative dynamics in chaotic systems, arxiv preprint
arxiv:2106.06898 .
li, z., zheng, h., kovachki, n., jin, d., chen, h., liu, b., azizzadenesheli, k., & anandkumar, a.,
2021b. physics-informed neural operator for learning partial differential equations, arxiv preprint
arxiv:2111.03794 .
lu, l., jin, p., pang, g., zhang, z., & karniadakis, g. e., 2021. learning nonlinear operators via
deeponet based on the universal approximation theorem of operators, nature machine intelligence,
3(3), 218â€“229.
mai, p. m. & beroza, g. c., 2002. a spatial random field model to characterize complexity in
earthquake slip, journal of geophysical research: solid earth, 107(b11), eseâ€“10.
moseley, b., markham, a., & nissen-meyer, t., 2020a. solving the wave equation with physicsinformed deep learning, arxiv preprint arxiv:2006.11894 .
moseley, b., nissen-meyer, t., & markham, a., 2020b. deep learning for fast simulation of seismic
waves in complex media, solid earth, 11(4), 1527â€“1549.
nakata, n. & beroza, g. c., 2015. stochastic characterization of mesoscale seismic velocity
heterogeneity in long beach, california, geophysical supplements to the monthly notices of the
royal astronomical society, 203(3), 2049â€“2054.
neal, b., mittal, s., baratin, a., tantia, v., scicluna, m., lacoste-julien, s., & mitliagkas, i., 2018.
a modern take on the bias-variance tradeoff in neural networks, arxiv preprint arxiv:1810.08591 .
olsen, k., 2000. site amplification in the los angeles basin from three-dimensional modeling of
ground motion, bulletin of the seismological society of america, 90(6b), s77â€“s94.
plessix, r.-e., 2006. a review of the adjoint-state method for computing the gradient of a functional
with geophysical applications, geophysical journal international , 167(2), 495â€“503.
pratt, r. g., 1990. frequency-domain elastic wave modeling by finite differences: a tool for crosshole
seismic imaging, geophysics, 55(5), 626â€“632.
rahman, m. a., ross, z. e., & azizzadenesheli, k., 2022. u-no: u-shaped neural operators, arxiv
preprint arxiv:2204.11127 .
15

raissi, m., perdikaris, p., & karniadakis, g. e., 2019. physics-informed neural networks: a deep
learning framework for solving forward and inverse problems involving nonlinear partial differential
equations, journal of computational physics, 378, 686â€“707.
rasht-behesht, m., huber, c., shukla, k., & karniadakis, g. e., 2022. physics-informed neural
networks (pinns) for wave propagation and full waveform inversions, journal of geophysical
research: solid earth, 127(5), e2021jb023120.
ren, p., rao, c., chen, s., wang, j.-x., sun, h., & liu, y., 2022. seismicnet: physics-informed neural
networks for seismic wave modeling in semi-infinite domain, arxiv preprint arxiv:2210.14044 .
ronneberger, o., fischer, p., & brox, t., 2015. u-net: convolutional networks for biomedical image
segmentation, in medical image computing and computer-assisted interventionâ€“miccai 2015:
18th international conference, munich, germany, october 5-9, 2015, proceedings, part iii 18 , pp.
234â€“241, springer.
smith, j. d., azizzadenesheli, k., & ross, z. e., 2020. eikonet: solving the eikonal equation
with deep neural networks, ieee transactions on geoscience and remote sensing, 59(12),
10685â€“10696.
smith, j. d., ross, z. e., azizzadenesheli, k., & muir, j. b., 2022. hyposvi: hypocentre inversion with stein variational inference and physics informed neural networks, geophysical journal
international , 228(1), 698â€“710.
song, c. & alkhalifah, t. a., 2021. wavefield reconstruction inversion via physics-informed neural
networks, ieee transactions on geoscience and remote sensing, 60, 1â€“12.
song, c. & wang, y., 2022. high-frequency wavefield extrapolation using the fourier neural operator,
journal of geophysics and engineering, 19(2), 269â€“282.
song, c., alkhalifah, t., & waheed, u. b., 2021. solving the frequency-domain acoustic vti wave
equation using physics-informed neural networks, geophysical journal international , 225(2),
846â€“859.
song, c., alkhalifah, t., & waheed, u. b., 2022. a versatile framework to solve the helmholtz
equation using physics-informed neural networks, geophysical journal international , 228(3),
1750â€“1762.
song, c., liu, y., zhao, p., zhao, t., zou, j., & liu, c., 2023. simulating multicomponent elastic
seismic wavefield using deep learning, ieee geoscience and remote sensing letters, 20, 1â€“5.
sun, h., yang, y., azizzadenesheli, k., clayton, r. w., & ross, z. e., 2022. accelerating time-reversal
imaging with neural operators for real-time earthquake locations, arxiv preprint arxiv:2210.06636 .
von karman, t., 1948. progress in the statistical theory of turbulence, proceedings of the national
academy of sciences, 34(11), 530â€“539.
wei, w. & fu, l.-y., 2022. small-data-driven fast seismic simulations for complex media using
physics-informed fourier neural operators, geophysics, 87(6), t435â€“t446.
16

white, c., tu, r., kossaifi, j., pekhimenko, g., azizzadenesheli, k., & anandkumar, a., 2023.
speeding up fourier neural operators via mixed precision, arxiv preprint arxiv:2307.15034 .
winovich, n., ramani, k., & lin, g., 2019. convpde-uq: convolutional neural networks with
quantified uncertainty for heterogeneous elliptic partial differential equations on varied domains,
journal of computational physics, 394, 263â€“279.
yang, y., gao, a. f., castellanos, j. c., ross, z. e., azizzadenesheli, k., & clayton, r. w., 2021.
seismic wave propagation and inversion with neural operators, the seismic record , 1(3), 126â€“134.
yang, y., gao, a. f., azizzadenesheli, k., clayton, r. w., & ross, z. e., 2023. rapid seismic
waveform modeling and inversion with neural operators, ieee transactions on geoscience and
remote sensing, 61, 1â€“12.
zhang, t., trad, d., & innanen, k., 2023a. learning to solve the elastic wave equation with fourier
neural operators, geophysics, 88(3), t101â€“t119.
zhang, y., zhu, x., & gao, j., 2023b. seismic inversion based on acoustic wave equations using
physics-informed neural network, ieee transactions on geoscience and remote sensing, 61, 1â€“11.
zhao, l., zou, c., chen, y., shen, w., wang, y., chen, h., & geng, j., 2021. fluid and lithofacies
prediction based on integration of well-log data and seismic inversion: a machine-learning approach,
geophysics, 86(4), m151â€“m165.
zhu, w., xu, k., darve, e., & beroza, g. c., 2021. a general approach to seismic inversion with
automatic differentiation, computers & geosciences, 151, 104751.
zhu, y. & zabaras, n., 2018. bayesian deep convolutional encoderâ€“decoder networks for surrogate
modeling and uncertainty quantification, journal of computational physics, 366, 415â€“447.

17

figure 1: u-no architecture. the input a comprises p- and s- wave velocity (vp and vs ), the source
location, and the frequency as a constant function. the output u comprises the frequency-domain
displacement wavefields. p denotes a point-wise lifting operator, q denotes a point-wise projection
operator, and g denotes an fno as the inner integral operator. smaller blue circles denote functionspace concatenations. inside the dotted box is the composition of each fno layer, where v is the
layer input, f is the fourier transform, f âˆ’1 is the inverse fourier transform, r and w are linear
operators, and Ïƒ is the nonlinear activation function.

18

figure 2: loss curves during the 2d u-no training for 100 epochs. the loss function is defined
as the relative l1 norm with a weight of 0.9 plus the relative l2 norm with a weight of 0.1. the
batch size is 32. an adam optimizer is employed with a learning rate of 0.001 and a learning rate
scheduler that decays the learning rate by half every 30 epochs.

19

figure 3: distribution of correlation coefficients between the 2d u-no predictions and ground truth
in the time domain for the test set. the red and black dashed lines indicate the mean and standard
deviation of the histograms.

20

figure 4: 2d elastic wave modeling with u-no evaluated in the frequency domain for an instance
from the test set. the first row displays the vp and vp /vs models with the source location marked
with a white star. the second, third, and fourth rows display the predicted results for the real parts
of displacement fields of 2 hz, 4 hz, and 6 hz, respectively. the relative loss of the u-no prediction
is 0.060 for 2 hz, 0.077 for 4 hz, and 0.126 for 6 hz.

21

figure 5: 2d elastic wave modeling with u-no evaluated in the time domain for an instance from
the test set. the first row displays the vp and vp /vs models with the source location marked with
a white star. the second, third, and fourth rows display the predicted results for displacement fields
at 0.7 s, 1 s, and 1.8 s, respectively. the cross-correlation coefficient between the u-no and sem
simulations is 0.997.

22

figure 6: loss curves during the 3d u-no training for 100 epochs. the loss function is defined
as the relative l1 norm with a weight of 0.9 plus the relative l2 norm with a weight of 0.1. the
batch size is 32. an adam optimizer is employed with a learning rate of 0.001 and a learning rate
scheduler that decays the learning rate by half every 30 epochs.

23

figure 7: 3d elastic wave modeling with u-no evaluated in the frequency domain for an instance of
random subpanels from the overthrust model. the first row displays the vp and vp /vs models with
the source location marked with a red star. the second, third, and fourth rows display the predicted
results for the real parts of displacement fields of 2 hz, 4 hz, and 6 hz, respectively. the relative
loss of the u-no prediction is 0.110 for 2 hz, 0.154 for 4 hz, and 0.241 for 6 hz.

figure 8: 3d elastic wave modeling with u-no evaluated in the time domain for an instance of
random subpanels from the overthrust model. the first row displays the vp and vp /vs models with
the source location marked with a red star. the second, third, and fourth rows display the predicted
results for displacement fields at 0.7 s, 1 s, and 1.5 s, respectively. the cross-correlation coefficient
between the u-no and sem simulations is 0.991.
24

(a)

(b)

figure 9: distribution of correlation coefficients between the 3d u-no predictions and ground truth
in the time domain for (a) random velocity fields from the test set and (b) random subpanels from
the 3d overthrust model. the red and black dashed lines indicate the mean and standard deviation
of the histograms.

figure 10: u-no based full-waveform inversion for a random subpanel from the overthrust model
with automatic differentiation. 30 sources are evenly distributed on the surface of a sphere with a
radius of 1.5 km (red stars) and receivers are configured at every grid point of the 64Ã—64Ã—64 mesh.
the true vs and vp models are plotted in the first row. the inverted velocity models with laplacian
smoothness regularization are plotted in the second row. the relative l2 norm of the misfit between
the true and inverted vs and vp is 0.03.

25

figure 11: u-no based full-waveform inversion for a random subpanel from the overthrust model
with automatic differentiation. 30 sources are evenly distributed on the surface of a sphere with
a radius of 1.5 km (red stars) and 64Ã—64 receivers are configured on the surface (blue area). the
true vs and vp models are plotted in the first row. the inverted velocity models with laplacian
smoothness regularization are plotted in the second row, where parts without ray path coverage are
masked by gray shadows. the relative l2 misfit for the ray-covered parts is 0.03.

26

figure 12: 3d elastic wave modeling on a 128Ã—128Ã—64 mesh with the u-no trained on a 64Ã—64Ã—64
mesh. the first row displays the vp and vp /vs models from random fields, with the source location
marked with a red star. the second, third, and fourth rows display the true and predicted ground
motions at 1.1 s, 1.3 s, and 1.5 s, respectively. the cross-correlation coefficient between the u-no
and sem simulations for full wavefields of this example is 0.977.

27

(a)

(b)

figure 13: (a) model architecture for predicting wavefields at the free surface comprising the u-no
in fig. 1 and a gno that queries the ground motions.(b) distribution of correlation coefficients
between true and predicted 3d ground motions for 1000 random velocity models using the u-no
with (red) and without (cyan) a gno layer.

figure 14: relative loss associated with frequencies for the 2d test set. the training set and 3d
cases exhibit similar behavior.

28

supplementary materials

figure s1: 2d elastic wave modeling with u-no evaluated in the frequency domain for an instance
from the test set. the first row displays the vp and vp /vs models with the source location marked
with a white star. the second, third, and fourth rows display the predicted results for the imaginary
parts of displacement fields of 2 hz, 4 hz, and 6 hz, respectively. the relative loss of the u-no
prediction is 0.060 for 2 hz, 0.077 for 4 hz, and 0.126 for 6 hz.

s1

figure s2: 3d elastic wave modeling with u-no evaluated in the frequency domain for an instance
of random subpanels from the overthrust model. the first row displays the vp and vp /vs models
with the source location marked with a red star. the second, third, and fourth rows display the
predicted results for the imaginary parts of displacement fields of 2 hz, 4 hz, and 6 hz, respectively.
the relative loss of the u-no prediction is 0.110 for 2 hz, 0.154 for 4 hz, and 0.241 for 6 hz.

figure s3: 3d elastic wave modeling with u-no evaluated in the frequency domain for an instance
of random velocity fields. the first row displays the vp and vp /vs models with the source location
marked with a red star. the second, third, and fourth rows display the predicted results for the real
parts of displacement fields of 2 hz, 4 hz, and 6 hz, respectively. the relative loss of the u-no
prediction is 0.096 for 2 hz, 0.149 for 4 hz, and 0.238 for 6 hz.
s2

figure s4: 3d elastic wave modeling with u-no evaluated in the frequency domain for an instance
of random velocity fields. the first row displays the vp and vp /vs models with the source location
marked with a red star. the second, third, and fourth rows display the predicted results for the
imaginary parts of displacement fields of 2 hz, 4 hz, and 6 hz, respectively. the relative loss of the
u-no prediction is 0.096 for 2 hz, 0.149 for 4 hz, and 0.238 for 6 hz.

figure s5: 3d elastic wave modeling with u-no evaluated in the time domain for an instance of
random velocity fields. the first row displays the vp and vp /vs models with the source location
marked with a red star. the second, third, and fourth rows display the predicted results for
displacement fields at 0.7 s, 1 s, and 1.5 s, respectively. the cross-correlation coefficient between the
u-no and sem simulations is 0.989.
s3



ieee transactions on geoscience and remote sensing, vol. 61, 2023

5919212

memory optimization in rnn-based full
waveform inversion using boundary
saving wavefield reconstruction
shaowen wang , graduate student member, ieee, yong jiang, peng song , jun tan ,
zhaolun liu, and bingshou he
abstractâ€” in wave equation modeling, wavefields propagating
over time can be regarded as feedforward in a recurrent neural
network (rnn). therefore, the seismic inversion problem based
on partial differential wave equations can be addressed using
automatic differentiation in the state-of-the-art deep learning
frameworks, eliminating the need for explicit backpropagating
the residual wavefield. however, one challenge that arises in the
context of automatic differentiation is the significant memory
usage due to the necessity of storing the hidden states of the rnn
(i.e., wavefields in seismic modeling) during forward computation
for constructing the computational graph and computing the
derivatives during backpropagation. this memory overhead can
become a bottleneck, particularly when dealing with large-scale
inversion problems. to mitigate this issue, we propose an effective
boundary-saving strategy that allows for the reconstruction of
the computational graph during the backpropagation process.
instead of storing all the intermediate wavefields at each time
step, we selectively save the necessary information at the boundaries, thereby significantly reducing the memory footprint. this
approach enables us to maintain the convenience and efficiency
of automatic differentiation computations while minimizing the
memory requirements. both 2-d and 3-d numerical experiments
validate the accurate reconstruction of wavefields with minimal
loss in precision, while the computational graph is simultaneously reconstructed. consequently, the gradients can also be
calculated correctly by automatic differentiation with minimal
cpu/graphics processing unit (gpu) memory occupation.
index termsâ€” automatic differentiation, boundary saving, full
waveform inversion.

f

i. i ntroduction
ull waveform inversion (fwi) aims to find a highresolution representation of the subsurface by iteratively

manuscript received 8 august 2023; revised 27 august 2023; accepted
16 september 2023. date of publication 20 september 2023; date of current
version 4 october 2023. this work was supported in part by wenhai program
of the st fund of laoshan laboratory (lskj) under grant lskj202204803,
in part by the national natural science foundation of china under grant
42074138, in part by the china national offshore oil corporation (cnooc)
under grant ccl2023xhps001em, grant kjgg-2022-0104, and grant
ccl2022rcps0513rcn, and in part by the open fund project of no. 1
geological team of shanong provincial bureau of geology and mineral
resources under grant 2022dy03. (corresponding author: peng song.)
shaowen wang is with the college of geoscience, ocean university of
china, qingdao 26600, china (e-mail: shaowinw@163.com).
yong jiang is with the cnooc ltd., shanghai 200050, china (e-mail:
jiangyong@cnooc.com.cn).
peng song, jun tan, zhaolun liu, and bingshou he are with the college of geoscience, ocean university of china, qingdao 266100, china,
also with the laoshan laboratory, qingdao 266200, china, and also with
the key laboratory of submarine geosciences and prospecting techniques
(ministry of education), qingdao 266100, china (e-mail: pengs@ouc.edu.cn;
tanjun0532@ouc.edu.cn; zhaolun.liu@ouc.edu.cn; hebinshou@ouc.edu.cn).
digital object identifier 10.1109/tgrs.2023.3317529

optimizing the model parameters until the modeled waveforms
best match the observed waveforms [1], [2]. to perform
waveform modeling using computers, particularly graphics
processing units (gpus), subsurface models are typically
discretized using a grid representation. when applying a finite
difference time domain (fdtd) method to discrete the wave
equations, the partial differentiation of the wavefields with
respect to the spatial coordinates can be implemented by
convolving a kernel with the wavefield, which is a typical
technique employed in convolutional neural networks (cnn).
the temporal differencing scheme of the wave equation
enables the iterative update of the wavefield, just like the
hidden states updated in a recurrent neural network (rnn).
consequently, the seismic forward modeling can be viewed
as wavefields propagating through rnn [3], [4], [5], [6].
when representing the forward modeling of wave equations
using an rnn, we can leverage the automatic differentiation
capabilities offered by state-of-the-art deep learning frameworks like pytorch to implement fsi without the need for
explicit backpropagation of the adjoint source. the gradients
calculated by automatic differentiation are consistent with
the adjoint method [7]. in rnn-fwi, the velocity model
building in traditional fsi transformed to training the parameters of an rnn. the rnn-fwi framework can be easily
incorporated with other neural networks to use more complicated loss functions and regularizations for better inversion
results [8].
the grid models, such as compressional wave velocity (vp ),
shear wave velocity (vs ), and density are considered as
trainable parameters in the rnn-fwi framework. however,
in the pure (default) automatic differentiation mode, fwirnn requires large gpu/cpu resources to store the hidden
states (i.e., the complete wavefields in fwi-rnn) for gradient calculations when backpropagating the loss between the
predicted data and the observed data [5]. it should be noted
that the advantages of fwi-rnn diminish when there are
multiple hidden states to store or when the wavefields are
large and require extensive storage capacity, particularly in 3-d
cases.
boundary-saving strategies [9], [10], [11] have been widely
used in traditional seismic modeling algorithms based on
finite difference methods, such as reverse time migration (rtm) [12], [13] and fwi [14], [15]. these strategies
selectively save the necessary values of the wavefields
and reconstruct the whole snapshot during backpropagating,

1558-0644 Â© 2023 ieee. personal use is permitted, but republication/redistribution requires ieee permission.
see https://www.ieee.org/publications/rights/index.html for more information.
authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

5919212

fig. 1.

ieee transactions on geoscience and remote sensing, vol. 61, 2023

rnn (a) and cell (b) of the forward rnn.

where the adjoint source is propagated to calculate the correlation of the adjoint wavefield and forward wavefield.
in this study, we first analyzed the feasibility of boundarysaving strategies in fwi-rnn. then we provided a detailed
description of the effective boundary-saving-based automatic
differentiation and compared it with the pure version of
automatic differentiation. finally, we show some results of the
proposed method in rnn-based acoustic fwi with two 2-d
benchmark models and a 3-d model.
ii. m ethod
a. forward modeling
the second-order acoustic wave partial equation can be
written as:
âˆ‚u (x)/âˆ‚t = v(x) âˆ‡ u(x) + s(x0 , t0 )
2

2

2

2

(1)

where u is the pressure wavefield, x is spatial coordinates,
v(x) is the velocity field at location x, and s(x0 , t0 ) represents
the source excited at location x0 and time t0 .
discretizing (1) using the finite difference method with a
second-order in time and 2n th-order in spatial scheme, we can
derive the following equation:
u t+1 (x) = v 2 (x)1t 2 u t (x) âˆ— k + 2u t (x) âˆ’ u tâˆ’1 (x) + s(x0 , t0 )
(2)
where the spatial derivatives are expressed as convolutions, âˆ—
is the convolutional operator, k is the convolutional kernel
with size (2n + 1) Ã— (2n + 1) where 2n represents the
differential order and the values at the cross-directions in
k are the differential coefficients obtained from the taylor
expansion.
in rnn, a cell represents the minimum computation unit
applied to the trainable parameters. the forward propagation
of the wavefields is same at each timestep, so a simpler version
of (2) can be formulated as:
ut+1 = cell(ut , utâˆ’1 , st ; m)

(3)

where cell(âˆ—) means a basic unit of rnn and maintains hidden
states (i.e., u t ).

fig. 1(a) shows the forward modeling rnn which takes
wavelets as inputs and outputs seismic records. fig. 1(b) shows
a cell of (a) which calculates the right-hand of (2), st is the
source amplitude at each timestep and the wavefield ut+1 are
hidden states that stores knowledge that the rnn currently
holds, yt are the records at specified locations.
b. pure automatic differentiation
the fwi tries to find a model that minimizes the discrepancy between the observed data and the predicted data which
is modeled by the current model parameters. the optimization
of the model parameters can be expressed as
arg min j (rnn(m, s; xr ), dobs (xr ))

(4)

m

where m is the model parameter (v p in acoustic case), j is
a scalar which is usually named objective function or loss
function, rnn represents the forward modeling operator, s is
the source function, xr is the locations of the receivers, and
dobs is the observed records.
take the derivative of the objective function with respect
to the model parameters, we can obtain the back-propagation
equation as follows:
âˆ‚j
= backward(j )
âˆ‚m

(5)

where backward(j ) means we need backward the loss calculated by objective function to compute the gradients of the
variables in the computational graph without calculating the
gradients of j w.r.t ut explicitly. it is the pure automatic
differentiation in the currently popular deep learning (dl)
frameworks, which allows users to perform fwi in a simpler way without manually defining adjoint propagations and
gradient formulas.
the gradients of the model parameters can be calculated by
the chain rule [6]

nt 
x
âˆ‚j
âˆ‚ut
âˆ‚j
=
.
(6)
âˆ‚m
âˆ‚u
t
m âˆ‚m ut â€² Ì¸=t
t=1

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

wang et al.: memory optimization in rnn-based fsi

fig. 2.

5919212

directed graph of fwi based on pure automatic differentiation.

take the derivative of j with respect to the wavefield at
timestep t + 1
âˆ‚j
âˆ‚j
âˆ‚ j âˆ‚ut+1
âˆ‚ j âˆ‚ut+1
=
+
+
.
âˆ‚ut+1
âˆ‚ut+1
âˆ‚ut+1 âˆ‚ut
âˆ‚ut+1 âˆ‚utâˆ’1

(7)

equations (6) and (7) show that the objective function
depends on all the ordered hidden states. when we backward
the loss j at timestep t + 1, the wavefields at previous times
must be available for calculating (6).
an unrolled fwi-rnn with pure automatic differentiation
is shown in fig. 2. in the forward propagation, for the purpose
of building the computational graph, the wavefields at each
timestep are stored in cpu or gpu memory to facilitate the
construction of the computational graph. the computational
graph plays a crucial role in tracking and organizing the
flow of computations within the rnn. however, it is important to note that this process incurs a significant memory
overhead due to the storage of wavefields. during backward
propagation, the computational graph and stored wavefields
are used by the automatic differentiation mechanism (ad)
to compute and accumulate gradients by (6). it effectively
traces the dependence between the output (wavefields) and the
model parameters of the fwi-rnn, enabling the computation
of gradients for optimization. the solid and wide orange
line linked to the memory represents heavy memory usage
and input/output (io).
c. effective boundary saving-based automatic
differentiation
actually, by using the effective boundary-saving strategy
discussed by yang et al. [10], wave propagation can be
reversible under certain initial and boundary conditions, which
can be expressed as

(8)
urtâˆ’1 = cell urt+1 , urt , stâˆ’1 ; m, btâˆ’1
where b is the wavefield boundaries at timestep t âˆ’ 1 and urtâˆ’1
represents the reconstructed wavefield at t âˆ’ 1. besides, the
last two wavefields are also necessary as the initial condition:
urn tâˆ’2 = cell(u n tâˆ’1 , u n t , s n tâˆ’2 ; m, b n tâˆ’2 ).
in fig. 3, the blue zone i is the absorbing boundary zone
for eliminating the boundary reflections, the gray zone ii is
the effective boundary zone with width as n (where 2n
is the differential order of the taylor expansion), and the

fig. 3. illustration of the effective boundary saving. (i) absorbing boundary
zone, (ii) effective boundary zone, and (iii) wavefield update zone.

orange zone iii is the wavefield update zone whose data will
be updated by the wave equation. in other words, instead
of storing all the wavefields (the whole area in fig. 3) and
building the computational graph in the forward modeling,
we only need to save the effective boundaries (the gray zone
in fig. 3) and the last two wavefield snapshots. using the
last two wavefield snapshots as initial conditions and effective boundaries as the boundary conditions of each timestep,
we can propagate the wavefields in the reverse-time direction
and rebuild the computational graph, and then backward the
reconstructed wavefield by using automatic differentiation.
the directed graph of the effective boundary saving-based
automatic differentiation described earlier is shown in fig. 4.
an extra modeling is needed in the loss function backward
propagation. the computational direction of this modeling is
in the reverse-time direction, aligning with the direction of the
loss back propagating. a snapshot of the computational graph
at each timestep is rebuilt for calculating and accumulating the
gradients. so, it is actually a trade-off between computation
and storage, where computations are performed on-the-fly
instead of explicitly storing the whole wavefield calculated
in the forward propagation. the dashed and slim orange line
linked to the memory represents lower memory usage and io
compared with fig. 2.
fig. 5 illustrates the pseudocodes for the two methods
mentioned above. in pure automatic differentiation, gradient calculation is enabled during the forward modeling for
building the computational graph. however, in boundarysaving-based automatic differentiation, gradient calculation

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

5919212

ieee transactions on geoscience and remote sensing, vol. 61, 2023

fig. 4.

directed graph of fwi based on effective boundary saving-based automatic differentiation.

fig. 5.

pseudo-code of the pure automatic differentiation (algorithm1) and effective boundary saving based automatic differentiation (algorithm2).

should be disabled during forward modeling, and boundary
values should be saved. during loss backpropagation, gradient
calculation is enabled, values of boundaries are assigned to
the reconstructed wavefields, and gradients are computed and
accumulated at each time step. in algorithm 1, we perform
a direct backpropagation of the scalar value of the objective
function. however, in algorithm 2, we necessitate the use
of additional application programming interfaces (apis) to
compute the gradient of the wavefield at timestep t âˆ’ 1 with
respect to the leaves within the computational graph.
iii. n umerical e xamples
in our examples, the spatial differentiation order is set to
2n = 2, so only one layer of the boundaries is needed to
store for reconstruction. in all our examples, an amd epyc
7543 32-core cpu and a nvidia a40 (48 gb) gpu are
used for tests. pytorch is used for implementing the modeling

and inversion tests. all the examples are running on gpu for
acceleration.
a dynamic source encoding strategy [16] is performed
to accelerate the computation of fwi. for multiscale inversion [16], data from six frequency bands with dominant
frequencies of 1, 3, 5, 8, and 10 hz, and the full frequency
band data are used to gradually obtain the final velocity model.
we perform 100 iterations at each scale, with 20 randomly
encoded shots in a batch. an adam optimizer with initial step
size of 5 and a decay rate of 0.75 for each scale is utilized for
optimization. a mean squared error (l2) objective function is
used for matching the predicted and observed data.
we use the following evaluation metrics for quantitative
analysis. the first one is a model error, which is defined as the
mean squared error of the true model and the inverted model
errork =

âˆ¥mtrue âˆ’ mk âˆ¥2
size(m)

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

(9)

wang et al.: memory optimization in rnn-based fsi

5919212

where mtrue is the ground truth model and mk is the inverted
model of the kth epoch. size(m) means the number of grid
points in the model.
the coding loss, which is defined as the mean square error
between the predicted and filtered observed data in a batch:
lossk =

2
ik


x

b yipred (mk ) âˆ’ b diobs

(10)

i

where yipred (mk ) is the ith predicted records at the kth epoch
with the model mk , ik is the randomly subsampled shots at the
current epoch, and b(âˆ—) is a filter applied to original observed
data with the current frequency band.
the memory saving ratio, which is defined as the difference
between the memory usage of the pure automatic differentiation (pad) method and the effective boundary saving-based
automatic differentiation (ebs-ad) method, is divided by the
memory usage of the ad method.
ï£±

ï£´
m = mpad âˆ’ mebs-ad mebs-ad
ï£´
ï£´
ï£´
k
ï£´
y
ï£´
ï£´
ï£´
m
=
sizeof(dtype)
Â·
n
t
Â·
ni
ï£´
pad
ï£²
i=1
(11)
ï£´
mebs-ad = sizeof(dtype) Â· n t
ï£´
ï£´
"
#
ï£´
k
k
ï£´
y
y
ï£´
ï£´
ï£´
Â·
n
âˆ’
(n i âˆ’ 2n )
ï£´
i
ï£³
i=1

fig. 6.

marmousi model. (a) ground truth model and (b) initial model.

i=1

where n i represents the size of the ith dimension; k represents
the dimensions of the model, and sizeof(dtype) is the size of
the data type used to store each value (in bytes). the default
value is 4 in our examples, which corresponds to a 32-bit
floating-point number. n t is the number of timesteps and 2n
is the spatial differential order.
a. marmousi model (2-d case)
the marmousi model with a width of 9.2 km and depth
of 3.5 km is used in this example. the spatial step of the
model is d x = dz = 10 m, and the sampling rate is 1 ms.
a total of 90 shots, spaced at an interval of 100 m, are modeled
for inversion. a ricker wavelet with a dominant frequency
of 15 hz is employed as the source function. the receivers
are evenly placed at the bottom of the sea, with a spacing
of 10 m. a 1-d model is used as the start model for fwi.
the ground truth and initial marmousi velocity models are
shown in fig. 6(a) and (b), respectively.
fig. 7(a) and (b) depict the gradients obtained at the
first epoch of the pad and ebs-ad methods, respectively.
fig. 7(c) illustrates the difference between the gradients shown
in fig. 7(a) and (b). the non-zero difference between the
two gradients can be attributed to numerical errors that
arise between the reconstructed wavefield and the original
wavefield. however, the magnitude of the error is small,
resulting in minimal impact on the inversion results. it is
worth noting that since the first frequency band has a dominant
frequency of 1 hz, so the gradient only captures large-scale
(low wavenumber) information of the model.
fig. 8 shows the final velocity models obtained through
fwi using pad and ebs-ad, respectively. both methods

fig. 7. gradients of marmousi model at the first epoch. (a) automatic
differentiation, (b) effective boundary saving, and (c) the difference between
(a) and (b).

successfully generate highly accurate velocity models, even
when utilizing a 1-d model as the initial start.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

5919212

ieee transactions on geoscience and remote sensing, vol. 61, 2023

fig. 8. final velocity models obtained by using pure automatic differentiation
(a), and effective boundary saving-based automatic differentiation (b).

fig. 9. differences between the true and the inverted models using: (a) pure
automatic differentiation and (b) effective boundary saving-based automatic
differentiation.

the difference between the ground-truth model and the
inverted models using these two methods is shown in fig. 9.
the differences highlight that the accuracy in the shallow
region is significantly higher compared to the deeper portion.
moreover, while the velocities within layers are inverted
accurately, the interfaces remain somewhat indistinct. using
a denser grid with broader frequency data could potentially
address this issue.
fig. 10 shows trace comparisons at distances of 4.0, 6.0,
and 8.0 km, respectively. the inverted velocity curves closely
resemble the ground-truth model. the difference between

fig. 10. velocity profile comparison of the true velocity model (black),
initial model (green), pure automatic differentiation inverted model (red),
and effective boundary saving-based automatic differentiation inverted model
(blue) at (a) x = 4.0 km, (b) x = 6.0 km, and (c) x = 8.0 km.

velocities obtained through the ebs-ad-based fwi and
pad-based fwi is minimal, as they appear to be almost overlapped. however, some sharp interfaces and deep structures
with high velocities are not accurately recovered.
fig. 11(a) and (b) shows model errors and losses of the
two methods. a sudden change in the curvature of the
curve indicates a shift in the frequency band of multiscales.
the results demonstrate that boundary saving strategy does not
compromise the accuracy of the inversion; it achieves nearly
the same level of accuracy as pure automatic differentiation.
to implement effective boundary saving, we need to
recalculate the wavefields by (8) and assign boundaries to
the wavefield. however, pad simply extracts the wavefields

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

wang et al.: memory optimization in rnn-based fsi

5919212

fig. 12. 2-d overthrust model. (a) ground truth model and (b) initial model.

fig. 11. model error curve (a) and loss (b) of the inversion based on pure
automatic differentiation (red) and effective boundary saving-based automatic
differentiation (blue).
table i
c omparison of the pad and ebs-ad based on m armousi m odel

from memory, resulting in higher computational efficiency
compared to the ebs-ad approach. specifically, for one
epoch, pad needs 4.5 s while ebs-ad takes 7.9 s as shown
in table i. the pad method utilizes 5924 mb of storage
to hold the complete wavefield, while the ebs-ad method
achieves the same with a significantly smaller storage requirement of just 45 mb, mainly by storing the boundaries. the
ebs-ad saves 99.2% memory compared to the pad method.
b. overthrust model (2-d case)
a 2-d overthrust model with a width of 12.5 km and a
depth of 4 km is used for testing purposes. the grid size is
25 Ã— 25 m. the sampling rate is 2 ms, with a record length
of 4 s. in fwi, a total of 100 shots are employed, spaced at
intervals of 100 m. the receivers are placed at the surface of
the model, with intervals of 25 m. the ground-truth model
and the 1-d start model are shown in fig. 12(a) and (b),
respectively.

fig. 13. gradients of overthrust model obtained at the first epoch. (a) pure
automatic differentiation, (b) effective boundary saving-based automatic differentiation, and (c) the difference between (a) and (b).

the gradients obtained at the first epoch of the first
frequency band are depicted in fig. 13. specifically,
fig. 13(a) and (b) illustrate the gradients obtained from the

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

5919212

ieee transactions on geoscience and remote sensing, vol. 61, 2023

fig. 14. final velocity models obtained by using pure automatic differentiation (a) and effective boundary saving-based automatic differentiation (b).

fig. 15. differences between the true and the inverted models using. (a) pure
automatic differentiation and (b) effective boundary-saving-based automatic
differentiation.

fig. 16. velocity profile comparison of the true velocity model (black line),
initial model (green line), pure automatic differentiation inverted model (red
line), and effective boundary saving-based automatic differentiation inverted
model (blue line) at (a) x = 3.75 km, (b) x = 7.5 km, and (c) x = 11.25 km.

pad and ebs-ad methods, respectively. the difference
between the two methods is presented in fig. 13(c). there
is no significant difference between the two gradients, thereby
validating the effectiveness of the ebs for fwi within the
context of automatic differentiation.
the final inversion results of the pad and ebs-ad methods
are shown in fig. 14(a) and (b), respectively. it can be observed
that both methods have successfully reconstructed almost all
the structures compared with the ground truth model shown
in fig. 14(a).
the difference between the ground truth model and
the inverted models of pad and ebs-ad are shown in
fig. 15(a) and (b), respectively.

fig. 16 shows trace profiles at distances x = 3.75 km, x =
7.5 km, and x = 11.25 km for comparison. the inverted
models obtained through the pad and ebs-ad methods
exhibit similarities to each other and both closely resemble
the true model.
the model error curves and the coding l2 loss curves are
shown in fig. 17(a) and (b), respectively. in both subfigures,
the curves of the two methods overlap in the low-frequency
bands, but they separate at high frequency bands.
table ii presents a comparison between the pad and
ebs-ad methods in terms of various metrics. the pad
method achieved a final model error of 0.145 and a final loss

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

wang et al.: memory optimization in rnn-based fsi

fig. 17.
model error curve (a) and loss (b) of the inversion based on
pure automatic differentiation (red line) and effective boundary saving-based
automatic differentiation (blue line).
table ii
c omparison of the pad and ebs-ad based on overthrust m odel

of 0.47, with a computational time of 2.0 s per epoch. on the
other hand, the ebs-ad method resulted in a slightly lower
final model error of 0.132, a slightly higher final loss of 0.49,
and a longer computational time of 3.9 s per epoch. the pad
method employs 610 mb of storage for the entire wavefield,
whereas the ebs-ad method requires only 10 mb to store
the boundaries. it demonstrated a remarkable memory saving
of 98.35% compared with pad method.
c. modified marmousi model (3-d case)
we generate a 3-d velocity model based on the 2-d
marmousi2 model [17]. the width and the length of the
generated model are 2.5 km, and the depth is 1.25 km.
the grid size in the three dimensions is 12.5 m. a ricker
wavelet with a 10 hz dominant frequency is used as the
source. fig. 18(a) shows slices at y = 0.25 km, y = 1.25 km,
and y = 2.25 km, respectively. fig. 18(b) shows slices at
x = 0.25 km, x = 1.25 km, and x = 2.25 km, respectively.

5919212

fig. 18.
slices of the 3-d ground truth model. (a) velocity slices at
y = 0.25 km, y = 1.25 km, and y = 2.25 km, respectively (from left to
right). (b) velocity slices at x = 0.25 km, x = 1.25 km, and x = 2.25 km,
respectively (from right to left).

for the purpose of fwi, an initial linear model with a velocity
ranging from 1500 to 3200 m/s is adopted, which is illustrated
in fig. 19. just like the 2-d cases in sections ii-b and ii-c,
we utilized 6 frequency bands for the multiscale strategy.
the number of sources and receivers are 100 and 400,
respectively, uniformly distributed on the x0y plane.
fig. 20(a) and (b) shows the locations of the sources and
receivers, respectively.
fig. 21 shows slices selected from the final ebs-ad
inverted 3-d model. the inverted model is similar to the
ground truth shown in fig. 18, which validated the feasibility
of our proposed method in 3-d cases. in the 3-d case,
the pad method encountered â€œout of memoryâ€ because a
large amount of wavefields are needed to store. therefore,
we only display the results of the eba-ad inverted results.
the theoretical memory usage for storing the wavefields of the
pad method is 46 698 mb and that of the eba-ad method is
only 1831 mb.
fig. 22 shows trace profiles at distance (x, y) = (1.25 km,
1.25 km) and (x, y) = (0.625 km, 0.625 km) for comparison.
the inverted models obtained through the ebs-ad method
closely approximates the actual values. the 3-d inversion
results may not be as favorable as those in 2-d (as shown
in fig. 10), possibly due to the sparse distribution of receivers
since the grid cell of the receivers is 250 Ã— 250 m. employing
a denser arrangement of receivers should potentially lead to
an improved inversion result.
the model error curves and the coding l2 loss curves are
shown in fig. 23(a) and (b), respectively. the convergence

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

5919212

ieee transactions on geoscience and remote sensing, vol. 61, 2023

fig. 19. slices of the linear start model. (a) velocity slices at y = 0.25 km,
y = 1.25 km, and y = 2.25 km, respectively (from left to right). (b) velocity
slices at x = 0.25 km, x = 1.25 km, and x = 2.25 km, respectively (from
right to left).

fig. 20.

fig. 21. slices from the inverted model. (a) velocity slices at y = 0.25 km,
y = 1.25 km, and y = 2.25 km, respectively (from left to right). (b) velocity
slices at x = 0.25 km, x = 1.25 km, and x = 2.25 km, respectively (from
right to left).

locations of (a) sources and (b) receivers.

of both model error and loss function indicates that the
boundary saving strategy can be effectively integrated into the
rnn-based 3-d full waveform inversion.
iv. d iscussion
in the pad method, the computational graph is established
during the forward simulation process. in contrast, within
the eba-ad method, forward simulation serves the purpose
of storing boundary values for wavefield reconstruction, and
gradients are not computed at this stage. therefore, a computational graph is not constructed, and the stored wavefields are
not detached from any graph.
when backpropagating the loss, we first enable gradient
computation, followed by the inverse time reconstruction

fig. 22. velocity profile comparison of the true velocity model (black line),
initial model (green line), and effective boundary saving-based automatic
differentiation inverted model (blue line) at (a) x = 1.25 km, y = 1.25 km
and (b) x = 0.625 km, y = 0.625 km.

of the wavefield, and then invoke the appropriate apis to
compute the gradients. at this point, the computational graph
has been deactivated, allowing us to safely assign values to
the boundaries without affecting the gradient calculations.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

wang et al.: memory optimization in rnn-based fsi

5919212

c ode availability
the codes that support the findings of this study are openly
available at https://github.com/geophyai/seistorch.
r eferences

fig. 23. model error curve (a) and loss (b) of the inversion based on effective
boundary saving-based automatic differentiation.

therefore, if we aim to circumvent the impact of in-place operations on gradients, we need to devise a method that enables
in-place operations to occur outside the computational graph.
implementing this in the acoustic case is straightforward, as it
requires only a single variable for boundary assignments.
the conv2d/conv3d apis within deep learning frameworks
often support batch calculating, allowing us to concurrently
compute multiple shots to enhance the computational efficiency of classic fwi without source encoding. this avenue
also sets the stage for our forthcoming research endeavors.
v. c onclusion
the pad in dl framework needs large amount memory to
store hidden states of rnn (wavefield snapshots in seismic
inversion) for constructing the computational graph, out of
memory can arise when the model is very large. with effective boundary saving strategy-based automatic differentiation
(ebs-ad), we only need to store the wavefields at a thickness
equal to half of the difference order and rebuild the computational graph in the backward propagation. the trade-off
between computation and storage is important in rnn-based
seismic inversion since we always face large models. we performed acoustic inversion in the context of pad and ebs-ad
by using marmousi model (2-d), overthrust model (2-d) and a
modified marmousi2 model (3-d), respectively. results show
that ebs-ad can perform rnn-based fwi with low-memory
usage and no loss in accuracy, albeit the cost of computational
efficiency. the boundary saving methods can also be used for
more intricate wave equations in the context of rnn and deeplearning frameworks.
acknowledgment
the authors sincerely appreciate the assistance of dr. tang
from medical university of innsbruck for the help with the
programming.

[1] p. lailly and j. bednar, â€œthe seismic inverse problem as a sequence
of before stack migrations,â€ in proc. conf. inverse scattering-theory
appl., philadelphia, pa, usa, 1983, pp. 206â€“220.
[2] a. tarantola, â€œinversion of seismic reflection data in the acoustic
approximation,â€ geophysics, vol. 49, no. 8, pp. 1259â€“1266, aug. 1984,
doi: 10.1190/1.1441754.
[3] t. w. hughes, i. a. d. williamson, m. minkov, and s. fan, â€œwave
physics as an analog recurrent neural network,â€ sci. adv., vol. 5, no. 12,
dec. 2019, art. no. eaay6946, doi: 10.1126/sciadv.aay6946.
[4] w. wang, g. a. mcmechan, and j. ma, â€œelastic isotropic and anisotropic
full-waveform inversions using automatic differentiation for gradient
calculations in a framework of recurrent neural networks,â€ geophysics,
vol. 86, no. 6, pp. r795â€“r810, nov. 2021, doi: 10.1190/geo2020-0542.1.
[5] j. sun, z. niu, k. a. innanen, j. li, and d. o. trad, â€œa theoryguided deep-learning formulation and optimization of seismic waveform
inversion,â€ geophysics, vol. 85, no. 2, pp. r87â€“r99, mar. 2020, doi:
10.1190/geo2019-0138.1.
[6] a. richardson, â€œseismic full-waveform inversion using deep learning
tools and techniques,â€ 2018, arxiv:1801.07232.
[7] r.-e. plessix, â€œa review of the adjoint-state method for computing
the gradient of a functional with geophysical applications,â€ geophys.
j. int., vol. 167, no. 2, pp. 495â€“503, nov. 2006, doi: 10.1111/j.1365246x.2006.02978.x.
[8] j. sun, k. innanen, t. zhang, and d. trad, â€œimplicit seismic full
waveform inversion with deep neural representation,â€ j. geophys. res.,
solid earth, vol. 128, no. 3, mar. 2023, art. no. e2022jb025964, doi:
10.1029/2022jb025964.
[9] e. dussaud et al., â€œcomputational strategies for reverse-time migration,â€
in proc. seg tech. program expanded abstr., jan. 2008, pp. 2267â€“2271,
doi: 10.1190/1.3059336.
[10] p. yang, j. gao, and b. wang, â€œrtm using effective boundary saving:
a staggered grid gpu implementation,â€ comput. geosci., vol. 68,
pp. 64â€“72, jul. 2014, doi: 10.1016/j.cageo.2014.04.004.
[11] r. g. clapp. (2009). reverse time migration? saving the boundaries.
[online]. available: https://api.semanticscholar.org/corpusid:55466322
[12] y. wang et al., â€œcuq-rtm: a cuda-based code package for stable and
efficient q-compensated reverse time migration,â€ geophysics, vol. 84,
no. 1, pp. f1â€“f15, jan. 2019, doi: 10.1190/geo2017-0624.1.
[13] q. zhang, w. mao, and y. chen, â€œattenuating crosstalk noise
of simultaneous-source least-squares reverse time migration with
gpu-based excitation amplitude imaging condition,â€ ieee trans.
geosci. remote sens., vol. 57, no. 1, pp. 587â€“597, jan. 2019, doi:
10.1109/tgrs.2018.2858850.
[14] n. masmoudi and t. alkhalifah, â€œfull-waveform inversion in acoustic
orthorhombic media and application to a north sea data set,â€ geophysics,
vol. 83, no. 5, pp. c179â€“c193, sep. 2018, doi: 10.1190/geo2017-0738.1.
[15] p. yang, j. gao, and b. wang, â€œa graphics processing unit implementation of time-domain full-waveform inversion,â€ geophysics, vol. 80,
no. 3, pp. f31â€“f39, may 2015, doi: 10.1190/geo2014-0283.1.
[16] g. t. schuster, x. wang, y. huang, w. dai, and c. boonyasiriwat, â€œtheory of multisource crosstalk reduction by phase-encoded statics: theory
of multisource crosstalk reduction,â€ geophys. j. int., vol. 184, no. 3,
pp. 1289â€“1303, mar. 2011, doi: 10.1111/j.1365-246x.2010.04906.x.
[17] g. s. martin, r. wiley, and k. j. marfurt, â€œmarmousi2: an elastic
upgrade for marmousi,â€ lead. edge, vol. 25, no. 2, pp. 156â€“166,
feb. 2006, doi: 10.1190/1.2172306.

shaowen wang (graduate student member, ieee)
received the b.e. degree in exploration technology
and engineering from shandong university of science and technology, shandong, china, in 2018.
he is currently pursuing the ph.d. degree in
marine geophysics with ocean university of china,
qingdao, china.
his research interests include seismic data denoising, seismic inversion, and deep learning.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.

5919212

ieee transactions on geoscience and remote sensing, vol. 61, 2023

yong jiang received the b.e. and m.e. degrees
from ocean university of china, qingdao, china,
in 2003 and 2006, respectively.
he currently serves as the deputy manager of the
exploration department at cnooc (china) ltd.,
shanghai, china. his research interests include seismic data interpretation, full waveform inversion, and
deep learning.

zhaolun liu received the ph.d. degree in earth
science and engineering from king abdullah university of science and technology (kaust),
thuwal, saudi arabia, in 2019.
he is an associate professor at the college of marine geosciences, ocean university of
china, qingdao, china. he previously served as a
post-doctoral research associate at the department
of geosciences, princeton university, princeton, nj,
usa. his research interests include source-encoded
acoustic-elastic coupled fwi and its application to
the onshore and offshore seismic data, seismic data processing and migration
based on machine learning methods, and surface-wave inversion and migration.

peng song received the b.s., m.e., and ph.d.
degrees from ocean university of china, qingdao,
china, in 2002, 2005, and 2014, respectively.
he is a full professor with the college of
marine geosciences, ocean university of china. his
research interests include deep learning, seismic data
processing, seismic forward modeling, migration,
and inversion.

jun tan received the b.s., m.e., and ph.d. degrees
from ocean university of china, qingdao, china,
in 2005, 2008, and 2011, respectively.
he is an associate professor with the college of
marine geosciences, ocean university of china. his
research interests include multiple attenuation, deep
learning, and seismic data processing.

bingshou he received the b.s. and m.e. degrees
from china university of mining and technology,
xuzhou, china, in 1996 and 1999, respectively,
and the ph.d. degree from china university of
petroleum (beijing), beijing, china, in 2002.
he is a full professor with the college of marine
geosciences, ocean university of china, qingdao,
china. his research interests include deep learning and seismic multicomponents data processing,
migration, and inversion.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:16:05 utc from ieee xplore. restrictions apply.



geoscience frontiers 11 (2020) 1993â€“2001

h o s t e d by

contents lists available at sciencedirect

geoscience frontiers
journal homepage: www.elsevier.com/locate/gsf

research paper

physics informed machine learning: seismic wave equation
sadegh karimpouli a, pejman tahmasebi b, *
a
b

mining engineering group, faculty of engineering, university of zanjan, zanjan, iran
department of petroleum engineering, university of wyoming, laramie, wy, 82071, usa

a r t i c l e i n f o

a b s t r a c t

handling editor: masaki yoshida

similar to many ï¬elds of sciences, recent deep learning advances have been applied extensively in geosciences for
both small- and large-scale problems. however, the necessity of using large training data and the â€˜black boxâ€™
nature of learning have limited them in practice and difï¬cult to interpret. furthermore, including the governing
equations and physical facts in such methods is also another challenge, which entails either ignoring the physics
or simplifying them using unrealistic data. to address such issues, physics informed machine learning methods
have been developed which can integrate the governing physics law into the learning process. in this work, a 1dimensional (1d) time-dependent seismic wave equation is considered and solved using two methods, namely
gaussian process (gp) and physics informed neural networks. we show that these meshless methods are trained
by smaller amount of data and can predict the solution of the equation with even high accuracy. they are also
capable of inverting any parameter involved in the governing equation such as wave velocity in our case. results
show that the gp can predict the solution of the seismic wave equation with a lower level of error, while our
developed neural network is more accurate for velocity (p- and s-wave) and density inversion.

keywords:
gaussian process (gp)
physics informed machine learning (piml)
seismic wave
optimization

1. introduction
recent advances in recording data and computational power made a
revolution in machine learning applications in many ï¬elds of sciences
such as machine vision (aslam et al., 2019; nasirahmadi et al., 2019;
wang et al., 2020), speech recognition (lee et al., 2019; vishal and
aggarwal, 2019), geosciences (waldeland et al., 2018; xiong et al., 2018;
kamrava et al., 2019, 2019b; karimpouli et al., 2019; karimpouli and
tahmasebi, 2019a; kong et al., 2019; bai and tahmasebi, 2020;
rouet-leduc et al., 2020; tang et al., 2020) and medical engineering
(bernal et al., 2019; guo et al., 2019; maitra et al., 2019) to name a few
examples. convolutional neural networks and their derivatives are one of
those highly effective networks with high accuracy in both classiï¬cations
(garcia-garcia et al., 2017), estimation problems (kamrava et al., 2019a;
karimpouli and tahmasebi, 2019b), the problems related to uncertainty
quantiï¬cation in complex (geo)-materials (kamrava et al., 2020a), and
also the problems related to fracture modeling and phase transition in
complex materials (kamrava et al., 2020b). a comprehensive review of
the applications of the ai methods in geosciences at both small- and
large-scale systems has been reviewed recently and can be found elsewhere (tahmasebi et al., 2020). thus, such methods are not reviewed

here. these networks perform very well when a vast amount of data is
available. however, in most engineering applications, if not all, data
acquisition is an expensive and time-demanding task. thus, one has to
draw conclusions from partial information. in this case, deep networks
fail to converge mostly due to the complexity of the system and unoptimized parameters.
in most cases, deep learning algorithms are assumed as a black box,
usually without any contribution of prior knowledge of the system. this
knowledge could be signiï¬cant, varying from physical laws governing
the time-dependent dynamic of a system to empirical relations derived
from the experimental analysis. prior information is assumed as tool to
regularize the solution by constraining it into a physically meaningful
space (raissi et al., 2019). these constraints lead to rejecting nonrealistic
solutions, known as nonuniqueness issue in inverse problems, and steer
the learning process to the right solution. algorithms that encode this
information into their learning processes are called as physics-constrained
(zhu et al., 2019) or physics-informed (raissi et al., 2019) methods, which
are used either for data-driven estimation and/or parameter inversion.
the engaged governing equations conduct quickly the learning process to
the right solution and enable it to learn the whole rule between inputs
and outputs even in a small data scheme.

* corresponding author.
e-mail addresses: s.karimpouli@znu.ac.ir (s. karimpouli), ptahmase@uwyo.edu (p. tahmasebi).
peer-review under responsibility of china university of geosciences (beijing).
https://doi.org/10.1016/j.gsf.2020.07.007
received 30 april 2020; received in revised form 14 june 2020; accepted 15 july 2020
available online 5 august 2020
1674-9871/Â© 2020 china university of geosciences (beijing) and peking university. production and hosting by elsevier b.v. this is an open access article under the
cc by-nc-nd license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

s. karimpouli, p. tahmasebi

geoscience frontiers 11 (2020) 1993â€“2001

among all data-driven methods, gaussian process (gp) and physicsinformed machine learning (piml) are used here as intelligent meshless
methods for solving the seismic wave equation. a gaussian process is a
collection of random variables, any gaussian process ï¬nite number of
which have a joint gaussian distribution (rasmussen and williams,
2006). the gp, as a bayesian procedure, assumes prior distributions over
functions and estimates posterior covariance function by applying the
governed physical lows (e.g., differential equation). it inherently quantiï¬es the uncertainty using a fully probabilistic workï¬‚ow and a powerful
training procedure. the gps could be categorized either as a kernel
machine with regularization approaches (vapnik, 2013) or one-layer
feed-forward bayesian neural networks with an inï¬nite number of hidden neurons (neal, 2012). some researchers attempted to combine
probabilistic models and machine learning and proposed the gp as a
non-parametric bayesian machine learning approach to solve differential
equations (lawrence, 2005; titsias and lawrence, 2010; alvarez et al.,
2013). in contrast to previous models, other researchers placed the gp
prior to the solution of a differential equation where optimal model parameters and hyper-parameters are all learned directly from the data
(raissi et al., 2017a, b). they used such a gp model to solve ordinary and
partial differential, integro-differential, and fractional-order operators.
in the second group, the pimls are a class of artiï¬cial neural networks
(ann), where the learning process is reinforced by physical governing
laws. although some researchers solved differential equations with ann
(meade and fernandez, 1994; lagaris et al., 1998), recent studies instead
have tried to revise the loss function based on the observed data as well as
the governing equations, initial and boundary conditions (raissi et al.,
2019; zhu et al., 2019; haghighat et al., 2020). toward this goal, piml
has been used for data-driven solution and discovery of a collection of
classical problems in ï¬‚uids, quantum mechanics, reaction-diffusion systems, and the propagation of nonlinear shallow-water waves (raissi
et al., 2019). they extended their framework for learning ï¬‚uid velocity
and pressure ï¬eld from ï¬‚ow visualizations (raissi et al., 2020). in a
similar fashion, piml is used in solid mechanics to predict 2d displacement of a plate under a nonuniform force (haghighat et al., 2020). the
physical laws in linear elasticity were encoded in a loss function to induce
physical aspects of the problem. they found that piml can predict a wide
range of parameters, converges quickly when the network is re-training
via transfer learning and can be used for sensitivity analysis and surrogate modeling.
in this study, we also have implemented piml for solving the seismic
wave equation as a fundamental equation, which has many applications
in geosciences such as earthquake seismology, seismic exploration, rock
physics and etc. in this work, we have compared two physics-informed
deep learning methods, namely gp and piml, for solving a 1d form of
a linear differential equation of seismic wave and simultaneously for
inversion of the wave velocity.

where Î² is the hyper-parameter of the kernel kuu . the key point of gp is
that it is closed under linear transformations such as summation, multiplication, differentiation, and integration. for example, since uÃ°xÃ¾ is a
gp, the obtained function under differential equation operator l Î±x is still
gp (dondelinger et al., 2013; cockayne et al., 2017; raissi et al., 2017a):
vÃ°xÃ¾  gpÃ°0; kvv Ã°x; xâ€™ ; Î±; Î²Ã¾Ã¾
where


kvv Ã°x; xâ€™ ; Î±; Î²Ã¾ Â¼ l Î±x l Î±xâ€™ kuu Ã°x; xâ€™ ; Î²Ã¾



kvu Ã°x; xâ€™ ; Î±; Î²Ã¾ Â¼ l Î±x kuu Ã°x; xâ€™ ; Î²Ã¾

(5)



kuv Ã°x; xâ€™ ; Î±; Î²Ã¾ Â¼ l Î±xâ€™ kuu Ã°x; xâ€™ ; Î²Ã¾

(6)

we assume a squared exponential although there are many covariance functions (rasmussen and williams, 2006):
"
â€™

kuu Ã°x; x ; Î²Ã¾ Â¼ Ïƒ

2
u exp

n
1x

Î»i Ã°xi  xâ€™i Ã¾2
2 iÂ¼1

#
(7)

where x (and/or xâ€™) is a n-dimensional vector in space/time and
Î² Â¼ fÏƒ 2u ; Î»i Ã°i Â¼ 1â€¦nÃ¾g is a collection of hyper-parameters: signal variance and weights. this is a stationary covariance function in which the
covariance is almost unity between variables whose corresponding inputs are very close, and decreases as their distance in the input space
increases (rasmussen and williams, 2006).
both parameter Î± and hyper-parameters Î² can be obtained by minimizing the negative log marginal likelihood n l m l (rasmussen and
williams, 2006) via an optimization process (raissi et al., 2017b):
 
 1
1
n
n l m l Â¼  log p yÎ±; Î²; Ïƒ 2u ; Ïƒ 2v Â¼ logjkj Ã¾ yt k1 y Ã¾ log 2 Ï€
(8)
2
2
2
 

y
where y Â¼ u , pÃ°yÎ±; Î²; Ïƒ 2u ; Ïƒ 2v Ã¾ Â¼ n Ã°0; kÃ¾ and k is expressed as:
yv
"
kÂ¼

kuu Ã°xu ; xu ; Î²Ã¾ Ã¾ Ïƒ 2nu inu

kvu Ã°xv ; xu ; Î±; Î²Ã¾

kuv Ã°xu ; xv ; Î±; Î²Ã¾

kvv Ã°xv ; xv ; Î±; Î²Ã¾ Ã¾ Ïƒ 2nv inv

#
(9)

where Ïƒ 2nu and Ïƒ 2nv are the noise variances to cover noisy data.
after training the model and obtaining the optimized parameters and
hyper-parameters, one can predict uÃ°xÃ¾ and vÃ°xÃ¾ for new points x using
posterior distributions:

2.1. gaussian process



pÃ°uÃ°xÃ¾jyÃ¾ Â¼ n uÃ°xÃ¾; s2u

(10)



pÃ°vÃ°xÃ¾jyÃ¾ Â¼ n vÃ°xÃ¾; s2v

(11)

where

suppose uÃ°xÃ¾ is a function of spatial or temporal variable x and a
differential equation operator l Î±x with a-priori unknown parameters of Î±
acts on it. thus, a differential equation is expressed as:
(1)

uÃ°xÃ¾ Â¼ ztu k1 y;

s2u Â¼ kuu Ã°x; xÃ¾  ztu k1 zu

(12)

vÃ°xÃ¾ Â¼ ztv k1 y;

s2v Â¼ kvv Ã°x; xÃ¾  ztv k1 zv

(13)

and

where vÃ°xÃ¾ is the forcing term. thus, uÃ°xÃ¾ is an unknown solution of
differential equations, which is targeted to be found via gp. to this end,
uÃ°xÃ¾ is assumed to be a gp with mean 0 and covariance function of
kuu Ã°x; xâ€™ ; Î²Ã¾ (rasmussen and williams, 2006):
uÃ°xÃ¾  gpÃ°0; kuu Ã°x; xâ€™ ; Î²Ã¾Ã¾

(4)

and

2. methodology

l Î±x uÃ°xÃ¾ Â¼ vÃ°xÃ¾

(3)

(2)

1994

ztu Â¼ Â½kuu Ã°x; xu Ã¾kuv Ã°x; xv Ã¾

(14)

ztv Â¼ Â½kvu Ã°x; xu Ã¾kvv Ã°x; xv Ã¾

(15)

s. karimpouli, p. tahmasebi

geoscience frontiers 11 (2020) 1993â€“2001

the uÃ°xÃ¾ and vÃ°xÃ¾ are the predictions in each new point and s2u and s2v
are the uncertainties of each prediction. these variances are accounted as
a direct consequence of a bayesian procedure, which can be used to
quantify the uncertainty and to ï¬nd new positions of data acquisition.

3. results
3.1. seismic wave equation
the 1d time-dependent seismic wave equation in an isotropic and
homogeneous medium can be expressed by (sheriff and geldart, 1995):

2.2. physics informed machine learning

âˆ‚2 uÃ°x; tÃ¾
âˆ‚2 uÃ°x; tÃ¾
 v2
Â¼0
âˆ‚t 2
âˆ‚x2

suppose a differential equation such the one shown in eq. (1). a conventional neural network n n Ã°x; w; bÃ¾ is used to approximate uÃ°xÃ¾,
where w; b are weights and biases of the network. therefore, the network
inputs are n-dimensional vector of x and the output is uÃ°xÃ¾. in a feedforward network, the value of each layer is computed from the last one as:


qi Â¼ a f i w i xi Ã¾ bi ; i Â¼ 1â€¦l

where uÃ°x; tÃ¾ is the displacement in point x at time t. the general solution
to this differential equation can be expressed as uÃ°x; tÃ¾ Â¼ a cosÂ½2Ï€ Ã°t 
x =vÃ¾  Ï†, where a and Ï† are the amplitude and phase of the wave. in
this study, we assume a wave with unite amplitude, zero phase, and a
velocity of 2 km/s, which leads to:

(16)

where a f is the activation function and q vector contains the values of
i-th layer. the process of a neural network is an optimization problem
where weights and biases of the network are updated by minimizing a
loss function. for example, mean square error (mse) is a well-known loss
function and is deï¬ned as:
mse Â¼

nu
1 x
Ã°u  u* Ã¾2
nu iÂ¼1

(24)

uÃ°x; tÃ¾ Â¼ cosÂ½2Ï€ Ã°t  x = 2Ã¾; 0  x  1; 0  t  1

(25)

where the units of x and t are km and s. we aim to use physics informed
machine learning algorithms (i.e., gp and piml) to train a machine by
limited numbeq2rs of training points from uÃ°x; tÃ¾ for both inversion of the
velocity v and prediction of uÃ°x; tÃ¾.

(17)
3.2. gp results

*

where u and u are the ground truth and output value of the network. the
updated weights and biases are obtained using an optimization function
such as adam (kingma and ba, 2014):
Ã°wnew ; bnew Ã¾ â† Ã°wcurrent ; bcurrent Ã¾  Î¾

âˆ‚mse

âˆ‚Ã°wcurrent ; bcurrent Ã¾

according to section 2.1 as well as seismic wave equation shown in
eq. (24), Î± Â¼ v 2 and v Â¼ 0, and the corresponding covariance function is:
kuu Ã°x; xâ€™ ; t; tâ€™ ; Î»1 ; Î»2 Ã¾ Â¼ Ïƒ 2u e2 Ã°Î»1 Ã°xxâ€™Ã¾ Ã¾Î»2 Ã°ttâ€™Ã¾ Ã¾
1

(18)

2

(26)

other covariance functions (kuv ; kvu ; kvv ) are computed based on eqs.
(4)â€“(6). fifty numbers of training data (nu Â¼ nv Â¼ 50) are randomly

where Î¾ is the learning rate.
to induce physical differential equations into the network, one needs
ï¬rst to compute differentiation of uÃ°xÃ¾ in the network. to this end, the
network inputs must be space or time variables while differentiation of
output is physically meaningful. due to graph-based implementation of
feed-forward networks, partial differentiation of uÃ°xÃ¾ is naturally achieved at machine precision. thus, differential equations and physics laws
are involved in the loss function to penalize the network learning procedure. in fact, the outputs of the network must meet the physics
informed loss function to achieve the best results. consequently, the
mean square error of the loss function can be written based on the
network output (mseu ), differential equation (msev ) and initial (mse0 )
and boundary (mseb ) conditions (raissi et al., 2019):
mse Â¼ mseu Ã¾ msev Ã¾ mse0 Ã¾ mseb

2

selected from both x and t domain (Â½0; 12 ), and yu Â¼ uÃ°x; tÃ¾ are computed
from eq. (25). according to eq. (24), yv Â¼ 0 for all data. we used a lbfgs-b (zhu et al., 1997) as the optimization method to minimize the
negative log marginal likelihood (i.e., eq. (8)). results showed that in
noise-free data the inverted velocity is 2.0382 km/s. fig. 1a illustrates the
predicted surface of the wave as well as the spatial location of training
data. the results are also compared with the original data. fig. 1b shows
absolute differences between the original and predicted values (juÃ°x;tÃ¾ 
u* Ã°x; tÃ¾j). also, variances of predictions are shown in fig. 1c.
to add more complexity, we produced noisy data and generated yu Â¼
uÃ°x; tÃ¾ Ã¾ Îµu and yv Â¼ vÃ°x; tÃ¾ Ã¾ Îµv , where Îµu and Îµv are the noise values.
both noises are assumed to have a normal distribution with zero mean
and unite variance (Îµ  n Ã°0; 1Ã¾). results show that the inverted velocity
is 1.9109 km/s, which is less accurate than the noise-free data. fig. 1dâ€“f
illustrate the predicted surface, errors, and variances of predictions.

(19)

and
3.3. piml results

nu
1 x
Ã°u  u* Ã¾2
nu iÂ¼1

(20)

msev Â¼

nv

2
1 x
vÃ°xÃ¾  lÎ±x uÃ°xÃ¾*
nv iÂ¼1

(21)

mse0 Â¼

n0

2
1 x
uÃ°x0 Ã¾  u*0
n0 iÂ¼1

(22)

mseb Â¼

nb

2
1 x
uÃ°xb Ã¾  u*b
nb iÂ¼1

(23)

mseu Â¼

the utilized network in this study is illustrated in fig. 2. as can be
seen, the inputs are x and t based on our 1d assumption of the wave
equation, i.e. eq. (25), and the only output is uÃ°x;tÃ¾, which is the solution
of the differential equation, i.e. eq. (24). the hidden neurons consist of 2
layers with 40 neurons in each. we will come back to this point shortly.
finally, our piml network is constructed and the following loss function
is implemented based on which the ï¬rst and second derivatives of the
network output with regard to x and t (eqs. (19)â€“(23)) are computed:
nu
1 x
Ã°uÃ°x; tÃ¾  u* Ã°x; tÃ¾Ã¾2
nu iÂ¼1
 
2
nv  2 *
âˆ‚ u x; t
1 x
âˆ‚2 u* Ã°x; tÃ¾
Ã¾
 v2
2
2
nv iÂ¼1
âˆ‚t
âˆ‚x

loss function Â¼

where uÃ°x0 Ã¾, uÃ°xb Ã¾ and u*0 , u*b are the initial and boundary values of the
function and network, respectively.

(27)

where u* Ã°x; tÃ¾ is the output of the network.
the velocity value is introduced as an unknown parameter to the
1995

s. karimpouli, p. tahmasebi

geoscience frontiers 11 (2020) 1993â€“2001

fig. 1. the gp evaluation: (a, d) predicted uÃ°x; tÃ¾ and (b, e) the absolute error of predictions and (c, f) variance of predictions for noise-free and noisy data,
respectively.

data. to evaluate the performance of the conducted predictions, fig. 4b
shows the absolute difference between the original and predicted values.
the inverted velocity for noise-free data is 1.9994 km/s. we also
generated noisy data by adding noises to the training data (yu Â¼ uÃ°x;tÃ¾ Ã¾
Îµu ). the noises Îµu were sampled from a normal distribution with zero
mean and unite variance (Îµ  n Ã°0; 1Ã¾). figs. 3b, 4c and 4d show the mse
components, predicted wave and absolute difference between the original and predicted values. the inverted velocity for noisy data is 1.9991
km/s.

network, which allows the network to optimize it during the learning
procedure. this means that the velocity inversion and displacement
prediction are conducted simultaneously.
to ï¬nd the optimum numbers of hidden layers and neurons, we
generated 1000 numbers of training data in a regular pattern in x; t
domain. then, for 2, 4 and 6 hidden layers each with 20, 30 and 40
neurons, we trained the network and the obtained mse values as well as
inverted velocity are compared. these results are reported in table 1,
which indicates that the best performance can be obtained when the
number of layers and neurons are selected 4 and 20, respectively.
after ï¬nding the optimal architecture of the pinn, we randomly
generated 1000 noise-free samples from uÃ°x; tÃ¾ and trained the network
using adam optimization function. we also used tanh as the activation
function, 100 number of batch size, 0.001 for learning rate, 500 patience
epochs and 7000 epochs. fig. 3a shows the mses during the training
phase. as can be seen, after 5000 epochs, the loss function reaches a
plateau. fig. 4a also illustrates the predicted wave as well as the training

4. discussion
deep learning methods are accounted as data driven methods with
many cons and pros, but being a black box is one the most negative points
in scientiï¬c computations. in physics informed deep learning methods,
however, physical laws and empirical relations are involved in the
learning process, which makes these methods to be more scientiï¬cally

1996

s. karimpouli, p. tahmasebi

geoscience frontiers 11 (2020) 1993â€“2001

fig. 2. architecture of the proposed piml for solving 1d seismic wave equation.

highly probable that k becomes ill-positioned during optimization when
the parameters are changed. the optimization may converge to a local
minimum which is also another issue. in fact, these issues correspond to
the optimization problem, which is still assumed as an open problem
(raissi et al., 2017b). to have the best results, the problem is solved using
a variant of hyper-parameter initializations and the solution is the
smallest log marginal likelihood.
on the other hand, the optimization process in pinn is more stable
than gp, but pinn cannot predict uÃ°x; tÃ¾ with very small training data like
gp. for example, even with using 1000 samples, the prediction error of
pinn is higher than gp (compare figs. 1b and 4b), though both of them
are very small. to provide a more quantitative comparison, histograms of
such errors are plotted in fig. 5. givens the data presented here and those
in figs. 1b and 4b, high error points are distributed in boundaries of
prediction surface. based on mean values of errors, the error levels of
both methods are comparable, while the error by gp is smaller than pinn
more than two times. adding noise to the training data hence increases
the prediction error in both methods, which is due to random nature of
noises (see figs. 1e and 4d).
an inherent property of the gp method, as a bayesian procedure, is
computing the variances of predictions which is known as uncertainty
quantiï¬cation. fig. 1c and f illustrate the variances of prediction for noisefree or noisy data, which are indicators of how certain these predictions
are. in practice, the results of uncertainty quantiï¬cation can be used for
decision making process in a real project.
the other interesting point of these methods is their ability to invert
the parameters of the governing equations while training, which is
known as inversion or inverse problem. table 2 summarizes the inverted
velocities by two methods. according to these results, the pinn is more
reliable than the gp, where the difference between the true and inverted
velocities are 0.03% and 0.04% for noise-free and noisy data, respectively. these errors are 1.91% and 4.49% in the case of the gp method.

table 1
mse and inverted velocity values for networks with different hidden layers and
neurons (bold numbers are selected as optimal values).
number of neurons
layer
2
4
6

mse
v
mse
v
mse
v

20

30

40

2.84104
1.998
2.44Â£10Â¡5
1.9937
1.67104
1.9646

4.33104
1.9968
8.19105
1.9771
4.81105
1.9768

4.31104
1.9972
5.19105
1.9884
6.04104
1.9727

oriented. some of these methods such as gp are even tractable analytically. using the results of our study, we demonstrated such methods by
evaluating their abilities and ï¬‚exibilities to be used for solving seismic
wave equation as an example in geosciences.
prediction of a ï¬eld, function or parameter is one the main abilities of
deep learning methods. for example, here we aimed to predict displacements of points in x direction along time t; i.e. (uÃ°x; tÃ¾); where a
wave is propagated through a 1d medium with 2 km/s velocity. unlike
many of deep leaning methods, the gp can predict uÃ°x; tÃ¾ using a small
dataset (e.g., here nu Â¼ nv Â¼ 50) with a promising level of error either in
noise-free or noisy data as shown in fig. 1. adding noise results a higher
level of error in prediction which is expected. one can compare fig. 1b
and e where the prediction error increases by including a noise to
training data. a critical issue with the gp method is minimizing log
marginal likelihood - eq. (9) - where inverting the covariance matrix and
an optimization process are needed. when the number of training data is
increased, a direct inversion of k is not computationally feasible. instead,
it is suggested to use cholesky decomposition of k, since it is faster and
numerically more stable (rasmussen and williams, 2006). it is also

fig. 3. mse values of different components of the loss function during training for (a) noise free and (b) noisy data.
1997

s. karimpouli, p. tahmasebi

geoscience frontiers 11 (2020) 1993â€“2001

fig. 4. the pinn evaluation: (a) predicted uÃ°x; tÃ¾ and (b) the absolute error of predictions for noise free data. (c) predicted uÃ°x; tÃ¾ and (d) the absolute error of
predictions for noisy data.

fig. 5. histograms of prediction errors generated by (a) gp, and (b) pinn methods.

the pinn methods is also more ï¬‚exible than gp with regard to
involving more physical relations. for example, in this case, one could
beneï¬t from linear elastic relations for both p- and s-waves to invert
other parameters such as lame parameters and/or density. since we use
1d equations, it is not possible to invert lame parameters, but both p- and

s-waves as well as density could still be inverted. to this end, elastic
relations could be written as:

Îµxx Ã°x; tÃ¾ Â¼

Ïƒ xx Ã°x; tÃ¾ Â¼ vp2 ÏÎµxx Ã°x; tÃ¾

table 2
inverted velocities (the true velocity is 2 km/s).
gp

inverted velocity
difference (%)

âˆ‚ux Ã°x; tÃ¾
âˆ‚x

Îµxy Ã°x; tÃ¾ Â¼

pinn

noise free

noisy

noise free

noisy

2.0382
1.91

1.9109
4.49

1.9994
0.03

1.9991
0.04

âˆ‚uy Ã°x; tÃ¾
âˆ‚x

Ïƒ xy Ã°x; tÃ¾ Â¼ vs2 ÏÎµxy Ã°x; tÃ¾

1998

(28)
(29)

(30)
(31)

s. karimpouli, p. tahmasebi

geoscience frontiers 11 (2020) 1993â€“2001

where Îµxx Ã°x; tÃ¾ and Ïƒ xx Ã°x; tÃ¾ are strain and stress ï¬elds in x plane corresponding to displacement in x direction ux Ã°x; tÃ¾. similarly, Îµxy Ã°x; tÃ¾ and
Ïƒ xy Ã°x; tÃ¾ are strain and stress ï¬elds in x plane corresponding to
displacement in y direction; i.e. uy Ã°x; tÃ¾. vp , vs and Ï are p- and s-waves
velocities and the density of the medium, respectively.
to involve these relations into the learning process, one needs to
compute the strain by computing differential of estimated uÃ°x; tÃ¾ (in both
x and y directions) using eqs. (28) and (30). since velocities and density
are unknowns, stress must be estimated by the neural network and, then,
tied to strain by eqs. (29) and (31). the wave equation is similar to eq.
(24), but for each of p- and s-wave. this leads to simultaneous inversion
of p- and s-wave velocity and as well as density as shown in fig. 6.
to estimate the stress values, either one neural network with two
outputs or two neural networks each with one output might be used.
previous studies (haghighat et al., 2020) showed that one output per
network is more effective. here, we used four individual networks for
each of ux Ã°x;tÃ¾, Ïƒ xx Ã°x;tÃ¾, uy Ã°x; tÃ¾ and Ïƒ xy Ã°x; tÃ¾ as illustrated in fig. 6. thus,
the loss function is deï¬ned as:

fig. 7. mse values of different components of the loss function during training.

stopping procedure. fig. 8 shows the results of the pinn for prediction of
ux Ã°x; tÃ¾, Ïƒ xx Ã°x; tÃ¾, uy Ã°x;tÃ¾ and Ïƒ xy Ã°x;tÃ¾ and their corresponding errors.
according to this pinn, p- and s-wave velocities and density values
have been inverted as 1.9998 km/s, 1.2001 km/s and 2.1983 g/cm3
which are very close to their original values (2 km/s, 1.2 km/s and 2.2 g/
cm3). it can also be concluded from these results that not only pinn
optimization process is more stable and the inverted values are more
accurate, but it is also more ï¬‚exible then gp for involving more physical
relation into learning process.
the problem solved in this study is a 2d problem: 1d wave equation
in space and time Ã°x; tÃ¾. to solve more complex problems in 3d, the
networks and loss functions must be extended to, for example, more inputs and outputs. in fact, there is no limitations for extending the
methods beyond 2d problems except that the computational time may
increase for higher-dimensional problems. in this study, a system with
intel core i7 cpu, 32 gb ram and gtx1050ti gpu was used. the
computation time for gp depends on the number of input data and the
utilized optimization algorithm. in our study, however, it took few seconds to complete the process. the run time for training of our large pinn
(fig. 6) was less than 19 min.

nu
nu
2 1 x

  
 2
1 x
loss functionÂ¼
Ïƒ xx x;t  Ïƒ *xx x;t
ux Ã°x;tÃ¾  u*x Ã°x;tÃ¾ Ã¾
nu iÂ¼1
nu iÂ¼1

Ã¾

nu
nu
2
1 x
1 x
uy Ã°x;tÃ¾  u*y Ã°x;tÃ¾ Ã¾
Ïƒ xy x;t  Ïƒ *xy x;t
nu iÂ¼1
nu iÂ¼1

2

2
 2
nu 
nu 
*
âˆ‚u*y x;t 
1 x
1 x
*
2 âˆ‚ux x;t
*
2
Ã¾
Ïƒ xx Ã°x;tÃ¾  vp Ï
Ã¾
Ïƒ xy Ã°x;tÃ¾  vs Ï
nu iÂ¼1
nu iÂ¼1
âˆ‚x
âˆ‚x

Ã¾

!2
 
2
nv  2 *
nv
2 *
âˆ‚2 u*y x;t
âˆ‚2 u*y Ã°x;tÃ¾
âˆ‚ ux x;t
1 x
1 x
2 âˆ‚ ux Ã°x;tÃ¾
2

v
Ã¾

v
p
s
nv iÂ¼1
nv iÂ¼1
âˆ‚t 2
âˆ‚x2
âˆ‚t 2
âˆ‚x2
(32)

we supposed the medium with p- and s-wave velocity of 2 and 1.2
km/s and a density of 2.2 g/cm3 and used the same wave relation as in
eq. (25) for both p- and s-wave. therefore, the stress ï¬eld is deï¬ned as
2Ï€ Ã°v ÏÃ¾sinÂ½2Ï€ Ã°t  x =vÃ¾. using similar arguments as the previous pinn,
this network was also trained and tested. fig. 7 shows evolution of the
loss function and its components during the training, which indicates that
the training process has been stopped at 5348-th epoch using early

fig. 6. architecture of the pinn for solving 1d seismic wave equation involving with linear elastic equations for inversion of p- and s-wave velocities and density.
1999

s. karimpouli, p. tahmasebi

geoscience frontiers 11 (2020) 1993â€“2001

fig. 8. predicted values and the absolute error of predictions for (a, b) uÃ°x; tÃ¾ , (c, d) Ïƒ x Ã°x; tÃ¾, (e, f) uy Ã°x; tÃ¾ and (g, h) Ïƒ y Ã°x; tÃ¾.

5. conclusions

equation of a 1d seismic wave as one of the most popular equations in
geosciences. in overall, the following points are concluded by this work:

physics informed machine learning methods belong to those data
driven methods which are trained not only by the observed data, but also
by the induced physics laws. in this study, we implemented the gaussian
process and physics informed machine learning to solve differential

(1) both gp and pinn can solve the differential equation and predict
the uÃ°x; tÃ¾ with a small error. however, the error level of the gp is
smaller than pinn.
2000

s. karimpouli, p. tahmasebi

geoscience frontiers 11 (2020) 1993â€“2001

(2) the prediction error is increased by adding noise into the training
data, but both methods are robust to the random noise.
(3) in gp, minimizing the log marginal likelihood is likely to result in
an ill-positioned covariance matrix or local minima.
(4) due to probabilistic nature of gp, variance of predictions is also
computed which can be used for uncertainty quantiï¬cation.
(5) both gp and pinn can invert the velocity of wave as an unknown
parameter of the equation accurately. however, pinn is more
accurate than the gp.
(6) the pinn is ï¬‚exible enough to add more physical relations and,
therefore, can invert more unknown parameters.

kingma, d.p., ba, j., 2014. adam: a method for stochastic optimization. arxiv:
1412.6980.
kong, q., trugman, d.t., ross, z.e., bianco, m.j., meade, b.j., gerstoft, p., 2019. machine
learning in seismology: turning data into insights. seismol res. lett. 90, 3â€“14.
https://doi.org/10.1785/0220180259.
lagaris, i.e., likas, a., fotiadis, d.i., 1998. artiï¬cial neural networks for solving ordinary
and partial differential equations. ieee trans. neural network. 9, 987â€“1000. https://
doi.org/10.1109/72.712178.
lawrence, n., 2005. probabilistic non-linear principal component analysis with gaussian
process latent variable models. j. mach. learn. res. 6, 1783â€“1816.
lee, m., lee, j., chang, j.-h., 2019. ensemble of jointly trained deep neural networkbased acoustic models for reverberant speech recognition. digit. signal process. 85,
1â€“9. https://doi.org/10.1016/j.dsp.2018.11.005.
maitra, s., ghosh, r., ghosh, k., 2019. applications of deep learning in medical imaging.
in: balas, v.e., roy, s.s., sharma, d., samui, p. (eds.), handbook of deep learning
applications. springer international publishing, cham, pp. 111â€“127. https://doi.org/
10.1007/978-3-030-11479-4_6.
meade, a.j., fernandez, a.a., 1994. the numerical solution of linear ordinary differential
equations by feedforward neural networks. math. comput. model. 19, 1â€“25. https://
doi.org/10.1016/0895-7177(94)90095-7.
nasirahmadi, a., sturm, b., edwards, s., jeppsson, k.-h., olsson, a.-c., mÃ¼ller, s.,
hensel, o., 2019. deep learning and machine vision approaches for posture detection
of individual pigs. sensors 19 (17), 3738. https://doi.org/10.3390/s19173738.
neal, r.m., 2012. bayesian learning for neural networks, vol. 118. springer science &
business media.
raissi, m., perdikaris, p., karniadakis, g.e., 2019. physics-informed neural networks: a
deep learning framework for solving forward and inverse problems involving
nonlinear partial differential equations. j. comput. phys. 378, 686â€“707. https://
doi.org/10.1016/j.jcp.2018.10.045.
raissi, m., perdikaris, p., karniadakis, g.e., 2017a. inferring solutions of differential
equations using noisy multi-ï¬delity data. j. comput. phys. 335, 736â€“746. https://
doi.org/10.1016/j.jcp.2017.01.060.
raissi, m., perdikaris, p., karniadakis, g.e., 2017b. machine learning of linear differential
equations using gaussian processes. j. comput. phys. 348, 683â€“693. https://doi.org/
10.1016/j.jcp.2017.07.050.
raissi, m., yazdani, a., karniadakis, g.e., 2020. hidden ï¬‚uid mechanics: learning velocity
and pressure ï¬elds from ï¬‚ow visualizations. science 367 (6481), 1026â€“1030.
https://doi.org/10.1126/science.aaw4741.
rasmussen, c.e., williams, c.k., 2006. gaussian processes for machine learning. mit
press, massachusetts.
rouet-leduc, b., hulbert, c., mcbrearty, i.w., johnson, p.a., 2020. probing slow
earthquakes with deep learning. geophys. res. lett. 47, e2019gl085870. https://
doi.org/10.1029/2019gl085870.
sheriff, r.e., geldart, l.p., 1995. exploration seismology. cambridge university press,
uk.
tahmasebi, p., kamrava, s., bai, t., sahimi, m., 2020. machine learning in geo- and
environmental sciences: from small to large scale. adv. water resour. 142, 103619.
https://doi.org/10.1016/j.advwatres.2020.103619.
tang, v., seetharaman, p., chao, k., pardo, b.a., van der lee, s., 2020. automating the
detection of dynamically triggered earthquakes via a deep metric learning algorithm.
seismol res. lett. 91, 901â€“912. https://doi.org/10.1785/0220190165.
titsias, m., lawrence, n.d., 2010. bayesian gaussian process latent variable model. in:
proceedings of the 13th international conference on artiï¬cial intelligence and
statistics (aistats) 2010. chia laguna resort, sardinia, italy, pp. 844â€“851.
vapnik, v., 2013. the nature of statistical learning theory. springer science & business
media, p. 768.
vishal, p., aggarwal, k.r., 2019. a hybrid of deep cnn and bidirectional lstm for
automatic speech recognition. j. intell. syst. 29 (1), 1261â€“1274. https://doi.org/
10.1515/jisys-2018-0372.
waldeland, a.u., jensen, a.c., gelius, l.-j., solberg, a.h.s., 2018. convolutional neural
networks for automated seismic interpretation. lead. edge 37, 529â€“537. https://
doi.org/10.1190/tle37070529.1.
wang, j., tchapmi, l.p., ravikumar, a.p., mcguire, m., bell, c.s., zimmerle, d.,
savarese, s., brandt, a.r., 2020. machine vision for natural gas methane emissions
detection using an infrared camera. appl. energy 257, 113998. https://doi.org/
10.1016/j.apenergy.2019.113998.
xiong, w., ji, x., ma, y., wang, y., albinhassan, n.m., ali, m.n., luo, y., 2018. seismic
fault detection with convolutional neural network. geophysics 83 (5), o97â€“o103.
https://doi.org/10.1190/geo2017-0666.1.
zhu, c., byrd, r.h., lu, p., nocedal, j., 1997. algorithm 778: l-bfgs-b: fortran
subroutines for large-scale bound-constrained optimization. acm trans. math
software 23 (4), 550â€“560. https://doi.org/10.1145/279232.279236.
zhu, y., zabaras, n., koutsourelakis, p.-s., perdikaris, p., 2019. physics-constrained deep
learning for high-dimensional surrogate modeling and uncertainty quantiï¬cation
without labeled data. j. comput. phys. 394, 56â€“81. https://doi.org/10.1016/
j.jcp.2019.05.024.

declaration of competing interest
the authors declare that they have no known competing ï¬nancial
interests or personal relationships that could have appeared to inï¬‚uence
the work reported in this paper.
references
alvarez, m.a., luengo, d., lawrence, n.d., 2013. linear latent force models using
gaussian processes. ieee trans. pattern anal. mach. intell. 35 (11), 2693â€“2705.
https://doi.org/10.1109/tpami.2013.86.
aslam, m., khan, t.m., naqvi, s.s., holmes, g., naffa, r., 2019. on the application of
automated machine vision for leather defect inspection and grading: a survey. ieee
access 7, 176065â€“176086. https://doi.org/10.1109/access.2019.2957427.
bai, t., tahmasebi, p., 2020. hybrid geological modeling: combining machine learning
and multiple-point statistics. comput. geosci. 104519. https://doi.org/10.1016/
j.cageo.2020.104519.
bernal, j., kushibar, k., asfaw, d.s., valverde, s., oliver, a., martÃ­, r., llad
o, x., 2019.
deep convolutional neural networks for brain image analysis on magnetic resonance
imaging: a review. artif. intell. med. 95, 64â€“81. https://doi.org/10.1016/
j.artmed.2018.08.008.
cockayne, j., oates, c., sullivan, t., girolami, m., 2017. probabilistic numerical methods
for pde-constrained bayesian inverse problems. aip conf. proc. 1853, 060001.
https://doi.org/10.1063/1.4985359.
dondelinger, f., husmeier, d., rogers, s., filippone, m., 2013. ode parameter inference
using adaptive gradient matching with gaussian processes. in: sixteenth
international conference on artiï¬cial intelligence and statistics. scottsdale, az, usa,
pp. 216â€“228.
garcia-garcia, a., orts-escolano, s., oprea, s., villena-martinez, v., garcia-rodriguez, j.,
2017. a review on deep learning techniques applied to semantic segmentation.
https://doi.org/10.1007/978-1-4471-4640-7. arxiv:1704.06857.
guo, z., li, x., huang, h., guo, n., li, q., 2019. deep learning-based image segmentation
on multimodal medical imaging. ieee trans. radiat. plasma med. sci. 3, 162â€“169.
https://doi.org/10.1109/trpms.2018.2890359.
haghighat, e., raissi, m., moure, a., gomez, h., juanes, r., 2020. a deep learning
framework for solution and discovery in solid mechanics: linear elasticity. arxiv:
2003.02751.
kamrava, s., sahimi, m., tahmasebi, p., 2020a. quantifying accuracy of stochastic
methods of reconstructing complex materials by deep learning. phys. rev. e 101,
043301. https://doi.org/10.1103/physreve.101.043301.
kamrava, s., tahmasebi, p., sahimi, m., 2019a. linking morphology of porous media to
their macroscopic permeability by deep learning. transport porous media. https://
doi.org/10.1007/s11242-019-01352-5.
kamrava, s., tahmasebi, p., sahimi, m., 2019b. enhancing images of shale formations by
a hybrid stochastic and deep learning algorithm. neural netw. 118, 310â€“320.
https://doi.org/10.1016/j.neunet.2019.07.009.
kamrava, s., tahmasebi, p., sahimi, m., arbabi, s., 2020b. phase transitions, percolation,
fracture of materials, and deep learning. phys. rev. e 102 (1), 011001. https://
doi.org/10.1103/physreve.102.011001.
karimpouli, s., tahmasebi, p., 2019a. segmentation of digital rock images using deep
convolutional autoencoder networks. comput. geosci. 126, 142â€“150. https://
doi.org/10.1016/j.cageo.2019.02.003.
karimpouli, s., tahmasebi, p., 2019b. image-based velocity estimation of rock using
convolutional neural networks. neural netw. 111, 89â€“97. https://doi.org/10.1016/
j.neunet.2018.12.006.
karimpouli, s., tahmasebi, p., saenger, e.h., 2019. coal cleat/fracture segmentation
using convolutional neural networks. nat. resour. res. 29, 1675â€“1685. https://
doi.org/10.1007/s11053-019-09536-y.

2001



journal of physics: conference
series

you may also like

paper â€¢ open access

seismic waveform inversion using a neural
network-based forward

- an optimized scheme of dispersion
suppression for elastic-wave variableorder rotated staggered-grid forward
modeling
weizhong wang, tianyue hu, jianyong
song et al.

to cite this article: hongsun fu et al 2019 j. phys.: conf. ser. 1324 012043

- multi-component gaussian beam prestack
depth migration
jianguang han, yun wang and jun lu

view the article online for updates and enhancements.

- attenuation (1/q) estimation in reflection
seismic records
wasiu raji and andreas rietbrock

this content was downloaded from ip address 200.12.181.232 on 24/08/2024 at 17:29

the second international conference on physics, mathematics and statistics
iop publishing
iop conf. series: journal of physics: conf. series 1324 (2019) 012043 doi:10.1088/1742-6596/1324/1/012043

seismic waveform inversion using a neural network-based
forward
hongsun fu, yan zhang1 and mingyue ma
school of science, dalian maritime university, dalian 116026, china
1

email: zhangyan_@dlmu.edu.cn

abstract. the purpose of seismic waveform inversion is to obtain a geological model that is
optimally fitted to the predicted seismic record and the measured seismic record. since the
forward model is repeatedly called during the inversion process, in order to improve efficiency,
an efficient forward calculation method must be employed. in this study, we take a 2d wave
equation as an example and propose a deep learning method as a forward model to minimize
the prediction error value of seismic records. and the velocity inversion test of the marmousi
model is carried out by conjugate gradient method. numerical experiments show that
compared with the traditional finite difference method, the method can greatly reduce the
calculation amount and improve the calculation efficiency.

1. introduction
seismic wave forward modeling is an important basis for seismic exploration. it is widely used in
seismic exploration and natural seismic research. by seismic forward modeling, we can check the
rationality of design, and the reliability of the results of processing and interpretation, and the validity
of inversion method and results. the regularity of the forward modeling in a given geological model
can enhance people's understanding of the unknown model and thus contribute to the solution of
geological problems. generally, the forward modeling of seismic waves can be formulated as an
operator equation
d ï€½ g ( m)
(1)
where d is the seismic data vector, g is the forward operator, and m is the model parameter.
the main research methods of seismic wave field forward modeling are divided into ray tracing
method and wave equation method [1, 2]. and the wave equation plays an important role in the
propagation principle of seismic waves and the interpretation of complex formations [3, 4]. there are
many ways to solve the wave equation [5, 6, 7], including finite difference method and finite element
method [8-11]. however, the resolution of the finite difference operator is low in the time domain, and
the stability and convergence of the algorithm are affected by the spatial sampling rate and the
temporal sampling rate [12, 13]. the finite element method requires high memory and a large amount
of computation of the computer, and different boundary conditions need to be set according to
different situations. therefore, we propose using neural network-based forward to replace the
traditional finite difference method for seismic forward modeling.
deep learning is a new field of machine learning that has drawn widespread interests by showing
outstanding performances for recognition and classification in image and speech processing [14-17].
the forward problems in equation (1) describe a mapping between, which is usually, one set of
continuous parameters to another set of continuous parameters [18]. we will refer to these methods in

content from this work may be used under the terms of the creative commons attribution 3.0 licence. any further distribution
of this work must maintain attribution to the author(s) and the title of the work, journal citation and doi.
published under licence by iop publishing ltd
1

the second international conference on physics, mathematics and statistics
iop publishing
iop conf. series: journal of physics: conf. series 1324 (2019) 012043 doi:10.1088/1742-6596/1324/1/012043

general as regression networks, and it can be used to estimate the forward operator g in equation (1).
in the following, it is used to replace a computationally expensive forward problem with a regression
network. the main advantage is that this will allow a very fast evaluation of the forward problem,
making it possible to perform model inversion more efficiently. in addition, we use the adjoint-state
method to compute the gradient of the objective function, hence the fwi workflow is simplified so we
can focus mainly on the forward modeling and the model updating. finally, the neural network-based
forward model will be compared to using other widely used approximate forward models (based on
finite difference approximations) in formulating. numerical experiments performed on various data
demonstrate the applicability of our method.
the paper is organized as follows. section 2 gives a brief introduction to the basic inversion
problem, and the concept of seismic forward modeling. the main methods of this paper are introduced
in section 3, including mathematical framework of using neural networks, preparation of training data
sets, and forward modeling results based on a simple geological models. section 4 shows the inversion
results of numerical experiments on complex the marmousi models using neural network forward
modeling. finally, the conclusion and future work are outlined in section 5.
2. 2d acoustic-wave equation
for a 2d velocity-depth model, the acoustic-wave equation is expressed as
2
ï‚¶ 2u
ï‚¶ 2u
2 ï‚¶ u
ï€½ v ( 2 ï€« 2 ) ï€« s (t )
ï‚¶t 2
ï‚¶x
ï‚¶z

(2)

where v( x, z ) is the velocity of the longitudinal wave at the point ( x, z ) , u is the wave field
describing the velocity bit or pressure, and s(t ) is the source functions.
indispensable conditions required for seismic forward modeling include seismic source functions,
formation velocity (wave velocity), and boundary conditions. in order to highlight theoretical values in
the model, this paper assumes that all noise, wave conversion, absorption, and diffraction are not
considered. during the iterative forward modeling procedure, we use a perfectly matched layer (pml)
absorbing boundary condition [19], the ricker wavelet as the seismic source function
2 2

s(t ) ï€½ e ( ï€­2ï° f/ï§ ) t cos 2ï° ft

(3)

where t is the time and f is the center frequency, generally taken as 20-40hz. in this paper, 20hz is
taken, r is the parameter of the control band width, and it is choose to be 4.
3. train data using neural networks
in this section, we first describe the mathematical framework of our novel method using neural
network-based forward modeling. then, we introduce the data preparation using a simple model and
the forward implementation of the model.
3.1. mathematical framework based on neural network model
an expected result of forward modeling is to minimize the simulation data records generated by the
network and the actual data records. we define the cost function as the sum of the squares of the
seismic record errors

eï€½

1 n
ïƒ¥ ( xk ï€­ d k ) 2
2 k ï€½1

(4)

where xk is the corresponding data computed using the exact, but expensive, forward model based on
finite-difference modeling, and d k is the simulated seismic data record.
as discussed, many types of machine-learning regression algorithms exist that can be used to
obtain forward modeling data. in this study, we use a simple two layer feed-forward neural network,

2

the second international conference on physics, mathematics and statistics
iop publishing
iop conf. series: journal of physics: conf. series 1324 (2019) 012043 doi:10.1088/1742-6596/1324/1/012043

that is, with one hidden layer, is considered to replace the accurate forward model [20]. such a neural
network that can be formulated as
nh

nm

j

i

d k ï€½ h1 ( ïƒ¥ ï¡ jk 2 h2 ( ïƒ¥ ï¡ jk 1 xi ))

(5)

where nm ï€½ 1000 is the number of model parameters and nh is the number of hidden units. in this
case, nh ï€½ 80 hidden units are considered, h1 and h 2 refer to two activation functions, in this case
both chosen to be of tanh type. ï¡ jk and ï¡ jk refer to the weights of the units in the first and second
layers. the main reason a two-layer feed-forward network is considered, is that the universal
approximation theorem [21], which states that a feed-forward network with a single hidden layer
containing a finite number of neurons can approximate any continuous function on compact subsets,
under mild assumption on the activation function.
in actual training, we take the seismic wavelet as an input and repeatedly correct its weight such
that the neural network will be able to estimate the data d given a specific model m in an optimal
way. this is achieved by training the neural network based on the training data set, which we usually
consider as an optimization problem, traditionally solved using the backpropagation algorithm.
the network learns by solving the optimization problem as
1

2

ï¡Ë† ï€½ arg min

1 n
( xi ï€­ h ( xi , ï¡ ))2
ïƒ¥
n i ï€½1

(6)

where h ( xi , ï¡ ) operator is implemented by using the neural network described above.
a commonly used minimization method is the gradient descent method to iteratively update the
parameter vector as follows

ï¡t ï€«1 ï€½ ï¡t ï€­ ï¤

ï‚¶le
ï‚¶ï¡

(7)

where ï¤ is the positive learning rate.
the neural network algorithm based on forward modeling is as follows:
initialize the weight ï¡ jk1 and ï¡ jk 2 , select the learning rate ï¤ , determine the error precision and the
maximum number of iterations, and let i ï€½ 1 ;
input training sample pairs and calculate the output d k according to equation (5);
calculate the network output error according to equation (4). when the total network error is less
than the error precision a within the maximum number of iterations, the algorithm converges to exit
the training, otherwise it goes to step 4;
the backpropagation method. adjust the weight of each layer according to equation (7), let
i ï€½ i ï€« 1, turn to step3.
3.2. forward implementation
in order to reflect the application effect of neural network based forward model, we simulated the
forward results of a simple geological model. in this paper, a three-layer horizontal layered
homogeneous medium model is used. based on the actual geological structure and the velocity data of
various rocks, we determined the velocity and depth parameters of the three layers of horizontal
layered uniformity. the seismic forward waveform records of the model are obtained under the
condition that the seismic simulation records parameters are unchanged. table 1 shows the structure
and parameters of the geological model.

3

the second international conference on physics, mathematics and statistics
iop publishing
iop conf. series: journal of physics: conf. series 1324 (2019) 012043 doi:10.1088/1742-6596/1324/1/012043

table 1. model construction and parameters.
velocity(m/s)
1500
2000
2500

depth(m)
1000
1200
1400

combined with the geological model, based on the matlab program, the seismic forward record
using a neural network is obtained as shown in figure 1. figure 2 is an error curve of actual seismic
records and simulated seismic records. figure 2 shows use of the neural network-based forward model
results in a modeling error bias is very close to zero, this is because the optimization of the neural
network minimizes the mean square error. it shows that the neural network-based forward model has
achieved good results.

figure 1. the seismic forward waveform
record of the geological.

figure 2. error curve of actual seismic
records and simulated seismic records.

4. inversion results based on marmousi model
in order to verify the effectiveness of the proposed method, we applied the conjugate gradient method
to the full-waveform inversion of the time domain acoustic wave and tested it using the marmousi
model (the marmousi original model was created by the french petroleum institute in 1988 and is
often used to test the correctness and rationality of advanced geophysical imaging methods). in time
domain forward modeling, ricker wavelet is used as source function, pml is used as absorbing
boundary condition, and neural network-based forward is used to synthesize seismic data sets.
for each velocity model, we use horizontal and vertical spacing of 5 m , a total of 60 shots, shots
are equally spaced on the interface of z ï€½ 50m , 180 detectors are equally spaced surface. the velocity
obtained by smoothing the real the marmousi model at large scale is used as the initial guess. the
inversion frequency is in uniform step from the minimum frequency of 5 hz to the maximum
frequency of 20hz. a total of 12 frequencies participate in the inversion.
the inversion results of the marmousi model are shown in figure 3. figures 3(a) and 3(b) are true
and initial velocity models respectively. figure 3(c) shows the results of the inversion velocity using
the neural network for forward modeling. the results show that the inversion results reconstruct the
general features of the model, and the boundaries part of the model is resolved reasonably well. it is
shown that the neural network based on the forward model also has a good inversion result.
in order to test the effect of using neural network-based forward model in this paper, table 2 lists
the consumption time for forward modeling seismic records between neural network method and finite
difference method, and shows the neural network-based forward has faster convergence velocity and
less time consuming than the finite difference method.

4

the second international conference on physics, mathematics and statistics
iop publishing
iop conf. series: journal of physics: conf. series 1324 (2019) 012043 doi:10.1088/1742-6596/1324/1/012043

table 2. comparison on time consuming.
method

time consuming

number of iterations

average time spent

finite difference

6933.8 s

643

10.78 s

neural network

4832.4 s

482

10.03 s

(a)

(b)

(c)

figure 3. inversion result of marmousi model. (a) true velocity model, (b) initial velocity model, (c)
inversion model of the proposed method.
5. conclusions
in this paper, we starting from the time domain wave equation, the error squared sum of the wave field
is taken as the objective function, and the minimum value is solved by using a neural network-based
forward model. a new velocity model construction is demonstrated using neural network that allows
the use of neural network forward modeling instead of computational complex forward modeling.

5

the second international conference on physics, mathematics and statistics
iop publishing
iop conf. series: journal of physics: conf. series 1324 (2019) 012043 doi:10.1088/1742-6596/1324/1/012043

under the premise of ensuring the same imaging results, the calculation amount is significantly
reduced, and the calculation efficiency is improved. however, due to the limited number of training
samples and oversimplied synthetic models and data, future work will focus on the improvements to
the dl-based neural networks that will be applied for precise velocity model building with real
seismic data.
acknowledgments
this work is partially supported by the national natural science foundation of china (grant nos.
41474102, 41304092).
references
[1] chapman c h 1985 ray theory and its extensions: wkbj and maslov seismograms geophysics
27-43
[2] cerveny v 2001 seismic ray theory cambridge university press cambridge uk
[3] zhang y g 2003 numerical simulation of seismic wave field geophysics 42(2) 143-148
[4] cao x l, li z, tian c k, huang j p and yang y 2014 forward modeling of p-and s-wave
separation in viscoelastic media 76th eage conference and exhibition
[5] komatitsch d, barnes c and tromp j 2000 wave propagation near a fluid-solid interface: a
spectral-element approach geophysics 65(2) 623-631
[6] li n, li z c, huang j p, guo z, tian k and li q 2014 numerical simulation with coupling
lebedev and standard staggered grid schemes for complex anisotropic media oil geophysical
prospecting 49 (1) 121-131
[7] witten b and shragge j 2018 wave equation imaging and adjoint-state inversion for microseismic monitoring preview
[8] bernth h and chapman c 2011 a comparison of the dispersion relations for anisotropic
elastodynamic finite-difference grids geophysics 76(3) 43-50
[9] zhang w and chen x 2006 traction image method for irregular free surface boundaries in
finite difference seismic wave simulation geophysical journal of the royal astronomical
society 167(1) 337-353
[10] meng w and fu l y 2017 seismic wavefield simulation by a modified finite element method
with a perfectly matched layer absorbing boundary journal of geophysics and engineering
14(4) 852-864
[11] liu s, li x, wang w and liu y 2014 a mixed-grid finite element method with pml absorbing
boundary conditions for seismic wave modelling journal of geophysics and engineering
11(5) 055009
[12] zhu q, jiang l and zhang w 2018 effects of media discretization method on finite difference
simulation for seismic wave field physical prospecting for petroleum 57(2) 198-207
[13] huang j p, liao w y and li c h 2018 a multi-block finite difference method for seismic wave
equation in auxiliary coordinate system with irregular fluid-solid interface engineering
computations 35(1) 334-362
[14] goodfellow i, y bengio and a courville 2016 deep learning mit press
[15] lecun y, bengio y and hinton g 2015 deep learning nature 521 436-444
[16] lecun y, kavukcuoglu k, and farabet c 2010 convolutional networks and applications in
vision: circuits and systems (iscas) proceedings of 2010 ieee international symposium
on circuits and systems ieee 253-256
[17] krasnopolsky v m and schiller h 2003 some neural network applications in environmental
sciences. part i: forward and inverse problems in geophysical remote measurements neural
netw 16(3) 321-334
[18] richardson a 2018 seismic full-waveform inversion using deep learning tools and techniques
geophysics

6

the second international conference on physics, mathematics and statistics
iop publishing
iop conf. series: journal of physics: conf. series 1324 (2019) 012043 doi:10.1088/1742-6596/1324/1/012043

[19] komatitsch d and tromp j 2003 a perfectly matched layer absorbing boundary condition for
the second-order seismic wave equation geophysics 154 146-153
[20] hansen t m and cordua k s 2017 efficient monte carlo sampling of inverse problems using a
neural network-based forward-applied to gpr crosshole traveltime inversion geophysics
211 1524-1533
[21] csaji b 2001 approximation with artificial neural networks faculty of sciences etvs lornd
university 24-48

7



deep learning seismic substructure detection using the frozen
gaussian approximation
james c. hateley, jay roberts, kyle mylonakis, xu yang

arxiv:1810.06610v4 [physics.geo-ph] 5 nov 2019

department of mathematics, university of california, santa barbara, ca 93106, usa

abstract
we propose a deep learning algorithm for seismic interface and pocket detection with neural
networks trained by synthetic high-frequency displacement data efficiently generated by the
frozen gaussian approximation (fga). in seismic imaging high-frequency data is advantageous since it can provide high resolution of substructures. however, generation of sufficient
synthetic high-frequency data sets for training neural networks is computationally challenging. this bottleneck is overcome by a highly scalable computational platform built upon the
fga, which comes from the semiclassical theory and approximates the wavefields by a sum
of fixed-width (frozen) gaussian wave packets.
training data for deep neural networks is generated from a forward simulation of the elastic wave equation using the fga. this data contains accurate traveltime information (from
the ray path) but not exact amplitude information (with asymptotic errors not shrinking to
zero even at extremely fine numerical resolution). using this data we build convolutional
neural network models using an open source api, geoseg, developed using keras and tensorflow. on a simple model, networks, despite only being trained on data generated by the
fga, can detect an interface with a high success rate from displacement data generated by
the spectral element method. benchmark tests are done for p-waves (acoustic) and p- and swaves (elastic) generated using the fga and a spectral element method. further, results with
a high accuracy are shown for more complicated geometries including a three-layered model,
a sine interface, and a 2d-pocket model where the neural networks are trained by both clean
and noisy data.
keywords: seismic tomography, convolutional neural network, elastic wave equation,
high-frequency wavefield, frozen gaussian approximation, image segmentation
1. introduction
various geophysical aspects, e.g., tectonics and geodynamics [1, 20, 19, 29], can be better understood by images of substructures (e.g. locations of seismic interfaces) of the earth
generated by seismic tomography. neural networks excel at recognizing shapes, patterns, and
âˆ—

corresponding author
email addresses: hateleyjc@gmail.com (james c. hateley), jayroberts@math.ucsb.edu (jay
roberts), kmylonakis@math.ucsb.edu ( kyle mylonakis), xuyang@math.ucsb.edu (xu yang)

preprint submitted to elsevier

november 6, 2019

sorting relevant from irrelevant data; this makes them good for image recognition and classification. in particular, convolutional neural networks allowed for rapid advances in image
classification and object detection [14], and in fact networks have been created for specific
tasks, such as, fault detection [2], earthquake detection, convnetquake [18], deepdetect [26]
and seismic phase arrival times, phasenet [30]. one obstacle in building a neural network to
detect seismic structures is having an ample data set for training. there is constant waveform
data being collected by seismic stations across the globe, and generating data by resampling
of this seismic data to train a network can be done, but is limited by the nyquist frequency.
seismic data can not be resampled with a nyquist frequency lower than the highest usable
frequency in the data, thus high frequency data is usually preferred as it tends to lead to improved resolution of the substructures. other difficulties of gathering an ample data lie within
the differences in geological locations, natural phenomenon (e.g. earthquakes) and unnatural phenomenon (e.g. fracking). using these data sets to train a general neural network is a
daunting task, and thus it is natural to use synthetic data for the training of neural networks.
the dominant frequency of a typical earthquake is around 5 hz [17] leading to demanding,
and at times, unaffordable computational cost. this makes generation of sufficient synthetic
high-frequency data sets for training neural networks computationally challenging to wellknown methods. we overcome this difficulty by building a highly scalable computational
platform upon the frozen gaussian approximation (fga) method for the elastic wave equation
[9], which comes from the semiclassical theory. the fga approximates the wavefields by a
sum of fixed-width (frozen) gaussian wave packets. the dynamics of each gaussian wave
packet follow ray paths with the prefactor amplitude equation derived from an asymptotic
expansion on the phase plane. the whole set of governing equations are decoupled for each
gaussian wave packet, and thus, in theory, each corresponding ode system can be solved on
its own process, making the algorithm embarrassingly parallel.
using synthetic data, araya-polo et al. perform inverse tomography via fully connected
neural networks with great success in [3] . their networks use low dimensional features
extracted from seismic data as input. using deeper convolutional neural networks trained
on seismogram data may allow the network to pick up on previously unknown signals. the
increase in input dimensionality necessitates more sophisticated deep learning techniques than
those presented in [3].
in this paper, we propose a deep learning algorithm for seismic interface detection, with
the neural networks trained by synthetic high-frequency seismograms. we first generate the
time series of synthetic seismogram data by the fga, which we use to train neural networks
made with an open source api, geoseg, developed using keras and tensorflow. despite only
being trained on fga generated data we observe the networks are able to detect a 1d interface with a high success rate on data generated by spectral element method. this method
more acucurately represents true seismic signals when fine time step and mesh sizes are used
in the computation. we conjecture that this robustness is due to the fact that although fga
does not carry exact amplitude information (with asymptotic errors proportional to the ratio
of wavelength over domain size), it contains accurate traveltime information. for this simple problem it is straight-forward in geophysics to identify the traveltime as a key factor in
interface location; however, this is not built into the network and so its use must be learned.
with the success of the 1d interface detection, we further apply the deep learning algorithms
for geometries with more complicated structures, including a three layered model and a 2d
2

pocket model, both of which show a high accuracy. we also investigate the effect of noise by
studying the performance of deep learning algorithms on noisy validation data, with the neural
networks trained using clean and noisy data, respectively.
the paper is outlined as follows: in section 2, we review briefly the mathematical background of fga and describe how the synthetic data is generated. in section 3, we describe
the details of the network design including network and block architectures. in section 4 we
show the performance of various networks on a series of geometries with different substructures, using both clean and noisy data. concluding remarks are made in section 5.
2. frozen gaussian approximation
we summarize the mathematical theory of fga in this section; for full exposition and
details for the elastic wave equation, see [9]; and for the acoustic wave equation, see [5].
the core idea of the fga is to approximate seismic wavefields by fixed-width gaussian
wave packets whose dynamics follow ray paths with the prefactor amplitude equation derived
from an asymptotic expansion on the phase plane. the ode system governing the dynamics
for each wave packet are decoupled. in theory, each ode system can be solved on its own
process, hence it is embarrassingly parallel. the implementation, as in previous works [9], is
with fortran using message passage interface (mpi). the implementation has a speed up factor
of approximately 1.94; hence, doubling the number of cores nearly halves the computational
time. the equation for the forward modeling to generate the training data set we use is the
elastic wave equation [7],
Ïâˆ‚t2 u = (Î» + Âµ)âˆ‡(âˆ‡ Â· u) + Âµâˆ†u + f,

(1)

where Ï, Î», Âµ, : r3 â†’ r is the material density, the first and second lameÌ parameters respectively and u : r Ã— r3 â†’ r3 is displacement. the differential operators are taken in terms of
the spacial variables. eq. (1) has a natural separation into divergence and curl free components
and can also be written as
âˆ‚t2 u = c2p âˆ‡(âˆ‡ Â· u) âˆ’ c2s âˆ‡ Ã— âˆ‡ Ã— u + fÏ .

(2)

this decomposition represents p-wave, and s-wave respectively with velocities
c2p (x) =

Î»(x) + 2Âµ(x)
,
Ï(x)

c2s (x) =

Âµ(x)
,
Ï(x)

(3)

with cp (x) representing the p-wave speed and cs (x) representing the s-wave speed.
2.1. the fga formulation
presented below is an outline for the fga. for derivation and benchmarking tests we
refer to [9, 5]. we introduce the fga formula for the elastic wave equation (2), with initial
conditions
(
u(0, x) = f k (x),
(4)
âˆ‚t u(0, x) = gk (x),

3

where the superscript k represents the wavenumber. for a sake of simplicity and clarity, we
shall also use the following notations:
âˆš
â€¢ i = âˆ’1 : the imaginary unit;
â€¢ subscripts/superscripts â€œpâ€ and â€œsâ€ indicate p- and s-waves, respectively;
â€¢ Â± indicates the two-way wave propagation directions correspondingly;
â€¢ nÌ‚ p,s (t): unit vectors indicating the polarized directions of p- and s-waves;
â€¢ nÌ‚p,s : the initial directions of p- and s-waves.
the fga approximates the wavefield uk (t, x) in eq. (1) by a summation of dynamic frozen
gaussian wave packets,
x

ukf (t, x) â‰ˆ

ap nÌ‚ p Ïˆpk ikp p Â·(xâˆ’q )âˆ’ k |xâˆ’q |2
p
p Î´qÎ´p
2
e
9/2
p (2Ï€/k)

(q,p)âˆˆgÂ±

x

+

as nÌ‚ s Ïˆsk ikp s Â·(xâˆ’qs )âˆ’ k |xâˆ’qs |2
2
e
Î´qÎ´p,
(2Ï€/k)9/2
s

(5)

(q,p)âˆˆgÂ±

with the weight functions
k
Ïˆp,s
(q, p) =

z

k
Î±p,s
(y, q, p) =

k

2

k
Î±p,s
(y, q, p)eâˆ’ikpÂ·(yâˆ’q)âˆ’ 2 |yâˆ’q| dy,

1


k
k
kf
(y)c
|p|
Â±
ig
(y)
Â· nÌ‚p,s .
p,s
2kcp,s |p|3

(6)
(7)

in eq. (5), gp,s
Â± refers to the initial sets of gaussian center q and propagation vector p for pand s-waves, respectively. in eq. (7), the â€œÂ±â€ on the right-hand-side of the equation indicate
k
correspond to (q, p) âˆˆ gp,s
that the Î±p,s
Â± . we refer [9] for the derivation, accuracy and explanation of fga, and only summarize the formulation as follows. the ray path is given by the
hamiltonian system with hamiltonian h(q, p ) = Â±cp,s (q)|p |. the â€œÂ±â€ give the two-way
wave propagation directions; e.g. for the â€œ+â€ wave propagation, (q, p) âˆˆ gp,s
+ , the gaussian
center qp,s (t, q, p) and propagation vector p p,s (t, q, p) follow the ray dynamics
ï£±
dqp,s
p p,s
ï£´
ï£²
= cp,s (qp,s )
,
dt
|p p,s |
ï£´
ï£³ dp p,s = âˆ’âˆ‚ c (q )|p |,
q p,s
p,s
p,s
dt

(8)

with initial conditions
qp,s (0, q, p) = q

and p p,s (0, q, p) = p.

4

(9)

wave reconstruction

offset

time propagation

transmitted

reflected
incident

initial decomposition

offset

figure 1: a cartoon illustration of fga algorithms: step 1, decompose the initial wavefield into a sum of
gaussian wave packets with corresponding weights given by (6); step 2, propagate gaussian wave packets
following (8), (10), (11) and (12), with the reflection-transmission conditions described in section 2.2; step 3,
reconstruct the wavefield by summing all gaussian wave packets using (5).

the prefactor amplitudes ap,s (t, q, p) satisfy the following equations, where s-waves have
been decomposed into sh- and sv-waves,



âˆ‚qp cp Â· p p 1
dap
âˆ’1 dzp
= ap Â±
+ tr zp
,
(10)
dt
|p p |
2
dt



âˆ‚qs cs Â· p s 1
dnÌ‚ sh
dasv
âˆ’1 dzs
= asv Â±
+ tr zs
âˆ’ ash
Â· nÌ‚ sv ,
(11)
dt
|p s |
2
dt
dt



âˆ‚qs cs Â· p s 1
dash
dnÌ‚ sh
âˆ’1 dzs
= ash Â±
+ tr zs
Â· nÌ‚ sv ,
(12)
+ asv
dt
|p s |
2
dt
dt
with the initial conditions ap,sv,sh = 23/2 , and nÌ‚ sv and nÌ‚ sh are the two unit directions perpendicular to p s , referring to the polarized directions of sv- and sh-waves, respectively. with
the short-hand notations,
âˆ‚z = âˆ‚q âˆ’ iâˆ‚p ,

zp,s = âˆ‚z (qp,s + ip p,s ).

(13)

we illustrate the algorithm by figure 1, and refer to the figures 5 and 6 in [9] for the
performance of efficiency of fga.
2.2. interface conditions
interface conditions are important as the direct and reflected waves from an interface are
picked up by the receiver, which records the time series of wavefield at certain location. this
gives travel time information; which in turn enables the depth of an interface to be computed.
for this exposition we only consider a flat interface, in general, we can use tangential-normal
5

z
gtrs
Ï†t

gtrp

Î¸t

layer 2 (câˆ¨
p,s )
z = z0

x

layer 1 (câˆ§
p,s )
Î¸i

Î¸r
Ï†r

gin
p

gre
p
gre
s

figure 2: cartoon illustration of an incident gaussian wave packet for p-wave hitting the interface at z = z0 ,
stands for
and then reflected and transmitted as gaussian wave packets for p- and sv-waves. here the gin,re,tr
p,s
the gaussian wave packet for the incident, reflected and transmitted p- and sv-waves, respectively. we denote
Î¸i , Î¸r , Î¸t to be the incident, reflection and transmission angles of p-waves, and Ï†r , Ï†t to be the reflection and
transmission angles of sv-waves, respectively.

coordinates. the derivation was detailed in appendix b in [9], with the idea of using the
continuity of level set functions corresponding to the hamiltonian dynamics (8). a cartoon
illustration on the behavior of gaussian wave packet is given in figure 2. for a flat interface
z = z0 , the wave speeds of the two layers near the interface are assumed to be,
(
(
câˆ¨s (x) z > z0
câˆ¨p (x) z > z0
.
(14)
,
c
(x)
=
cp (x) =
s
câˆ§s (x) z < z0
câˆ§p (x) z < z0
as a gaussian wave packet hits an interface, several of its quantities need to be defined. first,
ap,s and p p,s , are determined by snellâ€™s law and the zoeppritz equations [27]. if one denotes
Î¸i , Î¸r , Î¸t to be the p-wave incident, reflection and transmission angles, and Ï†r , Ï†t to be the
sv-wave reflection and transmission angles, respectively, then the zoeppritz equations read as
ï£« re ï£¶ ï£«
ï£¶
ap
cos(Î¸r )
ï£¬ are
ï£· ï£¬ sin(Î¸r ) ï£· in
s ï£·
ï£¬
ï£·
(15)
mï£¬
tr
ï£­ ap ï£¸ = ï£­ cos(2Ï†r ) ï£¸ ap ,
cos(2Î¸r )
atr
s
with the matrix m as
ï£«
câˆ§
p
cos(Î¸r )
sin(Ï†r )
câˆ§
s
ï£¬
âˆ§
cp
ï£¬ âˆ’ sin(Î¸ )
cos(Ï†r )
ï£¬
r
câˆ§
s
m =ï£¬
ï£¬ âˆ’ cos(2Ï†r )
âˆ’ sin(2Ï†r )
ï£­
câˆ§
sin(2Î¸r ) âˆ’( cpâˆ§ )2 cos(2Ï†r )
s

câˆ§
p
cos(Î¸t )
câˆ¨
p
âˆ§
cp
sin(Î¸t )
câˆ¨
p
Ï2
cos(2Ï†t )
Ï1
âˆ¨ 2
Ï2 (câˆ§
p cs )
âˆ§ 2 sin(2Î¸t )
Ï1 (câˆ¨
p cs )

6

câˆ§

âˆ’ cpâˆ¨ sin(Ï†t )
s

ï£¶

câˆ§
p
cos(Ï†t ) ï£·
ï£·
câˆ¨
s
ï£·,
Ï2
âˆ’ Ï1 sin(2Ï†t ) ï£·
ï£¸
2
Ï2 (câˆ§
p)
cos(2Ï†
)
t
Ï1 (câˆ§ )2
s

ï£·

(16)

where Ï1,2 are the densities for the layers 1 and 2, respectively. let n denote the normal to
the interface at the point of incidence then qin,re,tr is the gaussian center at the point of incidence, and pin,re,tr corresponds to the propagation vector of incident, reflected and transmitted
gaussian wave packet for either p- or s-waves. qin = qre = qtr and pre,tr is updated as
follows

q
in
tr,re
in |ntr,re âˆ’ |pin | âˆ’ (pin Â· n)2 âˆ’ (p Â· n) n,
|p
(17)
ptr,re
=
p
+
sgn(p
)
p,s
p,s
p,s

tr
âˆ¨ âˆ§
where ntr,re
p,s denotes the index of refraction for the new respective direction, e.g. np = cp /cp .
also zp,s needs to be updated, requiring use of conservation of level set functions defined in
the eulerian frozen gaussian approximation formula [15, 25].

âˆ‚z qre,tr = âˆ‚z qin f,

|pre,tr |
re,tr
re,tr
in
in
âˆ‚
q
Â·
âˆ‡c(q
)
âˆ’
âˆ‚
q
Â·
âˆ‡c(q
)
n,
z
z
c(qre,tr )pre,tr Â· n
(18)
t
âˆ’1
f and w are two 3 Ã— 3 matrices, f = w , and
ï£¹
ï£®

2
1
0
0
c(qre,tr )
ï£º
ï£¯
0
1
0
.
f =ï£°
ï£» , with Îº =
re,tr
c(qin )
(Îº âˆ’ 1) ppinx (Îº âˆ’ 1) ppiny Îº ppz in
âˆ‚z pre,tr = âˆ‚z pin w âˆ’

z

z

z

2.3. advantage of fga for generating training data
the data points used for our experiments are generated from the forward simulation of the
elastic wave equation using the fga. we record the displacement data from the wavefield at
various points near the surface; these points represent the receiver locations. given an initial
condition, as in eq. (4), the initial wave packet decomposition can be saved for generating
a data set for training. that is, the same data can be loaded as the parameters which vary
from data point to data point; e.g. interface height, pocket location, pocket size, etc. furthermore, if the initial condition is independent of the wave velocities, the same wave packet
decomposition can be used to generate data from simulation with varying velocities.
for a single forward simulation; after the initial wave packet decomposition generated and
saved, loading the initial wave packet decomposition, running an ode solver, and recording
the displacement are the only tasks required to generate a data point. for generation of a data
set, the simulation can be restarted at t = 0 with another set of parameters. as the initial wave
packet decomposition is already loaded in memory, all that is required to generate the rest of
the data set is running an ode solver, and recording the displacement. the ode system for
the fga is uncoupled for each wave packet, the speed of a single simulation greatly benefits
from a parallel implementation.
3. network design
the goal of full waveform inversion (fwi) is to extract wave speed data from seismic
data. in its purest form, this is a regression type problem and was addressed with fully con7

nected networks in [3]. our work approaches the problem from a segmentation perspective.
we address a simplified version of fwi and attempt to detect subsurface structures by classifying them as regions of low or high wavespeed, thus transforming the regression problem
into a segmentation problem. these sorts of segmentation problems have been addressed with
great success by cnns [22]. semantic segmentation of images is the process of labeling each
pixel in an image with a class label for which it belongs. in semantic segmentation problems
the correct pixel label map is referred to as the ground truth. in our work the â€œimage is the
n-dimensional slice in the depth direction which is partitioned into n bins. the i, j th â€œpixelâ€
is the signal value from receiver i at depth bin j.
each bin is then labeled depending on whether it came from a region of high or low velocity. these velocity regions are our classes. our work diverges substantially from traditional
semantic segmentation of images, as our input is time series data which must be transformed
by the network. this is opposed to the traditional case where the input itself is labeled. the
goal of our network is to infer the presence of high and low wavespeed regions and the interfaces between them from seismogram data. the input to the network is x âˆˆ rm Ã—dÃ—r , where
m is the number of timesteps, d is the spatial dimension of media, and r is the the number of
receiver. the output of the network is
n (x) = (pki1 ...in ) âˆˆ rm1 Ã—Â·Â·Â·Ã—mn Ã—n ,

ij âˆˆ {1, ..., mj }
,
k âˆˆ {1, . . . , n }

(19)

where pki1 Â·Â·Â·in is the probability that bin i1 Â· Â· Â· in belongs to the k th class. in this paper d = 3,
n = 1, 2, and n = 1, 2, 3.
for example, possible output and groundtruth could be
ï£®
ï£¹
ï£® ï£¹
0.1 0.9
1
ï£° 0.2 0.8 ï£» , ï£°1ï£» .
0.55 0.45
1
here, at depth indexed by 1, the network believes with 10% probability that this bin is a low
speed region and with 90% probability that it is a high speed region, and similarly for the other
rows. the accuracy of a given inference is found by taking the argmax along the last axis of
the output tensor and comparing against the groundtruth. taking a max along the last axis
recovers the probability, interpreted as a confidence of the prediction. the above example has
66.67% accuracy, and the confidence is [0.9, 0.8, 0.55]t .
in [3], araya-polo et al. perform inverse tomography via deep learning and achieve
impressive results. our model is fundamentally different than geodnn in that: geodnn
is a fully connected network whereas geosegâ€™s is fully convolutional, and geodnn uses
semblance panels from cmp data as features for the network and geoseg uses the raw seismograph data. moreover, araya-polo et al. address the fwi problem and provide the wave
speeds in a two dimensional region and we tackle high and low velocity detection, shifting the
problem from regression to segmentation.
1
the networks were built using an open source api, geoseg , developed using keras and
1

https://github.com/kylemylonakis/geoseg

8

tensorflow. geoseg supports unet, fully convolutional segmentation network, or feed forward cnns as a base meta-architecture, using any of residual, dense, or convolutional blocks,
with or without batch normalization [21, 22, 11, 12, 13]. geoseg also allows for easy hyperparameter selection for network and block architectures, and for training optimizers and parameters. the optimizers used were nadam with default parameters [6], sometimes followed
by minibatch stochastic gradient descent (sgd), or sgd alone. the network structures are
described by their meta-architecture and their blocks. the meta-architecture describes the
global topology of the network and how the blocks interact with each-other. each block either
begins or ends with a decoding or encoding transition layer respectively. encoding transition
layers downsample their inputs with a strided convolution. decoding transition layers upsample thier inputs with a strided deconvolution. tranistion layers will not have dropout.
meta-architectures. while geoseg supports many kinds of feed-forward cnnâ€™s and
encoder-decoder networks with different choices of blocks, unet architectures with dense
blocks performed the best and will be the only type of network reported.
geodude-l refers to a unet architecture from [21]. these architectures have proven
highly efficient at image segmentation for road detection [28] and in biomedical applications [21]. these networks feed their input into a transfer branch, then an encoder branch
of length l, bridge block, and then a decoder branch of length l. the last layer is a convolutional layer followed by a softmax which outputs predictions as described above. the defining
feature of these networks are the â€œrungsâ€ connecting the encoder and decoder branches (see
figure 3). in this way, the network can incorporate both low and high resolution data [21, 28].
for the one dimensional problems the transfer branch is not necessary and can be omitted.
convolutional layers. the layer is broken first into a bottleneck convolution followed by
the main convolution. the bottleneck is a convolution which uses a 1x1 kernel to expand the
number of feature channels before performing the full convolution. it is suggested in [10, 23]
that such a bottleneck can reduce the number of necessary feature maps and so improve computational efficiency. we use rectified linear units (relus) [8] for our activation and size
3x3 (3x1 for 1d interface problems) filter kernels for our convolutions. as in [12], we use
batch-normalization [13] to help smooth training. the setup is shown in figure 4
dense blocks. though geoseg supports multiple block types, all the networks reported
in this paper use dense blocks. these are stacks of convolutional layers as shown in figure 4.
the defining features of these blocks, introduced in [12] is that every layer receives input
from all previous layers in the block via concatenation. such architectures have been shown
to greatly improve results in image classification while reducing computational burden [12].
transfer branch. all of our meta-architectures preserve resolution of their input and so
our detection resolution is limited by input resolution. this is not a problem in the temporal
axis, which translates to the z axis in output, since we have a large number of time samples;
however, the x-axis resolution is limited by the number of receivers we have for our input. to
increase the resolution in this direction, we place a small l-layer cnn before the main network
which upsamples the receiver axis, via strided deconvolutions, by a factor of 2l .

9

figure 3: meta-architecture of a two-layered unet, geodude-2, with transfer branch used in deep learning
algorithms. for 2d problems the input is upsampled along the receiver axis by deconvolutions in the transfer
branch. unetâ€™s have â€œrungsâ€ that connects the encoder and decoder branches. in this way, the network can
incorporate both low and high resolution data.

(a) convolutional layer

(b) dense block

figure 4: the type of blocks used in geoseg for this paper: (a) block compositions of a basic convolutional
layer using a bottleneck convolution to expand the filter channels before the full convolution; (b) a corresponding
dense block. each layer of the block recieves input from all previous layers allowing information to flow through
the whole block.

10

4. numerical experiments
here we present the performance of deep learning algorithms for the three detection experiments: 1d interface problme, three-layered media model, and a 2d single cylindrical pocket
model. the architecture used for all experiments is a unet with dense blocks (geodude).
each dense block will be made of four constituent bottle-necked convolutional layers with a
bottle neck factor of 4. for all 1d networks the dense blocksâ€™ convolutions use a kernel size
of 3 Ã— 1 in the base of the block and 2 Ã— 1 at each transition layer, while for the 2d networks a
3 Ã— 3 kernel size is used in the base block with a 2 Ã— 2 kernel size in the transition layer. the
meta-architectures had 16 filter channels except for the 1d interface model with p-wave data
which only used 4.
our primary evaluation metric is accuracy which is the number of correctly predicted
pixels over total pixels, i.e.,
accuracy =

number of correct pixels
,
total number of pixels

where we set the ground truth as follows for measuring accuracy; if any part of a pixel contains
a low velocity region, that pixel is counted as part of the low velocity region.
for the 2d pocket model, we will also consider the intersection over union metric which
better captures segmentation performance.
in the 2d pocket model, a two-layer transfer branch was used. each layer was a convolution, two-strided in the receiver direction with a kernel size of 3 Ã— 3 with 4 filter channels.
during training, these layers had a drop out probability of 0.2.
the wavespeeds cp and cs are given by (3), which will be specified as piecewise linear
functions (or constants) detailed in each numerical example. the initial p-wave data is generated with source function

fjk (x) = cos(k(xj âˆ’ x0,j )) exp âˆ’ 2k|x âˆ’ x0 |2 ,
(20)
and the p,s-wave initial data is generated from the greenâ€™s function
fjk (x) =

3
x
(xi âˆ’ xi,0 )(xj âˆ’ xj,0 )
i=1

4Ï€Ïc2p r3

fj (t0 âˆ’ r/cp )+

r2 Î´ij âˆ’ (xi âˆ’ xi,0 )(xj âˆ’ xj,0 )
fj (t0 âˆ’ r/cs )+
4Ï€Ïc2s r3
z
3(xi âˆ’ xi,0 )(xj âˆ’ xj,0 ) âˆ’ r2 Î´ij r/cs
sfj (t0 âˆ’ s) ds,
4Ï€Ïr5
r/cp

(21)

p
where fj (t) = cos (kt) exp(âˆ’2kt2 ), Î´ij is the kronecker delta, and t0 = 2 1/k,
x = (x1 , x2 , x3 ), x0 = (x0,1 , x0,2 , x0,3 ) is the location of the source, r = kx âˆ’ x0 k and Ï is
the density.
the data is generated on the cluster, pod, at the center for scientific computing at uc santa

11

2

barbara using 64 processes with a 4th order runge-kutta solver for the ode system. as the
initial condition is independent of the wavespeed, only one wave packet decomposition needs
to be computed and saved for all data points to be generated. this saves a tremendous amount
of time as only the ode system needs to be solved for various wavespeeds and interface
heights. for example to generate the p-wave data, when 804672 total beams are used, each
data point is generated in approximately 2.5 minutes. this is compared to specfem3d
which takes is approximately 45 minutes to generate a data point.
all of the networks were trained on the google cloud platform, or on the cluster pod at the
center for scientific computing at uc santa barbara with keras 2.2.2 and tensorflow 1.10.0
as a backend using a single nvidia tesla v100 gpu.
4.1. 1d interface
to provide a proof of concept we first experimented with a two-layered flat interface
model. we also use this case to investigate whether our network is simply inverting the fga
by comparing performance of a network trained on fga but evaluated on data generated by
sem.
4.1.1. p-wave data
dataset. the p-wave data set is generated with a computation domain of [0, 2] kmÃ—[0, 2]
kmÃ—[0, 2.5] km with a source centered at x0 = (0.5, 0.5, 0.5) km and k = 128 in (20),
which corresponds to approximately 20.37 hz. the stations are located on the surface at
s1 : (1.5, 1.5, 0) km, s2 : (1.8, 1.5, 0) km, s3 : (1.6, 1.9, 0) km. the interface is a plane,
z = z0 that varies from depth 1 km to 2.5 km. above the interface the wavespeed cp varies
linearly from .78 km/s to 1.22 km/s, below the interface the wavespeed cp varies linearly from
1.29 km/s to 1.56 km/s. see figure 5.
each data point is a (6000, 3, 3) tensor. prior to training, we further down sample the temporal dimension by a factor of 25 and normalize the amplitude of the seismogram data. there
were a total of 7790 examples. the mini-batch size during training was 256 examples.
network details. as described above our architecture was a 1d geodude-3 where each
convolutional layer in the dense block had 4 feature channels. the during training the dropout
probability was set to 0.5 and a nadam optimizer was used with default parameters.
results. network evaluations were performed with data generated by the fga and specfem.
notably, the networks are never trained on any specfem data. this was to investigate
whether the network was sensitive to the asymptotic error produced by the fga.
after 3500 epochs of training geodude-3 achieved a 96.97% evaluation accuracy on data
generated by the fga. when evaluated on data generated by specfem dataset geodude-3
achieved a 94.29% evaluation accuracy, only a 2.68% decrease. we remark in [16, 24], it
was shown even small perturbations in input can affect network classification results. this
suggests that the asymptotic errors present in the fga do not greatly affect the segmentation
problem. visualizations of the output for geodude-3 are shown in figure 6. figure 7 shows
the heatmap. recall this displays the confidence the network places on the pixels prediction.
2

http://csc.cnsi.ucsb.edu/clusters/pod

12

(a) computational domain

(b) seismogram

figure 5: the locations of source and receivers, and the generated synthetic p-wave seismograms for the 1d
interface problem. we take k = 128 for generating the synthetic data. (a) the source is located at (.5,.5,.5) km
as a star and the 3 receivers are located on the surface. the interface presented is at a depth of 2 km. (b) a
visualization of typical data point, which is a collection of 3 seismograms from the forward simulation using the
fga.

(a) fga: actual

(b) fga: predicted

(c) fga: difference

(d) sem: actual

(e) sem: predicted

(f) sem: difference

figure 6: 1d interface predicted by geodude-3 using p-wave data. each column of pixels represents a data
point. the value of each pixel describes whether the material at the depth corresponding to that pixelâ€™s column
belongs to either the high or low velocity region. the blue pixels represent the low velocity region, while the
yellow represent the high velocity region. subfigures (c), (f) show the difference between the predicted and
actual velocity profile, where the accuracy is measured by the wrong-labeled pixels (blue) over the total number
of pixels in the figures (c), (f). in fact, after 3500 epochs of training geodude-3 achieved a 96.97% evaluation
accuracy on data generated by the fga.

13

(a) fga

(b) sem

figure 7: p-wave confidence distribution comparison produced by geodude-3 for 1d interface problem. regions of low confidence correspond to areas where an interface is likely. the color bar is a probability spectrum
from 0.5 to 1. in general, the closer the network gets to the interface the less confident its prediction becomes.

4.1.2. p,s-wave data set
dataset. the p,s-wave dataset is generated with a computation domain of [0, 2] kmÃ—[0, 2]
kmÃ—[0, 3] km with a source centered at x0 = (0.5, 0.5, 0.5) km, and wavenumber k = 32, or
approximately 5.09 hz. the stations lie in a plane and are located just below the surface at
s1 : (1.1, 0.5, 0.1) km, s2 : (1.4, 0.5, .1) km, s3 : (1.8, 0.5, 0.1) km. the interface is a plane,
z = z0 that varies from depth 1 km to 2 km. above the interface cp varies linearly from 0.75
km/s to 1.10 km/s, below the interface cp varies linearly from 1.12 km/s to 1.48 km/s and we
fix cs = cp /1.7 (corresponding the case Î» â‰ˆ Âµ). see figure 8. there are a total of 6,400
data points in the p,s-wave dataset. each data point is a (2048,3,3) tensor. prior to training
each example is down-sampled along the temporal axis by a factor of 8. each network used a
mini-batch training size of 256. similarly to the p-wave dataset, 100 additional samples were
generated using specfem3d for evaluation after training.
network details. geodude-2 and geodude-3 with default parameters were used.
both networks were trained using a nadam optimizer with a dropout probability of 0.5.
results. both networks were trained for 3500 epochs. the most successful network
was geodude-2, with 98.26 % evaluation accuracy on fga data, and 97.55 % evaluation
accuracy on the specfem data . we find that the evaluation accuracy goes down for deeper
networks. in particular, geodude-3 performed worse with only a 92.34 % evaluation accuracy, especially compared to the same network architecture on the p-wave dataset. this is
likely due to overfitting of the data causing an increase in generalization error. similarly to the
p-wave dataset, evaluation accuracies on specfem3d data are only marginally worse than
their fga counterparts, with a max difference of 1.17% between the datasets. see table 1 for
the summary of the results and figures 9, 10 and 11.

14

(a) computational domain

(b) seismogram

figure 8: the locations of source and receivers, and the generated synthetic p- and s-wave seismograms for the
1d interface problem. we take k = 32 for generating the synthetic data. (a) the source is located at (.5,.5,.5)
as a star and the 3 receivers are located on the surface. the interface presented is at a depth of 2 km. (b) a
visualization of typical data point, which is a collection of 3 seismograms from the forward simulation using the
fga.

table 1: p,s-data network comparisons for 1d interface problem. here eval. acc. = evaluation accuracy,
train. acc. = training accuracy, and sem acc. = evaluation accuracy tested by sem synthetic data.

network eval. acc. train. acc. sem acc.
geodude-2
98.26 %
99.97 %
97.55 %
geodude-3
97.64 %
99.90 %
96.47 %

(a) evaluation accuracy

(b) training accuracy

figure 9: ps-wave training results for 1d interface problem, with synthetic data generated for k = 32 in (20):
the evaluation data set for this figure only contains data generated by the fga.

15

(a) fga: actual

(b) fga: predicted

(c) fga: difference

(d) sem: actual

(e) sem: predicted

(f) sem: difference

figure 10: 1d interface predicted by geodude-2 using p,s-wave data. each column of pixels represents a
sample. the value of each pixel describes whether the material at the depth corresponding to that pixelâ€™s column
belongs to either the high or low velocity region. the blue pixels represent the low velocity region, while the
yellow represent the high velocity region. subfigures (c), (f) show the difference between the predicted and
actual velocity profile, where the accuracy is measured by the wrong-labeled pixels (blue) over the total number
of pixels in the figures (c), (f). we give the statistical accuracy in table 1, which shows an accuracy of over 96%.

(a) geodude-2:fga

(b) geodude-2:sem

figure 11: p,s-wave heat-map distribution comparison produced by geodude-2 for 1d interface problem.
regions of low confidence correspond to areas where an interface is likely. the color bar is a probability spectrum
from 0.5 to 1. in general, the closer the network gets to the interface the less confident its prediction becomes.

16

(a) computational domain

(b) seismograph

figure 12: the locations of source and receivers, and the generated synthetic p- and s-wave seismograms for
the three-layered media model. we take k = 32 for generating the synthetic data. (a) the source is located at
(.5, 1, .5) km as a star, the 32 receivers are located on the surface on the plane y = 1 km, and the interfaces
presented are at a depth of 1.5 km and 2 km. (b) a visualization of typical data point, which is a collection of 32
seismograms from the forward simulation using the fga.

4.2. three-layered media
dataset. a natural extension of the model is to include one or more low velocity regions
in the computational domain. for this experiment we consider a three-layered media with a
low velocity region in the middle, the velocities in each region will be fixed.the p-wave speed
is cp = 1.3, 0.9, 1.7 km/s for the top, middle, and bottom layers, receptively. the s-wave
speed is set to cs = cp /1.7 for each layer. the lower interface will be in a rage of 1.8 km and
2.8 km by an increment of 1 m. similarly the upper interface will vary from .2 km to 1.2 km
by an increment of 1m. see figure 12. there were 10201 samples with a batch size of 64.
network details. geodude-3 was used. during training the dropout probability was
0.12. training was performed with stochastic gradient descent with a learning rate of 0.001.
results. the network achieved a training accuracy of 99.51% and an evaluation accuracy
of 95.51% after 3000 epochs. see figures 13 and 14.
4.3. sine interface model
dataset. for this experiment we consider a more complicated interface, which is given
by the level set function; f (x, z) = z âˆ’ 0.1 sin(Ï€f x). the level set values f (x, z) = d vary
between 0.3 km and 2.3 km by an increment of .001 km. and the phase factor f will ranges
from 1 to 2 by an increment of .001. the nondimensionalized is set to wavenumber k = 64
and cp = 1.1, 1.5 for the top and bottom layers will all be fixed. as before, cs will be a fixed
multiple of cp by 1.7. we record the displacement for 10 s; see figure 15 for source, receiver
details.
network details. geodude-4 was used with a dropout probability of 0.2. training
was performed with 1500 epochs using the nadam optimizer followed by 1000 epochs of
17

(a) actual

(b) predicted

(c) difference

figure 13: predictions for three-layered media by geodude-3: each column of pixels represents a sample. the
value of each pixel describes whether the material at the depth corresponding to that pixelâ€™s column belongs to
either the high or low velocity region. the color bar is a probability spectrum from 0.5 to 1. in general, the closer
the network gets to the interface the less confident its prediction becomes. there is a slight loss of confidence for
the network detecting the lower interface.

figure 14: confidence map for three-layered media model produced by geodude-3. regions of low confidence
correspond to areas where an interface is likely. the color bar is a probability spectrum from 0.5 to 1. in general,
the closer the network gets to the interface the less confident its prediction becomes.

18

(a) computational domain

(b) displacement color channel

figure 15: the locations of source and receivers, and the generated synthetic p- and s-wave seismograms for the
sine interface model. we take k = 64 for generating the synthetic data. (a) the source is located at (0.4, 1, 2.7)
km as a star, the 32 receivers are located on the surface on the plane y = 1 km, and the interfaces presented are
at a depth of 1.5 km. (b) visualization of network input as image for sine interface model. each color channel
(inverse rgb) represents a coordinate of the displacement.

stochastic gradient descent with a learning rate of 0.001. there were 10050 date points generated. 9150 data points are used for training with a mini-batch size of 25; 900 data points were
used for evaluation.
results. the network achieved a training accuracy of 99.92% and an evaluation accuracy
of 99.28% after 2500 epochs. see figure 16 for evaluation of a typical data point.
4.4. 2d low velocity pocket
dataset: we now investigate whether the network can learn more complex 2d geometries.
the considered models each will be a three-layered problem with a low velocity cylindrical
region in the middle layer. the source will be located at (.5, 1, 1.5) km. the interfaces located
at 1 km and 2.5 km will be fixed. a cylinder with center (x, z) and radius r will be randomly

(a) actual

(b) predicted

(c) confidence map

figure 16: typical results from training phase factor f = 1.83, level set value d = .8898. (a), (b) ground
truth and prediction for sine interface geodude-4. (c) confidence map for sine interface model. regions of low
confidence correspond to areas where an interface is likely.

19

(a) computational domain

(b) displacement color channel

figure 17: the locations of source and receivers, and generated synthetic p- and s-wave seismograms for the
2d pocket model. we take k = 32 for generating the synthetic data. (a) the source is located at (.5, 1, 1.5) km
as a star and the receivers are located on the surface on the plane y = 1 km. the interfaces are fixed at a depth
of 1 km and 2.5 km. visualization of network input as image for 2d pocket model. each color channel (inverse
rgb) represents a coordinate of the displacement.

generated x âˆˆ [0.85, 1.65] km, z âˆˆ [1.35, 2.15] km, and r âˆˆ [.05, .3] km with samples taken
from a uniform distribution. see figure 17. 11350 data points are generated with 1000 being
saved for evaluation. the p-wave speeds will be fixed and are cp = 1.1, 1.3, 1.7 km/s, for
the top, middle and bottom layers respectively. the s-wave speed, cs will be a fixed multiple
of cp by 1.7 for each layer. inside the pocket the p-wave speed is set to cp = 0.5 km/s and
the s-wave speed is set to zero, cs = 0. only p -waves will propagate through the cylinder;
however, s-wave can transmit to p-wave going in the pocket and p-wave can transmit to p,swaves coming out of the pocket. unlike previous models the goal is to identify a low velocity
region in a three layered media in a 2d slice of the computational domain. a batch size of 20
examples was used.
network design. a geodude-4 network was used with a two layer transfer branch
before its input. the dropout probability was 0.2.
results. the network achieved a training accuracy of 99.95% and an evaluation accuracy
of 99.73% after 1428 epochs. in figure 19 we see the networks are indeed learning geometry.
this is particularly interesting given that the network only â€seesâ€ images like figure 18(b).
these results suggest the network is transforming the data in some way which we hope to
explore in future work.
4.5. effect of noisy data
we now consider the 2d pocket example with additive white noise. normally, noise is
added to the training data set to increase the size of the set and lead to a more robust network. we take an evaluation set of 1000 data points and add i.i.d. (independent identically
distributed) gaussian noise to each time step of the displacement field data. for an individual

20

(a) x-coordinate

(b) y-coordinate

(c) z-coordinate

figure 18: visualization of network input using normalized displacement data for 2d pocket model.

(a) actual

(b) predicted

(c) confidence map

figure 19: 2d pocket results predicted by geodude-4, with a typical data point chosen for visualization. the
pocket is recovered with the networks confidence wavering on the boundary of the pocket.

21

(a) no noise

(b) noise strength of 1% of maximum recorded data

figure 20: comparison of seismograms with noise and no noise for the 2d pocket model. (a) seismogram with
no noise. (b) additive gaussian white noise at 1% of max |u|. this shows that 1% of the maximum recorded
displacement is enough to mask the reflected data from the pocket.

data point, the noise strength can be calculated by
wi =

Ïƒ
,
r max |ur |

(22)

where r is the reflection coefficient and max |ur | is the maximum displacement from the reflected wave. the noise strength will be given by w , which is the approximate average value
of wi across the data set. the standard deviation Ïƒ is chosen so that w can be interrupted as
a percentage of the reflected wave displacement, e.g., w = 20 gives of the a noise strength of
20% of the average max displacement of the reflected wave. we notice that with noise generated with a strength of 1% of the maximum of direct recorded displacement, the reflected
data from the pocket is the same order of magnitude of the noise, effectively masking it. see
figure 20.
network design: to compare results, we use the same model as in the previous section 4.4 and train a network with the same parameters, with a noise strength of w = 20.
results. a geodude-4 was trained for 2000 epochs with additional noise for a final
evaluation accuracy of 99.731% evaluation accuracy. however, evaluation accuracy can be
a misleading metric for network performance in pocket detection since assigning the high
velocity class to every pixel could get an accuracy up to 80% on some samples. instead
intersection over union (iou) is used (see [4] for a more detailed explanation). figures 21
and 22 show the histograms the iou scores of networks trained with and without white noise
evaluated on the evaluation data with no additional noise, additional noise strength w =
10, and additional noise strength w = 50 respectively. while both networks display good
iou scores on the unperturbed data and when the data is only perturbed with noise strength
w = 10, the benefits of additional noise in training become clear when the noise strength
is increased to w = 50: the iou scores of the network trained without noise on noisy data
22

unperturbed perturbed by w = 10 perturbed by w = 50
trained without noise
0.8163
0.7335
0.1308
trained with noise
0.8706
0.7576
0.5249
table 2: iou scores for geodude-4 trained with and without noise for the 2d pocket model.

(a) ground truth

(b) iou=0.1403

(c) iou=0.2052

figure 21: visualization of ious by geodude-4 for the 2d pocket model. results taken from network trained
with noise. data is augmented with noise with a noise strength of 50%. (a) ground truth for comparision. (b)
iou=0.1403. (c) iou= 0.2052. for each displayed results, the networks are able to detect the location of the
pocket. with additional noise the network is unable to resolve the geometry.

plummets, effectively misclassifying almost every pocket, while the iou score of the network
trained with noise decreases, but maintains many correct classifications. the average iou
scores are summarized in table 2. evaluating on higher noise strength collapses the networkâ€™s
output to no pocket detected.
4.6. structured noise
we now consider the 2d pocket example with additive structured noise. from our noiseless
evaluation data set, we add structured low frequency noise to each receiver. numbering each

(a) no noise

(b) w=10

(c) w=50

figure 22: network with trained without noise, 1000 data points are plotted in each histogram. subfigures (a),
(b), (c) show the iou metric with no noise, 10% noise strength, and 50% noise strength respectively.

23

(a) no noise

(b) w=10

(c) w=50

figure 23: performance of the network geodude-4 trained with noise strength at 20% of the average max
displacement of the reflected wave for the 2d pocket model. 1000 data points are plotted in each histogram.
subfigures (a), (b), (c) show the iou metric, with no noise, 20% noise strength, and 50% noise strength, respectively.

(a) actual

(b) predicted, iou=0.7585

(c) confidence map

figure 24: a visualization of a result from the structured noise (23) added to the noiseless evaluation dataset.
the network used is geodude-4 trained with noise from section 4.5. (a) ground truth. (b) prediction, with an
iou value of 0.7585. (c) confidence map.

receiver, sj = (j2/31, 1, 0), for j = 0, . . . , 31 we add noise to receiver sj as
a
4
cos(2t)
,
2
3(32 âˆ’ j)

(23)

where the amplitude a is modulus of the maximum displacement of the wavefield reflected
from the interface. we add this noise to each component of the wavefield. we remark that at
receiver at x = 2, the noise is the strongest at with an amplitude of 2a/3. that is, the noise
strength is two thirds of the height of the modulus of the maximum displacement wavefield
reflected from the interface. this is a stronger noise than the w = 50 case of additive white
noise. the structured noise decreases to an amplitude of a/48 at the receiver located at x = 0
the network is able to detect the pocket with good success. the network has full confidence, as can be seen from fig. 24c; however, the boundary for the prediction is perturbed.
this gives a slight false result which is reflected in the iou metric, see fig. 25. with the
structured noise, the prediction has a lower average iou value of .7443 compared to 0.8706
with no noise.
24

figure 25: the iou metric for structured noise as the input into geodude-4 for the 2d pocket model trained
with noise from section 4.5. the average iou is .7443.

5. conclusions and future work
the use of the fga to generate large amounts of seismic data provides a quick way to generate labeled synthetic data for statistical learning of the inverse tomography problem. casting
the inverse problem as a segmentation problem resulted in high evaluation accuracy networks
for piecewise constant two-layer models on both fga and sem datasets. the unet architectures with dense blocks displayed superior accuracy compared to simpler network architectures, however, deeper networks did not necessarily outperform their shorter counterparts. on
the two layer benchmark problem the networks exhibited good invariance of prediction in regard to which numerical method was used to generate the dataset, likely because the fga and
sem exhibit the same traveltime information. having a network independent of numerical
method is important, and the fga can help to train such a network as it generates synthetic
seismic data that carries the correct traveltime information of the real-world data. further,
analogous meta-architectures also exhibit high evaluation and iou accuracy for pocket detection in noisy data.
the success of the networks on the substructure geometries in the paper act as a stepping
stone to tackle more complicated and realistic geological models. by developing the api
geoseg, available at https://github.com/kylemylonakis/geoseg, it is easy to implement neural networks designed for the reported example models and more general segmentation problems of seismogram data than those discussed in this paper. together with the fga, the task
of training a deep neural network on sufficiently large amounts of seismogram data becomes a
computationally affordable task. immediate future directions to be explored are multi-pocket
models, multi-nonlinear interface models with and without pockets present. long term goal
is to develop a neural network model to tackle fully 3d substructure geometries and develop
a neural network trained on synthetic seismic data capable of making inferences from real
seismic data.

25

acknowledgement
we acknowledge support from the center for scientific computing from the cnsi, mrl:
an nsf mrsec (dmr-1720256) and nsf cns-1725797. the work was partially supported
by the nsf grant dms-1818592. xy also thanks professors haizhao yang and kui ren for
useful discussions.
references
[1] k. aki and w. lee. determination of the three-dimensional velocity anomalies under a
seismic array using first p arrival times from local earthquakes 1. a homogeneous intial
model. j. geophys. res., 81:4381â€“4399, 1976.
[2] m. araya-polo, t. dahlke, c. frogner, c. zhang, t. poggio, and d. hohl. automated
fault detection without seismic processing. the leading edge, 36(3):208â€“214, 2017.
[3] m. araya-polo, j. jennings, a. adler, and t. dahlke. deep-learning tomography. the
leading edge, 37(1):58â€“66, 2018.
[4] m. atiqur rahman and y. wang. optimizing intersection-over-union in deep neural
networks for image segmentation. volume 10072, pages 234â€“244, 12 2016.
[5] l. chai, p. tong, and x. yang. frozen gaussian approximation for 3-d seismic wave
propagation. geophysical journal international, 208(1):59â€“74, 2017.
[6] t. dozat. incorporating nesterov momentum into adam. 2016.
[7] a. m. dziewonski and d. l. anderson. preliminary reference earth model. physics of
the earth and planetary interiors, 25(4):297â€“356, 1981.
[8] x. glorot, a. bordes, and y. bengio. deep sparse rectifier neural networks. in aistats,
2011.
[9] j. c. hateley, x. yang, l. chai, and p. tong. frozen gaussian approximation for 3d elastic wave equation and seismic tomography. geophysical journal international,
216(2):1394â€“1412, 11 2019.
[10] k. he, x. zhang, s. ren, and j. sun. deep residual learning for image recognition.
corr, abs/1512.03385, 2015.
[11] k. he, x. zhang, s. ren, and j. sun. deep residual learning for image recognition. in
proceedings of the ieee conference on computer vision and pattern recognition, pages
770â€“778, 2016.
[12] g. huang, z. liu, and k. q. weinberger. densely connected convolutional networks.
corr, abs/1608.06993, 2016.
[13] s. ioffe and c. szegedy. batch normalization: accelerating deep network training by
reducing internal covariate shift. corr, abs/1502.03167, 2015.
26

[14] y. lecun, y. bengio, and g. e. hinton. deep learning. nature, 521(7553):436â€“444,
2015.
[15] j. lu and x. yang. frozen gaussian approximation for general linear strictly hyperbolic
systems: formulation and eulerian methods. multiscale model. simul., 10:451â€“472,
2012.
[16] s. moosavi-dezfooli, a. fawzi, o. fawzi, and p. frossard. universal adversarial perturbations. corr, abs/1610.08401, 2016.
[17] h. nakamichi, h. hamaguchi, s. tanaka, s. ueki, t. nishimura, and a. hasegawa.
source mechanisms of deep and intermediate-depth low-frequency earthquakes beneath
iwate volcano, northeastern japan. geophysical journal international, 154(3):811â€“828,
2003.
[18] t. perol, m. gharbi, and m. denolle. convolutional neural network for earthquake
detection and location. science advances, 4(2), 2018.
[19] n. rawlinson, s. pozgay, and s. fishwick. seismic tomography: a window into deep
earth. phys. earth planet. inter., 178(3-4):101â€“135, 2010.
[20] b. romanowicz. seismic tomography of the earthâ€™s mantle. annu. rev. earth planet.
sci., 19:77â€“99, 1991.
[21] o. ronneberger, p. fischer, and t. brox. u-net: convolutional networks for biomedical
image segmentation. corr, abs/1505.04597, 2015.
[22] e. shelhamer, j. long, and t. darrell. fully convolutional networks for semantic segmentation. 2015 ieee conference on computer vision and pattern recognition (cvpr),
pages 3431â€“3440, 2015.
[23] j. t. springenberg, a. dosovitskiy, t. brox, and m. a. riedmiller. striving for simplicity: the all convolutional net. corr, abs/1412.6806, 2014.
[24] c. szegedy, w. zaremba, i. sutskever, j. bruna, d. erhan, i. j. goodfellow, and r. fergus. intriguing properties of neural networks. corr, abs/1312.6199, 2013.
[25] d. wei and x. yang. eulerian gaussian beam method for high frequency wave propagation in heterogeneous media with discontinuities in one direction. commun. math. sci.,
10:1287â€“1299, 2012.
[26] y. wu, y. lin, z. zhou, d. c. bolton, j. liu, and p. johnson. deepdetect: a cascaded
region-based densely connected network for seismic event detection. ieee transactions
on geoscience and remote sensing, pages 1â€“14, 2018.
[27] o. yilmaz. seismic data analysis: processing, inversion, and interpretation of seismic
data. society of exploration geophysicists, 2001.
[28] z. zhang, q. liu, and y. wang. road extraction by deep residual u-net. corr,
abs/1711.10684, 2017.
27

[29] d. zhao. tomography and dynamics of western-pacific subduction zones. monogr.
environ. earth planets, 1:1â€“70, 2012.
[30] w. zhu and g. c. beroza. phasenet: a deep-neural-network-based seismic arrival
time picking method. arxiv e-prints, mar. 2018.

28



acta geophysica (2024) 72:673â€“687
https://doi.org/10.1007/s11600-023-01123-3

research article - applied geophysics

parametric elastic full waveform inversion with convolutional neural
network
keting guo1,2 Â· zhaoyun zong1,2

Â· jidong yang1,2 Â· yuanyuan tan1

received: 6 october 2022 / accepted: 12 april 2023 / published online: 9 june 2023
Â© the author(s) under exclusive licence to institute of geophysics, polish academy of sciences & polish academy of sciences 2023

abstract
elastic full waveform inversion (efwi) is a powerful tool for estimating elastic models by reducing the misfit between
multi-component seismic records and simulated data. however, when multiple parameters are updated simultaneously, the
gradients of the loss function with respect to these parameters will be coupled together, the effect exacerbate the nonlinear
problem. we propose a parametric efwi method based on convolutional neural networks (cnn-efwi). the parameters
that need to be updated are the weights in the neural network rather than the elastic models. the convolutional kernel in the
network can increase spatial correlations of elastic models, which can be regard as a regularization strategy to mitigate local
minima issue. furthermore, the representation also can mitigate the cross-talk between parameters due to the reconstruction
of frechÃ©t derivatives by neural networks. both forward and backward processes are implemented using a time-domain finitedifference solver for elastic wave equation. numerical examples on overthrust models, fluid saturated models and 2004 bp
salt body models demonstrate that cnn-efwi can partially mitigate the local minima problem and reduce the dependence
of inversion on the initial models. mini-batch configuration is used to speed up the update and achieve fast convergence. in
addition, the inversion of noisy data further verifies the robustness of cnn-efwi.
keywords full waveform inversion Â· parametric Â· machine learning Â· convolutional neural network

introduction
elastic full waveform inversion (efwi) is a powerful tool
for estimating the elastic parameters such as p-wave and
s-wave velocity (mora 1987; tarantola 1986). efwi problem is mathematically equivalent to partial differential
edited by prof. junlun li (associate editor) / prof.
gabriela fernÃ¡ndez viejo (co-editor-in-chief).
* zhaoyun zong
zongzhaoyun@upc.edu.cn
keting guo
s20010079@s.upc.edu.cn
jidong yang
jidong.yang@upc.edu.cn
yuanyuan tan
tyy17865422842@163.com
1

school of geosciences, china university of petroleum (east
china), qingdao, china

2

pilot national laboratory for marine science
and technology, qingdao, china

equation (pde) constrained optimization problem. it solves
elastic wave equation and minimizes the misfit between the
multicomponent records and simulated seismic data. efwi
can be implemented in frequency domain (brossier et al.
2009; pratt et al. 1998), the time domain (shipp and singh
2002), or in a hybrid domain (nihei and li 2007; sirgue
et al. 2008). efwi can utilize the long-offset reflection data
to estimate the amplitude variation result with offset (avo)
(borisov and singh 2015; chen et al. 2021, 2022). however,
compared with the acoustic case, numerical simulation in
elastic media may be computationally expensive 4â€“5 times.
when multiple parameters are involved in the inversion, the
nonlinearity increases, and the inversion result of efwi may
fall into a local minimum due to the cycle-skipping issue
(forgues and lambarÃ©, 1997; innanen 2014; prieux et al.
2013; wang and cheng 2017; zhang et al. 2021), and leads
to increasing ill-posedness of the inversion problem.
to mitigate these problems, many approaches have been
proposed in recent years. for example, one can use the hessian operator for mitigating the parameter cross-talk because
that can be regarded as a decoupling operator on the gradients (innanen 2014; operto et al. 2013). the algorithm based

13

vol.:(0123456789)

674

on hessian operator has a quadratic convergence rate, but
it needs a strong computational consumption. to avoid this
problem, many scholars use the approximate substitutions
of hessian operator such as the pseudo-hessian (choi et al.
2007; shin et al. 2001) or a quasi-newton method (brossier
et al. 2009). another popular solution to the cross-talk problem is based on subspace method, which considers local
projection onto a subspace of model parameters (baumstein
2014; kennett et al. 1988). in addition, the hierarchically
inversion (freudenreich and singh 2000; ren and liu 2016;
sears et al. 2008; tarantola 1986) and mode decomposed
(wang and cheng 2017) are demonstrated have the ability
to reduce the cross-talk.
in recent study, there are many studies to apply the deep
learning to fwi. in general, those deep learning-based
methods for fwi can be categorized into two types: (1) to
learn a direct map from seismic data to velocity models. for
instance, yang and ma (2019) establish an inverse regression with convolutional neural network (cnn) from synthetic training data set. fabien-ouellet and sarkar (2020)
estimate seismic velocity from cmp gathers using recurrent neural network (rnn). these methods are purely
data-driven methods which can perform inversion directly
after training networks. however, massive training data
set are needed and the accuracy and generalization cannot
be guaranteed without the physical constraints. in order to
address these issues, sun et al. (2021) proposed a physicsguided training strategy, which incorporates physical laws
in the training of neural network, and takes advantage of
data-driven deep learning and conventional physics-driven
methods. wu and lin (2019) used an encodingâ€“decoding
network structure to model the map from seismic data to
velocity structures. (2) to regard deep learning as an effective signal processing tool and does not need any training
data set. richardson (2018) and sun et al. (2020) implement
fwi in the framework of recurrent neural network (rnn)
and use automatic differentiation technology for the gradient
calculating instead of adjoint state method. zhu et al. (2021)
uses reverse-mode automatic differentiation to invert p-wave
velocity from acoustic fwi, and demonstrate the process of
reverse mode automatic differentiation is same with conventional adjoint state method. zhang et al. (2021) and wang
et al. (2021) extend the rnn-based fwi to isotropic-elastic
medium and anisotropic elastic medium. the latter proves
the minibatch configuration is faster and more accurate than
full-batch efwi. these approaches allow for efficient calculation of the derivatives of the residual through automatic
differential backpropagation method. however, they need
to store all intermediate variables for gradient back-propagation, which is demanding for large memory resources.
complex neural networks can be constrained by physical
rules to reduce its degrees of freedom. for example, wu and
mcmechan (2019) and zhu et al. (2022) reparametrize the

13

acta geophysica (2024) 72:673â€“687

velocity model by cnn, the unknown parameters become
the weights and bias in cnn instead of physics models, this
reparameterization has the ability to suppresses the local
minima issue.
in this paper, we demonstrate that the representation of
elastic models by cnns can mitigate the cycle-skipping and
the cross-talk effect in efwi. we first generate the elastic models from neural network and fed to the pde solvers to simulate the multicomponent seismic data. the l2
norm between synthetic data and true data is defined as cost
function and gradient-based optimization method such as
adam optimization method (kingma and ba 2014) is used to
decrease the value of cost function. in contrast to the traditional efwi, the parameters to be optimized in cnn-efwi
are the weight and bias in neural network. by decomposing
and reconstructing the frechÃ©t derivatives, cnns can decouple the p-and s-wave modes, leads to restrain the cross-talk
between parameters. in addition, structure of cnn framework can be regarded as an implicit regularization method,
which is crucial to for solving the local minimum problem
of efwi.
in the follow section, we first review the base theory of
efwi, including the forward and inversion parts. then, we
illustrate the methodology and workflow of cnn-efwi.
numerical examples on overthrust model, fluid model and
2004 bp model verify the reparameterization and regularization of the cnn-efwi inversion scheme.

theory
in the time domain, the 2d wave equation of the velocitystress form is:

ï¿½
ï¿½
â§ ğœ•t vx = 1âˆ•ğœŒ ğœ•x ğœxx + ğœ•z ğœxz
âª
ï¿½
ï¿½
âª ğœ•t vz = 1âˆ• ğœ•x ğœxz + ğœ•z ğœzz
ğœŒ
âª
â¨ ğœ•t ğœxx = (ğœ† + 2ğœ‡)ğœ•x vx + ğœ†ğœ•z vz + fx
âª
âª ğœ•t ğœzz = (ğœ† + 2ğœ‡)ğœ•z vz + ğœ†ğœ•x vx + fz
ï¿½
ï¿½
âª
â© ğœ•t ğœxz = ğœ‡ ğœ•z vx + ğœ•x vz ,

(1)

where, vx and vz are the particle velocity along the x and z
axis, respectively. ğœxx and ğœzz are two component of normal
stress tensor. ğœxz is shear stress tensor. ğœ•t , ğœ•x and ğœ•z are the
derivatives. ğœŒ is the density. ğœ† and ğœ‡ are the lamÃ© parameters. we solve eq. (1) using time domain finite-difference
(fd), which allows for an accurate modelling of p- and swaves in elastic models. at the process of forward propagate, we use convolution perfectly matched layers (cpml)
(komatitsch and martin 2007) to reduce unrealistic boundary reflected waves.

acta geophysica (2024) 72:673â€“687

675

mathematically, the inversion process of efwi can be
expressed as a pde constrained optimization problem,
which consists of minimizing the misfit between observed
data dobs (x, t) and prediction data dsyn (x, t):
ns
âˆ‘

min j =

i=1

ns nt
âˆ‘
âˆ‘â€–
â€–2
ji =
â€–diobs âˆ’ di â€–
â€–
â€–2
i=1 t=1

s.t.di (x, t) = s(ui (x, t))

(2)

ğœ•t ui (x, t) = f(ui (x, t), ğœ†(x), ğœ‡(x), ğœŒ(x)),

ns
âˆ‘
i=1

ji =

nbs nt
âˆ‘
âˆ‘â€–
â€–2
â€–diobs âˆ’ di â€–
â€–
â€–2
i=1 t=1

s.t.di (x, t) = s(ui (x, t))

(3)

ğœ•t ui (x, t) = f(ui (x, t), ğœ†(x), ğœ‡(x), ğœŒ(x)),
where the nbs is the number of batch size. in full-batch efwi
algorithms, the gradients of cost function with respect to
elastic models are calculated from all source gathers. however, in minibatch strategy, source gathers are distributed
into small batches and the parameters are updated after one
batch. in other words, this strategy has more update times
than the traditional full-batch efwi (wang et al. 2021).
equation (3) gives a quantitative measure of the misfit.
the gradient of misfit function with respect to elastic parameters is calculated by adjoint state method, whose details are
presented in appendix i. the gradientsâ€™ formulas are:
ğœ•ğœ† j = âˆ’

nt
âˆ‘
( t+1
)( t
)
t+1
ğœÌƒ zz + ğœÌƒ xx
ğœ•z vz + ğœ•x vxt Î´t

vs = cnnvs (x, wvs ) + vsinit ,

nt
âˆ‘
[ t+1 (
)
]
t+1
ğœÌƒ xz ğœ•x vzt + ğœ•z vxt + 2ğœÌƒ zzt+1 ğœ•z vzt + 2ğœÌƒ xx
ğœ•x vxt Î´t
t=1

âˆ‘
nt

ğœ•ğœŒ j =

t=1

[
( t
)
( t
)]
t
ğœŒâˆ’2 vÌƒ t+1
ğœ•z ğœxz + ğœ•x ğœxx
+ vÌƒ t+1
ğœ•x ğœxz + ğœ•z ğœzzt Î´t,
x
z

(4)

where vx and vz are the forward partial velocity vectors
in eq. (1). vÌƒ t+1
are the adjoint velocity vectors at
Ìƒ t+1
x and v
z
t+1
the time step t + 1, ğœÌƒ xx
and ğœÌƒ zzt+1 are adjoint normal stress

(5)

gradient-based inversion algorithm (such as l-bfgs,
adam) can be used to update velocity parameters. however,
updating multiple parameters simultaneously is a challenging task because they can influence the seismic response
together. the coupling of multiple elastic parameters can
cause an update error in one parameter to affect other parameters. the various parameters have different sensitivities,
which can exacerbate the crosstalk problem. besides, due
to the introduction of more degrees of freedom in the model
space, the nonlinearity increases when multiple parameters
are involved. in order to address these two issues, we reparametrize the velocity models using neural networks. the fundamental principle of neural network is the universal approximation theorem. in other words, neural networks have the
ability to approximate any function using nonorthogonal
basis functions. in the structure of convolutional neural networks (cnn), the deep image prior be used as regularization
for tasks such as denoising (lempitsky 2018). therefore, the
reparameterization using convolutional neural networks can
introduce the regularization into efwi, which is crucial to
mitigate the local minimum problem in efwi. in addition,
cnn provide a sparse representation of the velocity models
by introducing convolutional layers, so we can extract some
specific features by constructing specific layers of the network. the diagram of traditional efwi and proposed cnnefwi are shown in fig. 1. the structure of cnn-efwi
is more complex than traditional efwi. velocity models
vp and vs are reparametrized by generative neural network
structure, the process is expressed as:

vp = cnnvp (x, wvp ) + vpinit

t=1

ğœ•ğœ‡ j = âˆ’

ğœ•vp j = 2ğœŒvp ğœ•ğœ† j
ğœ•vs j = âˆ’4ğœŒvs ğœ•ğœ† j + 2ğœŒvs ğœ•ğœ‡ j,

where j is the scalar cost function.ns and nt is the number
of shots and time steps, ui (x, t) is the wavefield at the time
step t and the position x , which could be partial velocity or
stress. s denotes a sampling function that is only non-zeros
at the position of receivers and f is the solution of eq. (1).
the mini-batch strategy is used for speed-up the convergence, we split the shot gathers into small batches and
update parameters from each batch. the minibatch setup
is widely used in traditional deep learning field, and it has
a faster convergence speed than using the entire data set as
a batch (bishop 2006). with mini-batch strategy, the cost
function written as:

min j =

t+1 is three adjoint shear stress vectors. t and t are
vectors. ğœÌƒ xz
the discrete time step and maximum recording time, respectively. it is well known that the model parameterization is
important for multi-parameter efwi (tarantola 1986). we
choose p-wave velocity (vp), s-wave velocity (vs ) and density (ğœŒ) as the parameterization. the corresponding gradients
are derived by chain rule in the manner of mora (1987):

(6)

where cnnvp and cnnvs are generative neural networks
with respect to vp and vs , respectively. x is a random latent
vector, which is transformed to a tensor by fully connected
layer. wvp and wvs contain the weights and bias in neural
networks. the velocity models generated by convolutional
neural networks are input to the elastic wave eq. (1) used
in conventional efwi. vpinit and vsinit are the initial p-wave
and s-wave velocity models respectively. the initial model
is directly combined with the output of the neural network,

13

676

fig. 1â€‚â€‰the diagram of traditional efwi (a) and proposed cnnefwi (b). downward arrow denotes the forward computation. red
upward arrow denotes the gradient calculation by automatic differ-

which means that the neural networks are trained to predict
the residuals between the initial models and the real models.
a detailed of the structure is shown in fig. 1. different with
conventional inversion methods, which update vpinit and vsinit
models directly, the parameters to be optimized in cnnefwi are the wvp and wvs in eq. (6).
the gradients of cost function with respect to parameters
have been decomposed and reconstructed in the process of
inversion. with reparameterization by cnns, the cost function
of cnn-efwi is defined by:

13

acta geophysica (2024) 72:673â€“687

entiation (ad) and green upward arrow denotes gradient calculation
by adjoint state method (asm). subblock(n, n) denotes the kernel of
convolution layer in subblock is (n Ã— n)

min j =

ns
âˆ‘
i=1

ns nt
âˆ‘
âˆ‘â€–
â€–2
ji =
â€–diobs âˆ’ di â€–
â€–
â€–2
i=1 t=1

s.t.di (x, t) = s(ui (x, t))

(7)

ğœ•t ui (x, t) = f(ui (x, t), n(vp(x)), n(vs(x)), n(ğœŒ(x)))
to minimize the eq. (7) using the gradient-based
method, the gradient of the cost function with respect to
the learnable parameters wvp and wvs should be calculated.
the gradient formulas as:

acta geophysica (2024) 72:673â€“687

ğœ•wvp j =

ğœ•nvp

677

ğœ•vp j

ğœ•wvp
(8)
ğœ•nvs
ğœ•vs j,
ğœ•wvs j =
ğœ•wvs
/
/
where ğœ•nvp ğœ•wvp, ğœ•nvs ğœ•wvs are gradients of neural network
with respect to weight wvp and wvs respectively, which are
obtained by automatic differentiation back-propagation in
the deep learning framework such as tensorflow or pytorch.
ğœ•vp j and ğœ•vs j are the frechÃ©t derivatives in the eq. (5). equation (8) is a process of reconstructing the frechÃ©t derivatives, which can help mitigate the cross-talk problem as
demonstrated in next section. we use adam algorithm to
minimize the cost function (eq. 7) efficiently. adam applied
the concept of momentum and adaptively adjusted the gradient using the exponentially decaying average of the previous
squared gradient. the update formula is as follows:
wk+1 = wk âˆ’ ğ›¼pk ,

(9)

where ğ›¼ is learning rate, k denotes the update number, pk
denotes the update direction of the cost function, which is:

pk = ğœ•w j + ğ›½pkâˆ’1 ,

(10)

where ğ›½ denotes the momentum parameter and ğœ•w j is the
gradient of cost function with respect to neural network
weights w.

numerical experiments
in this section, we illustrate the reparameterization and
regularization capabilities of cnn-efwi with three comprehensive examples: seg/eage overthrust models, fluid
saturated models and 2004 bp salt body models. in these
implementations, we assume that the source functions are
well known and do not address this particular challenge of
full waveform inversion.

seg/eage overâ€‘thrust model
we take 2d overthrust models as the first example to evaluate the cnn-efwi algorithm. the models are resampled with a gird size of 68 Ã— 202 and spatial sampling of
Î´x = Î´z = 50m. figure 2 shows the true p-wave and s-wave
velocity models. to simulate offshore synthetic seismic
data, we use 68 explosive ricker wavelets with a domain
frequency 3 hz as the sources, and 199 receivers were fixed

fig. 2â€‚â€‰seg/eage overthrust
p- and s-wave velocity models.
(a)â€“(b) are the true models;
(c)â€“(d) are the initial models;
(e)â€“(f) is the inverted models
by cnn-efwi; (g)â€“(h) is the
inverted models by conventional
efwi

13

678

acta geophysica (2024) 72:673â€“687

on the seabed to simulate obc survey. the total recording
time is 5 s with an interval of 2.5 ms. in order to demonstrate
the decoupling ability of proposed cnn-efwi method, we
insert a high velocity layer at the bottom of p-wave velocity

table 1â€‚â€‰the total computational
costs of proposed cnn-efwi
and traditional efwi

fig. 3â€‚â€‰vertical profiles of overthrust p- wave velocity model
(a) and s-wave velocity model
(b). the area between two black
lines in (a) is the high-velocity
layer in p-wave velocity. the
subplots (a): x = 2.02 km. (b):
x = 4.04 km. (c): x = 6.06 km.
(d): x = 8.08 km

13

method

time
spent
(min)

cnn-efwi
efwi

5.50
5.26

model. the initial models are the 1 âˆ’ d increase models
(fig. 2). the same initial model and observed seismic data
are used for conventional efwi.
table 1 lists the total computing times on gpus with
these two methods. we set the maximum iteration number
as 40. we found that cnn-efwi requires more computational time due to the additional neural network structure. but average time only increases about 5 percent. the
inverted results are shown in fig. 2. both approaches have
recovered p-wave velocity and s-wave velocity very well
in the shallow part (above 2 km). the high-velocity thin
layer in p-wave velocity has been recovered clearly in the
results of cnn-efwi. in contrast, conventional efwi

acta geophysica (2024) 72:673â€“687

679

losses some resolution for the layer at the left and right
boundaries of the model. due to the influence of cross-talk
effect, the resolution at the bottom of s-wave wave velocity is low. the vertical profiles of the p-wave velocity and
s-wave velocity (fig. 3) further verify that the cnn-efwi
outperforms the conventional approach at the position of
inserted layer.
in order to quantitatively compare the accuracy of these
inverted results, we calculate the mean square error ( mse )
and structural similarity index measure ( ssim ) between
the inverted models and the true models. they are defined
as:
n
âˆ‘

[x(i) âˆ’ xÌ‚ (i)]2

(11)

(
)(
)
2ğœ‡x ğœ‡xÌ‚ + c1 2ğœxÌ‚x + c2
ssim(x, xÌ‚ ) = (
)(
),
ğœ‡x2 + ğœ‡x2Ì‚ + c1 ğœx2 + ğœxÌ‚2 + c2

(12)

mse(x, xÌ‚ ) =

i=1

where x is the true model and xÌ‚ is the estimated model,
ğœ‡x and ğœ‡xÌ‚ are the meaning of true model and inverted
model, respectively. ğœx and ğœxÌ‚ are variance of true model
and inverted model, respectively. ğœxÌ‚x denotes the covariance
between true and estimated model. n denotes the total number of points in the model. mse is measure of l2 norm distance between true and inverted models. ssim evaluates the
similarity of two images from local statistics, which close to

table 2â€‚â€‰mse and ssim between true and inverted overthrust models
using cnn-efwi and conventional efwi
p-wave velocity
cnn-efwi

mse
294.54
ssim
0.9754

s-wave velocity
conventional
efwi

ccn-efwi

conventional
efwi

459.4

154.71

180.34

0.9411

0.9862

0.981

one indicate high quality inversion results. in table 1, cnnefwi achieves more accurate inversion than conventional
efwi (table 2).

fluid saturated lens model
we consider a simple example of fluid saturated lens model.
in many cases, fwi suffers from insufficient data to fully
constrain inversion results. for example, inaccurate initial models can trap the optimization in the local minima
and raise cycle skipping problem. as shown in fig. 4, a
square homogeneous background is perturbed two lens
inclusions. we assume that the upper lens is gas saturated (vp = 2.65 km Â­sâˆ’1, vs = 1.66 km Â­sâˆ’1), and lower lens
is water-saturated (vp = 3.0 km Â­sâˆ’1,vs = 1.66 km Â­sâˆ’1). in
order to evaluate the regularization effect of cnn-efwi,
two initial velocity models are used. one is the smooth version of true velocity models (fig. 5a,d) and another is set
to the background homogeneous models (fig. 5g,j). the
grid size of the model is 100 Ã— 250 and the spatial sample
is Î´x = Î´z = 15m . 32 cells are added to the boundary of
models for cpml conditions. on the surface of the model,
50 shots are triggered with a horizontal interval of 30 m and
receivers are deployed with an interval of 15 m. the sources
are ricker wavelets with a dominant frequency of 4.5 hz.
the seismic simulation time step is 2.5 ms, and the whole
simulation time is 2.5 s.
at the case of using smooth initial model, fig. 5b,e show
the p-wave velocity models inverted by cnn-efwi and
conventional efwi, respectively. figure 5c,f show the inversion results of corresponding s-wave velocity models. we
observe that both p- and s-wave velocity are recovered very
well by the cnn-efwi, the sharpness of recovered lens
outlines and the boundary is encouraging. in contrast, the
inverted lens using conventional method are not as accurate
as that obtained by cnn-efwi especially at the low boundaries of the lens. figure 6a,b,c,d show the difference between
the true and inverted models. the figure indicate that the
proposed approach estimates accurately for most parts of the
models. the vertical profiles at the middle part of the models
are shown in fig. 7. we can see that the boundary of lens in
cnn-efwi results is sharper than results from conventional

fig. 4â€‚â€‰the true fluid-saturated lens models. (a) is the true p-wave velocity model. (b) is the true s-wave velocity model

13

680

acta geophysica (2024) 72:673â€“687

fig. 5â€‚â€‰the initial fluid-saturated lens models and corresponding
inverted results. (a)â€“(c) are smooth initial p-wave velocity model,
inverted result with cnn-efwi and conventional efwi, respectively. (d)â€“(f) are corresponding s-wave velocity models. (g)â€“(i) are

constant initial p-wave velocity model, inverted result with cnnefwi and inverted result with conventional efwi, respectively. (j)â€“
(l) are corresponding s-wave velocity models

efwi, which indicates that cnn-efwi is effective and can
provide higher resolution models compared with traditional
methods. quantitatively comparation is shown in table 3.
from this table one could get the same conclusion with
above comparison.
the inverted models using constant initial models is
shown in fig. 5. we can see that the inversion results using
conventional method hardly image the structure of the lens
in s-wave velocity model because of inadequate initial models. the cross-talk between p- and s-wave velocity lead to
incorrect imaging lens in the s-wave velocity model. in addition, the lower boundaries of the lens inverted by conventional method is fuzzy. the lack of low-frequency content
will accentuate the cycle-skipping problem, the optimization
is trapped at a local minimum. in the early stage of optimization, due to the influence of parameter crosstalk, both
p- and s-wave velocity cannot be updated correctly. thus,
the inversion eventually inadequate converges due to limited
iterations. however, cnn-efwi successfully mitigated the
crosstalk and recovered sharper both p-and s-wave velocity
models, especially at the upper and lower boundaries of the
lens. residual between true and inverted models are shown
in fig. 6. it is interesting to note that inverted s-wave velocity using constant initial models is more accurate than results

of using smooth initial models. vertical profiles across the
lens (fig. 7) further verify that the cnn-efwi has the ability to image fluid saturated lens even using constant initial
models. the mse and ssim between models and seismogram are presented in table 4. these comparisons verify
the regularization effect and reparameterization ability from
the cnn.

13

bp salt body model
2004 bp models (billette and brandsberg-dahl 2005) are
difficult to image for efwi because their complex salt bodies, which is based on a geological cross section through
the western gulf of mexico. the sharp contrast of large
salt bodies leads to serious cycle-skipping and amplitude
discrepancy. we slice the origin models and extract the left
parts for tests. the s-wave velocity (fig. 8c) was derived
from p-wave velocity model (fig. 8a) using an empirical
formula (mavko et al. 2020). the field covers the area of
9.4 km Ã— 3.4 km (x- and z-directions, respectively), the size
of each cell is 50 m for both dimensions, representing a uniform mesh of 68 Ã— 188 grid points. c-pml boundary condition is also used in the test with 20 grid points. 63 sources
wavelets with a 1.2 hz domain frequency is used to generate

acta geophysica (2024) 72:673â€“687

681

fig. 6â€‚â€‰the residuals of true models minus inverted models. (a)â€“(b)
are true p-wave velocity model minus inverted p-wave velocity models using smooth initial models with cnn-efwi and conventional
efwi, respectively. (c)â€“(d) are corresponding s-wave velocity mod-

els. (e)â€“(f) are p-wave velocity models using constant initial models
with cnn-efwi and conventional efwi, respectively. (g)â€“(h) are
corresponding s-wave velocity models

multicomponent seismic waveform data. 185 receivers are
deployed with an interval of 15 m. the time step is 9.2 ms,
which is small enough to avoid dispersion. it is assumed that
we know nothing of the salt bodies and the initial models are
set to constant value: p-wave velocity = 1.486 km Â­sâˆ’1 and
s-wave velocity = 0.891 km Â­sâˆ’1, respectively.

because the salt body is very complex, the learning rate
setting is a trial-and-error process. if the learning rate is too
small, the convergence speed of network parameters will be
very slow. however, a high learning rate leads to divergence
of the loss function. to permit a cross-talk between the
speed of convergence and stability, we implement a multiple

13

682

acta geophysica (2024) 72:673â€“687

fig. 7â€‚â€‰the vertical profiles at the middle of the models. (a) and (b) are inverted p- and s-wave velocity results with smooth initial models.
(c) and (d) are inverted p- and s-wave velocity results with constant initial models

table 3â€‚â€‰mse and ssim between true and inverted fluid-saturated
models using cnn-efwi and conventional efwi at the case of
smooth initial models
p-wave velocity
cnn-efwi

mse
17.9
ssim
0.9716

s-wave velocity
conventional
efwi

ccn-efwi

conventional
efwi

27.39

16.24

33.56

0.9255

0.9884

0.9397

table 4â€‚â€‰mse and ssim between true and inverted fluid-saturated
models using cnn-efwi and conventional efwi at the case of
constant initial models
p-wave velocity
cnn-efwi

mse
22.94
ssim
0.9522

s-wave velocity
conventional
efwi

ccn-efwi

conventional
efwi

66.06

14.7

95.78

0.7368

0.9901

0.3878

learning rate strategy within the process of optimization. the
learning rate become smaller when loss function decrease.
a comparison of the inverted results from different
approaches is shown in fig. 8. as we can see, convolutional
method cannot recover the shape of salt body of both p- and
s- wave velocity models. only the upper boundary of salt
body is recovered. there are no updates for both velocity

13

models in the case of conventional efwi. in contrast, cnnefwi correctly images the salt body, especially the lower
boundary of salt body and the left u-shaped target. it is demonstrated that cnn-efwi can image the p- and s- velocity
of complex salt body using constant initial models. figure 9
shows the features learned from one of the cnns of the 50
iterations and 100 iterations. noted that with the increasing
of iterations, the features learned from the network are closer
to the shape of salt body.
next, we added two different levels of noise to the cmp
gathers to verify the robust of cnn-efwi. as shown in
fig. 10. the snr (signal/noise ratio) are 25 and 20 db,
respectively. figure 11 shows the final inverted p- and
s-wave velocity models using different noisy data. we
observe that a significant error in the lower right corner of
p-wave velocity results with the increase of noise. however,
the position of main salt body is located successfully, even
the u-shape target is correctly imaged. the example demonstrates that spatial regularization from convolutional neural
networks could reduce the influence of noise on inversion
results.

discussion
proposed method provides a framework for combining the
convolutional neural networks and efwi applications. different with purely data-driven method, which rely on a large
number of training data set, including data pairs of velocity
models and corresponding seismic data. cnn-efwi can
introduce elastic physical information into neural networks
and does not need any extra data set.

acta geophysica (2024) 72:673â€“687

683

fig. 8â€‚â€‰2004 bp salt body models. (a) and (b)the true p- and s-wave
velocity models. (c) and (d) are initial p- and s-wave velocity models.
(e) and (f) are inverted p- and s-wave velocity models using cnn-

efwi. (g) and (h) inverted p- and s-wave velocity models using
conventional efwi

the efwi also could be applied in the frequency domain,
which is mathematically equivalent to the time domain
method (pratt et al. 1998). however, the implementation
of the frequency domain approach is memory consuming.
in addition, another crucial problem is that the inversion of
a set of sparse frequencies is susceptible to the gibbs phenomenon (brenders et al. 2012). therefore, the time domain
approach is more suitable for inverting complex elastic models. in addition, in the time domain method, the number of
shots and the number of time steps are the key factors of
parallel computation, which makes it easy to use gpu to
accelerate the computation.

compare to traditional efwi, cnn-efwi can provide
more stable inversion results because of the convolutional
structure in neural network, which increase spatial correlations of elastic models and can be regard as a regularization
strategy to mitigate local minima issue, even if the initial
models are not accurate enough. but it deeply relies on the
low frequency data in seismic data, cnn-efwi canâ€™t invert
reasonably results without it.
models parameterized using neural networks increases
the flexibility of inversion. however, more flexibility makes
the method less robust. to mitigate the problem, the maximum and minimum of models should be defined to constrain

13

684

acta geophysica (2024) 72:673â€“687

fig. 9â€‚â€‰features learned by neural networks. (a) and (b) are features of one layer in cnnvp after 50 and 100 iterations, respectively. (c) and (d) are
features of one layer in cnnvs after 50 and 100 iterations, respectively

fig. 10â€‚â€‰the seismogram generated from true models at the position of (x, z) = (5.0, 0.0) km. (a) and (d) are noise free horizontal
and vertical particle-velocity common-source gathers, respectively.
(b) and (e) are horizontal and vertical particle-velocity common-

the optimization in the process of inversion. otherwise, the
models will update in the wrong direction and be trapped in
local minima. typically, the minimum value of the models
is set to zero, while the maximum value of models is set as:

13

source gathers with 10 db noise, respectively. (c) and (f) are horizontal and vertical particle-velocity common-source gathers with 20 db
noise, respectively

vpmax =

0.6 min(dx, dz)
âˆš
2dt

(13)

acta geophysica (2024) 72:673â€“687

685

fig. 11â€‚â€‰inverted models with
noise data using cnn-efwi.
(a) and (c) are inverted p- and
s-wave velocity models with
25 db noise. (b) and (d) are
inverted p- and s-wave velocity
models with 20 db noise

âˆš
vsmax = vpmax âˆ• 2,

(14)

where the vpmax and vsmax denote the maximum value of pand s-wave velocity models respectively. min(dx, dz) is the
minimum between dx and dz . formula (12) is the stability
condition of finite difference.
noted that density is not inverted in our tests, which is
difficult to reconstruct because its perturbations have hardly
any effect on the phases or travel times of p- and s-waves.
for simplicity, many studies assume that models have the
constant density. (brossier et al. 2009; sears et al. 2008;
shipp and singh 2002) or calculate by the empirical relationship with p-wave velocity (borisov and singh 2015).
however, there also are same scholars consider density as
an inversion parameter. for example, xu and mcmechan
(2014) use a multistep-length efwi approach to mitigate
the cross-talk between p-wave velocity, s-wave velocity and
density. recently, zhang et al. (2021) invert the density in
the framework of rnn. extension to density inversion is one
direction of future works.
cnn-efwi is not limited to the specific configuration
used in this paper. the tune of hyperparameters of neural
networks is a crucial challenge. a variety of types of hyperparameters in cnn-efwi should be considered, including
the layers of neural networks, the learning rate, the scale
parameters, the activation function and the optimization
algorithm. the hyperparameters in our implementation are
not unique, because the turning of these hyperparameters is
a trial-and-error process. it should be noted that the resolution of p-wave velocity is less than s-wave velocity because
the spatial wavelengths of p-wave propagating in medium

are greater than wavelengths of s-wave. therefore, the convolutional kernels in cnns with respect to p-wave velocity
should be greater than that of s-wave velocity.

conclusion
in this work, we introduce a method cnn-efwi to mitigate the cycle-skipping problem in efwi by combining the
convolutional neural network and pdes. the weights and
bias in cnns are updated by connecting the gradient from
adjoint state method and automatic differentiation. the convolutional kernel in cnn increases the spatial correlation
of models, which can be regarded as an implicit regularization, which is significant for efwi. in addition, cnns also
have the ability to decouple the p-wave and s-wave modes
by reconstructing the frechÃ©t derivatives. numerical example on overthrust models demonstrate the reparameterization effect of proposed cnn-efwi. then, we design a fluid
saturated lens model, inversion results show that proposed
method outperforms than conventional method. even the
initial model is a constant model, the proposed method can
image the lens clearly. finally, the example of 2004 bp salt
body model further verify the regularization ability of neural
network. in addition, the features learned by neural network
automatically filter out noise, which improves the robustness of inversion. cnn-efwi can be directly applied to the
same datasets as conventional efwi to improve the inversion performance.

13

686

acta geophysica (2024) 72:673â€“687

appendix i
in this appendix, we derive the gradient of cost function with
respect to lamÃ© parameters (ğœ† and ğœ‡) and density (ğœŒ). for
simplicity, we ignore the non-reflecting pml regions. the
sum of squared errors is used as the cost function:

min j =

ns
âˆ‘
i=1

ji =

nbs nt
âˆ‘
âˆ‘â€–
â€–2
â€–diobs âˆ’ di â€– ,
â€–
â€–2

(15)

i=1 t=1

where nt is the number of time points. in the adjoint state
method algorithms, the gradient for elastic model parameters
ğœ•jâˆ•ğœ•ğœ†, ğœ•jâˆ•ğœ•ğœ‡ and ğœ•jâˆ•ğœ•ğœŒ depend on the adjoint wavefields,
which is calculated by chain rule. we obtain the adjoint
equations:
ğœ•Ìƒvx (r, t) 1
Î´t + sx (t)
ğœ•âˆ’ x
ğœŒ
ğœ•Ìƒvz (r, t) 1
ğœÌƒ zz (r, t âˆ’ Î´t) =ğœÌƒ zz (r, t) +
Î´t + sz (t)
ğœ•âˆ’ z
ğœŒ
)
(
ğœ•Ìƒvx (r, t) ğœ•Ìƒvz (r, t)
1
+
Î´t
ğœÌƒ xz (r, t âˆ’ Î´t) =ğœÌƒ xz (r, t) +
ğœ•+ z
ğœ•+ x
ğœŒ
]
[
ğœ• ğœÌƒ zz (r, t)
ğœ• ğœÌƒ xz (r, t)
ğœ• ğœÌƒ xx (r, t)
(ğœ† + 2ğœ‡) +
ğœ†+
ğœ‡ Î´t
vÌƒ x (r, t âˆ’ Î´t) =Ìƒvx (r, t) +
ğœ•+ x
ğœ•+ x
ğœ•âˆ’ z
]
[
ğœ• ğœÌƒ xz (r, t)
ğœ• ğœÌƒ zz (r, t)
ğœ• ğœÌƒ xx (r, t)
vÌƒ z (r, t âˆ’ Î´t) =Ìƒvz (r, t) +
(ğœ† + 2ğœ‡) +
ğœ†+
ğœ‡ Î´t,
ğœ•+ z
ğœ•+ z
ğœ•âˆ’ x

ğœÌƒ xx (r, t âˆ’ Î´t) =ğœÌƒ xx (r, t) +

)
(
nt
âˆ‘
) ğœ•vzt ğœ•vxt
( t+1
ğœ•j
t+1
+
Î´t
=âˆ’
ğœÌƒ zz + ğœÌƒ xx
ğœ•ğœ†
ğœ•z
ğœ•x
t=1
)
]
( t
nt [
âˆ‘
ğœ•vzt
ğœ•vz ğœ•vxt
ğœ•vxt
ğœ•j
t+1
t+1
+
+ 2ğœÌƒ zzt+1
+ 2ğœÌƒ xx
Î´t
ğœÌƒ xz
=âˆ’
ğœ•ğœ‡
ğœ•x
ğœ•z
ğœ•z
ğœ•x
t=1
[ ( t
( t
)]
nt
t )
ğœ•ğœxz ğœ•ğœzzt
ğœ•j âˆ‘ âˆ’2 t+1 ğœ•ğœxz ğœ•ğœxx
t+1
=
+
+
ğœŒ vÌƒ x
+ vÌƒ z
Î´t.
ğœ•ğœŒ t=1
ğœ•z
ğœ•x
ğœ•x
ğœ•z

(18)

acknowledgements we would like to acknowledge the sponsorship of
national natural science foundation of china (42174139,41974119,
42030103) and science foundation from innovation and technology
support program for young scientists in colleges of shandong province and ministry of science and technology of china (2019ra2136)
and support by the marine s&t fund of shandong province for pilot
national laboratory for marine science and technology(qingdao)
(grant no.2021qnlm020001-6).(forgues and lambarÃ©, 1997).
data availability data associated with this research are available and
can be accessed via the following url: https://â€‹github.â€‹com/â€‹guokeâ€‹ting/â€‹
cnn-â€‹fwi.

declarations
conflict of interest the authors declare that they have no competing
interests. no infringement of others' intellectual property rights.

(16)

where the wavefields with a tilde denote the adjoint wavefield. in other words, they are the partial derivatives of j
with respect to corresponding wavefields. for example, vÌƒ z
indicates ğœ•jâˆ•ğœ•vz . Î´t is discrete time interval. sx (t) and sz (t)
are adjoint source.
as the adjoint wave fields propagate backward in time,
the derivatives of the objective function with respect to the
elastic parameters are calculated as:
(
)
ğœ•ğœzz
ğœ•vx ğœ•vz
ğœ•ğœxx
Î´t
=
=
+
ğœ•ğœ†
ğœ•ğœ†
ğœ•x
ğœ•z
)
(
ğœ•vx
ğœ•ğœxx ğœ•ğœxz
= âˆ’ğœŒâˆ’2
+
Î´t
ğœ•ğœŒ
ğœ•x
ğœ•z
(
)
ğœ•ğœzz ğœ•ğœxz
ğœ•vz
= âˆ’ğœŒâˆ’2
+
Î´t
(17)
ğœ•ğœŒ
ğœ•z
ğœ•x
ğœ•ğœzz
ğœ•vz
ğœ•v
ğœ•ğœxx
= 2 x Î´t,
=2
Î´t
ğœ•ğœ‡
ğœ•x
ğœ•ğœ‡
ğœ•z
)
(
ğœ•ğœxz
ğœ•vx ğœ•vz
Î´t.
=
+
ğœ•ğœ‡
ğœ•z
ğœ•x
note that the system is solved backwards in time. connect eqs. (3) and (2), the gradients of the objective function
j with respect to elastic models ğœ†, ğœ‡ and ğœŒ are:

13

references
baumstein a (2014) extended subspace method for attenuation of
crosstalk in multi-parameter full wavefield inversion in: proceedings 2014 seg annual meeting 2014, all days: seg-2014â€“0546.
billette fj, and brandsberg-dahl s (2005) the 2004 bp velocity
benchmark in: extended abstracts https://â€‹doi.â€‹org/â€‹10.â€‹3997/â€‹2214-â€‹
4609-â€‹pdb.1.â€‹b035.
bishop cm (2006) pattern recognition and machine learning (information science and statistics). springer, new york
borisov d, singh sc (2015) three-dimensional elastic full waveform
inversion in a marine environment using multicomponent oceanbottom cables: a synthetic study. geophys j int 201(3):1215â€“1234.
https://â€‹doi.â€‹org/â€‹10.â€‹1093/â€‹gji/â€‹ggv048
brenders aj, albertin u and mika j (2012) comparison of 3-d timeand frequency-domain waveform inversion: benefits and insights
of a broadband, discrete-frequency strategy in: proceedings 2012
seg annual meeting, onepetro.
brossier r, operto s, virieux j (2009) seismic imaging of complex
onshore structures by 2d elastic frequency-domain full-waveform
inversion. geophysics 74(6):105â€“118. https://â€‹doi.â€‹org/â€‹10.â€‹1190/1.â€‹
32157â€‹71
chen f, zong z, jiang m (2021) seismic reflectivity and transmissivity
parametrization with the effect of normal in situ stress. geophys j
int 226(3):1599â€“1614. https://â€‹doi.â€‹org/â€‹10.â€‹1093/â€‹gji/â€‹ggab1â€‹79
chen f, zong z, yang y, gu x (2022) amplitude-variation-with-offset
inversion using p- to s-wave velocity ratio and p-wave velocity. geophysics 87(4):n63â€“n74. https://â€‹doi.â€‹org/â€‹10.â€‹1190/â€‹geo20â€‹
21-â€‹0623.1
choi y, shin c, min d-j (2007) frequency-domain elastic full-waveform inversion using the new pseudo-hessian matrix: elastic

acta geophysica (2024) 72:673â€“687
marmousi-2 synthetic test, seg technical program expanded
abstracts 2007. soc explor geophys 65:1908â€“1912
fabien-ouellet g, sarkar r (2020) seismic velocity estimation: a deep
recurrent neural-network approach. geophysics 85(1):u21â€“u29.
https://â€‹doi.â€‹org/â€‹10.â€‹1190/â€‹geo20â€‹18-â€‹0786.1
forgues e, lambarÃ© g (1997) parameterization study for acoustic and
elastic ray plus born inversion. j seism explor 6(2â€“3):253â€“277
freudenreich y, singh s (2000). full waveform inversion for seismic
dataâ€”frequency versus time domain. https://â€‹doi.â€‹org/â€‹10.â€‹3997/â€‹
2214-â€‹4609-â€‹pdb.â€‹28.â€‹c54
innanen ka (2014) seismic avo and the inverse hessian in precritical
reflection full waveform inversion. geophys j int 199(2):717â€“734.
https://â€‹doi.â€‹org/â€‹10.â€‹1093/â€‹gji/â€‹ggu291
kennett bln, sambridge ms, williamson pr (1988) subspace methods for large inverse problems with multiple parameter classes.
geophys j int 94(2):237â€“247. https://â€‹doi.â€‹org/â€‹10.â€‹1111/j.â€‹1365-â€‹
246x.â€‹1988.â€‹tb058â€‹98.x
kingma dp and ba jj (2014) adam: a method for stochastic optimization. arxiv preprint arxiv:1412.6980
komatitsch d, martin r (2007) an unsplit convolutional perfectly
matched layer improved at grazing incidence for the seismic wave
equation. geophysics 72(5):sm155â€“sm167. https://â€‹doi.â€‹org/â€‹10.â€‹
1190/1.â€‹27575â€‹86
lempitsky, v., a. vedaldi, and d. ulyanov. 2018, deep image prior.
paper read at 2018 ieee/cvf conference on computer vision and
pattern recognition. pp. 18â€“23.
mavko g, mukerji t, dvorkin j (2020) the rock physics handbook.
cambridge university press
mora p (1987) nonlinear two-dimensional elastic inversion of multioffset seismic data 52(9):1211â€“1228. https://â€‹doi.â€‹org/â€‹10.â€‹1190/1.â€‹
14423â€‹84
nihei kt, li x (2007) frequency response modelling of seismic waves
using finite difference time domain with phase sensitive detection
(tdâ€”psd). geophys j int 169(3):1069â€“1078. https://dâ€‹ oi.oâ€‹ rg/1â€‹ 0.â€‹
1111/j.â€‹1365-â€‹246x.â€‹2006.â€‹03262
operto s, gholami y, prieux v, ribodetti a, brossier r, metivier l,
virieux j (2013) a guided tour of multiparameter full-waveform
inversion with multicomponent data: from theory to practice. lead
edge 32(9):1040â€“1054. https://â€‹doi.â€‹org/â€‹10.â€‹1190/â€‹tle32â€‹091040.1
pratt rg, shin c, hick gj (1998) gauss-newton and full newton
methods in frequencyâ€“space seismic waveform inversion. geophys j int 133(2):341â€“362. https://â€‹doi.â€‹org/â€‹10.â€‹1046/j.â€‹1365-â€‹246x.â€‹
1998.â€‹00498.x
prieux v, brossier r, operto s, virieux j (2013) multiparameter full
waveform inversion of multicomponent ocean-bottom-cable data
from the valhall field part 2: imaging compressive-wave and
shear-wave velocities. geophys j int 194(3):1665â€“1681. https://â€‹
doi.â€‹org/â€‹10.â€‹1093/â€‹gji/â€‹ggt178
ren z, liu y (2016) a hierarchical elastic full-waveform inversion
scheme based on wavefield separation and the multistep-length
approach. geophysics 81(3):r99â€“r123. https://â€‹doi.â€‹org/â€‹10.â€‹1190/â€‹
geo20â€‹15-â€‹0431.1
richardson a (2018) seismic full-waveform inversion using deep
learning tools and techniques. arxiv preprint arxiv:.07232.
sears tj, singh sc, barton pj (2008) elastic full waveform inversion of multi-component obc seismic data. geophys. prospect
56(6):843â€“862. https://dâ€‹ oi.oâ€‹ rg/1â€‹ 0.1â€‹ 111/j.1â€‹ 365-2â€‹ 478.2â€‹ 008.0â€‹ 0692.x

687
shin c, jang s, min dj (2001) improved amplitude preservation for
prestack depth migration by inverse scattering theory. geophy
prospect 49(5):592â€“606. https://â€‹doi.â€‹org/â€‹10.â€‹1046/j.â€‹1365-â€‹2478.â€‹
2001.â€‹00279.x
shipp rm, singh sc (2002) two-dimensional full wavefield inversion of wide-aperture marine seismic streamer data. geophys j
int 151(2):325â€“344. https://â€‹doi.â€‹org/â€‹10.â€‹1046/j.â€‹1365-â€‹246x.â€‹2002.â€‹
01645.x
sirgue l, etgen jt, albertin u (2008) 3d frequency domain waveform
inversion using time domain finite difference methods. google
pat. https://â€‹doi.â€‹org/â€‹10.â€‹3997/â€‹2214-â€‹4609.â€‹20147â€‹683
sun j, niu z, innanen ka, li j, trad do (2020) a theory-guided
deep-learning formulation and optimization of seismic waveform
inversiontheory-guided dl and seismic inversion. geophysics
85(2):r87â€“r99
sun j, innanen ka, huang c (2021) physics-guided deep learning for
seismic inversion with hybrid training and uncertainty analysis.
geophysics 86(3):r303â€“r317. https://â€‹doi.â€‹org/â€‹10.â€‹1190/â€‹geo20â€‹
20-â€‹0312.1
tarantola a (1986) a strategy for nonlinear elastic inversion of seismic
reflection data. geophysics 51(10):1893â€“1903. https://dâ€‹ oi.oâ€‹ rg/1â€‹ 0.â€‹
1190/1.â€‹14420â€‹46
wang tf, cheng jb (2017) elastic full waveform inversion based on
mode decomposition: the approach and mechanism. geophys j int
209(2):606â€“622. https://â€‹doi.â€‹org/â€‹10.â€‹1093/â€‹gji/â€‹ggx038
wang w, mcmechan ga, ma j (2021) elastic isotropic and anisotropic
full-waveform inversions using automatic differentiation for gradient calculations in a framework of recurrent neural networks. geophysics 86(6):795â€“810. https://â€‹doi.â€‹org/â€‹10.â€‹1190/â€‹geo20â€‹20-â€‹0542.1
wu y, lin y (2019) inversionnet: an efficient and accurate data-driven
full waveform inversion. ieee trans comput imaging 6:419â€“433
wu y, mcmechan gajg (2019) parametric convolutional neural network-domain full-waveform inversion. geophysics 84(6):881â€“896
xu k, mcmechan gajg (2014) 2d frequency-domain elastic fullwaveform inversion using time-domain modeling and a multisteplength gradient approach. geophysics 79(2):41â€“53
yang f, ma j (2019) deep-learning inversion: a next-generation seismic velocity model building method. geophysics 84(4):r583â€“
r599. https://â€‹doi.â€‹org/â€‹10.â€‹1190/â€‹geo20â€‹18-â€‹0249.1
zhang t, sun j, innanen ka and trad d (2021) numerical analysis
of a deep learning formulation of elastic full waveform inversion
with high order total variation regularization in different parameterization. arxiv preprint arxiv:2101.08924
zhu w, xu k, darve e, beroza g (2021) a general approach to seismic inversion with automatic differentiation: computers. comput
eosci 151:104751
zhu w, xu k, darve e, biondi b, beroza gcjg (2022) integrating
deep neural networks with full-waveform inversion: reparameterization, regularization, and uncertainty quantification. geophysics
87(1):93â€“109
springer nature or its licensor (e.g. a society or other partner) holds
exclusive rights to this article under a publishing agreement with the
author(s) or other rightsholder(s); author self-archiving of the accepted
manuscript version of this article is solely governed by the terms of
such publishing agreement and applicable law.

13



an empirical analysis of the influence of seismic data modeling for
estimating velocity models with fully convolutional networks
luan rios campos
manufacturing and technology integrated campus â€“ senai cimatec
salvador, bahia 41650-010, brazil
peterson nogueira
manufacturing and technology integrated campus â€“ senai cimatec
national institute of science and technology for geophysics of petroleum - ufba
salvador, bahia 41650-010, brazil
davidson moreira
manufacturing and technology integrated campus â€“ senai cimatec
salvador, bahia 41650-010, brazil
erick giovani sperandio nascimento
manufacturing and technology integrated campus â€“ senai cimatec
salvador, bahia 41650-010, brazil

abstract
seismic modeling is the process of simulating wave
propagations in a medium to represent underlying structures of
a subsurface area of the earth. this modeling is based on a set of
parameters that determine how the data is produced. recent
studies have demonstrated that deep learning methods can be
trained with seismic data to estimate velocity models that give a
representation of the subsurface where the seismic data was
generated. thus, an analysis is made on the impact that different
sets of parameters have on the estimation of velocity models by
a fully convolutional network (fcn). the experiments varied
the number of sources among four options (1, 10, 25 or 50 shots)
and used three different ranges of peak frequencies: 4, 8 and 16
hz. the results demonstrated that, although the number of
sources have more influence on the computational time needed
to train the fcn than the peak frequency, both changes have
significant impact on the quality of the estimation. the best
estimations were obtained with the experiment of 25 sources
with 4 hz and increasing the peak frequency to 8 hz improved
even more the results, especially regarding the fcnâ€™s loss
function.
keywords: deep learning, geophysics, velocity model
estimation, seismic data analysis, fully convolutional
networks.

1. introduction
the exploration of subsurfaces of the earth is an expensive
process. the first step is to place sources and receivers along a
certain area and then propagate waves from one equipment to be
recorded by the other. this process generates seismograms that

26

systemics, cybernetics and informatics

have much information of the structures underneath the region
where the acquisition was made. the understanding of these
seismic data may lead oil and gas companies to more assertively
drill an area that may contain, for example, petroleum. however,
two problems arise: the raw data by itself does not provide such
kind of detailed information and they are too big and complex to
be analyzed by humans.
in this scenario, computer simulations aim to replicate the
process of seismic data modeling so certain methods, such as the
reverse time migration (rtm) or full-waveform inversion
(fwi), can be used. these techniques try to alleviate the
aforementioned problems when a subsurface section is
investigated. the rtm is a method that outputs an image where
it is possible to identify the underlying structures of a subsurface,
whilst the fwi is an iterative method that tries to solve a
nonlinear inversion problem to output a high-resolution model
of velocities of the subsurface. the latter complements the
former, since its output is an input for the other, and both
methods require a signal representation of the subsurface in
order to operate, i.e., the seismic data. moreover, the fwi also
needs an initial velocity model in order to produce a new one
with higher resolution. offering an optimal initial velocity
model to the fwi can diminish the computational power
required to perform the method, increase its convergence rate by
avoiding local minima and produce a high-resolution velocity
model.
in the geophysics literature there are methods that help the
production of initial velocity models. authors such as [2] and [3]
have, respectively, studied the use of reflection tomography and
migration-based velocity analysis for such tasks. there are also
approaches that consider the use of global methods such as
genetic algorithms [4] and simulated annealing [5]. however,

volume 17 - number 4 - year 2019

issn: 1690-4524

the first two methods pose as a high time-consuming task and
the last two demand more computational resources as the
subsurface being analyzed increases in size, since they will
require a larger population and, consequently, more modeling
steps to carry the search on. more recently, researchers have
been experimenting the use of deep learning techniques to solve
geophysics problems [6], including seismic inversion [7] [8] [9]
[10]. as far as it is of our concern, the first use of a fully
convolutional network (fcn) for the velocity model estimation
problem was addressed by [11], on which the fcn is trained
with the seismic data of 1000 velocity models and tested with 20
examples of seismic data not seen during training. the seismic
data was generated with sources and receivers placed on both the
top and the bottom layers of the subsurface, which characterizes
a well log seismic acquisition. the work of [12] shows how the
same network used by [11] can be applied to a more
conventional seismic acquisition, where the sources and
receivers are positioned only on the top of the subsurface.
none of the works previously mentioned address the
consequences of changing the number of sources or the peak
frequency when training a deep learning method, except for [11],
which compares only the case of seismic data with 1 and 10
shots. this comparison led the authors to conclude that training
the fcn with 10 shots not only offers better results than using
only 1 shot, but also contributes to reduce overfitting.
the goal of this study is to empirically analyze how the seismic
data generated from synthesized velocity models can influence
the estimation of such models using a fcn. this can contribute
to the oil and gas industry by either demonstrating that deep
learning methods may not necessarily require a high number of
seismic shots, as it happens with other techniques, in order to be
able to estimate a comprehensible velocity model, which can
lead to reduce the expense to simulate, store and process nonsynthetic seismic data, or offering a technique that perhaps is
less sensitive to higher frequencies. moreover, a brief
comparison with the results of the experiments of different shots
made by [11] is carried in this work. however, it is important to
state that, because the scheme to generate the seismic data and
the velocity models used in this work differ from the ones used
by [11], their results are only discussed, not reproduced.
the experiments discussed here consider a finite-differences
approach for the seismic modeling and alterations on some of its
parameters, such as the number of sources and peak frequency,
with the former varying from one central shot to 10, 25 and 50
equally spaced shots and the latter varying from 4 to 8 and 16
hz. the seismic data is generated with basis on the same dataset
of velocity models independently of changes on the modeling
parameters, which consequently yields the same training and
testing dataset throughout the entire analysis with modifications
only on the resolution of the seismic data due to the differences
of parameters.
the analysis is twofold: to compare the graphical results of the
estimated velocity models of each experiment made as well as
their metrics obtained after the fcn is completely trained.
analyzing the metrics can offer a statistical and more precise
evaluation of the results obtained after training the neural
network, since only a graphical analysis can mislead the
interpretation of how changing the modeling parameters effects
on the neural network training.

issn: 1690-4524

this study is organized as follows: the following section
presents the mathematical and physical theory behind the
seismic modeling; section three briefly presents the importance
of velocity models; section four overviews fully convolutional
networks applications and theory; section five describes the
methodology and experiments; in section six a discussion of the
results obtained with the experiments is made; and section seven
concludes this work and points new directions of research based
on the results obtained.

2. seismic modeling
seismic modeling simulates the process of propagating waves
on a subsurface area. this is done so researches can advance on
processes that aid the understanding of subsurface areas prior to
going into expeditions to them. this section is dedicated to
briefly present some of the equations considering the modeling
via the acoustic wave equation.
1

ğœ•2 ğ‘ƒğ‘  (ğ‘¥,ğ‘¡)

ğ‘£(ğ‘¥)2

ğœ•ğ‘¡ 2

âˆ’ âˆ‡2 ğ‘ƒğ‘  (ğ‘¥, ğ‘¡) = ğ‘ (ğ‘¥, ğ‘¡)

(1)

the acoustic wave equation [13] [14] is described by eq. (1), of
which ğ‘¥ = (ğ‘¥â€², ğ‘§â€²) is the position on the subsurface for a 2d
representation, ğ‘£(ğ‘¥) is the velocity at a given position, ğ‘ƒğ‘  (ğ‘¥, ğ‘¡)
is the source wave field and ğ‘ (ğ‘¥, ğ‘¡) defines the seismic source of
the acoustic wave. eq. (2) denotes the second spatial derivatives,
i.e., the laplacian operator (ïƒ‘Â²), for the two-dimensional case
as:
âˆ‡2 =

ğœ•2
ğœ•ğ‘¥ â€²2

+

ğœ•2
ğœ•ğ‘§ â€²2

(2)

one way to perform the seismic modeling is with the finitedifferences method [14], which offers a simple and easy
implementation [15] through the taylor series, consequently
leading to a discretization of the equations. both eq. (1) and eq.
(2) can be expanded by a taylor series, but some conditions must
be met in order to avoid the numerical dispersion and instability
that may arise when discretizing a continuous-time equation
[15].
on one hand, eq. (3) [15] denotes the conditions to avoid the
numerical instability of a 2d model, on which ï„ğ‘¡ is the time
sampling interval, ğ‘šğ‘ğ‘¥(ğ‘£) is the maximum velocity of the
model, ï„ğ‘¥â€² and ï„ğ‘§â€² are the spatial sampling interval respectively
on the x and z axes.
1

Î´ğ‘¡ â‰¤

max(ğ‘£)âˆš

1
1
+
Î´ğ‘¥â€²2 Î´ğ‘§â€²2

(3)

on the other hand, eq. (4) [15] illustrates the conditions to avoid
the numerical dispersion problem of a bi-dimensional model:
ğ‘“ğ‘šğ‘ğ‘¥ is the maximum value of frequency allowed so the
dispersion does not occur considering a given model, i.e., its
maximum spatial sampling interval (ğ‘šğ‘ğ‘¥(ï„ğ‘¥â€², ï„ğ‘§â€²)) and its
minimum velocity (ğ‘šğ‘–ğ‘›(ğ’—)). the parameter ğ¹ is constant
according to the order used for the taylor series and it decreases
as the order increases.

systemics, cybernetics and informatics

ğ‘“ğ‘šğ‘ğ‘¥ =

1

min(ğ‘£)

ğ¹ max(Î´ğ‘¥ â€² ,Î´ğ‘§ â€² )

volume 17 - number 4 - year 2019

(4)

27

the peak frequency (ğ‘“ğ‘ğ‘’ğ‘ğ‘˜ ) is defined as approximately half of
the max frequency (eq. (5)) and represent the point of the
spectrum of frequency with maximum amplitude.
ğ‘“ğ‘ğ‘’ğ‘ğ‘˜ =

ğ‘“ğ‘šğ‘ğ‘¥

(5)

2.3

the information generated by the simulated wave propagation is
translated into the seismic data, which corresponds to the values
of transit time of the wave, the amplitudes and the phase of the
events. the seismic data varies and respects undulation
phenomes such as reflection, refraction and transmission

3. velocity model
a velocity model offers a representation of the structures present
in a subsurface based on the velocity of propagation of the waves
emitted from the sources and recorded by the receivers that are
placed on the surface when the seismic data is being modeled.
this is because the velocity of propagation directly depends on
the type of medium through which a wave travels. therefore, it
is possible to determine a structure, i.e., rock, water, salt body,
etc., according to its velocity.
as said before, there are different approaches to handle the initial
velocity model problem in the geophysics literature and an
optimal model can help when applying the full-waveform
inversion. these models, however, are said to be smoothed
(figure 1a) and, although they can display an initial guess of the
velocities of the subsurface, they lack details on its structural
composition. in that sense, estimated models that have their
structures clearly identified by their velocity values and are
highly correlated to their ground-truth (figure 1b) counterpart
are known as high resolution models.

a)

b)

figure 1 - an example of a) smoothed and b) ground-truth
velocity models

4. fully convolutional networks
convolutional neural networks (cnn) were firstly introduced
by [16] as an option for recognizing handwritten digits from the
u.s. postal service. later it was proved that cnns can handle,
besides images, speech and time-series problems [17]. in the
recent years, deep learning has gained even more importance,
especially after the imagetnet contest in 2012 and the
development of alexnet [18]. since then, different proposals of
deep learning methods with cnns have been made, including
the fully convolutional networks (fcns).
the first proposition of use of an fcn was for handling semantic
segmentation problems [19], which is the task of segmenting an

28

systemics, cybernetics and informatics

image into parts and classifying those parts into one of the
predetermined classes.
eq. (6) demonstrates the operation of the basic components of
cnns as [19] point out. in this case, ğ‘¥ğ‘–ğ‘— is the data vector, ğ‘¦ğ‘–ğ‘— is
the next layer, ğ‘˜is the size of the kernel, ğ‘ the subsampling factor
and ğ‘“ğ‘˜ğ‘  defines the type of the layer (convolution, pooling or
activation function). therefore, [19] nominate cnns that
contain only layers ruled by eq. (6) as fully convolutional or
deep filter, since, differently from conventional approaches that
use cnns, the fcn does not contain fully connected (dense)
layers, producing with its operations a nonlinear filter instead of
a nonlinear function and reducing the number of parameters,
computational time and dependency of the size of the image.
ğ‘¦ğ‘–ğ‘— = ğ‘“ğ‘˜ğ‘  ({ğ‘¥ğ‘ ğ‘–+Î´i,sj+Î´j }0â‰¤Î´ğ‘–,Î´ğ‘—â‰¤ğ‘˜ )

(6)

5. methodology and experiments
in this section we describe the methodology and experiments of
this work. firstly, the synthetic velocity models are presented
with details regarding their construction such as number of
layers, minimum and maximum velocities and other
characteristics. then, the description moves on how the seismic
data is generated and how the fcn is configured to handle it as
inputs and estimate velocity models.
the velocity models and seismic data are both synthetic and they
are built in different occasions. we first generate 1020 random
velocity models and then we apply the finite-differences seismic
modeling on each one of the recently-generated velocity models
to create its corresponding seismic data.
the subsurface area being represented by the synthetic velocity
models is a marine region of 3000 m in length by 3000 m in
depth. the models are two-dimensional grids of 150 samples on
both x (ğ‘›ğ‘¥â€²) and z (ğ‘›ğ‘§â€²) axes and their number of layers vary
from 8 to 12 layers, of which the first layer represents a water
blade of 100 m deep and velocity of 1500 m/s. subsequent layers
have their depth randomly defined and their velocity is
incremented (ğ‘‰ğ‘–ğ‘›ğ‘ğ‘Ÿ ) in a crescent order, from the first layer
velocity onwards, depending on how many layers (ğ‘›) the model
has and on its maximum (ğ‘‰ğ‘šğ‘ğ‘¥ = 3500 ğ‘š/ğ‘ ) and minimum
(ğ‘‰ğ‘šğ‘–ğ‘› = 1500 ğ‘š/ğ‘ ) velocities (eq. 7), e.g., if the model has 12
layers, then the velocity will be incremented in 166,66 m/s at
each layer. furthermore, the models can have their layers
inclined, undulated or containing fault structures. figure 2
displays an example of such model.
ğ‘‰

ğ‘‰ğ‘–ğ‘›ğ‘ğ‘Ÿ = ğ‘šğ‘ğ‘¥

âˆ’ğ‘‰ğ‘šğ‘–ğ‘›
ğ‘›

(7)

the seismic modeling is conducted on two fronts and it
considers an arrangement of sources and receives as used by
[12], i.e., they are simulated as they were placed on the top of
the subsurface. the first front is to make different modeling
changing only the number of sources and fixing a low frequency
of 4 hz. since the sources are positioned on points (ğ‘¥â€², ğ‘§â€²) of the
subsurface, by decreasing their quantity we might inflict on the
acquisition of information belonging to certain regions of the
given subsurface. hence, the goal is to analyze how the changes
on the number of sources will affect and how much of the
velocity model the fcn can estimate.

volume 17 - number 4 - year 2019

issn: 1690-4524

the experiments of the second front consider modifications of
the frequency using three different bands: 4 hz, 8 hz and 16 hz.
as said before, the frequency is important to avoid the numerical
dispersion that may occur when calculating the acoustic wave
equation through the taylor series. the frequency of 16 hz is
the frequency of peak obtained from eq. (5) after calculating the
maximum frequency needed to avoid such dispersion when
applying the parameters depicted in table 1 in eq. (4) using ğ¹ =
2 as we considered a 32-order finite-differences. it is safe to say
that any value below this threshold does not disperse the wave
equation modeling, whereas frequencies above it disperse.

figure 2 - a synthetic velocity model containing 10 layers,
undulations, inclinations and fault structures
the frequency influences on how much of detail of the structures
the modeling will be able to capture. it is expected that by
lowering the frequency, the seismic data becomes smoother and
consequently the non-linearity of the problem is decreased.
hence, the values picked for the experiments represent low-band
(4 hz), medium-band (8 hz) and high-band (16 hz) frequencies
and aim to aid the understanding of how different bands can
determine the level of details of the estimated models.

gradually reduces the size of the image at the same time it
determines what are the features of the input data. the decoder
also has convolution layers, but the max-pooling are replaced by
up sampling layers. this results in an increasing of the image
size, to match the original image, and consequent localization of
the features identified during encoding.
this study relies on the same u-net proposed by [11], having
the same quantity of layers and the same number of filters on
each convolutional layer. however, two major changes were
made in order to improve the results. firstly, the stochastic
gradient descent (sgd) optimization function was replaced by
adamax [21], which computes adaptive learning rates for each
parameter and offers a more robust solution than the sgdâ€™s
fixed learning rate when training a neural network model.
secondly, the rectified linear unit (relu) activation function
was replaced by the parametric rectified linear unit (prelu).
by using the relu activation function one can avoid the
vanishing gradient problem that might occur in neural networks
trained with gradient-based optimizers, such as adamax.
however, such function has another issue called the dead relu,
which might compromise a netwok from learning since the
output of some of its neurons can be zero due to this functionâ€™s
nature (figure 3a). the prelu is an alternative to avoid this
issue as it learns to parameterize the negative inputs of the
neurons instead of assigning zeros to them as relu does (figure
3). a more detailed study demonstrating how these changes on
the activation function and optimizer leverage better velocity
model estimations can be seen in the work of [22].

table 1 - fixed parameters considered when modeling the
synthetic velocity models
a)
parameter

value

ğ‘›ğ‘¥â€²

150 samples

Î´ğ‘¥â€²

20 m

ğ‘›ğ‘§â€²

150 samples

Î´ğ‘§â€²

20 m

ğ‘›ğ‘¡

1500 samples

Î´ğ‘¡

0.002 s

b)

figure 3 - plot showing how the a) relu and b) prelu
activation functions work
the fcn is trained for 200 epochs with a batch size of 2 on 80%
of the total of seismic data generated, saving 20% for the testing
stage. the testing dataset is a portion of the original dataset
unknown to the fcn, i.e., that has never been presented to it
during the training phase, so it can offer an unbiased analysis of
the modelâ€™s performance. the batch size is small due to the size
of the input and, although it could increase as the number of
sources used during modeling decreases, since less sources
means a reduction of size of the seismic data, it was kept
unchanged throughout all experiments.

the fcn implementation takes the seismic data previously
described as input and tries to estimate the velocity model
corresponding to the input by minimizing the error between the
estimated model and the ground-truth that generated the seismic
data.

the evaluation of the fcn is made based on five different
metrics with respect to the testing dataset: mean squared error
(mse), which is also the loss function, mean absolute error
(mae), coefficient of determination (rÂ²), pearsonâ€™s coefficient
of correlation (r) and factor of two (fac2).

the work of [11] proposes the use of a u-net [20] to perform
the inversion of a seismic data into a velocity model. this fcn
consists of two parts: an encoder and a decoder. the encoder is
composed of convolution and max-pooling layers, which

in this context, the mse (eq. (8)) measures how far an estimated
model is from its respective ground-truth model. the bigger the

issn: 1690-4524

systemics, cybernetics and informatics

volume 17 - number 4 - year 2019

29

differences between one output and its corresponding target, the
greater the penalization and, consequently, the associated error.
ğ‘€ğ‘†ğ¸ =

1
ğ‘

âˆ‘ğ‘
Ì‚ğ‘˜ )2
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ ğ‘¦

(8)

the mae (eq. (9)) have lower values when compared to mseâ€™s
and indicates how much the difference of velocities between an
estimated and its ground-truth model vary, i.e., if the mae is of,
say, 100, it means the output have 100 m/s of average error
compared to the target.
ğ‘€ğ´ğ¸ =

1
ğ‘

âˆ‘ğ‘
Ì‚ğ‘˜ |
ğ‘˜=1|ğ‘¦ğ‘˜ âˆ’ ğ‘¦

the graphical results of the estimation of one ground-truth
model from the testing dataset can be seen in figure 3. the
ground-truth model contains undulated and inclined layers, and
a simple fault structure that is identified by the yellow ellipsis in
figure 4a. analyzing only the images leads to pointing out that
figure 4c, figure 4f and figure 4g obtained the best
representation of the ground-truth model because they contain
not only well-positioned layers, with identification of their
undulation and inclination, and a high precision of the velocities
on each layer, as the other estimations do, but also a fair
depiction of the fault structures.

(9)

the coefficient of determination (eq. (10)) indicates how better
the estimation is when compared to a baseline model - either ğ‘¦
or ğ‘¦Ì‚ variables of eq. (10).
ğ‘…2 =

Ì…)(ğ‘¦Ì‚ğ‘˜ âˆ’ğ‘¦Ì‚)]
[âˆ‘ğ‘
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ğ‘¦

2

(10)

2

2
âˆ‘ğ‘
Ì‚ ğ‘˜ âˆ’ğ‘¦Ì‚) âˆ‘ğ‘
ğ‘˜=1(ğ‘¦
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ğ‘¦)

a)

the pearsonâ€™s coefficient (eq. (11)) quantifies the linear
relationship between an estimated model and its ground-truth
counterpart, of which the value of -1 means opposite
correlations, 0 means no correlation at all and 1 means total
correlation
ğ‘Ÿğ‘¦ğ‘¦Ì‚ =

âˆ‘ğ‘
Ì‚ğ‘˜ âˆ’ğ‘¦Ì‚)
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ğ‘¦)(ğ‘¦
2

2 âˆšâˆ‘ğ‘ (ğ‘¦
âˆšâˆ‘ğ‘
Ì‚)
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ğ‘¦)
ğ‘˜=1 Ì‚ ğ‘˜ âˆ’ğ‘¦

(11)

the factor of two (eq. (12)) determines how much of the
estimation can be considered an outlier.
ğ‘“ğ‘ğ‘2 = 0.5 â‰¤

ğ‘¦Ì‚ğ‘˜
ğ‘¦ğ‘˜

â‰¤2

b)

c)

d)

e)

f)

g)

(12)

the parameters from eq. (8) to eq. (12) are as follows: ğ‘ is the
size of the velocity model grid, ğ‘¦ğ‘˜ is the ğ‘˜ğ‘¡â„ velocity of the
ground-truth model (target), ğ‘¦Ì‚ğ‘˜ is the ğ‘˜ ğ‘¡â„ velocity of the fcnâ€™s
model (estimated output), ğ‘¦ is the mean of velocities of the target
output and ğ‘¦Ì‚ is the mean of velocities of the estimated output
6. results
an analysis and comparison of the experiments discussed
previously are carried along this section. both the experiments
with variations on the number of shots and peak frequency are
discussed and compared amongst them. it is important to restate
that the experiments with adjustments of the number of shots
were made with a peak frequency of 4 hz, whereas when
changes on the peak frequency occurs the number of shots is
fixed at 25. the comparison is made both graphically and with
regards to the metrics presented beforehand that are calculated
after the deep learning model is fully trained.
the statistical comparison is to give a more reliable analysis,
since considering only the estimated image of the velocity model
can mislead the interpretation of the results. in this case, the goal
is to minimize both the loss (mse) and mae metrics at the same
time it maximizes ğ‘…Â², ğ‘Ÿ and ğ‘“ğ‘ğ‘2 to values as close to 1 as
possible. besides the metrics, the time (in hours) taken to train
the model also composes the analysis.

30

systemics, cybernetics and informatics

figure 4 - a) ground-truth velocity model and graphical results
obtained with the experiments of b) 1 source, c) 10 sources, d)
25 sources and e) 50 sources all modeled with ğ‘“ğ‘ğ‘’ğ‘ğ‘˜ = 4 hz
and f) 8 hz and g) 16 hz both having 25 shots
on one hand, it is not safe to infer so straightforwardly that these
representations are the best because this velocity model
represents only one example of the entire testing dataset. this
may indeed be a case where the fcn models estimated an
optimal velocity model from the seismic data they were trained
by, but there may also exist cases that the estimations greatly

volume 17 - number 4 - year 2019

issn: 1690-4524

differ from their ground-truth models. on the other hand, this
analysis indeed validates the use of fcns to produce velocity
models from unknown seismic data.
once the graphical investigation of many examples is imprecise
and impractical, a quantitative evaluation of the statistical
indicators belonging to each one of the experiments is
conducted. these metrics are measured after the training phase
using the entire testing dataset. table 2 displays the metrics, the
corresponding time it took for the models to be trained and the
peak frequency for each experiment.
before venturing into the comparison of the metrics, an
association between the experiments and the computational time
is conducted. it is possible to see from table 2 that as the number
of shots increases, so it increases the computational time taken
to train the fcn. this happens because the number of shots have
a direct influence on the size of the seismic data as additional
shots mean adding matrices of size ğ‘›ğ‘¡ Ã— ğ‘›ğ‘¥â€² to the seismic data.
on the other hand, the peak frequency does not seem to have
much importance to the computational time. considering the
experiment of 25 shots in table 2, since the modeling had a peak
frequency of 4 hz, and comparing it with the time of the
experiments of 8 hz and 16 hz, as both have 25 shots, there is
no clear relation of computational time and higher or lower
frequencies. in fact, the result that achieved the lowest time is
the one with the highest frequency and the experiment with
medium frequency took the longest to train.
table 2 - results of the evaluation metrics and the time for
training (in hours) for each one of the experiments with
changes on the number of sources (shots) with fixed peak
frequency of 4 hz and on the frequency with a fixed 25 number
of shots
1 shot

10
shots

25
shots

25
shots

25
shots

50
shots

ğ’‡ğ’‘ğ’†ğ’‚ğ’Œ
(hz)

4

4

4

8

16

4

time
(h)

7.19

7.43

8.10

8.14

8.09

9.07

mse

14172

7313

6837

6126

7578

7207

mae

75.39

45.41

44.19

46.79

54.69

49.72

rÂ²

0.954

0.975

0.977

0.980

0.974

0.976

r

0.983

0.989

0.990

0.991

0.990

0.990

fac2

0.999

1.0

1.0

0.999

1.0

0.999

the evaluation metrics of each experiment, in general,
demonstrated close values, but it is possible to notice that
training the fcn with seismic data that have more shots does not
necessarily indicate a better estimation. even though the
experiment with 50 shots demonstrates valuable results, i.e., it
accomplished values close to 1 for the r, r2 and fac2, and
relatively low values for mae and mse, other experiments were
able to surpass it. in this case, both experiments with 10 and 25

issn: 1690-4524

shots obtained better values in all metrics, of which the latter
bested the former. moreover, the experiment with 25 shots could
be further improved when the modeling was made with 8 hz,
reaching the lowest value with the loss function (mse) and the
highest with ğ‘…Â² and ğ‘Ÿ metrics for all experiments. this, however,
happened at the expense of slightly decreasing the mae and
ğ‘“ğ‘ğ‘2 metrics to values below the experiment of 25 shots and 4
hz.
on the other hand, neither reducing much the number of shots
nor increasing even more the peak frequency mean improvement
on the estimation either. the worst results belong to the
experiment with the central shot. in this case, the values of the
metrics ğ‘…Â², ğ‘Ÿ and ğ‘“ğ‘ğ‘2, though show little differences from the
same metrics of the other experiments, were the lowest and the
mse and mae were the highest amongst all. moreover, the
experiment with 16 hz resulted in worst metrics than the one
with 50 shots.
although having the worst metrics, the fcn successfully
inverted a seismogram of one shot into a velocity model. this
possibly happened due to the size of the subsurface and the
velocity model, which are considered small from a geophysics
perspective, but this cannot be confirmed to happen as the
subsurface becomes larger considering only the analysis made
in this work.
hence, considering the extent of the experiments conducted in
this work, it is possible to conclude that the fcn not only can
produce velocity models from unknown seismic data, but it can
also deliver high-resolution models. furthermore, the
parameters used to generate the seismic data, combined with the
size of the subsurface area and the size of its velocity model
representation, play an important role in determining how high
the modelâ€™s resolution is going to be.

7. conclusions
this work demonstrated how changing the number of sources
and peak frequency of the seismic modeling can affect the
training and evaluation of an fcn model that takes seismic data
as input to estimate 2d velocity models.
the experiments firstly fixed the peak frequency at 4 hz and
varied the number of shots amongst one central shot, 10, 25 and
50 shots and then fixed 25 shots and varied the peak frequency
to 8 and 16 hz. the results showed that the best metrics for the
fcn were obtained with the experiments of 10 and 25 sources
and increasing the peak frequency from 4 to 8 hz improved even
more the estimation, especially regarding the fcnâ€™s loss. when
the peak frequency was increased once again, the fcn reached
lower metrics than the experiment with 50 shots. nevertheless,
the worst results amongst all were obtained with the seismic data
produced by a single central shot. these results partially
contradict the affirmation made by [11], since the results were
indeed improved when increasing the number of shots from 1 to
10 and from 10 to 25 but they worsened when considering 50
shots. additionally, there is no clear evidence whether the
number of shots influences on the model overfitting or not. this
might have happened due to the size of the dataset used in their
work.
initial conclusions for the experiments addressed in this work
indicate that, depending on the size of the subsurface, training

systemics, cybernetics and informatics

volume 17 - number 4 - year 2019

31

the fcn with seismic data that have few shots is enough to
estimate a velocity model. however, as the size of the model and
subsurface increases, more shots may give a better
representation of the area. furthermore, the results imply that the
fcn is, up to a certain point, less sensitive to higher peak
frequencies as the results improved when the modeling was
changed from 4 to 8 hz, but they worsened when 16 hz was
considered.
in general, the results demonstrated to be valuable, since they
show the possibility of training deep learning models with
seismograms containing few shots and high frequencies to
estimate optimal velocity models.
further studies point to the need of analyzing whether few shots
are indeed enough to estimate velocity models of larger and
more complex subsurfaces. furthermore, improvements on the
training stage, such as mixing the dataset with low, medium and
high frequencies or substituting the max-pooling layers for
convolutional layers, can be made, and other deep learning
methods, such as generative adversarial networks (gan), may
be studied to determine whether they behave differently from the
fcn for the seismic inversion problem.

8. references
[1] carcione, jose m.; herman, gÃ©rard c.; ten
kroode, a. p. e. seismic modeling. geophysics, vol. 67,
no. 4, 2002, pp. 1304-1325.
[2] stork, christof. reflection tomography in the
postmigrated domain. geophysics, vol. 57, no. 5, 1992, pp.
680-692.
[3] al-yahya, kamal. velocity analysis by iterative profile
migration. geophysics, vol. 54, no. 6, 1989, pp. 718-729.
[4] sajeva, angelo et al. estimation of acoustic macro models
using a genetic full-waveform inversion: applications to the
marmousi model genetic fwi for acoustic macro models.
geophysics, vol. 81, no. 4, 2016, pp. r173-r184.
[5] datta, debanjan; sen, mrinal k. estimating a starting
model for full-waveform inversion using a global
optimization method. geophysics, vol. 81, no. 4, 2016, pp.
r211-r223.
[6] wang, wenlong; yang, fangshu; ma, jianwei
automatic salt detection with machine learning. in: 80th
eage conference and exhibition 2018. european
association of geoscientists and engineers, 2018.
[7] rÃ¶th, gunter; tarantola, albert. neural networks and
inversion of seismic data. journal of geophysical
research: solid earth, vol. 99, no. b4, 1994, pp. 67536768.
[8] lewis, winston; vigh, denes. deep learning prior models
from seismic images for full-waveform inversion. in: seg
technical program expanded abstracts 2017. society of
exploration geophysicists, 2017, pp. 1512-1517.
[9] araya-polo, mauricio et al. deep-learning tomography.
the leading edge, vol. 37, no. 1, 2018, pp. 58-66.
[10] wu, yue; lin, youzuo; zhou, zheng. inversionnet:
accurate and efficient seismic waveform inversion with
convolutional neural networks. in: seg technical
program expanded abstracts 2018. society of
exploration geophysicists, 2018, pp. 2096-2100.
[11] wang, wenlong; yang, fangshu; ma, jianwei.
velocity model building with a modified fully convolutional
network. in: seg technical program expanded

32

systemics, cybernetics and informatics

abstracts 2018. society of exploration geophysicists,
2018, pp. 2086-2090.
[12] campos, luan r.; nogueira, peterson; nascimento, erick.
estimating initial velocity models for the fwi using deep
learning. in: proceedings of the 16th international
congress of the brazilian geophysical society. brazilian
geophysical society.
[13] alford, r. m.; kelly, k. r.; boore, d. mt.
accuracy of finite-difference modeling of the acoustic wave
equation. geophysics, vol. 39, no. 6, 1974, pp. 834-842.
[14] baysal, edip; kosloff, dan d.; sherwood, john
wc. reverse time migration. geophysics, vol. 48, no. 11,
1983, pp. 1514-1524.
[15] dos santos, a. w. g. waveform inversion applied to
the analysis of seismic velocities using a multi-scale
approach. masterâ€™s thesis, universidade federal da bahia,
2013
[16] lecun, yann et al. handwritten digit recognition with a
back-propagation network. in: advances in neural
information processing systems, 1990, pp. 396-404.
[17] lecun, yann et al. convolutional networks for images,
speech, and time series. the handbook of brain theory and
neural networks, vol. 3361, no. 10, 1995, pp. 1995.
[18] krizhevsky, alex; sutskever, ilya; hinton,
geoffrey e. imagenet classification with deep convolutional
neural networks. in: advances in neural information
processing systems, 2012, pp. 1097-1105.
[19] long, jonathan; shelhamer, evan; darrell,
trevor. fully convolutional networks for semantic
segmentation. in: proceedings of the ieee conference on
computer vision and pattern recognition, 2015, pp. 34313440.
[20] ronneberger, olaf; fischer, philipp; brox,
thomas. u-net: convolutional networks for biomedical
image segmentation. in: international conference on
medical image computing and computer-assisted
intervention. springer, cham, 2015, pp. 234-241.
[21] kingma, diederik; ba, jimmy. adam: a method for
stochastic optimization. international conference on
learning representations, 2014.
[22] campos, luan r., nogueira, peterson, & nascimento,
erick. tuning a fully convolutional network for velocity
model estimation. in: offshore technology conference
brasil. offshore technology conference.

volume 17 - number 4 - year 2019

issn: 1690-4524



improving seismic wave simulation
and inversion using deep learning
lei huang(b) , edward clee, and nishath ranasinghe
department of computer science, prairie view a&m university,
prairie view, tx 77446, usa
{lhuang,niranasinghe}@pvamu.edu, t clee@acm.org

abstract. accurate simulation of wave motion for the modeling and
inversion of seismic wave propagation is a classical high-performance
computing (hpc) application using the ï¬nite diï¬€erence, the ï¬nite element methods and spectral element methods to solve the wave equations
numerically. the paper presents a new method to improve the performance of the seismic wave simulation and inversion by integrating the
deep learning software platform and deep learning models with the hpc
application. the paper has three contributions: 1) instead of using traditional hpc software, the authors implement the numerical solutions for
the wave equation employing recently developed tensor processing capabilities widely used in the deep learning software platform of pytorch. by
using pytorch, the classical hpc application is reformulated as a deep
learning recurrent neural network (rnn) framework; 2) the authors customize the automatic diï¬€erentiation of pytorch to integrate the adjoint
state method for an eï¬ƒcient gradient calculation; 3) the authors build a
deep learning model to reduce the physical model dimensions to improve
the accuracy and performance of seismic inversion. the authors use the
automatic diï¬€erentiation functionality and a variety of optimizers provided by pytorch to enhance the performance of the classical hpc application. additionally, methods developed in the paper can be extended
into other physics-based scientiï¬c computing applications such as computational ï¬‚uid dynamics, medical imaging, nondestructive testing, as
well as the propagation of electromagnetic waves in the earth.
keywords: machine learning Â· inverse problem Â· wave propagation

1

introduction

physical simulation and inversion are classical scientiï¬c computing applications
to discover the physical phenomenon and reveal the underlying properties. the
simulation solves the partial diï¬€erential equations (pde) that governs the physical phenomenon using numerical approximation methods, while the inversion
applies the gradient-based optimizations to ï¬nd the underlying properties by
minimizing the observed data and the simulated results. the entire process takes
c springer nature switzerland ag 2020

j. nichols et al. (eds.): smc 2020, ccis 1315, pp. 3â€“19, 2020.
https://doi.org/10.1007/978-3-030-63393-6_1

4

l. huang et al.

signiï¬cant computing resources to achieve the satisï¬ed accuracy. however, the
inverse problem is naturally challenging since it is ill-posed and nonlinear for
most cases.
recent advances in high-performance tensor processing hardware and software are providing new opportunities for accelerated linear algebra calculations
as used in machine learning, especially for deep learning neural networks, that
contributes signiï¬cantly to the success of data science. such calculations are
also at the heart of many simulations of physical systems such as wave propagation. the use of tensor processing in neural networks, with its need for backpropagation through multi-layered networks, has led to capabilities for automatic
diï¬€erentiation [1] for gradient calculations in deep learning software.
motivations: the motivations of the work have twofold. the ï¬rst one is to
understand the new deep learning software package such as pytorch and tensorflow, and their capacity of solving a scientiï¬c computational problem. especially, we are interested in how to model the traditional partial diï¬€erential equations (pdes) used in the scientiï¬c computational problem with a deep learning
model. the other is to study how to integrate the machine learning models that
are data-driven into the scientiï¬c computational model that are physics-driven.
the diï¬€erentiable programming has the potential to smoothly integrate them
together with a global optimization. the authors believe the study will lead to
more interesting research ï¬ndings in the topic of scientiï¬c machine learning
(sciml) and to ï¬nd an eï¬ƒcient way to combine the power of these two diï¬€erent
methods to facilitate scientiï¬c discovery.
in this paper, we study how to use the tensor-based machine learning software to formulate the physical simulation and to compute the gradients for
optimizations to solve the inverse problem. we use the seismic wave propagation simulation and the full wave inversion (fwi) as the physical case study.
we have adapted the techniques of others in this area of wave propagation [2,3]
to demonstrate how direct ï¬nite diï¬€erence integration can be implemented via a
deep learning software platform, allowing the gradients calculated by automatic
diï¬€erentiation to be used for the fwi of seismic reï¬‚ection survey data as an
augmentation to the well-known pysit [4] seismic research platform.
we summarize the paperâ€™s contributions in the following:
i) we formulate the pde solver in the seismic forward model using the recurrent neural network (rnn) implemented with the deep learning software
package pytorch, which allows us to take advantages of the tensor processing software and its accelerator implementation.
ii) we apply the automatic diï¬€erentiation implemented in pytorch to solve the
seismic inverse problem to uncover the earthâ€™s interior physical properties.
iii) we improve the automatic diï¬€erentiation eï¬ƒciency by creating a hybrid
back propagation method with the adjoint-state method to calculate the
gradients.
iv) we implement an autoencoder network to reduce the dimensions of the
inverted parameters to argument the convergence process and get more
accurate results for the ill-posed problem.

seismic simulation, inversion and deep learning

2

wave equations and rnn

2.1

wave equations

5

the wave motion is governed by physical rules that can be expressed in the
following partial diï¬€erential equation (pde) (1) and the boundary conditions
(2) and (3). we use the 1d scalar wave equation for simplicity purpose in this
paper:
1

âˆ‚ 2 u(x, t) âˆ‚ 2 u(x, t)
âˆ’
= f (x, t)
âˆ‚t2
âˆ‚x2
1 âˆ‚u(0, t) âˆ‚u(0, t)
âˆ’
=0
c(0) âˆ‚t
âˆ‚x
1 âˆ‚u(1, t) âˆ‚u(1, t)
âˆ’
=0
c(1) âˆ‚t
âˆ‚x

c2 (x)

(1)
(2)
(3)

where c(x) is the spatial velocity distribution, u(x, t) is the wave ï¬eld distribution
in space and time, and f (x, t) is the energy source distribution in space and time.
the eq. (1) can be solved numerically using a ï¬nite diï¬€erence approximation:
u(x âˆ’ Î´x, t) âˆ’ 2u(x, t), +u(x + Î´x, t)
Î´x2
1 u(x, t âˆ’ Î´t) âˆ’ 2u(x, t) + u(x, t + Î´t)
+ 2
.
c
Î´t2

f (x, t) = âˆ’

(4)

after factoring, the eq. (4) can be expressed as
u(x, t + Î´t) = f (x, t)c2 Î´t2 + (2u(x, t) âˆ’ u(x, t âˆ’ Î´t))
+c2

Î´t2
(u(x âˆ’ Î´x, t) âˆ’ 2u(x, t) + u(x + Î´x, t))
Î´x2

(5)

which shows that the next wave ï¬eld in time u(x, t+Î´t) can be calculated based
on the current and prior wave ï¬elds, as well as spatial neighbors in the current
wave ï¬eld. the wave motion simulation follows the time sequence to produce
the next state based on the prior ones, which is similar to the recurrent neural
network (rnn) in deep learning to model a time sequence function.
2.2

recurrent neural network

recurrent neural network (rnn) is used to model the pattern in a sequence of
data, mostly in time sequence. in recent years, rnn and its variants have been
applied successfully to problems such as speech recognition, machine translation,
and text-to-speech rendering. it has an internal cell that repeatedly processes an
input, carries a hidden state, and produces an output at each step. the rnn cell

6

l. huang et al.

can be designed to be simple or complex to model a problem with a forgettable
memory mechanism (long short-term memory (lstm) [5]) or/and a gating
mechanism (gated recurrent unit (gru) [6]).

fig. 1. a simple rnn model (a) with feedback loop, and (b) with loop unfolded

figure 1(a) shows a typical rnn structure that repeatedly takes an input,
updates its hidden state, and produces an output at every step. the rnn model
can be unfolded as shown in fig. 1(b) that learns the recurrence relationship from
a sequence of data. the hidden state hi remembers the prior state of the process
and is updated at each step. the hidden state enables rnn to learn the temporal
relationships among the inputs since most of the time sequence data do contain
temporal patterns. lstm allows rnn to forget long-term relationships built up
in the hidden state and emphasizes the short-term relationships, which can be
useful for many cases.
a simple rnn can be expressed in the eq. (6):
ht = Ïƒh (wh xt + wh htâˆ’1 + bh )
yh = Ïƒy (wy ht + by )

(6)

where xt is the input, ht is the hidden state, w is the weights, b is the bias, and
Ïƒ is the activation function.
looking back to the eq. (5), there are two hidden states u(x, t) and u(x, t âˆ’
Î´t) if we can restructure the ï¬nite diï¬€erence method using an rnn. there is
also a spatial stencil relationship of neighboring velocity distribution. we deï¬ne
a new function f with input of f (x, t), two hidden states u(x, t) and u(x, t âˆ’ 1),
and the constant velocity distribution c:

seismic simulation, inversion and deep learning

7

f (f (x, t), u(x, t), u(x, t âˆ’ 1), c)
= f (x, t)c2 Î´t2 + (2u(x, t) âˆ’ u(x, t âˆ’ 1))
Î´t2
+ c2
(u(x âˆ’ 1, t) âˆ’ 2u(x, t) + u(x + 1, t)).
Î´x2

(7)

then, the eq. (5) can be restructured as an rnn format:
ht+1 = Ïƒ(f (f (t), h(t), h(t âˆ’ 1), c))
yt+1 = p (ht+1 )

(8)

where p is the projection function to get the sample of a trace from a receiver.
the eq. (8) is then a non-learnable, deterministic physical solution represented
as the deep learning rnn model. figure 2 shows the rnn model we designed
that solves the wave equation with four inputs f (x, t), h(t), h(t âˆ’ 1), and c, the
velocity distribution which is constant in the equation. the output yt is the trace
sample of a receiver at each time step.

fig. 2. a rnn model for wave equation

2.3

pytorch rnn implementation

the wave equation rnn model we designed in fig. 2 enables us to utilize the
deep learning software platform to solve the wave equations. the beneï¬ts of using
a deep learning model to represent an hpc application include: (1) we will be
able to leverage the hpc implementation of the deep learning model exploiting
the advantages of gpus/multicores and vectorization for better performance;
(2) have an automatic gradients calculation using the built-in automatic diï¬€erentiation package in deep learning; (3) utilize the variety of built-in optimizers
to apply the gradients to ï¬nd the global/local optimums; (4) use the data- and

8

l. huang et al.

model- parallelism framework implemented in deep learning package to run the
application on a hpc cluster.
the following shows a code snippet of our rnn-similar implementation of wave equation using pytorch. there are two classes derived from
torch.nn.module for rnn cell and rnn driver respectively. we called them
wave pgnncell and wave propagator in our code. the wave pgnncell implemented a cell function in rnn that computes the waveï¬eld at a time step. the
wave propagator iterates over all time steps and takes the ricker source waveform sample as the input at each time step. the hidden state (self.h) contains
the next and current waveï¬elds, which are fed into the cell for the next iteration.
the trace is collected by projecting the current waveï¬eld based on the receiver
location. the program returns the simulated waveï¬eld and sampled trace at the
end.
c l a s s wave pgnncell ( t o r c h . nn . module ) :
def f o r w a r d ( s e l f , h, s r c ) :
uc , up = [ h [ 0 ] , h [ 1 ] ]
...
return [ un , uc ]
c l a s s wave propagator ( t o r c h . nn . module ) :
s e l f . c e l l = wave pgnncell (c, c o n f i g )
def f o r w a r d ( s e l f ) :
us = [ ]
# l i s t of output wavefields
traces = [ ]
rcv = s e l f . rcvrs
f o r i t in r a n g e ( s e l f . nt ) :
s e l f . h = s e l f . c e l l . f o r w a r d ( s e l f . h, s e l f . ws [ i t ] )
us . append ( s e l f .h [ 0 ] . d e t a c h ( ) . numpy ( ) )
# e x t r a c t w a v e f i e l d sample a t each r e c e i v e r
samps = r c v . sample ( s e l f . h [ 0 ] . c l o n e ( ) )
t r a c e s . append ( samps )
t r c = t o r c h . s t a c k ( t r a c e s , dim=1)
return us , t r c

2.4

seismic wave simulation

for seismic wave simulation, we use our rnn model to simulate the acoustic
wave propagation for the scalar wave equation. we create a â€œtrueâ€ synthetic
model and an initial model, which can be a smoothed version of the true model
or some other separately chosen function. we use the ricker wavelet as a waveform for one or more energy sources (shots) and create an array of receivers for
collecting traces. we assume the constant density in these models.
as we stated earlier, one beneï¬t of using deep learning software is to take
advantage of its multiple cpus and gpus implementation. we only need to specify which devices the code will operate on and deï¬ne tensors to these devices. all
remaining device-speciï¬c implementation and optimizations are done internally
by pytorch. we do not need to use cuda or openacc to port the code to
these devices.
another beneï¬t is to use the data-parallelism implemented in pytorch. we
can parallelize the code by the number of the sources/shots to run the code on
multiple gpus and distributed clusters.

seismic simulation, inversion and deep learning

9

in our implementation, we use pytorch1 1.5 to build the rnn model.
pytorch is an open source machine learning framework developed by facebook by merging torch and caï¬€e2, which supports a variety of hardware platforms including multiple cpus, gpus, distributed systems, and mobile devices.
besides the machine learning and deep learning functions, one unique feature
of pytorch is that it contains a just-in-time compiler to optimize the code if it
complies with torchscript, which is a subset of python. it has a built-in automatic diï¬€erentiation package for calculating derivatives, as well as a distributed
training module to train a model on a hpc cluster. pytorch has both python
and c++ frontends.
figure 3 shows a 1d seismic velocity inversion case applying our physicsruled rnn implementation. the fig. 3(a) shows a true synthetic velocity model
and an initial model; fig. 3(b) shows the inverted model comparing with the
true model (up) and a slightly smoothed ï¬nal inverted model (down); fig. 3(c)
shows the comparison of the true traces and the inverted traces; and fig. 3(d)
shows the waveï¬eld on how the seismic wave propagates with respect to space
and time.

(a) a true model and an initial model

(b) the inverted model comparison

(c) trace comparison

(d) waveï¬eld

fig. 3. applying rnn for 1d seismic velocity inversion
1

https://pytorch.org/.

10

l. huang et al.

the 1d inversion experiment ï¬nds a close-to the true model solution after 100
iterations. we use adam optimizer [7] with l2 regularization. we are currently
working on 2d cases by revising pysit package. we continue performing more
testing cases to evaluate the performance with both data and model parallelism
provided by pytorch on a cpu cluster and multiple gpus.

3

diï¬€erentiable programming

3.1

automatic diï¬€erentiation and adjoint-state method

the automatic diï¬€erentiation (ad) is also called algorithmic diï¬€erentiation that
calculates the derivatives of any arbitrary diï¬€erentiable program. unlike using
the numerical diï¬€erentiation of the adjoint state method that is an approximation to calculate the derivatives, the automatic diï¬€erentiation returns the
exact answer of the derivatives, though subject to the intrinsic rounding error.
machine learning software such as tensorflow and pytorch all have the built-in
implementation of ad as the core functionality of backpropagation to optimize
machine learning models. accurate gradients are critical to the gradient-based
optimizations used in both scientiï¬c computing and machine learning.
in order to calculate the derivatives of any diï¬€erentiable programs, ad needs
to store all operations on the execution path along with the intermediate results.
it then propagates derivatives backward from the ï¬nal output for every single
operation connected with the chain rule. for large scale application, ad faces
the challenge of meeting the demands of fast-growing storage in proportion to
the executed operations. furthermore, the individual derivative function for each
operation also slows down the computation with intrinsic sequential execution.
more work needs to be done if ad can be directly applied to a real scientiï¬c
application.
computationally expensive scientiï¬c applications typically use the adjoint
state method to calculate the gradient of a function with much better computation eï¬ƒciency, although it is a numerical approximation. in fwi, the adjoint
state method calculates the derivative of a forward function j(m) that depends
on u(m). the forward function j can be deï¬ned using h, as following [8]:
j(m) = h(u(m), m)

(9)

where m is the model parameter, which belongs to the model parameter space
m and u belongs to the state variable space, u. the state variables, u follow
the state equations outlined with the mapping function, f, which is also known
as the forward problem or forward equation [8]:
f (u(m), m) = 0.

(10)

the mapping function f is mapping from u * m to u and is satisï¬ed by the
state variable u. if the condition f(u, m) = 0 is satisï¬ed, the state variable u
becomes a physical realization. then, the adjoint state equation can be given as
following, where Î» is the adjoint state variable and uÌƒ is any element of u [8]:

seismic simulation, inversion and deep learning

[

Î´f (u, m) âˆ—
Î´h(u, m)
] Î»=
.
Î´ uÌƒ
Î´ uÌƒ

11

(11)

this adjoint-state gradient calculation involves computing the reverse-time
propagated residual waveï¬eld, combining with the saved forward-propagated
waveï¬eld snapshots at speciï¬ed time intervals to provide adjustments to the
medium properties (the gradient) at each spatial mesh point. in summary, the
forward propagation computes data observations representing the response of
the model, and the residual between the model response and actual observed
data is backward propagated and combined with the forward model response to
compute adjustments to the current model estimate.
intervening in the calculation of the gradient in this manner allows for management of the required computational resources by saving the forward waveï¬elds only as often as numerically required, explicitly managing data resources
through staging to disk or check-pointing as needed, implementing shot-level
parallelism, and other specially tailored techniques.
3.2

extended automatic diï¬€erentiation

a diï¬ƒculty with the auto-diï¬€erentiation (ad) procedure is that memory requirements for the back-propagation graph can become excessive, as noted by
richardson [2]. applying chain-rule diï¬€erentiation on elemental network nodes
over thousands of rnn time steps for a large mesh of physical parameter values is a reasonably-sized task for 1d problems, but the graph quickly becomes
intractable for 2d and 3d models. this issue renders impractical the use of pure
ad for such model inversion problems.
in order to solve the problem, we extended the ad backward process using
pytorch ad workï¬‚ow to integrate the adjoint-state method for the more eï¬ƒcient
gradient calculation. in pytorch, we can customize the ad workï¬‚ow by providing a backward function to calculate the gradients of any function. we need to
pass the required parameters of the forward function, the model parameters and
loss function to allow the backward function to pick up these parameters for the
adjoint-state calculation.
control over this auto-diï¬€erentiation process is available through use of a
pytorch extension to the autograd feature pictured conceptually in fig. 4,
wherein the rnn layer of the network can be replaced by a forward propagation loop and corresponding adjoint back-propagation loop for an equivalent
gradient calculation provided by the user. this alternative gradient calculation
can take advantage of well-known techniques in seismic inversion processing,
enabling existing performance enhancements to be applied using the extended
pytorch capability for specially designed back-propagation.
in the present case, the physical medium properties to be optimized are provided to the â€œforwardâ€ wave propagation problem implemented using the publicly available pysit seismic inversion toolkit [4], creating a simulated seismic
response. the corresponding â€œbackwardâ€ propagation consists in using the residual waveï¬eld represented by the diï¬€erence between the simulated data and the

12

l. huang et al.

fig. 4. adjoint gradient: automatic diï¬€erentiation vs. adjoint gradient calculation.
diï¬€erentiation respect to model parameters are replaced by gradients from adjoint
state in the backward automatic diï¬€erentiation.

observed seismic trace data from the corresponding actual ï¬eld data recording
(or recordings from a â€œtrueâ€ model in our synthetic studies), and implementing the â€œadjoint-stateâ€ solution to provide the required gradient of the model
parameters. other implementations of wave propagation solutions may also be
used in this framework, such as spectral-element methods [9] for 2d, 3d and
spherical 3d wave propagation.
the beneï¬cial end result is that traditional adjoint-state solution methods
are incorporated into the ad workï¬‚ow, so that seismic inversion calculations can
be integrated within the broader deep learning process with eï¬ƒcient calculation.

4

seismic inversion

4.1

seismic inversion

seismic inversion [10] is the method to reconstruct the earth subsurface image
by inverting seismic data observed via the multiple distributed sensors on the
surface. it is typically implemented using the adjoint state method [8] to calculate the gradients. as described in sect. 2 and sect. 3, by reconstructing the
forward problem using deep learning software, the seismic inversion problem can
be solved by the automatic diï¬€erentiation package, a variety of optimizers provided by pytorch, and a customized loss function. the automatic diï¬€erentiation
package in pytorch implements the methodology of automatic diï¬€erentiation
by recording all the forward operations in sequence and performing backward
derivative computation based on the chain rule.
figure 5 shows the workï¬‚ow of seismic inversion. the initial model m 0 is a
guess of the true model m that needs to be inverted. in these early experiments
using several shots of a synthetic seismic reï¬‚ection survey over a small 2d earth
model, we used for convenience an initial model guess that is a smoothed version

seismic simulation, inversion and deep learning

13

fig. 5. the full waveform inversion workï¬‚ow

of the true model. the seismic traces are either observed via distributed sensors
on top of the earth surface in the real-world application or are simulated using
the seismic wave forward function in this paper. the residual is obtained by
âˆ‚u
is calculated
comparing the synthetic data and observed data. the gradient âˆ‚m
based on the residual with respect to the initial model. the gradients are used
by a gradient-based optimizer to update the initial model to get a step close
to the real model. the entire process ends when the initial model and the true
model are converged or exceeded the speciï¬ed number of iterations.
4.2

autoencoder for dimensionality reduction

the seismic inversion process needs to uncover the physical properties at every
point represented in the geological space, which quickly leads to a large number
of model parameters to optimize in the traditional fwi process. the nature of
the nonlinear and ill-posed inverse problem often falls into the local minimum
traps. it is a sound solution to apply the dimensionality-reduction technique to
reduce the optimization parameters to improve the optimization accuracy by
engaging with machine learning models.
since we have customized the automatic diï¬€erentiation workï¬‚ow by integrating the adjoint state method for the fwi gradients (described in sect. 3), it is
now feasible to integrate the machine learning models into the fwi workï¬‚ow
and keep the program diï¬€erentiable. since the autoencoder a(x) is diï¬€erentiable
and the forward model f (x) is diï¬€erentiable, the composition of the f (a(x))
is diï¬€erentiable. we choose the autoencoder as the dimensionality-reduction
method and apply it before the forward model as shown in fig. 6.
the autoencoder contains 743,938 parameters as shown in fig. 7a and b. the
autoencoder is an unsupervised learning model that compresses the information
representation of the input data to a sparse latent variable with less dimensions
at the middle of the encoded layer. it then reconstructs the data from the encoded
latent variable to the original or enhanced data. the compression process is
called encoder and the reconstruction is called decoder. the encoder learns how
to compress the input data and describes it with the latent variable, while the
decoder learns how to reconstruct the data from the latent variable.

14

l. huang et al.

fig. 6. the full waveform inversion workï¬‚ow

8

1

4
i/

4

encoded
256

256

2

128
64

i

i

64

128

i/

i/

2

i/

i/

i/

8

10

8

1

1

input

output

(a) the autoencoder network structure
(b) the autoencoder model parameters

fig. 7. traditional seismic velocity inversion

we start the autoencoder training by generating a large number of random
seismic velocity models. in this work, we are using some simple and ï¬‚at velocity
layers representing the velocities of diï¬€erent earth interiors including water and
rocks. speciï¬cally, these models contain one or more low velocity layers in the
middle or bottom of these layers that is challenging for the low velocity inversion.
all of these models have the ï¬xed dimensions of 60 Ã— 80. as indicated in fig. 7a,
the autoencoder has two components: a encoder and a decoder. the encoder
compresses the input model with dimension of 60 Ã— 80 to an encoded latent
variable with dimension of 8 Ã— 10, which is 1/60 of the original dimension. the
latent variable is then decompressed by the decoder to restore to its original
dimension.
the loss function we used to train the autoencoder is the mean-square-error
(mse) loss and the optimizer is adam with learning rate of 0.001. the batch
size used is 128. the loss values during the training process is shown in fig. 8.

seismic simulation, inversion and deep learning

15

fig. 8. the autoencoder training loss

figure 6 shows the autoencoder enhanced fwi process, where the autoencoder is inserted before the forward function simulation starts. note that the
encoder is only applied to the ï¬rst iteration to get the encoded latent variable.
for the rest of optimization iterations, the decoder is applied to decompress the
encoded latent variable to get a new velocity model with the original dimension.
during the gradient-based optimization process, the gradients are calculated
with respected to the encoded latent variable, instead of the original model,
which reduced the dimensionality of the optimization search space to 1/60. we
use the mse loss and adam optimizer during the process.
4.3

results

pytorch has a list of optimizers including adam [7], rmsprop [11], stochastic
gradient descent (sgd), adadelta [12], adagrad [13], lbfgs, and their variants. the learning rate, scheduler and regularizations can be speciï¬ed to ï¬t
diï¬€erent optimization problems. there are also multiple regression and classiï¬cation loss functions implemented in pytorch. all of these packages provide a
rich environment to solve inverse problems.
in our implementation, we have demonstrated how to invoke the extended
automatic gradient calculation for the velocity model. we choose the adam
optimizer and the mse loss function to compare the misï¬t of the simulated
traces and observed traces after each iteration of the forward model. the partial
derivative (the gradient) of the loss function with respect to the initial model
and the encoded latent variable is calculated by the automatic diï¬€erentiation
process, which is applied by the optimizer to minimize the misï¬t. these iterations
gradually ï¬nd an approximation of the true velocity distribution.
figure 9 and fig. 10 show the diï¬€erences of the traditional fwi and the
autoencoder enhanced fwi results. fig. 9(a) shows the initial model, the true
model, and the inverted model; the loss graph fig. 9(b) shows the loss values
(at diï¬€erent scales) after each optimization iteration, and fig. 9(c) shows the
diï¬€erence between the inverted model and the initial model (top), as well as
the diï¬€erence between the inverted model and the true model. it appears that

16

l. huang et al.

(a) the initial, true and inverted model comparison

(b) loss function value

(c) diï¬€erences

fig. 9. traditional seismic velocity inversion

the traditional fwi does not optimize well in the low velocity layer case after
40 iterations ended with a high loss value, which falls into a local trap. the
autoencoder-enhanced fwi discovers the low velocity layer very well and continues to optimize the misï¬t for all 100 iterations. the diï¬€erence graphs also conï¬rm that the autoencoder case identiï¬es all layers well showing less structured
misï¬ts. noticeably, there are also less artifacts introduced in the autoencoder
enhanced fwi compared with the traditional fwi.
as described in sect. 3, the automatic diï¬€erentiation provided by the pytorch
software does not provide suï¬ƒcient eï¬ƒciency to solve the fwi 2d problem. the
gradients calculated for the whole program takes too long and too much space
to store them. we use the hybrid method describe in sect. 3.2 to overcome the
problem by incorporating the adjoint state method. as the result, the gradient
calculation using the hybrid approach achieves both accuracy and eï¬ƒciency,

seismic simulation, inversion and deep learning

17

(a) the initial, true and inverted model comparison

(b) loss function value

(c) diï¬€erences

fig. 10. the autoencoder enhanced seismic velocity inversion

which is feasible to be used for a large scale scientiï¬c computation problem
integrating with machine learning models.

5

discussion

there are a few of points that worth noting for the work. the ï¬rst is that the
automatic diï¬€erentiation is key for diï¬€erentiable programming, which can bridge
the physics-based scientiï¬c computing with the machine learning (ml)/artiï¬cial
intelligence (ai) technologies. ml/ai methods do not have physics principles
built in that may create an infeasible solution given the fact that most of the scientiï¬c inverse problems may be ill-posed. in our prior work [14], the convergence
of ml with a scientiï¬c application without diï¬€erentiable programming may not
ï¬nd a generalized solution since optimizations of the two diï¬€erent methods are
disconnected.

18

l. huang et al.

the second point we would like to make is that the automatic diï¬€erentiation
needs additional improvements to make it feasible to other applications. in our
method, we integrate the adjoint-state method to make it feasible to solve a large
case, however the solution is an approximation. if the automatic diï¬€erentiation
method can be more memory-eï¬ƒcient and parallelizable, it can be much more
useful to compute the exact gradients for the large complex problems.
the last point is the deep learning model autoencoder requires a revisit
to reduce the loss during decoding. although it reduces the dimension by compressing the input data into a sparse latent variable, the reconstruction is not
lossless. there are some errors introduced during the reconstruction process that
may hinder the optimization process. there is a trade-oï¬€ to take into the consideration when designing the convergence of ml/ai with scientiï¬c computing.
the good news is that there are many options to integrate them waiting for us
to explore.

6

conclusion and future work

we have successfully demonstrated two case studies of restructuring the wave
equation using ï¬nite diï¬€erence method in a deep learning rnn model framework
and an autoencoder enhanced fwi process. the beneï¬ts of the work include
fully utilizing the high-performance tensor processing and optimization capabilities implemented in the deep learning package pytorch, as well as the deep
integration of machine learning models with the inverse problem. by integrating
an hpc application with a deep learning framework with diï¬€erential programming, we can explore a large number of combinations of machine learning models
with physical numerical solutions to achieve better accuracy and eï¬ƒciency.
acknowledgment. this research work is supported by the us national science foundation (nsf) awards ##1649788, #1832034 and by the oï¬ƒce of the assistant secretary of defense for research and engineering (oasd(r&e)) under agreement number
fa8750-15-2-0119. the u.s. government is authorized to reproduce and distribute
reprints for governmental purposes notwithstanding any copyright notation thereon.
the views and conclusions contained herein are those of the authors and should not
be interpreted as necessarily representing the oï¬ƒcial policies or endorsements, either
expressed or implied, of the us nsf, or the oï¬ƒce of the assistant secretary of defense
for research and engineering (oasd(r&e)) or the u.s. government. the authors
would also like to thank the xsede for providing the computing resources.

references
1. baydin, a.g., pearlmutter, b.a., radul, a.a., siskind, j.m.: automatic diï¬€erentiation in machine learning: a survey. j. mach. learn. res. 18(1), 5595â€“5637
(2017)
2. richardson, a.: seismic full-waveform inversion using deep learning tools and techniques (2018). https://arxiv.org/pdf/1801.07232v2.pdf

seismic simulation, inversion and deep learning

19

3. hughes, t.w., williamson, i.a.d., minkov, m., fan, s.: wave physics as an analog
recurrent neural network (2019). https://arxiv.org/pdf/1904.12831v1.pdf
4. hewett, r.j., demanet, l., the pysit team: pysit: python seismic imaging
toolbox (january 2020). https://doi.org/10.5281/zenodo.3603367
5. hochreiter, s., schmidhuber, j.: long short-term memory. neural comput. 9(8),
1735â€“1780 (1997). https://doi.org/10.1162/neco.1997.9.8.1735
6. chung, j., gulcehre, c., cho, k., bengio, y.: empirical evaluation of gated recurrent neural networks on sequence modeling (2014)
7. kingma, d.p., ba, j.: adam: a method for stochastic optimization (2014)
8. plessix, r.-e.: a review of the adjoint-state method for computing the gradient
of a functional with geophysical applications. geophys. j. int. 167(2), 495â€“503
(2006). https://doi.org/10.1111/j.1365-246x.2006.02978.x
9. tromp, j., komatitsch, d., liu, q.: spectral-element and adjoint methods in seismology. commun. comput. phys. 3(1), 1â€“32 (2008)
10. schuster, g.: seismic inversion. society of exploration geophysicists (2017).
https://library.seg.org/doi/abs/10.1190/1.9781560803423
11. ruder, s.: an overview of gradient descent optimization algorithms (2016)
12. zeiler, m.d.: adadelta: an adaptive learning rate method (2012)
13. duchi, j., hazan, e., singer, y.: adaptive subgradient methods for online learning
and stochastic optimization. j. mach. learn. res. 12, 2121â€“2159 (2011)
14. huang, l., polanco, m., clee, t.e.: initial experiments on improving seismic data
inversion with deep learning. in: 2018 new york scientiï¬c data summit (nysds),
august 2018, pp. 1â€“3 (2018)



chapter 7

applying scientific machine learning
to improve seismic wave simulation
and inversion
lei huang, edward clee, nishath ranasinghe

department of computer science, prairie view a&m university, prairie view, tx, united states

abstract
accurate simulation of wave motion for the modeling and inversion of seismic wave propagation is a
classical high-performance computing (hpc) application using the finite difference, finite element, and
spectral element methods to solve the wave equation numerically. the article presents a new method
to improve the performance of seismic wave simulation and inversion by integrating the deep learning
software platform and deep learning models with the hpc application. the paper has three contributions: (1) instead of using traditional hpc software, the authors implement the numerical solutions for
the wave equation employing recently developed tensor processing capabilities widely used in the
deep learning software platform of pytorch. by using pytorch, the classical hpc application is reformulated as a deep learning recurrent neural network framework; (2) the authors customize the automatic
differentiation of pytorch to integrate the adjoint state method for an efficient gradient calculation;
(3) the authors build a deep learning model to reduce the physical model dimensions to improve the
accuracy and performance of seismic inversion. the automatic differentiation functionality and a variety
of optimizers provided by pytorch are used to enhance the performance of the classical hpc application. additionally, methods developed in the paper can be extended into other physics-based scientific
computing applications, such as computational fluid dynamics, medical imaging, nondestructive testing, as well as the propagation of electromagnetic waves in the earth.

keywords
inverse problem; scientific machine learning; seismic imaging; seismic wave propagation

7.1 introduction
this chapter explores the use of scientific machine learning (sciml)1 concepts in
addressing the problem of estimating properties of the earth associated with the propagation of seismic waves. sciml has recently emerged as a new method to solve the scientific computing problems using machine learning models and vice versa. the method
leverages the success of traditional scientific computational models and the advances in
data-driven machine learning models to augment the efficiency and accuracy of scientific simulation and inversion. moreover, it facilitates the scientific discovery by modeling both well-known scientific rules and the unknown patterns based on observed data.
advances in subsurface data analytics
doi: https://doi.org/10.1016/b978-0-12-822295-9.00011-x

Â© 2022 elsevier inc.
all rights reserved.

167

168

advances in subsurface data analytics

in this chapter, sciml is studied and applied to simulate the wave propagation and
to enhance the quality of the seismic inversions. in seismic reflection surveys, sound
waves from controlled sources are recorded by a large number of surface- or boreholedeployed sensors (receivers) which capture signals both directly transmitted as well as
reflected from structures at depth, for the purpose of deducing structural variations in
the velocity and density properties of the subsurface rock formations.

7.1.1 seismic imaging
advanced seismic imaging is the process of constructing a model (or â€œimageâ€) of the
earth properties through an â€œinversionâ€ process2, starting with an initial guess, simulating
the wave propagation for the field survey and computing a misfit with respect to the
field-recorded data, and iterating through a gradient-based optimization process until an
acceptable earth model is achieved. the particular method of full-waveform inversion
(fwi) is typically implemented using the adjoint state method3 to calculate the gradients.
fig. 7.1 shows the workflow of seismic inversion. the initial model m0 is a guess of
the true model m that is the unknown â€œtargetâ€ of the inversion. these early experiments, comprising several shots of a synthetic seismic reflection survey over a small
2-d earth model, used for convenience an initial model guess that is a smoothed version of the true model. the seismic traces are either observed via real-world survey
recordings or are simulated using the seismic wave forward function in this paper. the
residual is obtained by comparing the synthetic data and observed data. the gradient
âˆ‚u
is calculated based on the residual with respect to the initial model. the gradients
âˆ‚m
are used by a gradient-based optimizer to update the initial model to get a step closer

fig. 7.1 the full waveform inversion workflow.

applying scientific machine learning to improve seismic wave simulation and inversion

to the real model. the entire process ends when the initial model and the true model
are adequately converged or the specified limit of iterations is exceeded. enhancements
to this process using machine learning are treated in a later section, after discussion of
additional required concepts.

7.1.2 computational issues for wave propagation
physical simulation and inversion are classical scientific computing applications to
examine the physical phenomenon and reveal the underlying medium properties. the
simulation solves the partial differential equations (pdes) that govern the physical
phenomenon using numerical approximation methods, while the inversion applies the
gradient-based optimizations to find the underlying properties by minimizing the misfit
between the observed data and the simulated results. the entire process takes significant
computing resources to achieve the required accuracy. however, the inverse problem
is naturally challenging since it is ill-posed and nonlinear for most cases4. in particular,
the optimization process in the non-convex system frequently attempts to converge to
a local minimum instead of the desired global minimum, due to the effect of â€œcycle
skippingâ€ in which the alignment of seismic events is improperly discerned because of
insufficient sampling in time or space. practical approaches to mitigate these problems
are discussed by virieux and operto5, and still need to be applied even when using
machine-learning alternative methods.
another challenge in implementing the seismic inversion process is the need to correctly compute the adjoint-state solution whereby the gradient for updating the physical parameters in the model space must be calculated by backward propagation of the
residual data field at each observation point for each source. this calculation is at least
as challenging as the original forward propagation through the model, and represents a
differential process with stringent accuracy requirements.

7.1.3 opportunity for data analytics
a potential application of data analytics technology in the analysis of seismic reflection imaging data is to utilize the characteristic patterns exhibited in seismic reflection
recordings to learn the full-waveform responses of various earth structure patterns for
different survey geometries by means of a large number of model experiments6, and
train a very deep neural network to recognize such responses in actual field-recorded
surveys and discern likely structures that are generating such responses. inference of
structure from such a data-driven approach could then be faster than traditional inversion operations that require many forward and back-propagated simulations in the
optimization process. such a model-driven process can be made efficient by the using
a recurrent neural network (rnn) implementation of the wave propagation forward
model simulations, which can be transparently implemented on specialized hardware
through the newer tensor processing systems, such as tensorflow and pytorch.

169

170

advances in subsurface data analytics

ultimately, the objective is to integrate the machine learning models that are datadriven into the scientific computational models that are physics-driven. the differentiable programming method has the potential to smoothly integrate the two models
together with a global optimization. it is expected that this study will lead to more
interesting research findings in the topic of sciml and to discovering ways to combine
the power of these two different methods to facilitate scientific discovery.

7.1.4 scientific machine learning
a recent push to incorporate domain specific knowledge with machine learn ing has
shown great promise and it is emerging as a sub-field under the name of sciml. in a
recent workshop organized by the us department of energy to identify basic research
needs for sciml, a group of leading researchers in the field identified domain-aware,
interpretable and robust machine learning as fundamental to the development of
sciml7. they further highlighted the growing enthusiasm on sciml among the wider
scientific community is due to the broad availability of hpc resources; developments
in computationally effective data analysis algorithms as well as the availability of large
amounts of high-quality discrete and continuous data collected from scientific instruments7. furthermore, sciml could replace meshing associated with classical numerical
discretization techniques, such as finite element method, finite difference method with
neural networks that could approximate the solutions of physical problems described
as pdes with the use of automatic differentiation8, and also break free from the curse
of dimensionality9. the main idea behind obtaining approximate solutions to pdes in
sciml is to constrain the neural network to minimize the residual of the pde. the
sciml research currently includes the physics-informed neural networks (pinns)10,11,
the universal differential equation (ude) method12, the hamiltonian neural networks
(hnns)13,14, and the neural ordinary differential equation (node)15 to learn the nonlinear dynamics in physics.
the sciml method with integration of data and physics modeling is demonstrated
in two ways in this chapter:
1. by expressing wave propagation in the form of a rnn, allowing the gradients
calculated by automatic differentiation to be used for the fwi of seismic reflection
survey data, and
2. incorporating existing forward wave propagation and adjoint-state inversion
solutions through extensions to the deep learning auto-differentiation tools, thus
allowing seismic inversion to be integrated with a deep learning model to create an
end-to-end differentiable programming environments.
the contributions of this work are summarized in the following:
1. the pde solver in the seismic forward model is reformulated using rnn and
implemented with the deep learning software package, pytorch, which allows taking advantage of the tensor processing software and its accelerator implementation.

applying scientific machine learning to improve seismic wave simulation and inversion

2. the automatic differentiation feature implemented in pytorch enables calculating a
model gradient to solve the seismic inverse problem to uncover the earthâ€™s interior
physical properties.
3. efficiency of the automatic differentiation feature is improved by extending it to create
a hybrid back propagation using the adjoint-state method to calculate the gradients.
4. the optimization is incorporated into an autoencoder network to reduce the
dimensions of the inverted parameters to augment the convergence process and get
more accurate results for the ill-posed problem.

7.2 related work
7.2.1 seismic wave simulation and inversion
in this study, the application of a tensor-based machine learning system for solving physical simulation and gradient-based optimization problems is illustrated using seismic wave
propagation simulation and full waveform inversion (fwi) as the physical case study.
both richardson16 and zhu and xu17 have mathematically demonstrated the equivalence of wave-propagation inversion by the adjoint-state method with a rnn and
its back-propagation gradient computed through automatic differentiation. richardson
uses this equivalence to incorporate an existing implementation of forward modeling
and adjoint-state inversion as an extension to the automatic differentiation feature of
tensor processing, allowing interventions for explicitly managing memory and computational demands. zhu and xu, taking an opposite approach, have formulated the
wave propagation problem as an rnn for which the adjoint-state inverse need not
be explicitly coded, but instead is implemented automatically using the automatic differentiation capability built into the tensor processing system. this allows both forward
and back propagation to be transparently managed on cpu and gpu processors, with
memory and computational demands satisfied simply by allocating more resources in
parallel as needed.

7.2.2 surrogate models using machine learning
longer run times and slower convergence inhibit the rapid integration of scientific
models in real time applications and critical decision making processes. additionally,
the â€œcurse of dimensionalityâ€ is encountered when the number of samples required to
cover the parameter space increases with the number of parameters in the model. the
issue of longer run times demands that models be simplified, ignoring physical process
and reduction in the numerical accuracy of the models. on the other hand, surrogate
models facilitate the execution of complex physical models while preserving numerical accuracy. surrogate models are also referred to as meta models18, model emulators19,
reduced order models20,21, physics based proxy models22, multifidelity models23, and
response surfaces24.

171

172

advances in subsurface data analytics

7.2.3 dimensionality reduction
large realistic seismic inversions in geologically complex settings often utilize global
optimization algorithms, such as grid search25, simulated annealing26, very fast simulated
annealing27, monte carlo optimization28, particle swarm optimization29 due to the presence of many local minima in the misfit function between the model and the data.
however, global optimization modeling suffers from slow convergence rates as the dimensionality of the model space is increased. in machine learning, autoencoders are widely utilized to reduce dimensionality of the model space. an autoencoder neural network30 is an
unsupervised learning algorithm that implements backpropagation in an effort to generate
output values that are equal to the input values. it consists of an encoder and decoder; the
encoder operator reduces the dimensionality of the model space and the decoder operator tries to regenerate the original input data from the low-dimensional representation.
moseley et al.31 used wavenet network architecture32 and a conditional autoencoder
to simulate the earth response to the propagation of acoustic waves in horizontally
layered mediums and more general faulted 2d mediums, respectively. the encoder
network in the autoencoder is composed of 10 convolutional layers, which reduces
the spatial dimension of the input velocity model to a latent vector of 1*1 with
1,024 hidden channels. the decoder network utilizes 14 convolutional layers to expand
the latent space back into the original dimen sion. all the hidden layers in the autoencoder network utilizes relu activation functions33 except the final output layer which
utilizes an identity activation function. for the both networks, they observed the resulting seismic inversion are 20â€“500 times faster than finite difference modeling. but the
authors noted, extending the deep neural network based seismic inversion into more
complex, elastic and 3-d models will be challenging.
gao et al.34 used an autoencoder neural network to efficiently conduct a 1-d seismic
impedance inversion implementing trace-by-trace inversion with regularization using
a fully connected convolutional neural network. they used differential evolution35, a
stochastic, population-based global optimization algorithm, which solves optimization
problems based on a population of individuals in which each is a candidate solution.the
authors initially trained the autoencoder network to learn to encode the large dimension problem into a reduced order dimension problem and then decode it back to the
initial large dimension problem. they solve the large dimension problem by searching
the best acceptable solution in the reduced order space and then map the selected solution back to the higher order space to obtain the most acceptable model for the initial
problem. they further observed a good initial model supplemented with well-log data
is necessary to obtain an accurate inversion. the authors further showed their method
converges faster and produces more robust results than common inversion methods for
both synthetic and field data.
another study by chen and schuster36 used skeletonized seismic data, which consists
of the low-rank latent-space variables produce by an adequately trained autoencoder

applying scientific machine learning to improve seismic wave simulation and inversion

network, to obtain a subsurface velocity model by inverting the wave equation. the
authors coined the name newtonian machine learning (nml) for their method as it
inverts for the model parameters by utilizing both forward and backward operations of
the newtonian wave propagation while utilizing dimension reduction capabilities of
machine learning. they used seismic traces as the input to the autoencoder network,
which calculates the perturbation of skeletonized data with regard to velocity perturbations (frÃ©chet derivative). then the gradient is calculated by migrating the shifted
observed traces weighted by the the skeletonized data residual. finally, the model which
best determines the observed latent-space parameters are chosen as the final velocity
model. the authors also noted the their method could reduce the effects of cycle skipping which plagues fwi and it does not require manual picking of important features as
the skeletal data are automatically generated by the autoencoder network. they further
noted that the velocity model produced from their model has low resolution and it can
be used as a starting model for fwi.
a recent fwi study by sun and alkhalifah37 used meta-learning to train a neural
network to learn the optimization algorithm instead of using a predesigned optimization algorithm. they used gradient of the misfit function as the input of a rnn and
the history information of the gradient was used as the hidden states in the rnn37.
the authors formulated the loss function for the training as a weighted summation of
the l2-norm of the data residuals. furthermore, they accelerated the training process
of the neural network by minimizing randomly generated quadratic functions by locally
approximating the optimization as a a linear convex problem. additionally, they used
variational autoencoder38 methods to project and represent the model in latent space
to achieve more accurate and robust velocity model. they obtained faster convergence
rates compared to conventional fwi for both marmousi39 and the overthrust40 models.
mosser et al.41 used deep generative models in the context of inverting the acoustic
wave equation to obtain subsurface models. they used wasserstein generative adversarial networks (gan)42â€“44 to create subsurface geological structures and their respective petrophysical properties as a priori model. then they combined the models with
the acoustic wave equation and performed bayesian inversion using an approximate
metropolis-adjusted langevin algorithm (mala)45 drawing samples from a posterior
seismic observations. the authors used the adjoint method to calculate the gradients
with respect to the model parameters governing the forward problem and they exploited the differential nature of the deep neural network to calculate the gradient mismatch.
swischuk et al. demonstrated projection-based model reduction.46

7.2.4 differentiable programming
differentiable programming is a programming paradigm that the whole numerical
computation in the program is differentiable via automatic differentiation (ad)47.
differentiability is the core to machine learning and inverse problems since the program

173

174

advances in subsurface data analytics

can be optimized via the gradient-based optimization algorithms. ad is also known as
algorithmic differentiation has been widely used in machine learning software to calculate the derivatives of a sequence of operations. ad stores the sequence of operations
and applies the chain-rule to calculate the derivative either in the forward or reverse
model. the back-propagation process, which is the backbone of machine learning,
relies on ad to calculate the accurate gradients for training a model. the development
of domain-aware sciml by combining capabilities of ad with the domain knowledge
will help to understand the full potential of sciml. ad is natural for scientists to apply
since it is automatically implemented, but it comes with a hefty price of efficiency48.the
computation of ad requires to store all operations, and it needs to follow the chain-rule
with a sequential execution fashion. in contrast, many scientific computing applications
use the adjoint-state method to calculate derivatives based upon numerical solutions
to the associated partial differential equation systems. the numerical solution methods
offer opportunity for critical time and memory savings that improve the efficiency
while allowing preservation of sufficient accuracy.
ad is the key component for a programming model that can potentially bridge
scientific computing and machine learning49. julia50 is such a programming model
built with ad as its core component to provide a high-level and high-performance
programming model for numerical computing with differentiable programming. xu
et al.51 developed the ad seismic package based on julia differential programming and
mathematically proved that the adjoint-state method is equivalent to the reverse-mode
ad, which can be used very effectively for seismic inverse problems. the authors also
learned hidden dynamics using intelligent ad52 that demonstrates a promising usage of
applying the differentiable programming on bridging scientific computing and machine
learning domains.

7.3 wave equations and rnn
7.3.1 wave equations
the wave motion is governed by physical rules that can be expressed in the following
partial differential equation (pde) (1) along with the boundary conditions (2)(3). the
1d scalar wave equation is used for simplicity purpose in this paper:
1 âˆ‚ u ( x, t ) âˆ‚ u ( x, t )
âˆ’
= f ( x, t )
2
c ( x ) âˆ‚t 2
âˆ‚x 2

(1.1)

1 âˆ‚u ( 0, t ) âˆ‚u ( 0, t )
âˆ’
=0
c ( 0 ) âˆ‚t
âˆ‚x

(1.2)

1 âˆ‚u (1, t ) âˆ‚u (1, t )
âˆ’
=0
c (1) âˆ‚t
âˆ‚x

(1.3)

2

2

applying scientific machine learning to improve seismic wave simulation and inversion

where, c(x) is the spatial velocity distribution, u(x, t) is the wave field distribution in
space and time, and f (x, t) is the energy source distribution in space and time.
the eq. (1.1) can be solved numerically using a finite difference approximation:
u( x âˆ’ âˆ†x, t ) âˆ’ 2u( x, t ), +u( x + âˆ†x, t )
âˆ†x 2
1 u( x, t âˆ’ âˆ†t ) âˆ’ 2u( x, t ) + u( x, t + âˆ†t )
+ 2
c
âˆ†t 2
after factoring, the eq (1.4) can be expressed as:
f ( x, t ) = âˆ’

(1.4)

u ( x, t + âˆ†t ) = f ( x, t ) c 2 âˆ†t 2 + ( 2u ( x, t ) âˆ’ u( x, t âˆ’ âˆ†t ))
+ c2

âˆ†t 2
(u( x âˆ’ âˆ†x, t ) âˆ’ 2u( x, t ) + u( x + âˆ†x, t ))
âˆ†x 2

(1.5)

which shows that the next wave field in time u(x, t + âˆ†t) can be calculated based on
the current and prior wave fields, as well as spatial neighbors in the current wave field.
the wave motion simulation follows the time sequence to produce the next state based
on the prior ones, which is similar to rnn in deep learning to model a time sequence
function.

7.3.2 recurrent neural network
rnn is used to model the pattern in a sequence of data, mostly in time sequence. in
recent years, rnn and its variants have been applied successfully to problems such as
speech recognition, machine translation, and text-to-speech rendering. it has an internal
cell that repeatedly processes an input, carries a hidden state, and produces an output at
each step. the rnn cell can be designed to be simple or complex to model a problem
with a forgettable memory mechanism (long short-term memory [lstm]53) or/and a
gating mechanism (gated recurrent unit [gru]54).
fig. 7.2a shows a typical rnn structure that repeatedly takes an input, updates its
hidden state, and produces an output at every step. the rnn model can be unfolded
as shown in fig. 7.2b that learns the recurrence relationship from a sequence of data.
the hidden state hi remembers the prior state of the process and is updated at each step.
the hidden state enables rnn to learn the temporal relationships among the inputs
since most of the time sequence data do contain temporal patterns. lstm allows rnn
to forget long-term relationships built up in the hidden state and emphasizes the shortterm relationships, which can be useful for many cases.
a simple rnn can be expressed in the eq. (1.6):
ht = Ïƒ h (wh xt + whht âˆ’1 + bh )
yh = Ïƒ y (wy ht + by )

(1.6)

175

176

advances in subsurface data analytics

fig. 7.2 a simple rnn model (a) with feedback loop and (b) with loop unfolded.

where xt is the input, ht is the hidden state, w represents the weights, b is the bias, and
Ïƒ is the activation function.
the discretized wave eq. (1.5) can be restructured as an rnn with two hidden
states u(x, t) and u(x, t âˆ’ âˆ†t). there is also a spatial stencil relationship of neighboring
velocity distribution. the internal operation of the rnn can be defined using a new
function f with input of f (x, t), two hidden states u(x, t) and u(x, t âˆ’ 1), and the constant
velocity distribution c:
f ( f ( x, t ) , u ( x, t ) , u ( x, t âˆ’ 1) , c )

= f ( x, t ) c 2 âˆ†t 2 + ( 2u ( x, t ) âˆ’ u ( x, t âˆ’ 1) )
+ c2

(1.7)

âˆ†t
(u ( x âˆ’ 1, t ) âˆ’ 2u ( x, t ) + u ( x + 1, t ) )
âˆ†x 2
2

then, the eq. (1.5) can be restructured as an rnn format:

(

ht +1 = Ïƒ f ( f (t ) , h (t ) , h (t âˆ’ 1) , c )
yt +1 = p ( ht +1 )

)

(1.8)

where, p is the projection function to get the sample of a trace from a receiver.
the eq. (1.8) is then a nonlearnable, deterministic physical solution represented as the
deep learning rnn model. fig. 7.3 shows the resulting rnn model that solves the wave
equation with four inputs f (x, t), h(t), h(t âˆ’ 1), and c, the velocity distribution, which is
constant in the equation. the output yt is the trace sample of a receiver at each time step.

7.3.3 pytorch rnn implementation
the wave equation rnn model presented in fig. 7.3 enables use of the deep learning
software platform to solve the wave equations. the benefits of using a deep learning

applying scientific machine learning to improve seismic wave simulation and inversion

fig. 7.3 a rnn model for wave equation.

model to represent an hpc application include the ability to: (1) leverage the hpc
implementation of the deep learning model exploiting the advantages of gpus/multicores and vectorization for better performance, (2) have an automatic gradients calculation using the built-in automatic differentiation package in deep learning, (3) utilize the
variety of built-in optimizers to apply the gradients to find the global/local optimums,
and (4) use the data- and model-parallelism framework implemented in deep learning
package to run the application on a hpc cluster.
the implementation in this work uses pytorch1 v1.5 to build the rnn model.
pytorch is an open source machine learning framework developed by facebook by
merging torch and caffe2, which supports a variety of hardware platforms including
multiple cpus, gpus, distributed systems, and mobile devices. besides the machine
learning and deep learning functions, one unique feature of pytorch is that it contains
a just-in-time compiler to optimize the code if it complies with torchscript, which is
a subset of python. it has a built-in automatic differentiation package for calculating
derivatives, as well as a distributed training module to train a model on a hpc cluster.
pytorch has both python and c++ frontends.
the following python code snippet illustrates an rnn-similar implementation of
wave equation using pytorch. the two classes derived from torch.nn. module for rnn
cell and rnn driver respectively are called wave pgnncell and wave propagator. the
wave pgnncell implemented a cell function in rnn that computes the wavefield at
a time step. the wave propagator iterates over all time steps and takes the seismic source
waveform sample as the input at each time step.the hidden state (self.h) contains the previous and current wave fields, which are fed into the cell for the next iteration.the trace is
collected by projecting the current wavefield based on the receiver location.the program
returns the simulated wavefield and sampled trace at the end.
1https://pytorch.org/

177

178

advances in subsurface data analytics

7.3.4 seismic wave simulation
for a simple demonstration of seismic wave simulation, the rnn model is used to
simulate the acoustic wave propagation for the scalar wave equation. a â€œtrueâ€ synthetic
model is used to generate a target â€œrecordedâ€ data set, and an initial model can be
constructed as a smoothed version of the true model or some other separately chosen
function. a time-symmetric ricker wavelet is used as a waveform for one or more
energy sources (shots), and the wavefield is sampled as traces at an array of receivers.
rock density is assumed constant in these models.
as stated earlier, one benefit of using deep learning technology is to take advantage
of its multiple cpus and gpus implementation. it is required only to specify which
devices the code will operate on and define tensors to these devices. all remaining
device-specific implementation and optimizations are done internally by pytorch; porting of application code using cuda or openacc is not required. another benefit is
to use the data-parallelism implemented in pytorch. we can parallelize the code by the
number of the sources/shots to run the code on multiple gpus and distributed clusters.
fig. 7.4 shows a 1d seismic velocity inversion case applying the physics-ruled
rnn implementation. the fig. 7.4a shows a true synthetic velocity model and an
initial model; 4b shows the inverted model comparing with the true model (up) and a
slightly smoothed final inverted model (down); 4c shows the comparison of the true
traces and the inverted traces; and 4d shows the wave field illustrating how the seismic

applying scientific machine learning to improve seismic wave simulation and inversion

fig. 7.4 applying rnn for 1d seismic velocity inversion.

wave propagates with respect to space and time. the 1d inversion experiment finds a
solution close to the true model after 100 iterations, using the adam optimizer55 with
l2 regularization.
ongoing efforts include working on 2d cases by revising pysit package, and performing more test cases to evaluate the performance with both data and model parallelism provided by pytorch on a cpu cluster and multiple gpus.

7.4 differentiable programming
7.4.1 automatic differentiation and adjoint-state method
the method of automatic differentiation (ad), also called algorithmic differentiation, calculates the derivatives for any arbitrary differentiable program. unlike using
the numerical differentiation of the adjoint state method that is an approximation to
calculate the derivatives, the automatic differentiation returns the exact answer of the

179

180

advances in subsurface data analytics

derivatives, though subject to the intrinsic rounding error. machine learning software
such as tensorflow and pytorch all have the built-in implementation of ad as the core
functionality of backpropagation to optimize machine learning models. accurate gradients are critical to the gradient-based optimizations used in both scientific computing
and machine learning.
in order to calculate the derivatives of any differentiable programs, ad needs to
store all operations on the execution path along with the intermediate results. it then
propagates derivatives backward from the final output for every single operation connected with the chain rule. for largescale applications, ad faces the challenge of meeting the demands of fast-growing storage in proportion to the executed operations.
furthermore, the individual derivative function for each operation also slows down the
computation with intrinsic sequential execution. more work needs to be done if ad
can be directly applied to a real scientific application.
computationally expensive scientific applications typically use the adjoint state
method to calculate the gradient of a function with much better computation efficiency,
although it is a numerical approximation. in fwi, the adjoint state method calculates
the derivative of a forward function j(m) that depends on the wavefield u(m). the forward function j can be defined using h, as following3:
j ( m ) = h (u ( m ) , m )

(1.9)

where m is the model parameter, which belongs to the model parameter space m and u
belongs to the state variable space, u.the the state variables, u follow the state equations
outlined with the mapping function, f, which is also known as the forward problem or
forward equation3:
f (u(m ), m ) = 0.

(1.10)

the mapping function f is mapping from u * m to u and is satisfied by the state
variable u. if the condition f(u, m) = 0 is satisfied, the state variable u becomes a physical
realization. then, the adjoint state equation can be given as following, where Î» is the
adjoint state variable and uÌƒ is any element of u3:
*

ï£® Î´ f ( u, m ) ï£¹
Î´ h ( u, m )
(1.11)
ï£¯
ï£º Î»=
Î´ u
ï£° Î´ u ï£»
this adjoint-state gradient calculation involves computing the reverse-time propagated residual wavefield, then combining with the saved forward-propagated wavefield
snapshots at specified time intervals to provide adjustments to the medium properties
(the gradient) at each spatial mesh point. in summary, the forward propagation computes
data observations representing the response of the model, and the residual between the
model response and actual observed data is backward propagated and combined with
the forward model response to compute adjustments to the current model estimate.

applying scientific machine learning to improve seismic wave simulation and inversion

intervening in the calculation of the gradient in this manner allows for management
of the required computational resources by saving the forward wave fields only as often
as numerically required, explicitly managing data resources through staging to disk or
check-pointing as needed, implementing shot-level parallelism, and other specially tailored techniques.

7.4.2 extended automatic differentiation
a difficulty with the autodifferentiation (ad) procedure is that memory requirements for the back-propagation graph can become excessive, as noted by richardson16.
applying chain-rule differentiation on elemental network nodes over thousands of
rnn time steps for a large mesh of physical parameter values is a reasonably-sized task
for 1d problems, but the graph quickly becomes intractable for 2d and 3d models.this
issue usually renders impractical the use of pure ad for such model inversion problems.
an alternative solution to the problem, involves extending the ad backward process
using the pytorch ad workflow to integrate the adjoint-state method for the more
efficient gradient calculation. in pytorch, the ad workflow is extended by providing
a backward function to calculate the gradients of the corresponding forward function.
the required parameters of the forward function, the model parameters and loss function are provided to allow the backward function to pick up these parameters for the
adjoint-state calculation.
control over this auto-differentiation process is available through use of a pytorch
extension to the autograd feature pictured conceptually in fig. 7.5, wherein the rnn
layer of the network can be replaced by a forward propagation loop and corresponding adjoint back-propagation loop for an equivalent gradient calculation provided

fig. 7.5 adjoint gradient vs automatic differentiation. derivatives with respect to model parameters
in the backward automatic differentiation are replaced by gradients from adjoint state.

181

182

advances in subsurface data analytics

by the user. this alternative gradient calculation can take advantage of well-known
techniques in seismic inversion processing, enabling existing performance enhancements to be applied using the extended pytorch capability for specially designed
back-propagation.
in the present case, the physical medium properties to be optimized are provided
to the â€œforwardâ€ wave propagation problem implemented using the publicly available
pysit seismic inversion toolkit56, creating a simulated seismic response. this simulated
response is to be compared with the observed seismic trace data from the corresponding
actual field data recording (or recordings from a â€œtrueâ€ model in our synthetic studies).
the corresponding â€œbackwardâ€ propagation consists in using the residual wavefield
represented by the difference between the simulated data and the observed seismic trace
data, and implementing the â€œadjoint-stateâ€ solution to provide the required gradient of
the model parameters. other implementations of wave propagation solutions may also
be used in this framework, such as spectral element methods57 for 2d, 3d, and spherical
3d wave propagation.
the beneficial end result is that traditional adjoint-state solution methods are incorporated into the ad workflow, so that seismic inversion calculations can be integrated
within the broader deep learning process with efficient calculation.

7.5 seismic inversion
7.5.1 seismic inversion using neural network
these new concepts from machine learning may now be applied to the problem of
seismic inversion as presented in section 1.1. as described in section 1.3 and section
1.4, by reconstructing the forward problem using deep learning software, the seismic
inversion problem can be solved by the automatic differentiation package, a variety of
optimizers provided by pytorch, and a customized loss function. the automatic differentiation package in pytorch implements the methodology of automatic differentiation
by recording all the forward operations in sequence and performing backward derivative computation based on the chain rule.

7.5.2 autoencoder for dimensionality reduction
the seismic inversion process needs to uncover the physical properties at every point
represented in the geological space, which quickly leads to a large number of model
parameters to optimize in the traditional fwi process. the nature of the nonlinear and
ill-posed inverse problem often falls into the local minimum traps. it is a sound solution
to apply the dimensionality reduction technique to reduce the optimization parameters
to improve the optimization accuracy by engaging with machine learning models.
since the automatic differentiation workflow has been customized by integrating
the adjoint state method for the fwi gradients (described in section 1.4), it is now

applying scientific machine learning to improve seismic wave simulation and inversion

fig. 7.6 full waveform inversion enhanced with autoencoder dimensionality reduction.

feasible to integrate the machine learning models into the fwi workflow and keep the
program differentiable. since the autoencoder a(x) is differentiable and the forward
model f (x) is differentiable, the composition of the f (a(x)) is differentiable. the autoencoder is applied as the dimensionality reduction method applied before the forward
model as shown in fig. 7.6.
for the 60 Ã— 80 2d model in the current example, the autoencoder contains
743,938 parameters as shown in fig. 7.7a and b. the autoencoder is an unsupervised
learning model that compresses the information representation of the input data to a
sparse latent variable with less dimensions at the middle of the encoded layer. it then
reconstructs the data from the encoded latent variable to the original or enhanced data.
the compression process is called encoder and the reconstruction is called decoder. the
encoder learns how to compress the input data and describes it with the latent variable,
while the decoder learns how to reconstruct the data from the latent variable.
we start the autoencoder training by generating a large number of random seismic
velocity models. in this work, we are using some simple and flat velocity layers representing the velocities of different earth interiors including water and rocks. specifically,
these models contain one or more low velocity layers in the middle or bottom of
these layers that is challenging for the low velocity inversion. all of these models have
the fixed dimensions of 60 Ã— 80. as indicated in fig. 7.7a, the autoencoder has two
components: a encoder and a decoder. the encoder compresses the input model with

183

fig. 7.7 traditional seismic velocity inversion.

184
advances in subsurface data analytics

applying scientific machine learning to improve seismic wave simulation and inversion

fig. 7.8 the autoencoder training loss.

dimension of 60 Ã— 80 to an encoded latent variable with dimension of 8 Ã— 10, which is
1/60 of the original dimension.the latent variable is then decompressed by the decoder
to restore to its original dimension.
the loss function we used to train the autoencoder is the mean-squre-error (mse)
loss and the optimizer is adam with learning rate of 0.001. the batch size used is 128.
the loss values during the training process are shown in fig. 7.8.
fig. 7.6 shows the autoencoder-enhanced fwi process, where the autoencoder is
inserted before the forward function simulation starts. note that the encoder is only
applied to the first iteration to get the encoded latent variable. for the remaining optimization iterations, the decoder is applied to decompress the encoded latent variable
to get a new velocity model with the original dimension. during the gradient-based
optimization process, the gradients are calculated with respect to the encoded latent
variable, instead of the original model, which reduces the dimensionality of the optimization search space by a factor of 60.

7.5.3 results
pytorch has a list of optimizers, including adam55, rmsprop58, stochastic gradient
descent (sgd), adadelta59, adagrad60, lbfgs, and their variants. the learning rate,
scheduler and regularizations can be specified to fit different optimization problems.
there are also multiple regression and classification loss functions implemented in
pytorch. all of these packages provide a rich environment to solve inverse problems.
the present implementation has demonstrated how to invoke the extended automatic gradient calculation for the velocity model. the adam optimizer and the mse

185

186

advances in subsurface data analytics

fig. 7.9 traditional seismic velocity inversion.

loss function are used to compare the misfit of the simulated traces and observed traces
after each iteration of the forward model. the partial derivative (the gradient) of the loss
function with respect to the initial model and the encoded latent variable is calculated by
the automatic differentiation process, which is applied by the optimizer to minimize the
misfit. these iterations gradually find an approximation of the true velocity distribution.
fig. 7.9 and fig. 7.10 show the differences of the traditional fwi and the autoencoder enhanced fwi results. within each figure (1) shows the initial model, the true
model (flat layers with a low velocity feature around 1 km depth), and the inverted

applying scientific machine learning to improve seismic wave simulation and inversion

fig. 7.10 the autoencoder-enhanced seismic velocity inversion.

model; the loss graph (b) shows the loss values (at different scales) after each optimization iteration, and figure (c) shows the difference between the inverted model and the
initial model (top), as well as the difference between the inverted model and the true
model. it appears that the traditional fwi does not optimize well in the low velocity
layer case after 40 iterations ended with a high loss value, which falls into a local trap.
the autoencoder-enhanced fwi discovers the low velocity layer very well and continues to optimize the misfit for all 100 iterations. the difference graphs also confirm that
the autoencoder case identifies all layers well showing less structured misfits. noticeably,

187

188

advances in subsurface data analytics

there are also less artifacts introduced in the autoencoder-enhanced fwi compared
with the traditional fwi.
as described in section 1.4, the automatic differentiation provided by the pytorch
software does not provide sufficient efficiency to solve the fwi 2d problem.the gradients calculated for the whole program takes too long and too much space to store them.
the hybrid method described in section 1.4.2 overcomes the problem by incorporating the adjoint state calculation. as the result, the gradient calculation using the hybrid
approach achieves both accuracy and efficiency, which is feasible for use in a large scale
scientific computation problem integrating with machine learning models.

7.6 discussion
there are a few points that worth noting for the work. the first is that the automatic
differentiation is key for differentiable programming, which can bridge the physicsbased scientific computing with the machine learning (ml)/artificial intelligence (ai)
technologies. ml/ai methods do not have physics principles built-in, which may create
an infeasible solution given the fact that most of the scientific inverse problems may
be ill-posed. in our prior work61, the convergence of ml with a scientific application
without differentiable programming may not find a generalized solution since optimizations of the two different methods are disconnected.
the second point is that the automatic differentiation needs additional improvements to make it feasible to other applications. in the present method, integrating the
adjoint-state method makes it feasible to solve a large case; however, the solution is an
approximation. if the automatic differentiation method can be more memory-efficient
and parallelizable, it can be much more useful to compute the exact gradients for the
large complex problems.
the last point is the deep learning model autoencoder requires a revisit to reduce
the loss during decoding. although it reduces the dimension by compressing the input
data into a sparse latent variable, the reconstruction is not lossless. there are some errors
introduced during the reconstruction process that may hinder the optimization process.
there is a trade-off to take into consideration when designing the convergence of ml/
ai with scientific computing.

7.7 conclusions
two cases have been demonstrated of restructuring the wave equation using finite
difference method in a deep learning rnn model framework and an autoencoder
enhanced fwi process. the benefits of the work include fully utilizing the highperformance tensor processing and optimization capabilities implemented in the deep
learning package pytorch, as well as the deep integration of machine learning models
with the inverse problem. by integrating an hpc application with a deep learning

applying scientific machine learning to improve seismic wave simulation and inversion

framework with differentiable programming, we can explore a large number of combinations of machine learning models with physical numerical solutions to achieve better
accuracy and efficiency.
moreover, the work can be extended to more applications in reservoir characterization, identification of sweet spots, and reservoir simulation. with the scientific principles
embedded in the machine learning, the method enhances the interpretability and reliability of machine learing models in the engineering fields.

7.8 acknowledgment
this research work is supported by the us national science foundation (nsf) awards ##1649788,
#1832034 and by the office of the assistant secretary of defense for research and engineering
(oasd(r&e)) under agreement number fa8750-15-2-0119. the u.s. government is authorized to
reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation
thereon. the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the
us nsf, or the office of the assistant secretary of defense for research and engineering (oasd(r&e))
or the u.s. government. this work used the extreme science and engineering discovery environment
(xsede), which is supported by national science foundation grant number aci-1548562. specifically,
it used the bridges system, which is supported by nsf award number aci-1445606, at the pittsburgh
supercomputing center (psc).

references
1. baker n, alexander f, bremer t, hagberg a, kevrekidis y, najm h, et al., workshop report on basic
research needs for scientific machine learning: core technologies for artificial intelligence. http://doi.
org/10.2172/1478744. https://www.osti.gov/biblio/1478744.
2. schuster g, seismic inversion, society of exploration geophysicists, 2017. doi: http://doi.
org/10.1190/1.9781560803423.
3. plessix r-e. a review of the adjoint-state method for computing the gradient of a functional
with geophysical applications. geophys j int 2006;167(2):495â€“503. http://doi.org/10.1111/j.1365246x.2006.02978.x.
4. adler j, oktem o, solving ill-posed inverse problems using iterative deep neural networks. https://
arxiv.org/pdf/1704.04058.pdf. accessed feb. 15, 2021.
5. virieux j, operto s. an overview of full-waveform inversion in exploration geophysics. geophysics
2009;74(6):wcc127â€“52. http://doi.org/10.1190/1.3238367.
6. moseley b, markham a, nissen-meyer t, fast approximate simulation of seismic waves with deep
learning. https://arxiv.org/pdf/1807.06873.pdf. accessed feb. 15, 2021.
7. baker n, alexander f, bremer t, hagberg a, kevrekidis y, najm h, et al., workshop report on basic
research needs for scientific machine learning: core technologies for artificial intelligence. http://doi.
org/10.2172/1478744.
8. baydin ag, pearlmutter ba, radul aa, siskind jm. automatic differentiation in machine learning: a
survey. j. mach. learn. res. 2017;18(1):5595â€“637.
9. poggio t, mhaskar h, rosasco l, miranda b, liao q. why and when can deep-but not shallownetworks avoid the curse of dimensionality: a review. int j autom comput 2017;14(5):503â€“19. http://
doi.org/10.1007/s11633-017-1054-2.
10. raissi m, perdikaris p, karniadakis g. physics-informed neural networks: a deep learning framework
for solving forward and inverse problems involving nonlinear partial differential equations. j comput
phys 2019;378:686â€“707. https://doi.org/10.1016/j.jcp.2018.10.045.

189

190

advances in subsurface data analytics

11. kharazmi e, zhang z, karniadakis ge, variational physics-informed neural networks for solving
partial differential equations (2019). arxiv: 1912.00873.
12. rackauckas c, ma y, martensen j, warner c, zubov k, supekar r, skinner d, et al., universal differential equations for scientific machine learning, arxiv preprint arxiv:2001.04385.
13. greydanus s, dzamba m, yosinski j, hamiltonian neural networks (2019). arxiv:1906.01563.
14. mattheakis m, protopapas p, sondak d, giovanni md, kaxiras e, physical symmetries embedded in
neural networks (2020). arxiv:1904.08991.
15. chen rtq, rubanova y, bettencourt j, duvenaud dk. neural ordinary differential equationsbengio s, wallach h, larochelle h, grauman k, cesabianchi n, garnett r. advances in neural
information processing systems, curran associates, inc. 2018:6571â€“83. https://proceedings.neurips.
cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-paper.pdf.
16. richardson a, seismic full-waveform inversion using deep learning tools and techniques. https://
arxiv.org/pdf/1801.07232v2.pdf. accessed feb. 15, 2021.
17. zhu w, xu k, darve e, beroza1 gc, a general approach to seismic inversion with automatic differentiation. https://arxiv.org/pdf/2003.06027.pdf. accessed feb. 15, 2021.
18. blanning rw. the construction and implementation of metamodels. simulation 1975;24(6):177â€“84.
http://doi.org/10.1177/003754977502400606.
19. oâ€™hagan a, bayesian analysis of computer code outputs: a tutorial, the fourth international
conference on sensitivity analysis of model output (samo 2004), reliab eng syst saf.
2006;91(10)1290â€“1300. https://doi.org/10.1016/j.ress.2005.11.025. url http://www.sciencedirect.com/science/article/pii/s0951832005002383.
20. willcox k, peraire j. balanced model reduction via the proper orthogonal decomposition. aiaa j
2002;40(11):2323â€“30. http://doi.org/10.2514/2.1570.
21. wilson kc, durlofsky lj. computational optimization of shale resource development using reducedphysics surrogate models, the spe western regional meeting. march 2012. bakersfield, california,
usa: society of petroleum engineers. https://doi.org/10.2118/152946-ms.
22. sethian ja. a fast marching level set method for monotonically advancing fronts. proc natl acad sci
1996;93(4):1591â€“5. http://doi.org/10.1073/pnas.93.4.1591.
23. robinson td, eldred ms, willcox ke, haimes r. surrogate-based optimization using multifidelity
models with variable parameterization and corrected space mapping. aiaa j 2008;46(11):2814â€“22.
24. regis rg, shoemaker c. constrained global optimization of expensive black box functions using
radial basis functions. j glob optim 2005;31:153â€“71.
25. sandvol e, seber d, calvert a, barazangi m. grid search modeling of receiver functions: implications for crustal structure in the middle east and north africa. j geophys res solid earth
1998;103(b11):26899â€“917. http://doi.org/10.1029/98jb02238.
26. sen mk, stoffa pl. nonlinear one-dimensional seismic waveform inversion using simulated annealing. geophysics 1991;56(10):1624â€“38.
27. zhao l-s, sen mk, stoffa p, frohlich c. application of very fast simulated annealing to the determination of the crustal structure beneath tibet. geophys j int 1996;125(2):355â€“70.
28. press f. earth models obtained by monte carlo inversion. journal of geophysical research (1896-1977)
1968;73(16):5223â€“34.
29. fernÃ¡ndez martÃ­nez jl, garcÃ­a gonzalo e, fernÃ¡ndez Ã¡lvarez jp, kuzma ha, menÃ©ndez pÃ©rez co.
pso: a powerful algorithm to solve geophysical inverse problems: application to a 1d-dc resistivity case.
j appl geophys 2010;71(1):13â€“25. https://doi.org/10.1016/j.jappgeo.2010.02.001.
30. kramer ma. nonlinear principal component analysis using autoassociative neural networks. alche
j 1991;37(2):233â€“43.
31. moseley b, nissen-meyer t, markham a. deep learning for fast simula-tion of seismic waves in
complex media. solid earth 2020;11(4):1527â€“49. http://doi.org/10.5194/se-11-1527-2020.
32. van den oord a, dieleman s, zen h, simonyan k, vinyals o, graves a, kalchbrenner n, et al.,
wavenet: a generative model for raw audio, arxiv e-prints (2016) arxiv: 1609.03499arxiv:1609.
03499.
33. nair v, hinton ge. rectified linear units improve restricted boltzmann machines. in: proceedings
of the 27th international conference on international conference on machine learning, icmlâ€™10,
omnipress, madison, wi, usa 2010:807â€“14.

applying scientific machine learning to improve seismic wave simulation and inversion

34. gao z, li c, liu n, pan z, gao j, xu z. large-dimensional seismic inversion using global optimization with autoencoder-based model dimensionality reduction. ieee trans geosci remote sens
2020:1â€“5. https://doi.org/10.1109/tgrs.2020.2998035.
35. storn r, price k. differential evolution â€“ a simple and efficient heuristic for global optimization over
continuous spaces. j. glob optim 1997;11(4):341â€“59. http://doi.org/10.1023/a:1008202821328.
36. cheny, schuster gt. seismic inversion by newtonian machine learning. geophysics 2020;85(4):wa185â€“
200.
37. sun b, alkhalifah t, ml-descent: an optimization algorithm for fwi using machine learning,
geophysics 85:r477-r492. arxiv: https://doi.org/10.1190/geo2019-0641.1.
38. kingma dp, welling m, auto-encoding variational bayes, corr abs/1312.6114.
39. versteeg r. the marmousi experience: velocity model determination on a synthetic complex data
set. the leading edge 1994;13(9):927â€“36.
40. appendix b. the seg-eage salt data set, 2012, pp. 217â€“8. arxiv: https://library.seg.org/doi/
pdf/10.1190/1.9781560801689.appendixb.
41. mosser l, dubrule o, blunt mj, stochastic seismic waveform inversion using generative adversarial
networks as a geological prior (2018). https://arxiv.org/abs/1806.03720. accessed feb 15, 2021.
42. arjovsky m, chintala s, bottou l, wasserstein gan (2017). https://arxiv.org/abs/1701.07875.
accessed feb. 15, 2021
43. gulrajani i, ahmed f, arjovsky m, dumoulin v, courville a, improved training of wasserstein gans
(2017). https://arxiv.org/abs/1704.00028. accessed feb. 15, 2021.
44. tolstikhin i, bousquet o, gelly s, scholkopf b, wasserstein autoencoders. https://arxiv.org/
pdf/1711.01558.pdf. accessed feb. 15, 2021.
45. roberts go, rosenthal js. optimal scaling of discrete approximations to langevin diffusions. j r stat
soc: series b statmethodol 1998;60(1):255â€“68.
46. swischuk r, mainini l, peherstorfer b, willcox k. projection-based model reduction: formulations
for physics-based machine learning. comput fluids 2019;179:704â€“17. http://doi.org/10.1016/j.
compfluid.2018.07.021.
47. baydin ag, pearlmutter ba, radul aa, automatic differentiation in machine learning: a survey,
corr abs/1502.05767. arxiv:1502.05767. http://arxiv.org/abs/1502.05767. accessed feb. 15,
2021.
48. margossian cc, a review of automatic differentiation and its efficient implementation, corr
abs/1811.05031. arxiv:1811.05031. http://arxiv.org/abs/1811.05031. accessed feb. 15, 2021.
49. innes m, edelman a, fischer k, rackauckas c, saba e, shah vb, et al., a differentiable programming system to bridge machine learning and scientific computing, corr abs/1907.07587.
arxiv:1907.07587. http://arxiv.org/abs/1907.07587. accessed feb. 15, 2021.
50. bezanson j, edelman a, karpinski s, shah vb. julia: a fresh approach to numerical computing. siam
rev 2017;59(1):65â€“98. https://doi.org/10.1137/141000671.
51. zhu w, xu k, darve ef, beroza g, a general approach to seismic inversion with automatic differentiation, arxiv: computational physics. https://arxiv.org/abs/2003.06027. accessed feb. 15, 2021.
52. xu k, li d, darve ef, harris jm, learning hidden dynamics using intelligent automatic differentiation, arxiv abs/1912.07547. https://arxiv.org/abs/1912.07547. accessed feb. 15, 2021.
53. hochreiter s, schmidhuber j. long short-term memory. neural comput 1997;9(8):1735â€“80.
54. chung j, gulcehre c, cho k, bengio y, empirical evaluation of gated recurrent neural networks
on sequence modeling (2014). arxiv:1412.3555. https://arxiv.org/abs/1412.3555. accessed feb. 15,
2021.
55. kingma dp, ba j, adam: a method for stochastic optimization (2014). arxiv:1412.6980. https://arxiv.
org/abs/1412.6980. accessed feb. 15, 2021.
56. hewett rj, demanet l, team tp, pysit: python seismic imaging toolbox (2020). http://doi.
org/10.5281/zenodo.3603367. accessed feb. 15, 2021.
57. tromp j, komatitsch d, liu q. spectral-element and adjoint methods in seismology. commun comput
phys 2008;3(1):1â€“32.
58. ruder s, an overview of gradient descent optimization algorithms (2016). arxiv:1609.04747.
https://arxiv.org/abs/1609.04747. accessed feb. 15, 2021.

191

192

advances in subsurface data analytics

59. zeiler md, adadelta: an adaptive learning rate method (2012). arxiv: 1212.5701. https://arxiv.org/
abs/1212.5701. accessed feb. 15, 2021.
60. duchi j, hazan e, singer y. adaptive subgradient methods for online learning and stochastic optimization. j. mach. learn. res. 2011;12:2121â€“59.
61. huang l, polanco m, clee te. initial experiments on improving seismic data inversion with
deep learning. new york scientific data summit (nysds) 2018;2018:1â€“3. http://doi.org/10.1109/
nysds.2018.8538956.



an empirical analysis of the influence of seismic data modeling for
estimating velocity models with fully convolutional networks
luan rios campos
manufacturing and technology integrated campus â€“ senai cimatec
salvador, bahia 41650-010, brazil
peterson nogueira
manufacturing and technology integrated campus â€“ senai cimatec
national institute of science and technology for geophysics of petroleum - ufba
salvador, bahia 41650-010, brazil
davidson moreira
manufacturing and technology integrated campus â€“ senai cimatec
salvador, bahia 41650-010, brazil
erick giovani sperandio nascimento
manufacturing and technology integrated campus â€“ senai cimatec
salvador, bahia 41650-010, brazil

abstract
seismic modeling is the process of simulating wave
propagations in a medium to represent underlying structures of
a subsurface area of the earth. this modeling is based on a set of
parameters that determine how the data is produced. recent
studies have demonstrated that deep learning methods can be
trained with seismic data to estimate velocity models that give a
representation of the subsurface where the seismic data was
generated. thus, an analysis is made on the impact that different
sets of parameters have on the estimation of velocity models by
a fully convolutional network (fcn). the experiments varied
the number of sources among four options (1, 10, 25 or 50 shots)
and used three different ranges of peak frequencies: 4, 8 and 16
hz. the results demonstrated that, although the number of
sources have more influence on the computational time needed
to train the fcn than the peak frequency, both changes have
significant impact on the quality of the estimation. the best
estimations were obtained with the experiment of 25 sources
with 4 hz and increasing the peak frequency to 8 hz improved
even more the results, especially regarding the fcnâ€™s loss
function.
keywords: deep learning, geophysics, velocity model
estimation, seismic data analysis, fully convolutional
networks.

1. introduction
the exploration of subsurfaces of the earth is an expensive
process. the first step is to place sources and receivers along a
certain area and then propagate waves from one equipment to be
recorded by the other. this process generates seismograms that

26

systemics, cybernetics and informatics

have much information of the structures underneath the region
where the acquisition was made. the understanding of these
seismic data may lead oil and gas companies to more assertively
drill an area that may contain, for example, petroleum. however,
two problems arise: the raw data by itself does not provide such
kind of detailed information and they are too big and complex to
be analyzed by humans.
in this scenario, computer simulations aim to replicate the
process of seismic data modeling so certain methods, such as the
reverse time migration (rtm) or full-waveform inversion
(fwi), can be used. these techniques try to alleviate the
aforementioned problems when a subsurface section is
investigated. the rtm is a method that outputs an image where
it is possible to identify the underlying structures of a subsurface,
whilst the fwi is an iterative method that tries to solve a
nonlinear inversion problem to output a high-resolution model
of velocities of the subsurface. the latter complements the
former, since its output is an input for the other, and both
methods require a signal representation of the subsurface in
order to operate, i.e., the seismic data. moreover, the fwi also
needs an initial velocity model in order to produce a new one
with higher resolution. offering an optimal initial velocity
model to the fwi can diminish the computational power
required to perform the method, increase its convergence rate by
avoiding local minima and produce a high-resolution velocity
model.
in the geophysics literature there are methods that help the
production of initial velocity models. authors such as [2] and [3]
have, respectively, studied the use of reflection tomography and
migration-based velocity analysis for such tasks. there are also
approaches that consider the use of global methods such as
genetic algorithms [4] and simulated annealing [5]. however,

volume 17 - number 4 - year 2019

issn: 1690-4524

the first two methods pose as a high time-consuming task and
the last two demand more computational resources as the
subsurface being analyzed increases in size, since they will
require a larger population and, consequently, more modeling
steps to carry the search on. more recently, researchers have
been experimenting the use of deep learning techniques to solve
geophysics problems [6], including seismic inversion [7] [8] [9]
[10]. as far as it is of our concern, the first use of a fully
convolutional network (fcn) for the velocity model estimation
problem was addressed by [11], on which the fcn is trained
with the seismic data of 1000 velocity models and tested with 20
examples of seismic data not seen during training. the seismic
data was generated with sources and receivers placed on both the
top and the bottom layers of the subsurface, which characterizes
a well log seismic acquisition. the work of [12] shows how the
same network used by [11] can be applied to a more
conventional seismic acquisition, where the sources and
receivers are positioned only on the top of the subsurface.
none of the works previously mentioned address the
consequences of changing the number of sources or the peak
frequency when training a deep learning method, except for [11],
which compares only the case of seismic data with 1 and 10
shots. this comparison led the authors to conclude that training
the fcn with 10 shots not only offers better results than using
only 1 shot, but also contributes to reduce overfitting.
the goal of this study is to empirically analyze how the seismic
data generated from synthesized velocity models can influence
the estimation of such models using a fcn. this can contribute
to the oil and gas industry by either demonstrating that deep
learning methods may not necessarily require a high number of
seismic shots, as it happens with other techniques, in order to be
able to estimate a comprehensible velocity model, which can
lead to reduce the expense to simulate, store and process nonsynthetic seismic data, or offering a technique that perhaps is
less sensitive to higher frequencies. moreover, a brief
comparison with the results of the experiments of different shots
made by [11] is carried in this work. however, it is important to
state that, because the scheme to generate the seismic data and
the velocity models used in this work differ from the ones used
by [11], their results are only discussed, not reproduced.
the experiments discussed here consider a finite-differences
approach for the seismic modeling and alterations on some of its
parameters, such as the number of sources and peak frequency,
with the former varying from one central shot to 10, 25 and 50
equally spaced shots and the latter varying from 4 to 8 and 16
hz. the seismic data is generated with basis on the same dataset
of velocity models independently of changes on the modeling
parameters, which consequently yields the same training and
testing dataset throughout the entire analysis with modifications
only on the resolution of the seismic data due to the differences
of parameters.
the analysis is twofold: to compare the graphical results of the
estimated velocity models of each experiment made as well as
their metrics obtained after the fcn is completely trained.
analyzing the metrics can offer a statistical and more precise
evaluation of the results obtained after training the neural
network, since only a graphical analysis can mislead the
interpretation of how changing the modeling parameters effects
on the neural network training.

issn: 1690-4524

this study is organized as follows: the following section
presents the mathematical and physical theory behind the
seismic modeling; section three briefly presents the importance
of velocity models; section four overviews fully convolutional
networks applications and theory; section five describes the
methodology and experiments; in section six a discussion of the
results obtained with the experiments is made; and section seven
concludes this work and points new directions of research based
on the results obtained.

2. seismic modeling
seismic modeling simulates the process of propagating waves
on a subsurface area. this is done so researches can advance on
processes that aid the understanding of subsurface areas prior to
going into expeditions to them. this section is dedicated to
briefly present some of the equations considering the modeling
via the acoustic wave equation.
1

ğœ•2 ğ‘ƒğ‘  (ğ‘¥,ğ‘¡)

ğ‘£(ğ‘¥)2

ğœ•ğ‘¡ 2

âˆ’ âˆ‡2 ğ‘ƒğ‘  (ğ‘¥, ğ‘¡) = ğ‘ (ğ‘¥, ğ‘¡)

(1)

the acoustic wave equation [13] [14] is described by eq. (1), of
which ğ‘¥ = (ğ‘¥â€², ğ‘§â€²) is the position on the subsurface for a 2d
representation, ğ‘£(ğ‘¥) is the velocity at a given position, ğ‘ƒğ‘  (ğ‘¥, ğ‘¡)
is the source wave field and ğ‘ (ğ‘¥, ğ‘¡) defines the seismic source of
the acoustic wave. eq. (2) denotes the second spatial derivatives,
i.e., the laplacian operator (ïƒ‘Â²), for the two-dimensional case
as:
âˆ‡2 =

ğœ•2
ğœ•ğ‘¥ â€²2

+

ğœ•2
ğœ•ğ‘§ â€²2

(2)

one way to perform the seismic modeling is with the finitedifferences method [14], which offers a simple and easy
implementation [15] through the taylor series, consequently
leading to a discretization of the equations. both eq. (1) and eq.
(2) can be expanded by a taylor series, but some conditions must
be met in order to avoid the numerical dispersion and instability
that may arise when discretizing a continuous-time equation
[15].
on one hand, eq. (3) [15] denotes the conditions to avoid the
numerical instability of a 2d model, on which ï„ğ‘¡ is the time
sampling interval, ğ‘šğ‘ğ‘¥(ğ‘£) is the maximum velocity of the
model, ï„ğ‘¥â€² and ï„ğ‘§â€² are the spatial sampling interval respectively
on the x and z axes.
1

Î´ğ‘¡ â‰¤

max(ğ‘£)âˆš

1
1
+
Î´ğ‘¥â€²2 Î´ğ‘§â€²2

(3)

on the other hand, eq. (4) [15] illustrates the conditions to avoid
the numerical dispersion problem of a bi-dimensional model:
ğ‘“ğ‘šğ‘ğ‘¥ is the maximum value of frequency allowed so the
dispersion does not occur considering a given model, i.e., its
maximum spatial sampling interval (ğ‘šğ‘ğ‘¥(ï„ğ‘¥â€², ï„ğ‘§â€²)) and its
minimum velocity (ğ‘šğ‘–ğ‘›(ğ’—)). the parameter ğ¹ is constant
according to the order used for the taylor series and it decreases
as the order increases.

systemics, cybernetics and informatics

ğ‘“ğ‘šğ‘ğ‘¥ =

1

min(ğ‘£)

ğ¹ max(Î´ğ‘¥ â€² ,Î´ğ‘§ â€² )

volume 17 - number 4 - year 2019

(4)

27

the peak frequency (ğ‘“ğ‘ğ‘’ğ‘ğ‘˜ ) is defined as approximately half of
the max frequency (eq. (5)) and represent the point of the
spectrum of frequency with maximum amplitude.
ğ‘“ğ‘ğ‘’ğ‘ğ‘˜ =

ğ‘“ğ‘šğ‘ğ‘¥

(5)

2.3

the information generated by the simulated wave propagation is
translated into the seismic data, which corresponds to the values
of transit time of the wave, the amplitudes and the phase of the
events. the seismic data varies and respects undulation
phenomes such as reflection, refraction and transmission

3. velocity model
a velocity model offers a representation of the structures present
in a subsurface based on the velocity of propagation of the waves
emitted from the sources and recorded by the receivers that are
placed on the surface when the seismic data is being modeled.
this is because the velocity of propagation directly depends on
the type of medium through which a wave travels. therefore, it
is possible to determine a structure, i.e., rock, water, salt body,
etc., according to its velocity.
as said before, there are different approaches to handle the initial
velocity model problem in the geophysics literature and an
optimal model can help when applying the full-waveform
inversion. these models, however, are said to be smoothed
(figure 1a) and, although they can display an initial guess of the
velocities of the subsurface, they lack details on its structural
composition. in that sense, estimated models that have their
structures clearly identified by their velocity values and are
highly correlated to their ground-truth (figure 1b) counterpart
are known as high resolution models.

a)

b)

figure 1 - an example of a) smoothed and b) ground-truth
velocity models

4. fully convolutional networks
convolutional neural networks (cnn) were firstly introduced
by [16] as an option for recognizing handwritten digits from the
u.s. postal service. later it was proved that cnns can handle,
besides images, speech and time-series problems [17]. in the
recent years, deep learning has gained even more importance,
especially after the imagetnet contest in 2012 and the
development of alexnet [18]. since then, different proposals of
deep learning methods with cnns have been made, including
the fully convolutional networks (fcns).
the first proposition of use of an fcn was for handling semantic
segmentation problems [19], which is the task of segmenting an

28

systemics, cybernetics and informatics

image into parts and classifying those parts into one of the
predetermined classes.
eq. (6) demonstrates the operation of the basic components of
cnns as [19] point out. in this case, ğ‘¥ğ‘–ğ‘— is the data vector, ğ‘¦ğ‘–ğ‘— is
the next layer, ğ‘˜is the size of the kernel, ğ‘ the subsampling factor
and ğ‘“ğ‘˜ğ‘  defines the type of the layer (convolution, pooling or
activation function). therefore, [19] nominate cnns that
contain only layers ruled by eq. (6) as fully convolutional or
deep filter, since, differently from conventional approaches that
use cnns, the fcn does not contain fully connected (dense)
layers, producing with its operations a nonlinear filter instead of
a nonlinear function and reducing the number of parameters,
computational time and dependency of the size of the image.
ğ‘¦ğ‘–ğ‘— = ğ‘“ğ‘˜ğ‘  ({ğ‘¥ğ‘ ğ‘–+Î´i,sj+Î´j }0â‰¤Î´ğ‘–,Î´ğ‘—â‰¤ğ‘˜ )

(6)

5. methodology and experiments
in this section we describe the methodology and experiments of
this work. firstly, the synthetic velocity models are presented
with details regarding their construction such as number of
layers, minimum and maximum velocities and other
characteristics. then, the description moves on how the seismic
data is generated and how the fcn is configured to handle it as
inputs and estimate velocity models.
the velocity models and seismic data are both synthetic and they
are built in different occasions. we first generate 1020 random
velocity models and then we apply the finite-differences seismic
modeling on each one of the recently-generated velocity models
to create its corresponding seismic data.
the subsurface area being represented by the synthetic velocity
models is a marine region of 3000 m in length by 3000 m in
depth. the models are two-dimensional grids of 150 samples on
both x (ğ‘›ğ‘¥â€²) and z (ğ‘›ğ‘§â€²) axes and their number of layers vary
from 8 to 12 layers, of which the first layer represents a water
blade of 100 m deep and velocity of 1500 m/s. subsequent layers
have their depth randomly defined and their velocity is
incremented (ğ‘‰ğ‘–ğ‘›ğ‘ğ‘Ÿ ) in a crescent order, from the first layer
velocity onwards, depending on how many layers (ğ‘›) the model
has and on its maximum (ğ‘‰ğ‘šğ‘ğ‘¥ = 3500 ğ‘š/ğ‘ ) and minimum
(ğ‘‰ğ‘šğ‘–ğ‘› = 1500 ğ‘š/ğ‘ ) velocities (eq. 7), e.g., if the model has 12
layers, then the velocity will be incremented in 166,66 m/s at
each layer. furthermore, the models can have their layers
inclined, undulated or containing fault structures. figure 2
displays an example of such model.
ğ‘‰

ğ‘‰ğ‘–ğ‘›ğ‘ğ‘Ÿ = ğ‘šğ‘ğ‘¥

âˆ’ğ‘‰ğ‘šğ‘–ğ‘›
ğ‘›

(7)

the seismic modeling is conducted on two fronts and it
considers an arrangement of sources and receives as used by
[12], i.e., they are simulated as they were placed on the top of
the subsurface. the first front is to make different modeling
changing only the number of sources and fixing a low frequency
of 4 hz. since the sources are positioned on points (ğ‘¥â€², ğ‘§â€²) of the
subsurface, by decreasing their quantity we might inflict on the
acquisition of information belonging to certain regions of the
given subsurface. hence, the goal is to analyze how the changes
on the number of sources will affect and how much of the
velocity model the fcn can estimate.

volume 17 - number 4 - year 2019

issn: 1690-4524

the experiments of the second front consider modifications of
the frequency using three different bands: 4 hz, 8 hz and 16 hz.
as said before, the frequency is important to avoid the numerical
dispersion that may occur when calculating the acoustic wave
equation through the taylor series. the frequency of 16 hz is
the frequency of peak obtained from eq. (5) after calculating the
maximum frequency needed to avoid such dispersion when
applying the parameters depicted in table 1 in eq. (4) using ğ¹ =
2 as we considered a 32-order finite-differences. it is safe to say
that any value below this threshold does not disperse the wave
equation modeling, whereas frequencies above it disperse.

figure 2 - a synthetic velocity model containing 10 layers,
undulations, inclinations and fault structures
the frequency influences on how much of detail of the structures
the modeling will be able to capture. it is expected that by
lowering the frequency, the seismic data becomes smoother and
consequently the non-linearity of the problem is decreased.
hence, the values picked for the experiments represent low-band
(4 hz), medium-band (8 hz) and high-band (16 hz) frequencies
and aim to aid the understanding of how different bands can
determine the level of details of the estimated models.

gradually reduces the size of the image at the same time it
determines what are the features of the input data. the decoder
also has convolution layers, but the max-pooling are replaced by
up sampling layers. this results in an increasing of the image
size, to match the original image, and consequent localization of
the features identified during encoding.
this study relies on the same u-net proposed by [11], having
the same quantity of layers and the same number of filters on
each convolutional layer. however, two major changes were
made in order to improve the results. firstly, the stochastic
gradient descent (sgd) optimization function was replaced by
adamax [21], which computes adaptive learning rates for each
parameter and offers a more robust solution than the sgdâ€™s
fixed learning rate when training a neural network model.
secondly, the rectified linear unit (relu) activation function
was replaced by the parametric rectified linear unit (prelu).
by using the relu activation function one can avoid the
vanishing gradient problem that might occur in neural networks
trained with gradient-based optimizers, such as adamax.
however, such function has another issue called the dead relu,
which might compromise a netwok from learning since the
output of some of its neurons can be zero due to this functionâ€™s
nature (figure 3a). the prelu is an alternative to avoid this
issue as it learns to parameterize the negative inputs of the
neurons instead of assigning zeros to them as relu does (figure
3). a more detailed study demonstrating how these changes on
the activation function and optimizer leverage better velocity
model estimations can be seen in the work of [22].

table 1 - fixed parameters considered when modeling the
synthetic velocity models
a)
parameter

value

ğ‘›ğ‘¥â€²

150 samples

Î´ğ‘¥â€²

20 m

ğ‘›ğ‘§â€²

150 samples

Î´ğ‘§â€²

20 m

ğ‘›ğ‘¡

1500 samples

Î´ğ‘¡

0.002 s

b)

figure 3 - plot showing how the a) relu and b) prelu
activation functions work
the fcn is trained for 200 epochs with a batch size of 2 on 80%
of the total of seismic data generated, saving 20% for the testing
stage. the testing dataset is a portion of the original dataset
unknown to the fcn, i.e., that has never been presented to it
during the training phase, so it can offer an unbiased analysis of
the modelâ€™s performance. the batch size is small due to the size
of the input and, although it could increase as the number of
sources used during modeling decreases, since less sources
means a reduction of size of the seismic data, it was kept
unchanged throughout all experiments.

the fcn implementation takes the seismic data previously
described as input and tries to estimate the velocity model
corresponding to the input by minimizing the error between the
estimated model and the ground-truth that generated the seismic
data.

the evaluation of the fcn is made based on five different
metrics with respect to the testing dataset: mean squared error
(mse), which is also the loss function, mean absolute error
(mae), coefficient of determination (rÂ²), pearsonâ€™s coefficient
of correlation (r) and factor of two (fac2).

the work of [11] proposes the use of a u-net [20] to perform
the inversion of a seismic data into a velocity model. this fcn
consists of two parts: an encoder and a decoder. the encoder is
composed of convolution and max-pooling layers, which

in this context, the mse (eq. (8)) measures how far an estimated
model is from its respective ground-truth model. the bigger the

issn: 1690-4524

systemics, cybernetics and informatics

volume 17 - number 4 - year 2019

29

differences between one output and its corresponding target, the
greater the penalization and, consequently, the associated error.
ğ‘€ğ‘†ğ¸ =

1
ğ‘

âˆ‘ğ‘
Ì‚ğ‘˜ )2
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ ğ‘¦

(8)

the mae (eq. (9)) have lower values when compared to mseâ€™s
and indicates how much the difference of velocities between an
estimated and its ground-truth model vary, i.e., if the mae is of,
say, 100, it means the output have 100 m/s of average error
compared to the target.
ğ‘€ğ´ğ¸ =

1
ğ‘

âˆ‘ğ‘
Ì‚ğ‘˜ |
ğ‘˜=1|ğ‘¦ğ‘˜ âˆ’ ğ‘¦

the graphical results of the estimation of one ground-truth
model from the testing dataset can be seen in figure 3. the
ground-truth model contains undulated and inclined layers, and
a simple fault structure that is identified by the yellow ellipsis in
figure 4a. analyzing only the images leads to pointing out that
figure 4c, figure 4f and figure 4g obtained the best
representation of the ground-truth model because they contain
not only well-positioned layers, with identification of their
undulation and inclination, and a high precision of the velocities
on each layer, as the other estimations do, but also a fair
depiction of the fault structures.

(9)

the coefficient of determination (eq. (10)) indicates how better
the estimation is when compared to a baseline model - either ğ‘¦
or ğ‘¦Ì‚ variables of eq. (10).
ğ‘…2 =

Ì…)(ğ‘¦Ì‚ğ‘˜ âˆ’ğ‘¦Ì‚)]
[âˆ‘ğ‘
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ğ‘¦

2

(10)

2

2
âˆ‘ğ‘
Ì‚ ğ‘˜ âˆ’ğ‘¦Ì‚) âˆ‘ğ‘
ğ‘˜=1(ğ‘¦
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ğ‘¦)

a)

the pearsonâ€™s coefficient (eq. (11)) quantifies the linear
relationship between an estimated model and its ground-truth
counterpart, of which the value of -1 means opposite
correlations, 0 means no correlation at all and 1 means total
correlation
ğ‘Ÿğ‘¦ğ‘¦Ì‚ =

âˆ‘ğ‘
Ì‚ğ‘˜ âˆ’ğ‘¦Ì‚)
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ğ‘¦)(ğ‘¦
2

2 âˆšâˆ‘ğ‘ (ğ‘¦
âˆšâˆ‘ğ‘
Ì‚)
ğ‘˜=1(ğ‘¦ğ‘˜ âˆ’ğ‘¦)
ğ‘˜=1 Ì‚ ğ‘˜ âˆ’ğ‘¦

(11)

the factor of two (eq. (12)) determines how much of the
estimation can be considered an outlier.
ğ‘“ğ‘ğ‘2 = 0.5 â‰¤

ğ‘¦Ì‚ğ‘˜
ğ‘¦ğ‘˜

â‰¤2

b)

c)

d)

e)

f)

g)

(12)

the parameters from eq. (8) to eq. (12) are as follows: ğ‘ is the
size of the velocity model grid, ğ‘¦ğ‘˜ is the ğ‘˜ğ‘¡â„ velocity of the
ground-truth model (target), ğ‘¦Ì‚ğ‘˜ is the ğ‘˜ ğ‘¡â„ velocity of the fcnâ€™s
model (estimated output), ğ‘¦ is the mean of velocities of the target
output and ğ‘¦Ì‚ is the mean of velocities of the estimated output
6. results
an analysis and comparison of the experiments discussed
previously are carried along this section. both the experiments
with variations on the number of shots and peak frequency are
discussed and compared amongst them. it is important to restate
that the experiments with adjustments of the number of shots
were made with a peak frequency of 4 hz, whereas when
changes on the peak frequency occurs the number of shots is
fixed at 25. the comparison is made both graphically and with
regards to the metrics presented beforehand that are calculated
after the deep learning model is fully trained.
the statistical comparison is to give a more reliable analysis,
since considering only the estimated image of the velocity model
can mislead the interpretation of the results. in this case, the goal
is to minimize both the loss (mse) and mae metrics at the same
time it maximizes ğ‘…Â², ğ‘Ÿ and ğ‘“ğ‘ğ‘2 to values as close to 1 as
possible. besides the metrics, the time (in hours) taken to train
the model also composes the analysis.

30

systemics, cybernetics and informatics

figure 4 - a) ground-truth velocity model and graphical results
obtained with the experiments of b) 1 source, c) 10 sources, d)
25 sources and e) 50 sources all modeled with ğ‘“ğ‘ğ‘’ğ‘ğ‘˜ = 4 hz
and f) 8 hz and g) 16 hz both having 25 shots
on one hand, it is not safe to infer so straightforwardly that these
representations are the best because this velocity model
represents only one example of the entire testing dataset. this
may indeed be a case where the fcn models estimated an
optimal velocity model from the seismic data they were trained
by, but there may also exist cases that the estimations greatly

volume 17 - number 4 - year 2019

issn: 1690-4524

differ from their ground-truth models. on the other hand, this
analysis indeed validates the use of fcns to produce velocity
models from unknown seismic data.
once the graphical investigation of many examples is imprecise
and impractical, a quantitative evaluation of the statistical
indicators belonging to each one of the experiments is
conducted. these metrics are measured after the training phase
using the entire testing dataset. table 2 displays the metrics, the
corresponding time it took for the models to be trained and the
peak frequency for each experiment.
before venturing into the comparison of the metrics, an
association between the experiments and the computational time
is conducted. it is possible to see from table 2 that as the number
of shots increases, so it increases the computational time taken
to train the fcn. this happens because the number of shots have
a direct influence on the size of the seismic data as additional
shots mean adding matrices of size ğ‘›ğ‘¡ Ã— ğ‘›ğ‘¥â€² to the seismic data.
on the other hand, the peak frequency does not seem to have
much importance to the computational time. considering the
experiment of 25 shots in table 2, since the modeling had a peak
frequency of 4 hz, and comparing it with the time of the
experiments of 8 hz and 16 hz, as both have 25 shots, there is
no clear relation of computational time and higher or lower
frequencies. in fact, the result that achieved the lowest time is
the one with the highest frequency and the experiment with
medium frequency took the longest to train.
table 2 - results of the evaluation metrics and the time for
training (in hours) for each one of the experiments with
changes on the number of sources (shots) with fixed peak
frequency of 4 hz and on the frequency with a fixed 25 number
of shots
1 shot

10
shots

25
shots

25
shots

25
shots

50
shots

ğ’‡ğ’‘ğ’†ğ’‚ğ’Œ
(hz)

4

4

4

8

16

4

time
(h)

7.19

7.43

8.10

8.14

8.09

9.07

mse

14172

7313

6837

6126

7578

7207

mae

75.39

45.41

44.19

46.79

54.69

49.72

rÂ²

0.954

0.975

0.977

0.980

0.974

0.976

r

0.983

0.989

0.990

0.991

0.990

0.990

fac2

0.999

1.0

1.0

0.999

1.0

0.999

the evaluation metrics of each experiment, in general,
demonstrated close values, but it is possible to notice that
training the fcn with seismic data that have more shots does not
necessarily indicate a better estimation. even though the
experiment with 50 shots demonstrates valuable results, i.e., it
accomplished values close to 1 for the r, r2 and fac2, and
relatively low values for mae and mse, other experiments were
able to surpass it. in this case, both experiments with 10 and 25

issn: 1690-4524

shots obtained better values in all metrics, of which the latter
bested the former. moreover, the experiment with 25 shots could
be further improved when the modeling was made with 8 hz,
reaching the lowest value with the loss function (mse) and the
highest with ğ‘…Â² and ğ‘Ÿ metrics for all experiments. this, however,
happened at the expense of slightly decreasing the mae and
ğ‘“ğ‘ğ‘2 metrics to values below the experiment of 25 shots and 4
hz.
on the other hand, neither reducing much the number of shots
nor increasing even more the peak frequency mean improvement
on the estimation either. the worst results belong to the
experiment with the central shot. in this case, the values of the
metrics ğ‘…Â², ğ‘Ÿ and ğ‘“ğ‘ğ‘2, though show little differences from the
same metrics of the other experiments, were the lowest and the
mse and mae were the highest amongst all. moreover, the
experiment with 16 hz resulted in worst metrics than the one
with 50 shots.
although having the worst metrics, the fcn successfully
inverted a seismogram of one shot into a velocity model. this
possibly happened due to the size of the subsurface and the
velocity model, which are considered small from a geophysics
perspective, but this cannot be confirmed to happen as the
subsurface becomes larger considering only the analysis made
in this work.
hence, considering the extent of the experiments conducted in
this work, it is possible to conclude that the fcn not only can
produce velocity models from unknown seismic data, but it can
also deliver high-resolution models. furthermore, the
parameters used to generate the seismic data, combined with the
size of the subsurface area and the size of its velocity model
representation, play an important role in determining how high
the modelâ€™s resolution is going to be.

7. conclusions
this work demonstrated how changing the number of sources
and peak frequency of the seismic modeling can affect the
training and evaluation of an fcn model that takes seismic data
as input to estimate 2d velocity models.
the experiments firstly fixed the peak frequency at 4 hz and
varied the number of shots amongst one central shot, 10, 25 and
50 shots and then fixed 25 shots and varied the peak frequency
to 8 and 16 hz. the results showed that the best metrics for the
fcn were obtained with the experiments of 10 and 25 sources
and increasing the peak frequency from 4 to 8 hz improved even
more the estimation, especially regarding the fcnâ€™s loss. when
the peak frequency was increased once again, the fcn reached
lower metrics than the experiment with 50 shots. nevertheless,
the worst results amongst all were obtained with the seismic data
produced by a single central shot. these results partially
contradict the affirmation made by [11], since the results were
indeed improved when increasing the number of shots from 1 to
10 and from 10 to 25 but they worsened when considering 50
shots. additionally, there is no clear evidence whether the
number of shots influences on the model overfitting or not. this
might have happened due to the size of the dataset used in their
work.
initial conclusions for the experiments addressed in this work
indicate that, depending on the size of the subsurface, training

systemics, cybernetics and informatics

volume 17 - number 4 - year 2019

31

the fcn with seismic data that have few shots is enough to
estimate a velocity model. however, as the size of the model and
subsurface increases, more shots may give a better
representation of the area. furthermore, the results imply that the
fcn is, up to a certain point, less sensitive to higher peak
frequencies as the results improved when the modeling was
changed from 4 to 8 hz, but they worsened when 16 hz was
considered.
in general, the results demonstrated to be valuable, since they
show the possibility of training deep learning models with
seismograms containing few shots and high frequencies to
estimate optimal velocity models.
further studies point to the need of analyzing whether few shots
are indeed enough to estimate velocity models of larger and
more complex subsurfaces. furthermore, improvements on the
training stage, such as mixing the dataset with low, medium and
high frequencies or substituting the max-pooling layers for
convolutional layers, can be made, and other deep learning
methods, such as generative adversarial networks (gan), may
be studied to determine whether they behave differently from the
fcn for the seismic inversion problem.

8. references
[1] carcione, jose m.; herman, gÃ©rard c.; ten
kroode, a. p. e. seismic modeling. geophysics, vol. 67,
no. 4, 2002, pp. 1304-1325.
[2] stork, christof. reflection tomography in the
postmigrated domain. geophysics, vol. 57, no. 5, 1992, pp.
680-692.
[3] al-yahya, kamal. velocity analysis by iterative profile
migration. geophysics, vol. 54, no. 6, 1989, pp. 718-729.
[4] sajeva, angelo et al. estimation of acoustic macro models
using a genetic full-waveform inversion: applications to the
marmousi model genetic fwi for acoustic macro models.
geophysics, vol. 81, no. 4, 2016, pp. r173-r184.
[5] datta, debanjan; sen, mrinal k. estimating a starting
model for full-waveform inversion using a global
optimization method. geophysics, vol. 81, no. 4, 2016, pp.
r211-r223.
[6] wang, wenlong; yang, fangshu; ma, jianwei
automatic salt detection with machine learning. in: 80th
eage conference and exhibition 2018. european
association of geoscientists and engineers, 2018.
[7] rÃ¶th, gunter; tarantola, albert. neural networks and
inversion of seismic data. journal of geophysical
research: solid earth, vol. 99, no. b4, 1994, pp. 67536768.
[8] lewis, winston; vigh, denes. deep learning prior models
from seismic images for full-waveform inversion. in: seg
technical program expanded abstracts 2017. society of
exploration geophysicists, 2017, pp. 1512-1517.
[9] araya-polo, mauricio et al. deep-learning tomography.
the leading edge, vol. 37, no. 1, 2018, pp. 58-66.
[10] wu, yue; lin, youzuo; zhou, zheng. inversionnet:
accurate and efficient seismic waveform inversion with
convolutional neural networks. in: seg technical
program expanded abstracts 2018. society of
exploration geophysicists, 2018, pp. 2096-2100.
[11] wang, wenlong; yang, fangshu; ma, jianwei.
velocity model building with a modified fully convolutional
network. in: seg technical program expanded

32

systemics, cybernetics and informatics

abstracts 2018. society of exploration geophysicists,
2018, pp. 2086-2090.
[12] campos, luan r.; nogueira, peterson; nascimento, erick.
estimating initial velocity models for the fwi using deep
learning. in: proceedings of the 16th international
congress of the brazilian geophysical society. brazilian
geophysical society.
[13] alford, r. m.; kelly, k. r.; boore, d. mt.
accuracy of finite-difference modeling of the acoustic wave
equation. geophysics, vol. 39, no. 6, 1974, pp. 834-842.
[14] baysal, edip; kosloff, dan d.; sherwood, john
wc. reverse time migration. geophysics, vol. 48, no. 11,
1983, pp. 1514-1524.
[15] dos santos, a. w. g. waveform inversion applied to
the analysis of seismic velocities using a multi-scale
approach. masterâ€™s thesis, universidade federal da bahia,
2013
[16] lecun, yann et al. handwritten digit recognition with a
back-propagation network. in: advances in neural
information processing systems, 1990, pp. 396-404.
[17] lecun, yann et al. convolutional networks for images,
speech, and time series. the handbook of brain theory and
neural networks, vol. 3361, no. 10, 1995, pp. 1995.
[18] krizhevsky, alex; sutskever, ilya; hinton,
geoffrey e. imagenet classification with deep convolutional
neural networks. in: advances in neural information
processing systems, 2012, pp. 1097-1105.
[19] long, jonathan; shelhamer, evan; darrell,
trevor. fully convolutional networks for semantic
segmentation. in: proceedings of the ieee conference on
computer vision and pattern recognition, 2015, pp. 34313440.
[20] ronneberger, olaf; fischer, philipp; brox,
thomas. u-net: convolutional networks for biomedical
image segmentation. in: international conference on
medical image computing and computer-assisted
intervention. springer, cham, 2015, pp. 234-241.
[21] kingma, diederik; ba, jimmy. adam: a method for
stochastic optimization. international conference on
learning representations, 2014.
[22] campos, luan r., nogueira, peterson, & nascimento,
erick. tuning a fully convolutional network for velocity
model estimation. in: offshore technology conference
brasil. offshore technology conference.

volume 17 - number 4 - year 2019

issn: 1690-4524



exploration geophysics

issn: (print) (online) journal homepage: www.tandfonline.com/journals/texg20

low-frequency marine seismic data reconstruction
based on the far-field signature using a modified
u-net
yunhui park, hye-jin moon & sukjoon pyun
to cite this article: yunhui park, hye-jin moon & sukjoon pyun (2024) low-frequency
marine seismic data reconstruction based on the far-field signature using a modified u-net,
exploration geophysics, 55:3, 263-276, doi: 10.1080/08123985.2024.2317129
to link to this article: https://doi.org/10.1080/08123985.2024.2317129

published online: 02 jul 2024.

submit your article to this journal

article views: 50

view related articles

view crossmark data

full terms & conditions of access and use can be found at
https://www.tandfonline.com/action/journalinformation?journalcode=texg20

exploration geophysics
2024, vol. 55, no. 3, 263â€“276
https://doi.org/10.1080/08123985.2024.2317129

low-frequency marine seismic data reconstruction based on the far-field
signature using a modified u-net
yunhui parka , hye-jin moona and sukjoon pyunb
a ocean climate response & ecosystem research department, korea institute of ocean science & technology, busan, south korea;
b department of energy resource engineering, inha university, incheon, south korea

abstract

article history

preserving low-frequency components in seismic data is challenging due to data acquisition
restrictions and the processing of low-cut filtering after surveys. if seismic data lack low frequencies, their resolution deteriorates, leading to inaccurate seismic interpretations. to resolve this
problem, we developed a low-frequency reconstruction that employs a modified u-net neural
network. to improve the training of the neural network, we generated training data analogous to
unseen data, based on the far-field signature as the wavelet source to retain the spectral characteristics of field data. we addressed potential overfitting by generating a large amount of various
synthetic data through wave equation-based modelling using a variety of velocity and density
models. after synthesising the seismic data, we implemented a filtering method to produce input
data with insufficient low frequencies and label data with sufficient low frequencies. the far-field
signature plays an important role in the successful reconstruction of low frequencies due to its
intrinsic field data features and greater low-frequency information compared to field data alone.
we tested the generalisation of the network using unseen synthetic and field data not used in the
training stage, and analyzed the results in the time and frequency domains. although the input
data did not retain frequencies below 10 hz, the trained network predicted low frequencies that
were similar to the desired data. we also produced post-stack sections via simple processing to
evaluate low-frequency reconstructions produced by our trained network. the low-frequency
reconstruction scheme led to a better understanding of subsurface media.

received 31 january 2024
accepted 6 february 2024

introduction
in seismic exploration, the acquisition of sufficient lowfrequency components has attracted great interest,
because they provide robust full waveform inversion
(fwi) and broadband seismic data with enhanced resolution (ten kroode et al. 2013). however, it is difficult to acquire sufficient offshore low-frequency seismic data due to the limitations of standard acquisition systems and low-cut filtering for noise removal
(fang et al. 2020). however, recent developments in
seismic acquisition systems have facilitated the recording of ultra-low-frequency components and acquisition of broadband seismic data. using ocean bottom
nodes (obns) and low-frequency seismic sources, lowfrequency data can now be obtained (brenders et al.
2022). however, because research groups frequently
exploit conventional seismic acquisition systems consisting of narrow-azimuth towed streamers and air
gun arrays, low-frequency acquisition remains challenging. in offshore seismic exploration, towed streamer
systems record important geological signals that are
often accompanied by noise (e.g. swell noise) in the
low-frequency band. to remove this noise, low-cut filtering is typically performed in the early stages of
contact yunhui park
korea

yunipark@kiost.ac.kr

Â© 2024 australian society of exploration geophysicists

keywords

band-limited data; deep
learning; far-ï¬eld signature;
low-frequency
reconstruction; modiï¬ed
u-net

seismic data processing, which creates an unavoidable
shortage of low frequencies. this lack of low frequencies
exacerbates reverberation, which in turn distorts signals
and degrades overall resolution. together, these phenomena complicate the accurate geological interpretation of subsurface structures. optimised problem-based
advanced imaging techniques such as fwi may not provide reliable solutions when the problem falls into local
minima.
prior to the advent of machine learning (ml), a variety of studies sought to improve the accuracy of lowfrequency predictions. for example, hu (2014) introduced beat-tone inversion, which extracts low frequencies from two different seismic wavefields. wu, luo,
and wu (2014) and hu et al. (2017) reconstructed
low frequencies using an envelope of signals. li and
demanet (2016) extracted low-frequency components
from higher frequencies using a phase-tracking method
suggested by li and demanet (2015). later studies successfully predicted lower frequencies using broadband
sources (wang and herrmann 2016; zhang et al. 2017).
however, these studies had a limited ability to
reconstruct realistic low-frequency components (sun
and demanet 2020). notably, it was difficult to make

korea institute of ocean science & technology, 385, haeyang-ro, yeongdo-gu, busan 49111, south

264

y. park et al.

connections between low and high frequencies and
to accurately report the behaviours of adjacent traces
in space (ovcharenko et al. 2019). to address these
problems, several novel ml-based approaches have
been developed. in the early stages of ml-based seismic research, ovcharenko et al. (2019) applied ml to
low-frequency extensions, synthesising common shot
gathers using random velocity models and then producing training datasets using fourier transform models. they extended the frequency bandwidth up to
0.25 hz using a convolutional neural network (cnn)
and applied the predicted low frequencies to fwi.
unlike ovcharenko et al. (2019), who adopted training datasets in the frequency-space domain, sun and
demanet (2020) selected timeâ€“space domain seismic
data as training datasets, and the frequencies of the
input and label data were 5â€“20 and 0.1â€“5 hz, respectively, such that the frequency bandwidths of the training datasets did not overlap. they demonstrated the
accuracy of their trained network by performing fwi
using the marmousi and bp 2004 benchmark models.
fang et al. (2020) also used timeâ€“space domain seismic
data for low-frequency extrapolation. however, unlike
sun and demanet (2020), the frequency bandwidths
of their training data overlapped: the input and label
patches had cut-off frequencies of 10 and 6 hz, respectively. their study was the first attempt to apply an mlbased low-frequency extension to unseen field data and
showed remarkable performance in predicting low frequencies in field data and presenting reliable results for
multi-scale fwi and least-squares reverse-time migration (fang et al. 2020). with few exceptions, other early
studies confined their test datasets to synthetic seismic
data, because when the training data consists of synthesised wavefields, the trained networks are unable
to explain actual physical phenomena or real-world
complexities, resulting in poor generalisation (aharchaou and baumstein 2020). to resolve this problem
and relay actual physics to ml, aharchaou and baumstein (2020) conducted an ml-based study using field
data acquired from obns. the training datasets were
created from obn data and the trained network was
verified using streamer data. however, unlike seismic
surveys that use only streamers, obn surveys require
a large budget, which makes them feasible only for a
few research groups. thus, it may be difficult to prepare
training datasets mirroring real-world characteristics for
the majority of generalised cases.
in this study, we developed a practical method
for streamer surveys, utilising the far-field signature
to reflect the characteristics of unseen data to training data. we used a wave equation-based modelling
scheme to synthesise training datasets that were
applied to train a modified u-net-based neural network (wang, xu, and zhou 2020). then, we validated
the trained network using field data and unseen data.
we provide detailed explanations of the generation of

training datasets for successful ml, along with brief
explanations of the velocity and density models used
to synthesise seismic data. we also introduce the usage
of the far-field signature, as the most important contribution to data generation proposed in this study. further, we explain the wave equation modelling scheme
and its two-dimensional (2d) time-domain, cell-based,
finite-difference method (fdm) for the generation of
training data, as well as the filtering method used to
divide the training data into input and label data. we
introduce the modified u-net and its hyperparameters
and explain the training results. finally, we provide synthetic testing and generalisation results, which were
used to verify the proposed ml method. in addition to
timeâ€“space domain seismic data, we also used spectral
data to evaluate the low-frequency extension. through
comparisons of post-stack sections with simple seismic
processing, we verified that low-frequency components
were properly constructed by the trained network.

method
training data generation
velocities, densities, and source wavelets were needed
to synthesise seismic data for use in training datasets.
to account for diversity, it was necessary to generate a large number of velocity models with various
distributions and without redundancy, in addition to
corresponding density models that were geologically
dependent on these velocities. to take the characteristics of the testing data (i.e. field data) into consideration during training data generation, it was necessary
to determine several parameters associated with sea
bottom depth, layer thickness, and subsurface structure. the source wavelets also required careful selection because they play a critical role in ensuring that
the training data accurately reflect the characteristics
of field data. the acquisition of correct source estimation from the obtained field data will contribute to the
generation of practical training datasets. rather than
relying on laborious source estimation, we took advantage of the far-field signature that was simultaneously
recorded during the acquisition of seismic data from
the north viking graben in the north sea (keys and
foster 1998). the far-field signature is suitable for producing realistic training datasets because it is acquired
using identical equipment (i.e. air gun arrays) under the
same environmental conditions in which the field data
were recorded, and contains lower frequency components than the field data. once the physical parameters (i.e. velocities, densities, and far-field signature)
were resolved, we solved the wave equation numerically to generate synthetic data and produced a large
number of input data without low frequencies, and
label data with low frequencies, through band-pass
filtering.

exploration geophysics

velocity and density models
to avoid bias in the training datasets, it was necessary to create a large number of numerical models
with various stratigraphic interfaces and velocity distributions without redundancy. we created numerical
2d velocity models with simple stratigraphy and folds
by partly following the method of wu et al. (2020),
which suggested the construction of three-dimensional
(3d) velocity models (choi and pyun 2021). thus, 1,000
different velocities were required to create the 1,000
unique common shot gathers that were used as training
data in this study (figure 1).
we also needed as many density models as velocity models. the mobil avo datasets provide well data
(measured p-wave, s-wave, and density logs) as well as
surface seismic data (hanitzsch et al. 1998). however,
it was beyond the scope of this study to calibrate density and/or establish the relationship between density
and velocity using limited data. therefore, we elected to
produce plausible training datasets with trivial effort. to
create density models, we adopted gardnerâ€™s equation

265

(gardner, gardner, and gregory 1974) to define empirical relationships between density and velocity, and set
the density of water at 1 g/cm3 .

far-field signature
the amplitude spectrum of the far-field signature
indicated that it contains low-frequency components
(figure 2(a)). because we produced synthetic data by
solving the wave equation numerically, high-frequency
components were not useful for the data generation stage. indeed, high frequencies created dispersion errors during wave equation modelling, such that
high-cut filtering was needed to generate a source signature with low-frequency components. considering
the dispersion analysis of our numerical modelling, we
selected a cut-off frequency of 30 hz for the source
wavelet (figure 2(c)). to address any concerns about
the potential loss of good data through the removal of
high-frequency components, we focused on a limited
bandwidth to improve the efficiency of training data

figure 1. automatically and randomly generated velocity models with diï¬€erent geological structures and distributions (wu et al.
2020; choi and pyun 2021).

figure 2. original far-ï¬eld signature (a) in the time domain and (b) its amplitude spectrum in the frequency domain and high-cut
ï¬ltered signature (c) in the time domain and (d) its amplitude spectrum in the frequency domain.

266

y. park et al.

generation and then recalled the data after predicting
low frequencies using ml. our efforts to avoid data loss
are described comprehensively below.

wave equation-based modelling with 2d
time-domain, cell-based fdm
to generate the training datasets, we applied the following 2d timeâ€“space domain acoustic wave equation:
1 âˆ‚ 2u
âˆ‚
=
2
k âˆ‚t
âˆ‚x



1 âˆ‚u
Ï âˆ‚x



âˆ‚
+
âˆ‚z



1 âˆ‚u
Ï âˆ‚z


+ f,

(1)

where k is the bulk modulus consisting of density and
p-wave velocity, and u, f , and Ï denote the pressure
wavefield, source signature, and density, respectively.
to solve the wave equation numerically, we adopted
cell-based fdm (min, shin, and yoo 2004; lee, min et al.
2008; lee, lim et al. 2009), which assigns physical properties in the cells and wavefields to their respective
points (figure 3).
when solving the wave equation using the conventional fdm for media with a heterogeneous density
model, it is usually necessary to differentiate densities
directly on interfaces where they vary. however, cellbased fdm has the advantage of direct differentiation
of densities because the properties are stored within the
cells (park and ha 2019). therefore, based on the cellbased fdm, equation (1) can be expressed as follows:
1
4



 l+1
ui,j âˆ’ 2uli,j + ulâˆ’1
1
1
1
1
i,j
+
+
+
2
2
2
2
2
Î´t
Ï1 v1
Ï2 v2
Ï3 v3
Ï4 v4




1
1 l
1
1
1 l
l
=
u
âˆ’
+
ui,j + uiâˆ’1,j
Î´x 2 Ï4 i+1,j
Ï4
Ï2
Ï2




1 l
1
1
1 l
1
l
u
âˆ’
+
+
u
u
+
i,j
Î´z2 Ï4 i,j+1
Ï4
Ï3
Ï3 i,jâˆ’1

+ fi,jl ,

(2)

where i and j are indices of x and z. we set the same
acquisition parameters as the field data to generate
training datasets.

filtering for training datasets
we implemented low-cut filtering, which erases frequencies below 10 hz to generate input data lacking low-frequency components. to avoid the gibbs
phenomenon, which distorts accurate interpretations,
tapering was applied from 10 to 25 hz (figure 4(a)).
unlike the input data, the label data do not require lowcut filtering because they should have sufficient low frequencies (figure 4(b)). thus, 800 training datasets and
200 validation datasets were generated.

modified u-net
we chose to use a modified architecture of the original u-net (ronneberger, fischer, and brox 2015) that
was suggested by wang, xu, and zhou (2020). compared to the original network, the modified u-net has
one fewer layer and nearly half the channel size. the
dimension and shape of our input images differed from
those generally used in ml, with 120 traces in the horizontal direction and 1,000 time samples in the vertical direction. a deeper network indicates stronger constraints, which makes the modified u-net a more suitable option for our rectangular-shaped training images
(figure 5). as explained above, timeâ€“space domain synthetic data (figure 4) were assigned to input and label
data. unlike label data, input data rarely have frequencies below 10 hz. therefore, to preserve the relative
amplitude characteristics, we used whole shot gathers
rather than patches, and selected the hyperparameters
listed in table 1.
training and validation loss curves during 30 epochs
are shown in figure 6. the loss function was based on
the root mean square error (rmse; table 1). the rmse
of both the training and validation datasets decreased
during the training stage until 30 epochs. unlike the
training error, the validation error did not decrease dramatically and almost converged at 30 epochs, indicating that the network was well trained within 30 epochs.

synthetic test

figure 3. schematic diagram of the cell-based ï¬nite-diï¬€erence
method (fdm). physical properties including density (Ï) and
velocity (v) are assigned to cells, and waveï¬elds (u) are calculated at grid points.

after the network is trained, the trained model must be
validated using unseen data that were not used during training. for the synthetic test, we used a small
portion of the marmousi-2 velocity and density data
(martin, larsen, and marfurt 2002), as shown in figure
7. the numerical models have complex geological features including huge fold structures, normal faults, and
unconformities.

exploration geophysics

267

figure 4. a pair of synthetic training data (common shot gathers). (a) input data. (b) label data. the input data were low-cut ï¬ltered
at 10 hz, whereas the label data have suï¬ƒcient low frequencies below 10 hz.

figure 5. modiï¬ed architecture of the original u-net (ronneberger, fischer, and brox 2015), suggested by wang, xu, and zhou
(2020). the modiï¬ed u-net is more appropriate for our data, with 120 horizontal and 1,000 vertical samples.
table 1. hyperparameter values for
predicting low-frequency content.
hyperparameter

value

learning rate
batch size
no. of epochs
loss function
optimizer

0.001
25
30
rmse
adam

because the low-frequency extension is highly
dependent on characteristics of the source wavelet,
we used the high-cut-filtered far-field signature (figure

2(c)) as the source wavelet. an unseen seismogram
was modelled using the 2d timeâ€“space domain acoustic wave equation and cell-based fdm. the input seismogram was generated using the same bandpass filtering method used for the training data. the input
data for the synthetic test is shown in figure 8(a),
and the predictions obtained using the trained model
are presented in figure 8(b). compared to the desired
data shown in figure 8(c), the overall resolution and
continuities were improved and waveforms became
discernible. figure 8(d) shows discrepancies between
the desired and predicted data. these differences are

268

y. park et al.

figure 6. loss curves for the training and validation data based on root mean square error (rmse).

from 15 to 28 hz were successfully reconstructed, and
some frequencies below 15 hz were also estimated.
there were inconsistencies in low-frequency extensions
when there was little low-frequency information in the
input data; however, the predicted data were generally
similar to the desired data.
to quantitatively assess the results, we calculated
the mean square error (mse), rmse, and mean absolute
error (mae) using input, predicted, and desired seismograms as follows:
1
(yÌ‚i âˆ’ yi )2 ,
n
n

mse =

(3)

i=1

n

rmse =

(yÌ‚i âˆ’ yi )2

i=1

n

,

(4)

and
1
|yÌ‚i âˆ’ yi |,
n
n

figure 7. small portion of the marmousi-2 (a) velocity and (b)
density data. source and receivers are indicated by a star and
circles, respectively.

primarily interpreted as ultra-low-frequency components of the events, which arose because actual ultralow frequencies are extremely difficult to reconstruct.
this phenomenon is also evident in the amplitude spectra shown in figure 9, which represents the amplitude
spectra of figure 8(a) without ultra-low frequencies.
figure 9(b) shows the amplitude spectra of figure 8(b)
with the low frequencies reconstructed and bandwidth
extended. figure 9(c) shows the amplitude spectra of
the desired data (i.e. figure 8(c)), and figure 9(d) shows
the differences between the predicted and desired
amplitude spectra. as shown in figure 9(d), the accuracy of the ultra-low frequency extension was limited;
however, frequencies in the range of 15â€“23 hz were
predicted without critical errors, and some frequencies
below 15 hz were also well estimated.
for inspection and comparison of the data, we calculated mean values for all traces (figure 10). amplitudes

mae =

(5)

i=1

where n is the number of pixels in the seismogram
(excluding negligible amplitudes); i is the index of the
pixel, such that yÌ‚i indicates the ith amplitude of the
desired seismogram, and yi indicates the ith amplitude
of input or predicted seismogram. table 2 shows the
mse, rmse, and mae values for the synthetic test. significant errors were caused by differences between the
input data and the desired data, whereas only minor
errors occurred when the predicted data were similar to
the desired data.

generalisation test
the field data used in the generalisation test were 2d
seismic data obtained from the north viking graben
in the north sea (keys and foster 1998; madiba and
mcmechan 2003). these data were released by exxonmobil and included 25 line km seismic data that were
used to compare seismic inversion schemes at the 1994
society of exploration geophysicists workshop (figure

exploration geophysics

269

figure 8. comparison of common shot gathers between (a) input data, (b) predicted data, and (c) desired data. (d) diï¬€erences
between the desired and predicted data.

11). we used approximately 4 s of data from the original
6 s recording.
figure 12 shows a variety of spectra compared to the
nearest trace shown in figure 11. the original spectrum
(black line) shows that the field data contain both highand low-frequency components. to ensure the generalizability of our low-frequency reconstruction method
using ml, we intentionally adopted both low-cut and

high-cut filtering (30 hz maximum frequency) for our
training data. as previously explained, the field data had
somewhat low frequencies; therefore, it was also possible to arrange the desired output data using the original
as a reference.
we generated a total of 1,001 input data containing frequencies from 10 to 30 hz (figure 13(a)). figure
13(b) shows the predicted results obtained using the

270

y. park et al.

figure 9. comparison of amplitude spectra between (a) input data, (b) predicted data, and (c) desired data. (d) diï¬€erences between
the predicted and desired data.

trained network. figure 13(c) shows the desired output,
including spectral characteristics that were illustrated as
the orange dotted line in figure 12. figure 13(a) lacks
very low frequencies, which made strong ringing artifacts more apparent. by contrast, figure 13(b), whose
low frequencies were reconstructed by ml, represents
a better shot gather, with reduced ringing artifacts
and increased dominance of important waveform signals that are discriminable in both near traces and faroffset traces. to evaluate the accuracy of the predicted

results, we compared these data with the desired output, which included actual low-frequency components
(figure 13(c)) by plotting their differences (figure 13(d)).
the predicted result was not identically reconstructed;
there were limitations in the ultra-low frequency extensions. however, a comparison with the input data
showed that the predicted results were still noteworthy.
the overall magnitude of amplitude on the predicted
seismogram was slightly lower than that of the original
data because the amplitude of the ultra-low frequencies

exploration geophysics

271

figure 10. comparison of amplitude spectra for mean values of the input data (black line), desired data (green dashed line), and
predicted data (red dotted line).
table 2. performance metrics calculated using the synthetic
data.

figure 11. common shot gather of two-dimensional seismic
data obtained from the north viking graben in the north sea,
used to test the generalisation of the trained network.

was less well reconstructed. despite the problem, the
predicted signals were significantly improved compared to the input data. for a different perspective,
we observed the results in terms of amplitude spectra
(figure 14). although the input data seldom contained
frequencies below 20 hz (figure 14(a)) due to low-cut filtering, they showed significant recovery (figure 14(b)).
compared to the amplitude spectra of the desired output (figure 14(c)) and the differences shown in figure

metric

input data

predicted data

mse
rmse
mae

0.0073072319
0.0854823510
0.0518436280

0.0015629645
0.0395343450
0.0260090010

14(d), frequencies from 15 to 20 hz on ml-predicted
traces were accurately recovered. although estimated
frequencies below 15 hz are rarely guaranteed, the
trends of the spectra in the frequencyâ€“space domain
approximated the desired result.
for more detailed inspection, we extracted the input,
desired, and predicted spectra from the traces and calculated their mean values (figure 15). the predicted
data showed frequencies similar to those of the input
data, with no large distortions above 25 hz and wellrecovered frequencies below 25 hz. actual frequencies
can be well estimated if they are present in the input
data; otherwise, frequency recovery becomes almost
impossible. the trained network also predicted ultralow frequencies below 5 hz; however, it was difficult
to assure their accuracy because the far-field signature,
which was used to generate label data, did not have sufficient frequency information below 5 hz. thus, our ml
method resulted in a fairly reasonable frequency spectrum despite poor low frequencies in the input data.

figure 12. comparison of amplitude spectra between original (black line), ï¬ltered (red dashed line), and desired nearest traces
(orange dotted line).

272

y. park et al.

figure 13. comparison of common shot gather (no. 79) between (a) input data, (b) predicted data, and (c) desired data. (d)
diï¬€erences between the predicted and desired data.

next, we evaluated the performance of the network
in terms of generalisation; the resulting mse, rmse, and
mae values are listed in table 3. in contrast to the synthetic test results (table 2), the errors did not converge
satisfactorily because the features of the field data are
much more complex.

table 3. performance metrics calculated using 2d seismic data
(shot gather no. 79) from the north viking graben in the north
sea.
metric

input data

predicted data

mse
rmse
mae

0.0039292928
0.0626840740
0.0290294980

0.0012996943
0.0360512730
0.0175647030

exploration geophysics

273

figure 14. comparison of amplitude spectra (shot gather no. 79) between (a) input data, (b) predicted data, and (c) desired data.
(d) diï¬€erences between the predicted and desired data.

to determine the impact of the extended low frequencies on seismic interpretations, we performed conventional seismic processing. first, we restored the high
frequencies, which were previously excluded, to their
correct input, desired, and predicted data points. after
this method was performed for all 1,001 shot gathers,
the spectral band became broader. unlike seismic processing typically conducted in industrial, commercial,

or in-depth research, the conventional seismic processing that we performed consisted of a few simple
tasks: direct wave muting, velocity analysis, surfacerelated multiple elimination, normal moveout correction, and fx migration. to confirm the accuracy of our
ml results and the versatility of our network, these processing methods were applied identically to the input,
desired, and predicted data and the three resulting

274

y. park et al.

figure 15. comparison of amplitude spectra between mean values of input data (black line), desired data (green dashed line), and
predicted data (red dotted line).

figure 16. comparison of post-stack sections derived from the input data (top), predicted data (middle), and desired data (bottom)
data, respectively.

exploration geophysics

275

figure 17. comparison of traces extracted from stack sections consisting of input data (black dotted line), predicted data (red dashed
line), and desired data (blue line).

post-stack sections were compared (figure 16). in the
first post-stack section, derived from input data without low frequencies, the overall amplitudes or events
were weak and geological structures were too ambiguous for accurate interpretation (figure 16(a)). this problem was caused by ringing effects in the shot gathers
that were reflected in the stack section. compared to
the input data, the second section, derived from the
predicted data, showed better continuity between geological structures, with considerably less ringing (figure
16(b)). thus, the low frequencies predicted using our ml
method contributed to significant improvement in geological interpretation compared to the desired section
(figure 16(c)).
as a more detailed inspection, we extracted traces
from each section to a horizontal distance of 1.5 km and
compared the results (figure 17). the input data lacked
low frequencies, whereas the desired data contained
sufficient low frequencies. although the ml-predicted
data did not perfectly reproduce the original data, their
trend converged with the desired data and discrepancies between the input and predicted data were more
conspicuous than those between the predicted and
desired data. thus, our proposed ml method accurately predicted low-frequency components, providing
opportunities for better interpretation and understanding of subsurface media that will improve the resolution
of seismic data and enhance the continuity of reflection
events.

conclusions
this study generated training datasets that enabled
successful ml-based low-frequency extensions using
wave equation-based modelling, a variant of subsurface modelling (i.e. velocity and density), and a far-field
signature containing shot excitation characteristics during the acquisition phase. the far-field signature played
an essential role in mitigating the chronic problem of
low-frequency data shortages and improved the resolution of unseen data. the far-field signature can be

obtained simultaneously with seismic data using the
same streamer system, depending on the acquisition
conditions. it provides more detailed low-frequency
data compared to multi-channel seismic data, and
therefore creates more satisfactory training data for
low-frequency extensions. however, it may be impossible to obtain a far-field signature with field data due
to suboptimal conditions (e.g. shallow water depth). in
such cases, it may be worthwhile to obtain the nearfield signature rather than the far-field signature for
the generation of proper training data. the near-field
signature also offers the opportunity to gather source
and low-frequency components, and does not produce ghost reflections, unlike the far-field signature.
thus, reasonable training data may still be obtained
using the near-field signature. instead of a source array
system with multiple sources, particularly if a single
source is shot, the near-field signature can be used
as a source wavelet for numerical modelling; next,
ml-based low-frequency extension and generalisation
would be required to validate the approach, and the
resulting data should be compared to far-field applications. these steps would allow future studies to verify the robustness of ml-based low-frequency extensions against ghosting effects when using the far-field
signature.

acknowledgements
this study was supported by the grant â€œstrengthening to analyze and assess marine environmental/ecosystem variabilities
in the surrounding seas of koreaâ€ from the korea institute of
ocean science and technology (pea0201).

disclosure statement
no potential conflict of interest was reported by the author(s).

funding
this study was supported by the grant â€œstrengthening to analyze and assess marine environmental/ecosystem variabilities
in the surrounding seas of koreaâ€ from the korea institute of
ocean science and technology (pea0201).

276

y. park et al.

references
aharchaou, m., and a. baumstein. 2020. deep learning-based
artificial bandwidth extension: training on ultrasparse obn
to enhance towed-streamer fwi. the leading edge 39 no.
10: 718â€“26. doi:10.1190/tle39100718.1.
brenders, a., j. dellinger, i. ahmed, e. dÃ­az, m. gherasim, h. jin,
m. vyas, and j. naranjo. 2022. the wolfspar experience with
low-frequency seismic source field data: motivation, processing, and implications. the leading edge 41 no. 1: 9â€“18.
doi:10.1190/tle41010010.1.
choi, w., and s. pyun. 2021. synthetic training data generation
for fault detection based on deep learning. geophysics and
geophysical exploration 24 no. 3: 89â€“97. doi:10.7582/gge.
2021.24.3.089.
fang, j., h. zhou, y. elita li, q. zhang, l. wang, p. sun, and
j. zhang. 2020. data-driven low-frequency signal recovery using deep-learning predictions in full-waveform inversion. geophysics 85 no. 6: a37â€“43. doi:10.1190/geo20200159.1.
gardner, g.f., l.w. gardner, and a.r. gregory. 1974. formation velocity and densityâ€”the diagnostic basics for stratigraphic traps. geophysics 39 no. 6: 770â€“80. doi:10.1190/1.
1440465.
hanitzsch, c., d. marion, a. tura, w.b. beydoun, d. jizba, s.
jin, and d.j. foster. 1998. avo migration/inversion analysis. in comparison of seismic inversion methods on a single
real data set: society of exploration geophysicists, ed. r. g.
keys and d. j. foster, 57â€“74. doi:10.1190/1.9781560802082.
ch1.
huâˆ—, w. 2014. fwi without low frequency data-beat tone
inversion. in seg technical program expanded abstracts 2014,
1116â€“20. society of exploration geophysicists. doi:10.1190
/segam2014-0978.1.
hu, y., l. han, z. xu, f. zhang, and j. zeng. 2017. adaptive multi-step full waveform inversion based on waveform
mode decomposition. journal of applied geophysics 139:
195â€“210. doi:10.1016/j.jappgeo.2017.02.017.
keys, r.g., and d.j. foster. 1998. comparison of seismic inversion methods on a single real data set. society of exploration
geophysicists. doi:10.1190/1.9781560802082.
lee, h.y., s.c. lim, d.j. min, b.d. kwon, and m. park. 2009.
2d time-domain acoustic-elastic coupled modeling: a cellbased finite-difference method. geosciences journal 13 no.
4: 407â€“14. doi:10.1007/s12303-009-0037-x.
lee, h.y., d.j. min, b.d. kown, and h.s. yoo. 2008. timedomain elastic wave modeling in anisotropic media using
cell-based finite-difference method. journal of the korean
society of mineral and energy resources engineers 45 no. 5:
536â€“45.
li, y.e., and l. demanet. 2015. phase and amplitude tracking
for seismic event separation. geophysics 80 no. 6: wd59â€“72.
doi:10.1190/geo2015-0075.1.
li, y.e., and l. demanet. 2016. full-waveform inversion with
extrapolated low-frequency data. geophysics 81 no. 6:
r339â€“48. doi:10.4043/26626-ms.

madiba, g.b., and g.a. mcmechan. 2003. processing, inversion,
and interpretation of a 2d seismic data set from the north
viking graben, north sea. geophysics 68 no. 3: 837â€“48.
doi:10.1190/1.1581036.
martin, g.s., s. larsen, and k. marfurt. 2002. marmousi-2: an
updated model for the investigation of avo in structurally
complex areas. in seg international exposition and annual
meeting, seg-2002. doi:10.1190/1.1817083.
min, d.j., c. shin, and h.s. yoo. 2004. free-surface boundary condition in finite-difference elastic wave modeling.
bulletin of the seismological society of america 94 no. 1:
237â€“50. doi:10.3997/2214-4609-pdb.5.p264.
ovcharenko, o., v. kazei, m. kalita, d. peter, and t. alkhalifah. 2019. deep learning for low-frequency extrapolation
from multioffset seismic. geophysics 84 no. 6: r989â€“1001.
doi:10.1190/geo2018-0884.1.
park, b., and w. ha. 2019. efficient 3d acoustic wave propagation modeling using a cell-based finite difference method.
geophysics and geophysical exploration 22 no. 2: 56â€“61.
doi:10.7582/gge.2019.22.2.056.
ronneberger, o., p. fischer, and t. brox. 2015. u-net: convolutional networks for biomedical image segmentation: munich, germany. in medical image computing and
computer-assisted interventionâ€“miccai 2015: 18th international conference, proceedings, part iii, vol.18, 234â€“41.
springer international publishing. doi:10.1007/978-3-31924574-4_28.
sun, h., and l. demanet. 2020. extrapolated full-waveform
inversion with deep learning efwi-cnn. geophysics 85 no.
3: r275â€“88. doi:10.1190/geo2019-0195.1.
ten kroode, f., s. bergler, c. corsten, j.w. de maag, f. strijbos,
and h. tijhof. 2013. broadband seismic dataâ€”the importance of low frequencies. geophysics 78 no. 2: wa3â€“14.
doi:10.1190/geo2012-0294.1.
wang, r., and f. herrmann. 2016. frequency down extrapolation with tv norm minimization. in seg technical program
expanded abstracts 2016. society of exploration geophysicists, 1380â€“84. doi:10.1190/segam2016-13879674.1.
wang, m., s. xu, and h. zhou. 2020. self-supervised learning for low frequency extension of seismic data. in
seg technical program expanded abstracts 2020, society of
exploration geophysicists, 1501â€“5. doi:10.1190/segam20203427086.1.
wu, x., z. geng, y. shi, n. pham, s. fomel, and g. caumon.
2020. building realistic structure models to train convolutional neural networks for seismic structural. geophysics 85
no. 4: wa27â€“39. doi:10.1190/segam2019-3214282.1.
wu, r.s., j. luo, and b. wu. 2014. seismic envelope inversion and modulation signal model. geophysics 79 no. 3:
wa13â€“24. doi:10.1190/geo2013-0294.1.
zhang, p., l. han, z. xu, f. zhang, and y. wei. 2017. sparse blind
deconvolution based low-frequency seismic data reconstruction for multiscale full waveform inversion. journal of
applied geophysics 139: 91â€“108. doi:10.1016/j.jappgeo.20
17.02.021.



ieee transactions on geoscience and remote sensing, vol. 62, 2024

5900717

seismic velocity inversion based on physically
constrained neural networks
yan zhang , decong meng, yifan zhou, liwei song , and hongli dong

abstractâ€” the propagation velocity of seismic waves is a
crucial parameter in seismic exploration, encompassing the entire
process of seismic data acquisition, processing, and interpretation. traditional model-driven full-waveform inversion (fwi)
methods, which rely on an initial velocity, suffer from low
computational efficiency. conversely, data-driven deep-learning
(dl) approaches heavily rely on extensive training data and
lack interpretability due to overreliance on training data for
generalization. to address these challenges, we present a seismic velocity inversion network model that incorporates prior
knowledge and constraints based on physical laws. the proposed
approach involves constructing a data-driven inversion network
with dual encoders and single decoder structure, enabling the
learning of nonlinear mappings from seismic data to velocity
models. by incorporating prior well-logging data and attention
mechanisms, the inversion process is improved. in addition,
a seismic forward modeling network based on recurrent neural
networks (rnns) is developed to solve the acoustic wave equation.
leveraging the advantages of parallel computing, the forward
modeling process achieves fast calculations. the automatic differentiation algorithm in dl facilitates gradient calculations,
specifically back propagation of the residuals to incorporate the
physical constraints. ultimately, the proposed seismic velocity
inversion network combines the two network structures while
incorporating the constraint of wave field extrapolation law.
numerical experiments demonstrate that this network exhibits
advantages in terms of result accuracy and model generalization.
index termsâ€” attention mechanism, physical constraints,
prior logging, recurrent neural network (rnn), velocity inversion.

i. i ntroduction

i

n the realm of seismic exploration, accurate determination of seismic wave velocity is a critical factor that
influences the imaging quality, the hypocenter localization
accuracy, and the effectiveness of geological interpretation.
it represents a challenging and significant subject within
the field of oil and gas resource exploration. full-waveform
inversion (fwi) stands as one of the established model-driven
manuscript received 16 july 2023; revised 25 october 2023; accepted
28 november 2023. date of publication 5 december 2023; date of current version 13 december 2023. this work was supported by the natural
science foundation of heilongjiang province under grant lh2023d009.
(corresponding author: yan zhang.)
yan zhang, decong meng, and yifan zhou are with the school of computer
and information technology, northeast petroleum university, daqing 163318,
china (e-mail: zhangyan1999@nepu.edu.cn; mdc741768803@163.com;
zyfiivv@outlook.com).
liwei song is with the school of physics and electronic engineering, northeast petroleum university, daqing 163318, china (e-mail:
zhidao90@163.com).
hongli dong is with the institute of artificial intelligence energy
research, northeast petroleum university, daqing 163318, china (e-mail:
shiningdhl@vip.126.com).
digital object identifier 10.1109/tgrs.2023.3339783

methods for wave velocity inversion, and it is widely regarded
as an advanced technique in seismic exploration. by harnessing the principles of the wave equation, fwi effectively
exploits the kinematics (travel time) and dynamics (amplitude
and phase) characteristics of seismic waves. consequently,
the inversion results are better suited for complex geological
conditions. notably, fwi offers numerous advantages, including high inversion accuracy and excellent imaging efficacy
for complex structures. however, fwi faces the challenge
of solving a strongly nonlinear problem of mapping seismic
wave velocities from observed seismic data. as a result, the
inversion outcomes heavily rely on the large-scale wave velocity information incorporated within the initial wave velocity
model. in cases where the accuracy of the initial model is
insufficient, it can lead to the inversion result being trapped in
local minimum [1], [2], [3]. to address this issue, researchers
have utilized tomographic inversion methods to provide initial
velocity models for fwi [4]. however, these methods often
rely heavily on human experience and struggle to meet the
accuracy requirements of fwi. to overcome these challenges,
xu et al. [5] based on a nonlinear iterative relaxation approach
where short and long wavelength components of the velocity
model are updated alternatively to improve the resolution from
fwi. alkhalifah and wu [6] described it in detail and combined it with image-based waveform inversion. chen et al. [7]
proposed the envelope-based sparse-constrained deconvolution
(e-scd) inversion method, which reduces the complexity of
the travel-time inversion and improves the reconstruction accuracy of the reflection sequences. chen et al. [8] proposed a salt
structure elastic fwi based on the multiscale signed envelope,
effectively overcoming the problems that restrict efwi, such
as the lack of low-frequency seismic data and multiparameter
coupling. li and alkhalifah [9] introduced a method that
combines extended waveform inversion with the matching
filter between the predicted and observed data. this approach
mitigates cycle skipping. dokter et al. [10] developed a 2-d
fwi approach for the simultaneous determination of s-wave
velocity and density models from sh- and love-wave data.
guitton [11] proposed a blocky regularization scheme for fwi,
effectively constraining the ill-posed inverse problems and not
smoothing the model. while these methods have effectively
mitigated the dependence of fwi on the initial velocity
model, these approaches still encounter challenges related to
computational cost and difficulty. furthermore, fwi involves
multiple calculations of seismic wave field forward modeling
during gradient calculations and model iteration updates, with
the computational requirements scaling with the number of
sources. consequently, fwi becomes highly computationally

1558-0644 Â© 2023 ieee. personal use is permitted, but republication/redistribution requires ieee permission.
see https://www.ieee.org/publications/rights/index.html for more information.
authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

5900717

ieee transactions on geoscience and remote sensing, vol. 62, 2024

intensive and time-consuming [12], [13], [14]. in response
to the aforementioned limitations of traditional methods,
data-driven deep-learning (dl) approaches have rapidly
emerged.
dl methods, including multilayer perception, convolutional neural networks (cnn), and recurrent neural networks
(rnns), have gained significant attention and adoption in
various fields. in the domain of seismic exploration, dl has
demonstrated remarkable achievements in tasks, such as
seismic denoising [15], [16], [17], [18], [19], interpolation
reconstruction [20], seismic migration [21], earthquake prediction [22], initial velocity model building [23], and fault
detection [24], [25], [26]. both theoretical research and practical applications have demonstrated that dl excels in fitting
nonlinear mapping relationships and has a strong potential for
solving geophysical inverse problems.
in the context of seismic velocity inversion, arayapolo et al. [27] proposed a 2-d velocity modeling method
based on deep neural networks. this approach utilizes manually extracted feature information as input and generates
a velocity model as output. through neural networks, the
nonlinear mapping between feature information and the
velocity model is approximated. kazei et al. [28] and
ovcharenko et al. [29] have shown early applications of
direct inversion on real data. yang and ma [30] introduced a
purely data-driven dl velocity inversion method using u-net
networks. by combining supervised training with extensive
datasets, the trained network model effectively maps seismic data to the velocity model. li et al. [31] constructed
seisivnet, a velocity inversion network based on deep neural networks. this method incorporates all information from
seismic channel data and global seismic profile features by
employing fully connected layers, leading to more accurate
velocity model inversion results. cao et al. [32] constructed
biinnet, a lightweight architecture for real-time velocity
inversion networks based on bisenet. the advantages of
biinnet lie in smaller volumes, lower computational complexity, and faster inference speed. yang et al. [33] proposed a
dl-based algorithm to build high-resolution velocity models
using low-resolution velocity models, migration images, and
well-log velocities as inputs.
moreover, traditional fwi suffers from limitations due to
local optimization algorithms that rely on gradient guidance
to solve nonlinear inversion problems. these algorithms tend
to converge to local minima near the initial value, which can
result in inaccurate or meaningless inversion outcomes when
the initial model is inadequate. this drawback underscores
the importance of prior knowledge in velocity inversion.
to address this issue, zhang et al. [34] developed a neural network that combines a prior initial velocity model
for seismic velocity inversion, achieving promising results.
zhang and gao [35] developed a deep-learning full-waveform
inversion (dlfwi) approach using seismic migration images,
which outperforms the conventional data-driven deep-learning
full-waveform inversion (dd-dlfwi) approach in terms of
reconstruction accuracy, antinoise, and generalization ability.
feng et al. [36] proposed inversionnet, a multiscale velocity
inversion network based on the fully convolutional network

architecture. their approach involved utilizing a style transfer
network to convert natural scenery images into underground
velocity models, laying the foundation for constructing an
extensive dataset required for dl. subsequently, the u-net
network was employed to obtain the initial velocity model, and
finally, the high-wavenumber components of the underground
velocity model were reconstructed by incorporating the initial
velocity model. these advancements in dl methodologies
have opened up new possibilities for improving seismic velocity inversion and addressing the challenges associated with
traditional methods.
the seismic velocity inversion method based on dl has
shown advantages such as reduced dependency on initial
models compared with traditional fwi methods, and higher
prediction efficiency. however, challenges remain in terms of
requiring a large number of training samples and ensuring
the generalization ability of deep neural networks in the
field of seismic exploration velocity inversion. as a result,
research is expanding to explore velocity inversion methods that combine model-based and data-driven approaches.
these methods aim to achieve automation, efficiency, and
relatively improved network generalization and interpretability. richardson [37] demonstrated the equivalence between
gradients calculated using traditional adjoint state methods
and dl automatic differentiation tools by constructing an
rnn network for fwi of the acoustic wave equation. sun
and alkhalifah [38] used rnn to enhance the convergence
of fwi. and, sun et al. [39] conducted comparative tests
on an rnn inversion network based on the acoustic wave
equation and flexibly used multiple optimization algorithms
within the dl framework, resulting in improved calculation efficiency. ren et al. [40] proposed swinet, a seismic
waveform inversion network, based on wave-equation-based
forward modeling network cells. wang et al. [41] implemented
an fwi network using rnn for isotropic and anisotropic
elastic wave media. song and alkhalifah [42] developed the
physics informed neural network (pinn)-based wri method,
and it is able to invert for a reasonable velocity with very
limited iterations and frequencies. rasht-behesht et al. [43]
proposed pinn based on the acoustic wave equation. the
meshless nature of solving partial differential wave equations
using neural networks enables the pinn network to handle
different types of boundary absorption conditions, resulting in
more accurate forward modeling results and addressing the
fwi problem. malovichko et al. [44] developed a general
approach to integrating petrophysical models in 3-d seismic
fwi based on the gramian constraints. the model-based and
data-driven method, when combined with the advantages of
fwi theory within the framework of dl algorithms, not only
potentially may improve inversion accuracy and generalization
but also may reduce the reliance of data-driven methods on
a large number of training samples, which is one of the
research highlights in the field of seismic velocity inversion
in conjunction with dl methods.
to address the nonuniqueness of the inversion problem and
accelerate model convergence, we propose a model-driven and
data-driven seismic velocity inversion network based on physical constraints and prior knowledge from well-logging data.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

zhang et al.: seismic velocity inversion based on physically constrained neural networks

this network incorporates wave field continuation constraints
to ensure consistency between the synthetic and observed
seismic data, improving the accuracy of the inversion results.
by integrating prior knowledge derived from well-log data,
which provides valuable information about subsurface rock
properties and seismic velocities, the solution space is further
constrained, enabling faster convergence to more accurate
velocity models. this combined approach offers a comprehensive and effective solution for high-precision seismic velocity
inversion in seismic exploration.
the major contributions include the following aspects.
1) we introduce the prior knowledge of well logs, observation systems, and seismic source location to reduce the
multiplicity in the inversion problem.
2) we combined attention mechanism and fully connected
layers to design a u-net network with two encoders and
one decoder, serving as a data-driven velocity inversion
network and demonstrating its effectiveness.
3) we combined the fwi theory with an rnn network
to simulate the seismic wave propagation process and
achieved an effective integration of physical laws and
neural networks.
4) integrating data-driven neural networks with
physics-driven rnn forward modeling networks
enables a data-model dual-driven approach for velocity
inversion.
ii. t heory
in this section, we will start by reviewing the fwi, as well
as the idea of using cnns for solving inverse problems. then,
our rnn-based forward modeling will be discussed.
a. fwi
fwi, initially introduced by tarantola [45], is a method for
seismic velocity inversion that operates in the time domain.
it iteratively updates the model by minimizing the discrepancy between the numerically simulated seismic data and the
observed data. the inversion process can be divided into two
main steps: wave field simulation and gradient calculation.
in this study, we utilize the 2-d constant density acoustic wave
equation for forward modeling, which can be expressed as
follows:
âˆ‚ 2 u(x, z, t)
1
âˆ’ âˆ‡ 2 u(x, z, t) = s(x, z, t)
v 2 (x, z)
âˆ‚t 2

(1)

where âˆ‡ 2 is the laplace operator, v(x, z) is the wave velocity,
and u(x, z, t) and s(x, z, t) are the wave field and source
at time t, respectively. by employing the second-order finite
difference scheme in the time domain, the wave field value
at the next time step can be calculated. this simplifies the
expression for the forward modeling simulation as follows:
ut+1t = 1t 2 v 2 (âˆ‡ 2 ut + st ) + 2ut âˆ’ utâˆ’1t

(2)

where 1t is the sampling time step, ut+1t is the wave field
value of the next time step, and st is the source function
value at time t. the prediction of seismic data is obtained by
recording the wave field values at each time point using the

5900717

detector mapping operator. as a result, the forward modeling
simulation process can be simplified as follows:
d pred = f (v)

(3)

where d pred is the predicting seismic data, f is the acoustic forward modeling operator, and v is the velocity model
parameter.
seismic velocity inversion involves solving the inverse
problem of (3) in the forward modeling process. fwi can
be viewed as an optimization problem that aims to minimize
the mismatch between the predicted seismic data generated by
forward numerical simulation and the observed seismic data.
by iteratively updating the model, the objective is to minimize
the error between the predicted and observed seismic data in
order to invert the parameters of the underground medium
velocity model. the objective function can be expressed as
follows:
1
(4)
j (v) = âˆ¥ f (v) âˆ’ d obs âˆ¥2
2
where d obs represents the actual observed seismic data. fwi
utilizes the adjoint state method [46] to calculate the gradient
during the inversion process and iteratively updates the model
parameters through local optimization algorithms.
b. principle of convolutional neural network for solving
inversion problems
compared with multilayer perceptron, cnns have a strong
advantage in processing data samples with spatial characteristics, such as images and 2-d seismic data, due to their strong
spatial feature extraction ability and spatial weight-sharing
characteristics [47]. in the purely data-driven velocity inversion problem, if we assume that the convolutional neural
network is denoted by net, then the inversion process can
be formulated as follows:
y = net(x, Î¸ )

(5)

where x is the observed seismic data, y is the 2-d velocity
model predicted by the network, and Î¸ is the parameter of the
network. the convolutional neural network approximates the
target by fitting extensive training data. the average loss on
the training dataset d is regarded as an empirical risk, and
remp is computed as follows:
n

remp (net) =

s
1 x
l(net(x i , Î¸ ), yÌƒi )
ns i=1

(6)

where yÌƒ is the 2-d velocity model label, l is the loss
function, and ns is the number of samples. the objective
of the convolutional neural network algorithm is to minimize
the empirical risk, and in the context of velocity inversion,
it aims to learn the nonlinear mapping relationship between
the seismic data and the velocity model. therefore, under the
guidance of empirical risk minimization, the objective function
j of the neural network used for seismic velocity inversion can
be defined as follows:
x
j (x, Î¸, yÌƒ) = arg min
l(net(x i , Î¸ ), yÌƒi )
(7)
Î¸

i

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

5900717

ieee transactions on geoscience and remote sensing, vol. 62, 2024

fig. 1.
rnn network structural diagram: after unfolding in timeline,
assuming that the current time step is t, xt represents the input to the current
network, h t represents the value of the hidden layer, and yt represents the
output at the current time step. u, g, and w are the trainable network weights.

where l is the loss function. gradient descent is used as
an iterative optimization method to approximate the optimal
solution of a convolutional neural network. the objective
is to minimize the empirical risk by updating the network
parameters in the direction of steepest descent of the loss
function. through the computation of gradients with respect to
the network parameters, we update the network in small steps
to gradually minimize the loss and improve the networkâ€™s performance. the iterative process continues until a satisfactory
convergence is achieved or a predefined stopping criterion is
met. the network parameter Î¸t+1 is expressed as follows:
Î¸ t+1 = Î¸ t + Âµ(âˆ‡Î¸ i=1,2,...,t j (x, Î¸ i , yÌƒ))

(8)

where Âµ represents the update direction and step, and âˆ‡Î¸ i=1,2,...,t
represents the gradient relative to parameter Î¸ i .
c. forward modeling principle of rnn
rnn is a suitable and effective model for processing
sequential data in the field of seismic exploration. unlike
cnn, rnn considers the sequential relationship between
input data, making it well-suited for temporal data processing.
in the context of seismic forward simulation, where the
wave field values at each time depend on the values at previous
times, rnn [48] can capture this temporal dependency.
rnn has a unique structure, as shown in fig. 1, with a
cyclic architecture that allows it to analyze and process sequential data effectively. this cyclic structure gives rnn a natural
advantage in handling sequences such as text, voice, waveform
data, and well-logging data. another characteristic of rnn is
parameter sharing, where the parameters of each layer (u, g,
and w ) are shared across time steps. this parameter sharing
enables rnn to perform the same operation at each step
but with different inputs and hidden states. consequently, the
number of parameters and network complexity to be learned
in rnn is reduced, enhancing its flexibility for sequences of
varying lengths.
we can describe the calculation process of the hidden layer
by assuming that at time t, the input of the network is xt , the
hidden state (hidden layer neuron activity value) h t is not only
related to the input xt at the current time but also to the saved
hidden state h tâˆ’1 at the previous time
z t = w htâˆ’1 + uxt + b
ht = Ïƒ (z t )
yt = g(ht ).

(9)
(10)
(11)

from a physical perspective, the process of seismic wave
propagation in the underground medium and its reception by
geophones involves a complex mapping relationship.
the observed seismic data represents the response of the
underground medium to the excitation caused by the artificial
source. however, it is important to note that this mapping
relationship is not necessarily reversible. in other words,
given the same source and geophone conditions, the observed
seismic data may correspond to scattered wave information
from different locations within the underground medium.
this one-to-many relationship between the observed data and
the underlying subsurface properties is a fundamental cause
of inversion instability, adding to the challenge of learning
accurate mappings using neural networks.
to overcome this challenge, it is necessary to consider both
the model space (underground medium) and the data space
(observed data) in the inversion process. the propagation of
seismic waves can only be fully described by combining these
two spaces. therefore, the inversion problem cannot solely
rely on approximating the data space, but must also incorporate
constraints from the model space to obtain more unique and
stable inversion results. the forward numerical simulation
process based on the wave equation exhibits similarities to the
forward propagation process of rnn. by introducing model
parameters, source functions, and geophone functions, and
solving the wave equation using rnn, it is possible to obtain
predicted seismic data through forward numerical simulation
calculations of the seismic wave field. by combining (2)
and (9), it can be concluded that

 
 

 
ut+1t
2 + 1t 2 v 2 âˆ‡ 2 âˆ’1
ut
2 2 st
=
Â·
+ 1t v
.
ut
1
0
utâˆ’1t
0
(12)
formula (12) represents the fundamental calculation step
within the rnn forward modeling network. this step involves
solving the wave equation to obtain and transmit the wave field
value at time t + 1t. subsequently, the geophone mapping
operator p is utilized to derive the output seismic data
represented as follows:
d t+1t = p(ut+1t ).

(13)

the output at each time step is combined to synthesize the
predicted seismic data, completing the forward numerical simulation. the rnn forward modeling utilizes the mean squared
error (mse) loss function to quantify the discrepancy between
the predicted and observed seismic data. the optimization
result is obtained by minimizing the mse loss function
1
[(d obs âˆ’ d pred )]2 .
(14)
n
in addition to forward numerical simulation, rnn can
also compute gradients through backpropagation to facilitate
the inversion process. rnn leverages automatic differentiation algorithms [49] to calculate gradients and updates the
model parameters using optimization algorithms within the
dl framework. the gradients obtained through automatic
differentiation are equivalent to those obtained through the
conventional adjoint state method [50].
l r (d obs , d pred ) =

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

zhang et al.: seismic velocity inversion based on physically constrained neural networks

5900717

compared with traditional methods, employing rnn for
forward modeling and inversion operations eliminates the need
for manually deriving gradient calculation expressions and
enables flexible utilization of various optimization algorithms
within the dl framework for experimental research. this
approach enhances computational efficiency and facilitates the
integration of other dl algorithms.
iii. c onstruction of v elocity i nversion n etwork
based on p rior p hysical c onstraints
in this section, we first focus on the construction of a
data-driven velocity inversion subnet. we then delve into
the construction intricacies of an rnn-based forward subnet,
laying the groundwork for robust predictive modeling within
our framework. finally, we delineate the construction of
the velocity inversion network, strategically integrating prior
physical constraints to enhance the accuracy and reliability of
our inversion methodology.
a. construction of a data-driven velocity inversion subnet
1) prior knowledge processing and application: deep
neural networks have the ability to learn distribution characteristics and mapping relationships between seismic data
and the corresponding velocity models using large amounts of
data, even without accurate initial velocity models. however,
incorporating prior knowledge obtained during seismic exploration can help constrain the solution space, reduce ambiguity,
and enhance algorithm convergence. in particular, the source
location and observation system information serve as valuable
prior knowledge that captures the specific location-related
details of seismic data and the underground medium. in addition, well-logging data represent an essential source of prior
knowledge in seismic exploration, offering rich geological
information with high vertical resolution and strong continuity.
it provides valuable insights into the local characteristics of
the underground medium. therefore, we introduce the source,
observation system information, and well-logging data as
reliable prior knowledge for the network.
to incorporate this prior knowledge, we utilize one-hot
vector encoding to represent the source and geophone location
information in the network. since the location information of
the source and geophone is closely associated with seismic
data and velocity models, we manually define two lines of
encoding vectors. one line represents the source location
information, while the other line represents the geophone
location information. these lines are concatenated with the
seismic trace corresponding to a specific source and receiver.
in addition, a 2-d matrix, matching the size of the velocity
model, is employed to encode the source and geophone locations using one-hot encoding. fig. 2 illustrates this process.
furthermore, well-log data serves as reliable prior knowledge in seismic exploration and is often employed as a
constraint in inversion methods. in this context, we utilize
well-log data from two shafts to obtain velocity values corresponding to the 30th and 70th columns of the actual seismic
velocity model. by applying the nearest-neighbor interpolation
algorithm and gaussian blur operation, we generate a rough

fig. 2.

source function and observation system prior.

initial velocity model that serves as the initial velocity prior
knowledge. the resulting model is depicted in fig. 3, showcasing the incorporation of well-log data into the inversion
process.
2) attention gate mechanism: to network performance
enhance the feature extraction of well-log areas and improve,
an attention gate (ag) [51] can be incorporated into the
encoding and decoding structure of the network. this attention
mechanism allows for focused feature extraction in areas of
interest by learning the weights of attention regions through
neural network gradient propagation. the specific structure of
the network with the ag is depicted in fig. 4.
given the presence of distinct location information and wave
velocity information within the well-log data, the attention
mechanism can effectively enhance the extraction of features
related to the well-log data. moreover, the characteristics of
different areas are integrated into feature map c as auxiliary
and complementary information. the adjusted feature map
encompasses crucial details from various spatial positions,
highlighting the salient features of specific well-logging areas
and improving the modelâ€™s sensitivity and prediction accuracy.
3) overall design of velocity inversion subnet: based on
the experience gained from traditional fwi, the recovery of
large-scale information in the velocity model heavily relies
on the initial velocity model, whereas small-scale structural
details primarily rely on the reflected wave information contained in seismic data. in the context of dl, neural networks
tend to learn low frequencies before gradually capturing higher
frequencies as they fit the data [52]. this frequency bias phenomenon holds significance when designing velocity inversion
networks.
to address this, we propose two encoder structures for
the network. the first encoder network utilizes prior logs
to provide a large-scale wave velocity model. on the other
hand, the second encoder network leverages the waveform
information present in the seismic data to capture small-scale
feature information. by combining the outputs of these two
encoders, we aim to enhance the learning performance of the
network, allowing it to effectively capture both the large-scale
and small-scale characteristics of the velocity model.
furthermore, seismic data represent the time-series information, capturing the propagation and variation of seismic waves
within the underground medium over time. in contrast, the
velocity model provides information about the spatial location
and structure of the underground medium. however, the correspondence between seismic data and the velocity model in the
element space is often weak, posing challenges for network

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

5900717

fig. 3.

ieee transactions on geoscience and remote sensing, vol. 62, 2024

velocity model is shown on the left. the well-logging data are shown in the middle. (right) initial speed after well-logging data processing.

fig. 4. ag mechanism combines both the information from the previous
context represented as x, and the subsequent context represented as g. x and g
are transformed into matrices a and b of the same size via a 1 Ã— 1 convolution
and added together. this sum is then passed through the rectified linear unit
(relu) activation function, producing matrix c. the attention coefficients Î±
are computed via convolution and sigmoid operations applied to matrix c.
finally, the output y is obtained by multiplying x with Î±.

training and learning. to address this issue, we leverage the
benefits of fully connected layers, which can effectively utilize
the full waveform information present in seismic data despite
the weak spatial correspondence. however, when dealing with
large input and output data, fully connected layers require
a substantial number of parameters, extensive computational
resources, and high memory consumption. to mitigate these
challenges, we adopt an encoding network to compress the
features of seismic data into a one-dimensional vector format
[bs, c, 1, 1], where bs represents the batch size and c represents
the number of feature channels. this compressed feature
vector is then passed through a fully connected layer for
feature mapping. by employing this approach, we can leverage
the high-level semantic features extracted from the seismic
data to map them to the structural features of the model space,
thus alleviating the resource consumption issues associated
with the large amount of data in the fully connected layer.
based on the aforementioned analysis, the structure of our
velocity inversion network, which incorporates prior knowledge, is illustrated in fig. 5. the network comprises two
encoders and one decoder, working in tandem to facilitate the
inversion process and enhance learning performance.

velocity inversion subnet to incorporate physical law constraints via the velocity model.
referring to (2), when the velocity model is known, the
forward modeling simulation process can be simplified as a
calculation involving the laplacian values of the wave field.
the calculation of the laplacian values can be achieved
through convolution operations within the neural network.
the convolution kernel used in the convolution operation is
typically determined by the finite difference format and the
dimensions of the velocity model. for a 2-d wave velocity
model using a spatial second-order finite difference format,
the convolutional kernel can be represented as a matrix
ï£®
ï£¹
0
1
0
ï£°1 âˆ’4 1ï£».
0
1
0

b. construction of rnn-based forward subnet

like traditional finite difference calculation methods, rnn
forward modeling networks utilize convolution operations to
compute the laplace operator of the wave field. this approach
offers the advantage of parallel computing, resulting in higher
computational efficiency, especially when dealing with multiple velocity models and multiple forward modeling for various
sources. furthermore, the use of gpus can further accelerate
the computations.
in the context of traditional seismic forward modeling,
absorption boundary conditions are employed to attenuate
the reflections of the wave field at artificial boundaries.
in our rnn forward modeling network, we introduce perfectly
matched layer (pml) absorption boundary conditions. the
acoustic wave equation with pml conditions can be expressed
as follows:
âˆ‚2u
âˆ‚u
+ 2m
+ m 2 u = v2âˆ‡ 2 u
(15)
2
âˆ‚t
âˆ‚t
where m is the cosine-type attenuation factor matrix. by using
the second-order finite difference method of time




2 âˆ’ m2 1t 2 + 1t 2 v 2 âˆ‡ 2
1 âˆ’ m1t
ut+1t =
ut âˆ’
utâˆ’1t .
1 + m1t
1 + m1t
(16)

traditional fwi has demonstrated its effectiveness in practical data applications by utilizing the wave field continuation
law as a driving force for velocity inversion. the physical
law-driven approach has yielded promising results. in our
proposed framework, we establish a connection between the
forward modeling subnet, implemented using rnn, and the

the rnn architecture is utilized to process the time series
data by storing hidden states. in the context of seismic velocity
modeling, each momentâ€™s wave field calculation is treated as
a layer in the rnn, referred to as a forward network unit.
the velocity model is considered a trainable parameter of the
network, and the internal operations of the forward modeling

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

zhang et al.: seismic velocity inversion based on physically constrained neural networks

fig. 5.

5900717

data-driven velocity inversion network structural diagram.

fig. 6. rnn forward modeling network process. sn represents the source value at time step n, which is used to compute the wavefield pn at time step n
through network units. the wavefield value dn at time step n is then obtained by applying the detector operator to pn and projected as a row of seismic data
output.

network unit involve convolution operations on the laplacian
values of the wave field and simple matrix operations on
the wave field. the results of the forward modeling network
unit calculations are then mapped to a line of seismic data
output using the detector operator p. the overall structure
of the rnn forward modeling network is depicted in fig. 6.
within each layer of the network, the wave fields from the
previous two moments are used as inputs, along with the
source function values at the current moment. by performing
the necessary calculations, new wave field values and corresponding predicted seismic data are obtained. this calculation
process aligns with (16).
c. construction of the velocity inversion network based on
prior physical constraints
the seismic velocity inversion network, which combines the
data-driven velocity inversion subnet and the rnn forward
modeling network, is designed to integrate physical laws

and prior knowledge constraints. the overall structure of
the network is illustrated in fig. 7. the predicted velocity
model generated by the inversion network serves as a trainable
parameter input for the rnn forward network, establishing a
connection between the two networks. during training, the
l2 loss is computed using the simulated seismic data and
the predicted seismic data obtained from the rnn forward
modeling network. the gradient of this loss function with
respect to velocity model parameters is obtained using automatic differentiation algorithm. by jointly training the two
networks, the gradient of the velocity model in the rnn can be
backpropagated to the inversion subnetwork, providing physical constraints for the inversion subnetwork. upon completion
of the final training, the predicted output from the inversion
subnet is used as the final result of the overall network.
the loss function of the inversion subnet combines the mse
function and the structural similarity function (ssim) to jointly
measure the difference between the output results and the real
velocity model. the specific form of the loss function is as

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

5900717

fig. 7.

ieee transactions on geoscience and remote sensing, vol. 62, 2024

overall network structural diagram.

follows:
2Âµ yv Âµ yÌƒv + c1 2Ïƒ yv yÌƒv + c2


Âµ2yv + Âµ2yÌƒ + c1 Ïƒ y2v + Ïƒ yÌƒ2 + c2



ssim yv , yÌƒv = 

v



(17)

v


1
(18)
l u ( yv , yÌƒv ) = [( yv âˆ’ yÌƒv )2 1 âˆ’ ssim( yv , yÌƒv ) ]
n
where Âµ yv and Âµ yÌƒv represent the average values of image
pixels within windows yv and yÌƒv , respectively, Ïƒ y2v and Ïƒ yÌƒ2
v
represent their variance, respectively, Ïƒ yv yÌƒv represents their
covariance, c1 and c2 represent the calculated stability constant, and n represents the number of batch samples. ssim is
a metric commonly used to assess the similarity between two
images, with a specific focus on edge and texture similarity.
in the context of velocity model inversion, the fine-scale
stratum interfaces are often the key areas of interest, which
aligns with ssimâ€™s emphasis on texture. therefore, ssim can
be employed to measure the structural difference between the
inversion results and the real velocity model. typically, ssim
values range between 0 and 1, where a value of 1 indicates
complete similarity between the two images. to align with
the optimization objective of minimizing the neural networkâ€™s
loss function, we utilize negative values in the loss function
formulation. this ensures that the optimization goal is met.
finally, the loss function of the overall network is
l total = l u ( yv , yÌƒv ) + Î» l r (d obs , d pred ).

(19)

iv. e xperiment
in this section, we will first share information on the data
organization, followed by how we trained the network. we also
share our accuracy evaluation metrics. we follow that by
showing the prediction results and analyzing the performance
of the network. we finally compare various algorithms.
a. data organization
to address the scarcity of real velocity models in the
context of velocity inversion, we employ a manual generation
approach to expand the dataset dv . this involves creating
2-d velocity models with different structures, including simple
layered models, fault models, and salt dome models. in the
generation of simple layered velocity models, we superpose

sine, cosine, and various linear functions to create layers
ranging from 4 to 7. the velocities of these layers are within
the range of 1500â€“4500 m/s. the size of these velocity models
is 1000 Ã— 640 m, with a grid spacing of 10 m in both
the horizontal and vertical directions. fig. 8(a) illustrates
an example of such a layered velocity model. in addition,
we generate fault models by introducing random parameters
such as fault length, position, and inclination angle within the
range of 30â—¦ â€“150â—¦ . fig. 8(b) demonstrates an example of a
fault model. to further diversify the dataset, we incorporate
salt dome objects. these objects involve the addition of a
randomly shaped salt dome with a size of 20 Ã— 20 grid
points and a velocity of 5 km/s to the previously generated velocity models. fig. 8(c) provides a visualization of
a salt dome model. by generating these artificial velocity
models with different structures, we can augment the dataset
and provide additional samples for the velocity inversion
training.
once a sufficient number of velocity models have been
obtained, the finite difference forward modeling method is
used to generate corresponding simulated seismic data. first,
for the setup of the source and observation system, we adopt
surface shooting and surface observation setup. geophones
are placed every 10 m horizontally on the surface (every
grid point on the surface), with a sampling time interval of
0.001 s. the total acquisition time is 1.024 s, implying that
there are 1024 sampling point records received. the seismic
source wavelet ricker adopts a 20-hz main frequency, with a
total of ten shots and a horizontal interval of 100 m between
the seismic sources that are shot point-by-point on the surface.
the final partial simulated seismic data results are shown in
fig. 9.
to include the effects of noise and missing channels in real
seismic data, we introduce random gaussian noise and random
channel dropout to the seismic data in the dataset dv . a 15%
of the seismic data are added with random gaussian noise,
with a maximum noise intensity Ïƒ of 0.15. furthermore, a
15% of the seismic data undergo random channel dropout,
with a maximum random missing ratio of 35%. the resulting
observed seismic data, processed with noise and missing
channels, is depicted in fig. 10. this augmentation of the
dataset enhances the robustness of the network model.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

zhang et al.: seismic velocity inversion based on physically constrained neural networks

fig. 8.

5900717

display of velocity model dataset. (a) simple layered model. (b) including a fault. (c) salt dome object.
table i
t est r esults of a blation e xperiments

fig. 9.

simulated seismic data generated by forward modeling.

b. training the network
we share here the training details that resulted in the
predictions we will see in section iv-d. we first individually
trained the data-driven inversion network using supervised
learning methods. we also designed ablation experiments to
verify the effectiveness of the network. we used an adam
optimizer with an initial learning rate of 0.001. when the loss
value did not decrease 5 consecutive times on the validation
set, we reduced the learning rate by a factor of 0.1. the batch
size was set to 10, and the total number of training iterations
was 200.
in the section on overall network performance below,
we consider that the high memory consumption of rnn networks and the difficulty of retraining concatenated networks.

fig. 10. seismic data processing. (a) noisy seismic data with Ïƒ = 0.15.
(b) randomly missing 30% of seismic data.

this study adopts the idea of transfer learning. we utilize a
pretrained velocity inversion subnetwork and combine it with
an rnn forward modeling network for retraining with a small
amount of data. the overall training process is as follows:
first, the inversion subnetwork is trained on the dv dataset for
70 iterations. then, the pretrained model is combined with the
rnn forward modeling network and trained for an additional
130 iterations. regarding the rnn forward modeling network,
we used the adam optimizer and conducted tests with learning
rates set to 1, 10, 50, 100, and 150, respectively. ultimately,
the learning rate of a 100 yielded good training results for the
network.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

5900717

ieee transactions on geoscience and remote sensing, vol. 62, 2024

fig. 11. comparison of test samples. the first line is the label. the second line is the result of model iii. the third line is the result of model ii. the fourth
line is the result of model i.

c. evaluation index
mse, mean absolute error (mae), and ssim metrics were
used to measure the recovery accuracy
b

l mse ( yÌƒ, y) =

1x
( yÌƒi âˆ’ yi )2
n i=1

l mae ( yÌƒ, y) =

1x
| yÌƒi âˆ’ yi |
n i=1

(20)

b

(21)

where n represents the number of batch samples, yÌƒ is the
network prediction output, and y is the real label data.
d. data-driven inversion network results
1) ablation experiment: to assess the impact of prior
knowledge and ag on the inversion results, we conducted
an ablation experiment by comparing model ii (without prior
knowledge inversion network) and model iii (without ag)
with model i, which incorporates prior knowledge into the
inversion subnet. this experiment aims to examine the influence of different mechanisms on network performance and
effectiveness.

table i displays the quantitative results of the three network
models on the test set. model ii (without prior knowledge of
inversion network) and model iii (without ag) exhibit inferior
performance compared with the proposed network models in
terms of mse, mae, and ssim indicators.
this indicates that the incorporation of the ag and prior
knowledge in model i (prior inversion subnet) has enhanced
the inversion accuracy of the network. fig. 11 presents the
test results of the three models on the simple layered velocity
model, fault velocity model, and salt dome velocity model.
observing the graphs, we can see that all three network models
perform well in restoring the simple layered model. however,
model iii (without ag) exhibits relatively poor restoration at
the edges, resulting in fuzziness. in the fault model, there are
variations in the restoration quality among the three models,
but model iii (without ag) lacks distinct fault features and
appears relatively blurry. in contrast, model ii (without prior
knowledge inversion network) and the proposed network in
this study exhibit more prominent fault features at the fault
location, and the network recovery effect in this study is better
than model ii (without prior knowledge inversion network).
regarding the salt dome model, model iii (without ag)
exhibits the worst restoration effect, particularly with a blurry

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

zhang et al.: seismic velocity inversion based on physically constrained neural networks

fig. 12. output results of the training set model corresponding to the first
35 training times. (a) ground truth. (b) output for the fifth round of training.
(c) output for the 15th round of training. (d) output for the 25th round of
training. (e) output for the 35th round of training.

bottom interface. model ii (without prior knowledge inversion
network) performs better in restoring the background layered
model but struggles with blurry boundaries and shape discrepancies when predicting the salt dome. based on these findings,
we conclude that the attention mechanism improves the networkâ€™s inversion performance in regions with pronounced
velocity changes. it enables the inversion subnet to restore
structural features, such as faults and salt domes, which exhibit
significant velocity variations. however, model ii, without
prior knowledge, primarily learns background layered features
and performs relatively poorly in high-frequency feature areas
with substantial velocity changes.
to gain a visual understanding of the network learning
process and the impact of prior knowledge and the attention
mechanism, we have selected a model from the training set
to showcase the prediction results in the initial 35 rounds of
network training, as illustrated in fig. 12. the graph reveals
several noteworthy observations. initially, the network begins
by learning large-scale, low-frequency features, and gradually transitioning toward higher-frequency feature learning.
as training progresses, the network starts recovering information about the layered structure of the underground media.
notably, the network demonstrates a preference for learning
relevant feature information near the two vertical wells, which
are based on the initial velocity model and well logging
data. the layered structure becomes increasingly apparent
around the location where the prior well-logging information is
present. the network successfully restores the velocity values
in this region, displaying significant distinctions from the
surrounding velocity information. this observation serves as
evidence of the effectiveness of the attention mechanism and

5900717

the integration of well-logging prior information in the velocity
inversion subnet. overall, fig. 12 shows a clear demonstration
of the networkâ€™s learning trajectory, highlighting its capability
to progressively recover the layered structure of the velocity
model based on the initial velocity model and well-logging
data. the attention mechanism and prior well-logging information play vital roles in enhancing the networkâ€™s effectiveness
in velocity inversion.
2) seismic data testing with different noise intensities
and sampling rates: to assess the modelâ€™s robustness in
handling real-world scenarios, we conducted tests using noisy
seismic data and data with missing seismic channels. the noise
intensities were set at 0.1, 0.3, and 0.5, while the missing
seismic data had missing proportions of 15%, 25%, and 35%.
the results are presented in figs. 13 and 14. from the
findings, we can see that model i network maintains excellent
performance when the noise intensity is 0.1, which falls within
the range of noise intensity in the training data. even with a
noise intensity of 0.3, the model still performs well, although
there are slight velocity variations in the finer details. however,
as the noise intensity increases, the inversion results of the
network become noticeably affected, particularly in regions
with significant structural changes. this indicates that noise
has a significant impact on the networkâ€™s inversion results.
in experiments involving missing traces, the overall prediction
results exhibit minimal changes when 15% of seismic channels
are missing, unlike the results affected by noise interference.
as the proportion of missing channels gradually increases,
the overall prediction results display only minor alterations.
this phenomenon can be attributed to the convolutional neural
networkâ€™s feature extraction process, which leverages spatial proximity to extract data features. the early layers of
the network, particularly in the downsampling stage, employ
larger-sized convolutional kernels with a broad receptive field.
this enables better learning of the spatial features of seismic
data and offers robustness against the absence of seismic
channels. furthermore, due to the weak spatial correspondence between seismic data and velocity models, disturbances
in spatial structures have a relatively small impact on the
inversion results compared to noise interference. the layered
characteristics of velocity models, on the other hand, are
sensitive to the time information of reflected waves in seismic
data. as a result, the network exhibits greater robustness
in handling missing seismic channels in spatial positions,
resulting in relatively minor changes in the inversion results.
overall, the experimental results demonstrate the modelâ€™s
performance under varying intensities of noise and missing
seismic channel data. the network displays resilience to the
absence of seismic channels in spatial positions, while noise
interference has a more significant impact on the inversion
results.
e. overall network results
the training indicators of the network on the training set
are presented in fig. 15. from the changes in mse, it is
observed that the network quickly converges within the first
20 iterations, after which the numerical changes become more
gradual but continue to trend downward. the ssim also

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

5900717

ieee transactions on geoscience and remote sensing, vol. 62, 2024

fig. 13. network performance for different noise intensities. the first column is the label. the second column shows the network output without noise
interference. the third column shows the network output when the noise intensity is 0.1. the fourth column shows the network output when the noise intensity
is 0.3. the fifth column shows the network output when the noise intensity is 0.5.

fig. 14. network performance under varying degrees of missing channels. the first column is the label. the second column shows the network output
without missing channel interference. the third column shows the network output when 15% of data is missing. the fourth column shows the network output
when 25% of data is missing. the fifth column shows the network output when 35% of data is missing.

exhibits rapid changes in the early stages of training, followed
by a more gradual but upward trend. overall, the network
demonstrates excellent convergence. a comparison of various
indicators on the test set between model a (physical law
constraint network) and model b (unconstrained network) is
shown in table ii. it can be observed that model a has
improved in mse, mae, and ssim results compared to

model b. this indicates that model a, with the incorporation
of physical constraints, exhibits comparable performance.
in the robustness testing of the network against noise and
missing sampling, figs. 16 and 17 present the results of
models a and b on the test set. we can see that model a
maintains excellent performance even when the missing ratio
exceeds 20%. at the maximum missing ratio, the network

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

zhang et al.: seismic velocity inversion based on physically constrained neural networks

5900717

table ii
e ffectiveness t esting of j oint p hysical l aw c onstraints d rive

fig. 15.

training indicator curve.

fig. 16. comparison of test results for different degrees of missing channels. (a) comparison of different missing faults, where the first line is a network
without physical constraints, and the second line is a network with physical constraints. (b) comparison of different missing salt domes, where the first line
is a network without physical constraints, and the second line is a network with physical constraints.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

5900717

ieee transactions on geoscience and remote sensing, vol. 62, 2024

fig. 17. comparison of test results for different noise intensities. (a) comparison of faults with different noise intensities, where the first line is a network
without physical constraints, and the second line is a network with physical constraints. (b) comparison of salt domes with different noise intensities, where
the first line is a network without physical constraints, and the second line is a network with physical constraints.
table iii
c omparison w ith s imilar a lgorithms

with physical constraints still outperforms the network without
physical constraints. in the testing of different noise data,
model a also demonstrates robustness against noise interference in seismic data. overall, the combination of the pretrained
prior inversion subnet and the rnn forward modeling network
through transfer learning achieves favorable results. model
a, with physical constraints, exhibits better performance than
model b in terms of mse, mae, and ssim while demonstrating improved robustness against noise and missing sampling.
f. algorithm comparison
in order to demonstrate the effectiveness of the algorithm
proposed in this article, a comparison is made between

the velocity inversion network based on physical law constraints (proposed method), the u-net inversion method,
and the inversionnet inversion network. the performance
of these three methods on the test set is illustrated in
fig. 18. from the figure, it is evident that all three networks
achieve satisfactory reconstructions on the simple layered
model and the salt dome model. however, on the fault
model, the inversionnet network exhibits fuzziness of the
deep layer and fault location, with an unclear fault structure.
the u-net network also produces unsatisfactory results in
capturing fault details. in contrast, the network model proposed in this article effectively reconstructs the fault velocity
model.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

zhang et al.: seismic velocity inversion based on physically constrained neural networks

5900717

fig. 18. comparison results of similar algorithms. the first line is labeled data. the second line presents the algorithm results of this study. the third line
presents the u-net algorithm results. the fourth line presents the inversion algorithm results.

of the method proposed in this article, showcasing better
performance compared to other methods in terms of numerical
evaluation and visual comparison.
v. c onclusion

fig. 19. comparison of velocity values in the 70th column of the fault model
using similar algorithms.

the specific indicator values are provided in table iii,
which reveals that the algorithm proposed in this article
outperforms similar methods in terms of mse, mae, and
ssim. furthermore, fig. 19 presents a comparison of the
velocity values in the 70th column of the fault velocity
model for the three network models. it demonstrates that the
algorithm proposed in this article exhibits the best fit with the
labels. the results collectively demonstrate the effectiveness

we propose a seismic velocity inversion network that
leverages physical law constraints to achieve high-precision
velocity inversion. the key points of our approach are summarized as follows.
1) utilizing prior knowledge in an attention mechanism:
to expedite model convergence, we incorporate prior
knowledge from well logging, observation systems, and
source locations. we combine this knowledge with an
u-net network that includes an attention mechanism,
enhancing the utilization of prior information. considering the limited spatial correspondence between seismic
data and velocity models, as well as the rich but weak
energy of reflected waves in seismic data, we design
a data-driven velocity inversion network. this network
employs deep convolution layers to extract features and
combines them with fully connected layers for spatial
mapping, effectively utilizing available features.
2) integration of fwi and rnn: by combining fwi theory
with the capabilities of rnn in processing time series

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

5900717

ieee transactions on geoscience and remote sensing, vol. 62, 2024

data, we develop a novel framework. we utilize the
wave equation to drive wave field continuation, and
rnn is employed to calculate the wave field laplacian
value using convolutional methods. we leverage the
hidden state layer of the rnn to store wave field
information from previous time steps, thereby improving computational efficiency. in addition, we employ
automatic differentiation and the adam optimization
algorithm to automatically obtain gradients and update
the model parameters. this approach replaces traditional
fwi methods and effectively combines physical laws
with neural networks.
3) data and model dual-driven network: we implement
a velocity inversion network that combines data-driven
neural networks with a physically informed rnn forward modeling network. first, we employ transfer
learning to pretrain the data-driven velocity inversion
network. subsequently, we integrate this pretrained network with rnn forward modeling networks for further
training. through this approach, we achieve highprecision velocity inversion even with limited samples.
by leveraging these methodologies, our proposed network
achieves accurate velocity inversion results with improved
efficiency. it provides the effective combination of data-driven
approaches and physical laws.
r eferences
[1] s. lian, s. yuan, g. wang, t. liu, y. liu, and s. wang, â€œenhancing lowwavenumber components of full-waveform inversion using an improved
wavefield decomposition method in the time-space domain,â€ j. appl.
geophys., vol. 157, pp. 10â€“22, oct. 2018.
[2] x. guo, y. shi, w. wang, and h. liu, â€œa robust source-independent
misfit function for time domain waveform inversion based on normalized convolved wavefield,â€ j. appl. geophys., vol. 166, pp. 129â€“146,
jul. 2019.
[3] r. biswas et al., â€œtwo-step velocity inversion using trans-dimensional
tomography and elastic fwi,â€ in proc. seg tech. program expanded
abstr., sep. 2020, pp. 3628â€“3633.
[4] t. alkhalifah and y. choi, â€œfrom tomography to fwi with a single
objective function,â€ geophysics, vol. 79, no. 2, pp. 55â€“61, feb. 2014.
[5] s. xu, d. wang, f. chen, g. lambare, and y. zhang, â€œinversion on
reflected seismic wave,â€ in proc. 82nd annu. int. meeting, seg, expand
abstr., sep. 2012, pp. 1â€“7.
[6] t. alkhalifah and z. wu, â€œthe natural combination of full and imagebased waveform inversion,â€ geophys. prospecting, vol. 64, no. 1,
pp. 19â€“30, jan. 2016.
[7] g. chen, w. yang, y. liu, j. luo, and h. jing, â€œenvelope-based sparseconstrained deconvolution for velocity model building,â€ ieee trans.
geosci. remote sens., vol. 60, 2022, art. no. 4501413.
[8] g. chen, w. yang, y. liu, h. wang, and x. huang, â€œsalt structure elastic
full waveform inversion based on the multiscale signed envelope,â€ ieee
trans. geosci. remote sens., vol. 60, 2022, art. no. 4508912.
[9] y. li and t. alkhalifah, â€œextended full waveform inversion with
matching filter,â€ geophys. prospecting, vol. 69, no. 7, pp. 1441â€“1454,
jun. 2021.
[10] e. dokter, d. kÃ¶hn, d. wilken, d. de nil, and w. rabbel, â€œfull waveform inversion of sh- and love-wave data in near-surface prospecting,â€
geophys. prospecting, vol. 65, no. 1, pp. 216â€“236, dec. 2017.
[11] a. guitton, â€œblocky regularization schemes for full-waveform inversion,â€ geophys. prospecting, vol. 60, no. 5, pp. 870â€“884, sep. 2012.
[12] z. meng and j. a. scales, â€œ2-d tomography in multi-resolution analysis
model space,â€ in proc. seg tech. program expanded abstr., jan. 1996,
pp. 1126â€“1129.
[13] x. li, a. y. aravkin, t. van leeuwen, and f. j. herrmann, â€œfast randomized full-waveform inversion with compressive sensing,â€ geophysics,
vol. 77, no. 3, pp. 13â€“17, may 2012.

[14] x. chai, g. tang, r. peng, and s. liu, â€œthe linearized bregman
method for frugal full-waveform inversion with compressive sensing and sparsity-promoting,â€ pure appl. geophys., vol. 175, no. 3,
pp. 1085â€“1101, mar. 2018.
[15] w. cao, y. shi, w. wang, x. guo, f. tian, and y. zhao, â€œself-supervised
multitask 3-d partial convolutional neural network for random noise
attenuation and reconstruction in 3-d seismic data,â€ ieee trans. geosci.
remote sens., vol. 60, 2022, art. no. 5924619.
[16] c. qiu, b. wu, n. liu, x. zhu, and h. ren, â€œdeep learning prior model
for unsupervised seismic data random noise attenuation,â€ ieee geosci.
remote sens. lett., vol. 19, pp. 1â€“5, 2022.
[17] o. m. saad and y. chen, â€œdeep denoising autoencoder for seismic
random noise attenuation,â€ geophysics, vol. 85, no. 4, pp. 367â€“376,
jul. 2020.
[18] y. zhang, x. li, b. wang, j. li, h. wang, and h. dong, â€œrobust seismic
data denoising based on deep learning,â€ oil. geophys. prospecting,
vol. 57, no. 1, pp. 12â€“25, 2022.
[19] c. birnie, m. ravasi, s. li, and t. alkhalifah, â€œthe potential of selfsupervised networks for random noise suppression in seismic data,â€ artif.
intell. geosci., vol. 2, pp. 47â€“59, dec. 2021.
[20] y. zhang, j. li, b. wang, x. li, and h. dong, â€œseismic data regularization based on deep learning combining wavelet domain,â€ oil. geophys.
prospecting, vol. 57, no. 4, pp. 777â€“788, 2022.
[21] h. kaur, n. pham, and s. fomel, â€œimproving the resolution of migrated
images by approximating the inverse hessian using deep learning,â€
geophysics, vol. 85, no. 4, pp. 173â€“183, jul. 2020.
[22] md. h. a. banna et al., â€œapplication of artificial intelligence in predicting earthquakes: state-of-the-art and future challenges,â€ ieee access,
vol. 8, pp. 192880â€“192923, 2020.
[23] c. wei et al., â€œseismic velocity inversion based on cnn-lstm fusion
deep neural network,â€ appl. geophys., vol. 18, no. 4, pp. 499â€“514,
dec. 2021.
[24] x. hu et al., â€œimage recognition-based identification of multifractal features of faults,â€ frontiers earth sci., vol. 10, may 2022, art. no. 909166.
[25] a. cunha, a. pochet, h. lopes, and m. gattass, â€œseismic fault detection
in real data using transfer learning from a convolutional neural network
pre-trained with synthetic seismic data,â€ comput. geosci., vol. 135,
feb. 2020, art. no. 104344.
[26] d. yang, y. cai, g. hu, x. yao, and w. zou, â€œseismic fault detection
based on 3d unet++ model,â€ in proc. seg tech. program expanded
abstr., sep. 2020, pp. 1631â€“1635.
[27] m. araya-polo, j. jennings, a. adler, and t. dahlke, â€œdeep-learning
tomography,â€ lead. edge, vol. 37, no. 1, pp. 58â€“66, 2018.
[28] v. kazei, o. ovcharenko, p. plotnitskii, d. peter, x. zhang, and t.
alkhalifah, â€œmapping seismic data cubes to vertical velocity profiles by
deep learning,â€ geophysics, vol. 86, no. 5, pp. 711â€“721, aug. 2021.
[29] o. ovcharenko, v. kazei, t. a. alkhalifah, and d. b. peter, â€œmulti-task
learning for low-frequency extrapolation and elastic model building from
seismic data,â€ ieee trans. geosci. remote sens., vol. 60, 2022.
[30] f. yang and j. ma, â€œdeep-learning inversion: a next-generation seismic velocity model building method,â€ geophysics, vol. 84, no. 4,
pp. 583â€“599, jul. 2019.
[31] s. li et al., â€œdeep-learning inversion of seismic data,â€ ieee trans.
geosci. remote sens., vol. 58, no. 3, pp. 2135â€“2149, mar. 2020.
[32] w. cao, y. shi, x. guo, f. tian, x. ke, and c. li, â€œbiinnet: bilateral
inversion network for real-time velocity analysis,â€ ieee trans. geosci.
remote sens., vol. 60, 2022, art. no. 5905617.
[33] s. yang, t. alkhalifah, y. ren, b. liu, y. li, and p. jiang, â€œwell-log
information-assisted high-resolution waveform inversion based on deep
learning,â€ ieee geosci. remote sens. lett., vol. 20, pp. 1â€“5, 2023.
[34] j. zhang, j. li, x. chen, y. li, g. huang, and y. chen, â€œrobust
deep learning seismic inversion with a priori initial model constraint,â€
geophys. j. int., vol. 225, no. 3, pp. 2001â€“2019, jan. 2021.
[35] w. zhang and j. gao, â€œdeep-learning full-waveform inversion using
seismic migration images,â€ ieee trans. geosci. remote sens., vol. 60,
2022, art. no. 3062688.
[36] s. feng, y. lin, and b. wohlberg, â€œmultiscale data-driven seismic fullwaveform inversion with field data study,â€ ieee trans. geosci. remote
sens., vol. 60, 2022, art. no. 4506114.
[37] a. richardson, â€œseismic full-waveform inversion using deep learning
tools and techniques,â€ 2018, arxiv:1801.07232.
[38] b. sun and t. alkhalifah, â€œml-descent: an optimization algorithm for
fwi using machine learning,â€ geophysics, vol. 85, no. 6, pp. 477â€“492,
oct. 2020.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.

zhang et al.: seismic velocity inversion based on physically constrained neural networks

[39] j. sun, z. niu, k. a. innanen, j. li, and d. o. trad, â€œa theoryguided deep-learning formulation and optimization of seismic waveform
inversion,â€ geophysics, vol. 85, no. 2, pp. 87â€“99, mar. 2020.
[40] y. ren, x. xu, s. yang, l. nie, and y. chen, â€œa physics-based neuralnetwork way to perform seismic full waveform inversion,â€ ieee access,
vol. 8, pp. 112266â€“112277, 2020.
[41] w. wang, g. a. mcmechan, and j. ma, â€œelastic isotropic and anisotropic
full-waveform inversions using automatic differentiation for gradient
calculations in a framework of recurrent neural networks,â€ geophysics,
vol. 86, no. 6, pp. 795â€“810, nov. 2021.
[42] c. song and t. alkhalifah, â€œwavefield reconstruction inversion via
physics-informed neural networks,â€ ieee trans. geosci. remote sens.,
vol. 60, 2022, art. no. 5908012.
[43] m. rasht-behesht, c. huber, k. shukla, and g. e. karniadakis,
â€œphysics-informed neural networks (pinns) for wave propagation and
full waveform inversions,â€ j. geophys. res., solid earth, vol. 127, no. 5,
pp. 1â€“21, may 2022.
[44] m. malovichko, n. khokhlov, n. yavich, and m. s. zhdanov, â€œincorporating known petrophysical model in the seismic full-waveform inversion
using the gramian constraint,â€ geophys. prospecting, vol. 68, no. 4,
pp. 1361â€“1378, may 2020.
[45] a. tarantola, â€œinversion of seismic reflection data in the acoustic
approximation,â€ geophysics, vol. 49, no. 8, pp. 1259â€“1266, aug. 1984.
[46] r.-e. plessix, â€œa review of the adjoint-state method for computing the
gradient of a functional with geophysical applications,â€ geophys. j. int.,
vol. 167, no. 2, pp. 495â€“503, nov. 2006.
[47] y. lecun, l. bottou, y. bengio, and p. haffner, â€œgradient-based learning applied to document recognition,â€ proc. ieee, vol. 86, no. 11,
pp. 2278â€“2324, nov. 1998.
[48] j. l. elman, â€œfinding structure in time,â€ cognit. sci., vol. 14, no. 2,
pp. 179â€“211, mar. 1990.
[49] a. g. baydin, b. a. pearlmutter, a. a. radul, and j. m. siskind,
â€œautomatic differentiation in machine learning: a survey,â€ j. mach.
learn. res., vol. 18, no. 1, pp. 5595â€“5637, jan. 2017.
[50] m. sambridge, p. rickwood, n. rawlinson, and s. sommacal, â€œautomatic differentiation in geophysical inverse problems,â€ geophys. j. int.,
vol. 170, no. 1, pp. 1â€“8, jul. 2007.
[51] o. oktay et al., â€œattention u-net: learning where to look for the
pancreas,â€ 2018, arxiv:1804.03999.
[52] n. rahaman et al., â€œon the spectral bias of neural networks,â€ 2018,
arxiv:1806.08734.

5900717

yan zhang was born dalian, liaoning, china,
in 1980. he received the b.s. degree in computer
science and technology, the m.s. degree in computer
application technology, and the ph.d. degree in
oil and gas engineering from northeast petroleum
university, daqing, china, in 2003, 2010, and 2018,
respectively.
he is currently an associate professor with the
school of computer and information technology,
northeast petroleum university. his research interests include artificial intelligence and seismic data
processing.

decong meng, photograph and biography not available at the time of
publication.

yifan zhou, photograph and biography not available at the time of
publication.

liwei song, photograph and biography not available at the time of publication.

hongli dong, photograph and biography not available at the time of
publication.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 16:06:03 utc from ieee xplore. restrictions apply.



solid earth, 11, 1527â€“1549, 2020
https://doi.org/10.5194/se-11-1527-2020
Â© author(s) 2020. this work is distributed under
the creative commons attribution 4.0 license.

deep learning for fast simulation of seismic waves in complex media
ben moseley1 , tarje nissen-meyer2 , and andrew markham1
1 department of computer science, university of oxford, oxford, uk
2 department of earth sciences, university of oxford, oxford, uk

correspondence: ben moseley (bmoseley@robots.ox.ac.uk)
received: 14 october 2019 â€“ discussion started: 13 november 2019
revised: 21 june 2020 â€“ accepted: 30 june 2020 â€“ published: 24 august 2020

abstract. the simulation of seismic waves is a core task in
many geophysical applications. numerical methods such as
finite difference (fd) modelling and spectral element methods (sems) are the most popular techniques for simulating
seismic waves, but disadvantages such as their computational
cost prohibit their use for many tasks. in this work, we investigate the potential of deep learning for aiding seismic simulation in the solid earth sciences. we present two deep neural networks which are able to simulate the seismic response
at multiple locations in horizontally layered and faulted 2-d
acoustic media an order of magnitude faster than traditional
finite difference modelling. the first network is able to simulate the seismic response in horizontally layered media and
uses a wavenet network architecture design. the second network is significantly more general than the first and is able to
simulate the seismic response in faulted media with arbitrary
layers, fault properties and an arbitrary location of the seismic source on the surface of the media, using a conditional
autoencoder design. we test the sensitivity of the accuracy
of both networks to different network hyperparameters and
show that the wavenet network can be retrained to carry out
fast seismic inversion in the same media. we find that are
there are challenges when extending our methods to more
complex, elastic and 3-d earth models; for example, the
accuracy of both networks is reduced when they are tested
on models outside of their training distribution. we discuss
further research directions which could address these challenges and potentially yield useful tools for practical simulation tasks.

1

introduction

seismic simulations are essential for addressing many outstanding questions in geophysics. in seismic hazard analysis, they are a key tool for quantifying the ground motion
of potential earthquakes (boore, 2003; cui et al., 2010). in
oil and gas prospecting, they allow the seismic response of
hydrocarbon reservoirs to be modelled (chopra and marfurt,
2007; lumley, 2001). in geophysical surveying, they show
how the subsurface is illuminated by different survey designs (xie et al., 2006). in global geophysics, they are used
to obtain snapshots of the earthâ€™s interior dynamics by tomography (hosseini et al., 2019; bozdagÌ† et al., 2016), to decipher source and path effects from individual seismograms
(krischer et al., 2017) and to model wave effects of complex
structures (thorne et al., 2020; ni et al., 2002). in seismic
inversion, they are used to estimate the elastic properties of a
medium given its seismic response (tarantola, 1987; schuster, 2017) and in full-waveform inversion (fichtner, 2010;
virieux and operto, 2009), a technique used to image the 3-d
structure of the subsurface, they are used up to tens of thousands of times to improve on estimates of a mediumâ€™s elastic
properties. in planetary science, seismic simulations play a
central role in understanding novel recordings on mars (van
driel et al., 2019).
numerous methods exist for simulating seismic waves, the
most popular in fully heterogeneous media being finite difference (fd) and spectral element methods (sems) (igel,
2017; moczo et al., 2007; komatitsch and tromp, 1999).
they are able to capture a large range of physics, including
the effects of undulating solidâ€“fluid interfaces (leng et al.,
2019), intrinsic attenuation (van driel and nissen-meyer,
2014a) and anisotropy (van driel and nissen-meyer, 2014b).
these methods solve for the propagation of the full seismic

published by copernicus publications on behalf of the european geosciences union.

1528

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

wavefield by discretising the elastodynamic equations of motion. for an acoustic heterogeneous medium, these are given
by the scalar linear equation of motion:


1
1 âˆ‚ 2p
âˆ‚ 2f
Ïâˆ‡ Â·
âˆ‡p âˆ’ 2 2 = âˆ’Ï 2 ,
(1)
Ï
v âˆ‚t
âˆ‚t
where p is the acoustic pressure, f is a point
âˆšsource of volume injection (the seismic source), and v = Îº/Ï is the velocity of the medium, with Ï the density of the medium and
Îº the adiabatic compression modulus (long et al., 2013).
whilst fd and spectral element methods are the primary
means of simulation in complex media, a major disadvantage
of these methods is their computational cost (bohlen, 2002;
leng et al., 2016). typical fd or sem simulations can involve billions of degrees of freedom, and at each time step the
wavefield must be iteratively updated at each 3-d grid point.
for many practical geophysical applications, this is often
prohibitively expensive. for example, in global seismology,
one may be interested in modelling waves up to 1 hz in frequency to resolve small-scale heterogeneities in the mantle
and a single simulation of this type with conventional techniques can cost around 40 million cpu hours (leng et al.,
2019). at crustal scales, industrial seismic imaging requires
wave modelling up to tens of hertz in frequency carried out
hundreds of thousands of times for each explosion in a seismic survey, and such requirements can easily fill the largest
supercomputers on earth. any improvement in efficiency is
welcome, not least due to the high financial and environmental costs of high-performance computing.
in some applications, large parts of the earth model may
be relatively smooth or simple. this simplicity can be taken
advantage of, for example, in the complexity-adapted sem
introduced by leng et al. (2016), and can deliver a large
speedup compared to standard numerical modelling. pseudoanalytical methods such as ray tracing and amplitude-versusoffset modelling (aki and richards, 1980; vinje et al., 1993)
are another approach which can provide significant speedups,
albeit being approximate. we note that many applications are
constrained and driven by a sparse set of observations on the
surface of an earth model. for these applications, we are
typically only interested in modelling the seismic response
at these points to decipher seismic origin or the 3-d structure beneath the surface, yet fully numerical methods still
need to iterate the entire wavefield through all points in the
model at all points in time. any shortcut to avoid computing
these massive 4-d wavefields might lead to drastic efficiency
improvements. in short, the points above suggest that alternative and advantageous methods to capture accurate wave
physics may be possible for these challenging problems.
the field of machine learning has seen an explosion in
growth over the last decade. this has been primarily driven
by advancements in deep learning, which has provided more
powerful algorithms allowing much more difficult problems
to be learned (goodfellow et al., 2016). this progress has led
to a surge in the use of deep learning techniques across many
solid earth, 11, 1527â€“1549, 2020

areas of science. in particular, deep neural networks have recently shown promise in their ability to make fast yet sufficiently accurate predictions of physical phenomena (guo
et al., 2016; lerer et al., 2016; paganini et al., 2018). these
approaches are able to learn about highly non-linear physics
and often offer much faster inference times than traditional
simulation.
in this work, we ask whether the latest deep learning techniques can aid seismic simulation tasks relevant to the solid
earth sciences. we investigate the use of deep neural networks and discuss the challenges and opportunities when using them for practical seismic simulation tasks. our contribution is as follows:
â€“ we present two deep neural networks which are able
to simulate seismic waves in 2-d acoustic media an order of magnitude faster than fd simulation. the first
network uses a wavenet network architecture (van den
oord et al., 2016) and is able to accurately simulate the
pressure response from a fixed point source at multiple locations in a horizontally layered velocity model.
the second is significantly more general; it uses a conditional autoencoder network design and is able to simulate the seismic response at multiple locations in faulted
media with arbitrary layers, fault properties and an arbitrary location of the source on the surface of the media. in contrast to the classical methods, both networks
simulate the seismic response in a single inference step,
without needing to iteratively model the seismic wavefield through time, resulting in a significant speedup
compared to fd simulation.
â€“ we test the sensitivity of the accuracy of both networks
to different network designs, present a loss function
with a time-varying gain which improves training convergence and show that fast seismic inversion in horizontally layered media can also be carried out by retraining the wavenet network.
â€“ we find challenges when extending our methods to
more complex, elastic and 3-d earth models and discuss further research directions which could address
these challenges and yield useful tools for practical simulation tasks.
in sect. 2, we consider the simple case of simulating seismic waves in horizontally layered 2-d acoustic earth models
using a wavenet deep neural network. in sect. 3, we move on
to the task of simulating more complex faulted earth models
using a conditional autoencoder network. in sect. 4, we discuss the challenges of extending our approaches to practical
simulation tasks and future research directions.

https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media
1.1

related work

the use of machine learning and neural networks in geophysics is not new (van der baan and jutten, 2000). for example, murat and rudman (1992) used neural networks to
carry out automated first break picking, dowla et al. (1990)
used a neural network to discriminate between earthquakes
and nuclear explosions and poulton et al. (1992) used them
for electromagnetic inversion of a conductive target. in seismic inversion, rÃ¶th and tarantola (1994) used a neural network to estimate the velocity of 1-d, layered, constant thickness velocity profiles from seismic amplitudes and nath et al.
(1999) used neural networks for cross-well travel-time tomography. however, these early approaches only used shallow network designs with small numbers of free parameters
which limits the expressivity of neural networks and the complexity of problems they can learn about (goodfellow et al.,
2016).
the field of machine learning has grown rapidly over the
last decade, primarily because of advances in deep learning. the availability of larger datasets, discovery of methods
which allow deeper networks to be trained and availability of
more powerful computing architectures (mostly gpus) has
allowed much more complex problems to be learnt (goodfellow et al., 2016), leading to a surge in the use of deep learning in many different research areas. for example, in physics,
lerer et al. (2016) presented a deep convolutional network
which could accurately predict whether randomly stacked
wooden towers would fall or remain stable, given 2-d images of the tower. guo et al. (2016) demonstrated that convolutional neural networks could estimate flow fields in complex computational fluid dynamics (cfd) calculations 2 orders of magnitude faster than a traditional gpu-accelerated
cfd solver, and paganini et al. (2018) used a conditional
generative adversarial network to simulate particle showers
in particle colliders.
a resurgence is occurring in geophysics too (bergen et al.,
2019; kong et al., 2019). early examples of deep learning
include devilee et al. (1999), who used deep probabilistic
neural networks to estimate crustal thicknesses from surface wave velocities and valentine and trampert (2012), who
used a deep autoencoder to compress seismic waveforms.
more recently, perol et al. (2018) presented an earthquake
identification method using convolutional networks which
is orders of magnitude faster than traditional techniques. in
seismic inversion, araya-polo et al. (2018) proposed an efficient deep learning concept for carrying out seismic tomography using the semblance of common midpoint receiver
gathers. wu and lin (2018) proposed a convolutional autoencoder network to carry out seismic inversion, whilst yang and
ma (2019) adapted a u-net network design for the same purpose. richardson (2018) demonstrated that a recurrent neural
network framework can be used to carry out full-waveform
inversion (fwi). sun and demanet (2018) showed a method
for using deep learning to extrapolate low-frequency seismic
https://doi.org/10.5194/se-11-1527-2020

1529

energy to improve the convergence of fwi algorithms. in
seismic simulation, zhu et al. (2017) presented a multi-scale
convolutional network for predicting the evolution of the full
seismic wavefield in heterogeneous media. their method was
able to approximate the wavefield kinematics over multiple
time steps, although it suffered from the accumulation of error over time and did not offer a reduction in computational
time. moseley et al. (2018) showed that a convolutional network with a recursive loss function can simulate the full
wavefield in horizontally layered acoustic media. krischer
and fichtner (2017) used a generative adversarial network to
simulate seismograms from radially symmetric and smooth
earth models.
in this work, we present fast methods for simulating seismic waves in horizontally layered and faulted 2-d acoustic
media, which offer a significant reduction in computation
time compared to zhu et al. (2017). we also present a fast
method for seismic inversion of horizontally layered acoustic media, which is more general than the original approach
proposed by rÃ¶th and tarantola (1994) because it is able to
invert velocity models with varying numbers of layers and
varying layer thicknesses. we restrict ourselves to 2-d acoustic media and discuss implications for 3-d elastic media below.
2

fast seismic simulation in 2-d horizontally layered
acoustic media using wavenet

first, we consider the simple case of simulating seismic
waves in horizontally layered 2-d acoustic earth models. we
train a deep neural network with a wavenet architecture to
simulate the seismic response recorded at multiple receiver
locations in the earth model, horizontally offset from a point
source emitted at the surface of the model. as mentioned
above, many seismic applications are concerned with sparse
observations similar to this setup. a key difference of this approach compared to fd and sem simulations is that the network computes the seismic response at the surface in a single
inference step, without needing to iteratively model the seismic wavefield through time, potentially offering a significant
speedup. whilst we concentrate on simple velocity models
here, more complex faulted earth models are considered in
sect. 3.
an example simulation we wish to learn is shown in fig. 1
and our simulation workflow is shown in fig. 2. the input to
the network is a horizontally layered velocity profile and the
output of the network is a simulation of the pressure response
recorded at each receiver location. we will now discuss deep
neural networks, our wavenet architecture, our simulation
workflow and our training methodology in more detail below.

solid earth, 11, 1527â€“1549, 2020

1530

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

figure 1. ground truth fd simulation example. (a) a 20 hz ricker seismic source is emitted close to the surface and propagates through
a 2-d horizontally layered acoustic earth model. the black circle shows the source location. a total of 11 receivers are placed at the same
depth as the source with a horizontal spacing of 50 m (red triangles). the full wavefield is overlain for a single snapshot in time. note
seismic reflections occur at each velocity interface. (b) the earth velocity model. the earth model has a constant density of 2200 kg mâˆ’2 .
(c) the resulting ground truth pressure response recorded by each of the receivers, using fd modelling. a t 2.5 gain is applied to the receiver
responses for display.

2.1

deep neural networks and the wavenet network

a neural network is a network of simple computational elements, known as neurons, which perform mathematical operations on multidimensional arrays or tensors (goodfellow
et al., 2016). the composition of these neurons together defines a mathematical function of the networkâ€™s input. each
neuron has a set of free parameters, or weights, which are
tuned using optimisation, allowing the networkâ€™s function to
be learned, given a set of training data. in deep learning, the
neurons are typically arranged in multiple layers, which allows the network to learn highly non-linear functions.
a standard building block in deep learning is the convolutional layer, where all neurons in the layer share the same
weight tensor and each neuron has a limited field of view of
its input tensor. the output of the layer is achieved by cross
correlating the weight tensor with the input tensor. multiple
weight tensors, or filters, can be used to increase the depth of
the output tensor. such designs have achieved state-of-the-art
performance across a wide range of machine learning tasks
(gu et al., 2018).
the wavenet network proposed by van den oord et al.
(2016) makes multiple alterations to the standard convolutional layer for its use with time series. each convolutional
layer is made causal; that is, the receptive field of each neusolid earth, 11, 1527â€“1549, 2020

ron only contains samples from the input layer whose sample
times are before or the same as the current neuronâ€™s sample time. furthermore, the wavenet exponentially dilates the
width of its causal connections with layer depth. this allows the field of view of its neurons to increase exponentially with layer depth, without needing a large number of
layers. these modifications are made to honour time series
prediction tasks which are causal and to better model input
data which vary over multiple timescales. the wavenet network recently achieved state-of-the-art performance in textto-speech synthesis.
2.2

simulation workflow

our workflow consists of a preprocessing step, where we
convert each input velocity model into its corresponding normal incidence reflectivity series sampled in time (fig. 2a),
followed by a simulation step, where it is passed to a
wavenet network to simulate the pressure response recorded
by each receiver (fig. 2b).
the reflectivity series is typically used in exploration seismology (russell, 1988) and contains values of the ratio of the
amplitude of the reflected wave to the incident wave for each
interface in a velocity model. for acoustic waves at normal
incidence, these values are given by

https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

1531

figure 2. our wavenet simulation workflow. given a 1-d earth velocity profile as input (a), our wavenet deep neural network (b) outputs
a simulation of the pressure responses at the 11 receiver locations in fig. 1. the raw input 1-d velocity profile sampled in depth is converted
into its normal incidence reflectivity series sampled in time before being input into the network. the network is composed of nine timedilated causally connected convolutional layers with a filter width of two samples and dilation rates which increase exponentially with layer
depth. each hidden layer of the network has the same length as the input reflectivity series, 256 channels and a rectified linear unit (relu)
activation function. a final causally connected convolutional layer with a filter width of 101 samples, 11 output channels and an identity
activation is used to generate the output simulation.

r=

Ï 2 v2 âˆ’ Ï 1 v1
,
Ï 2 v2 + Ï 1 v1

(2)

where Ï1 , v1 and Ï2 , v2 are the densities and p-wave velocities across the interface. the series is usually expressed in
time and each reflectivity value occurs at the time at which
the primary reflection of the source from the corresponding
velocity interface arrives at a given receiver. the arrival times
can be computed by carrying out a depth-to-time conversion
of the reflectivity values using the input velocity model.
we chose to convert the velocity model to its reflectivity series and use the causal wavenet architecture to constrain our workflow. for horizontally layered velocity models
and receivers horizontally offset from the source, the receiver
pressure recordings are causally correlated to the normal incidence reflectively series of the zero-offset receiver. intuitively, a seismic reflection recorded after a short time has
only travelled through a shallow part of the velocity model
and the pressure responses are at most dependent on the past
samples in this reflectivity series. by preprocessing the input velocity model into its corresponding reflectivity series
and using the causal wavenet architecture to simulate the
receiver response, we can constrain the network so that it
honours this causal correlation.
we input the 1-d profile of a 2-d horizontally layered velocity model, with a depth of 640 m and a step size of 5 m.
we use eq. (2) and a standard 1-d depth to time conversion
to convert the velocity model into its normal incidence reflectivity series. the output reflectivity series has a length of
1 s and a sample rate of 2 ms. an example output reflectivity
series is shown in fig. 2a.
https://doi.org/10.5194/se-11-1527-2020

figure 3. distribution of layer velocity and layer thickness over all
examples in the training set.

the reflectivity series is passed to the wavenet network,
which contains nine causally connected convolutional layers
(fig. 2b). each convolutional layer has the same length as
the input reflectivity series, 256 hidden channels, a receptive
field width of two samples and a rectified linear unit (relu)
activation function (nair and hinton, 2010). similar to the
original wavenet design, we use exponentially increasing dilations at each layer to ensure that the first sample in the input
reflectivity series is in the receptive field of the last sample of
the output simulation. we add a final causally connected convolutional layer with 11 output channels, a filter width of 101
samples and an identity activation to generate the output simulation, where each output channel corresponds to a receiver
prediction. this results in the network having 1 333 515 free
parameters in total.
2.3

training data generation

to train the network, we generate 50 000 synthetic ground
truth example simulations using the seismic_cpml code,
which performs second-order acoustic fd modelling (kosolid earth, 11, 1527â€“1549, 2020

1532

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

matitsch and martin, 2007). each example simulation uses a
randomly sampled 2-d horizontally layered velocity model
with a width and depth of 640 m and a sample rate of 5 m
in both directions. (fig. 1b). for all simulations, we use a
constant density model of 2200 kg mâˆ’2 .
in each simulation, the layer velocities and layer thickness are randomly sampled from log-normal distributions.
we also add a small velocity gradient randomly sampled
from a normal distribution to each model such that the velocity values tend to increase with depth, to be more earth realistic. the distributions over layer velocities and layer thicknesses for the entire training set are shown in fig. 3.
we use a 20 hz ricker source emitted close to the surface and record the pressure response at 11 receiver locations placed symmetrically around the source, horizontally
offset every 50 m (fig. 1a). we use a convolutional perfectly
matched layer boundary condition such that waves which
reach the edge of the model are absorbed with negligible
reflection. we run each simulation for 1 s and use a 0.5 ms
sample rate to maintain accurate fd fidelity. we downsample the resulting receiver pressure responses to 2 ms before
using them for training.
we run 50 000 simulations and extract a training example
from each simulation, where each training example consists
of a 1-d layered velocity profile and the recorded pressure
response at each of the 11 receivers. we withhold 10 000 of
these examples as a validation set to measure the generalisation performance of the network during training.
2.4

training process

the network is trained using the adam stochastic gradient
descent algorithm (kingma and ba, 2014). this algorithm
computes the gradient of a loss function with respect to the
free parameters of the network over a randomly selected subset, or batch, of the training examples. this gradient is used
to iteratively update the parameter values, with a step size
controlled by a learning rate parameter. we propose a l2 loss
function with a time-varying gain function for this task, given
by
l=

1
kg(yÌ‚ âˆ’ y )k22 ,
n

(3)

where yÌ‚ is the simulated receiver pressure response from
the network, y is the ground truth receiver pressure response
from fd modelling, and n is the number of training examples in each batch. the gain function g has the form g = t g ,
where t is the sample time and g is a hyperparameter which
determines the strength of the gain. we add this to empirically account for the attenuation of the wavefield caused by
spherical spreading, by increasing the weight of samples at
later times. in this section, we use a fixed value of g = 2.5.
we use a learning rate of 1 Ã— 10âˆ’5 , a batch size of 20 training examples and run training over 500 000 gradient descent
steps.
solid earth, 11, 1527â€“1549, 2020

2.5

comparison to 2-d ray tracing

we compare the wavenet simulation to an efficient, quasianalytical 2-d ray-tracing algorithm which assumes horizontally layered media. we modify the 2-d horizontally layered
ray-tracing bisection algorithm from the consortium for research in elastic wave exploration seismology (crewes)
seismic modelling library (margrave and lamoureux, 2018)
to include zoeppritz modelling of the reflection and transmission coefficients at each velocity interface (aki and richards,
1980) and 2-d spherical spreading attenuation (gutenberg,
1936; newman, 1973) during ray tracing. the output of the
algorithm is a primary reflectivity series for each receiver,
which we convolve with the source signature used in fd
modelling to obtain an estimate of the receiver responses.
2.6

results

whilst training the wavenet, the losses over the training and
validation datasets converge to similar values, suggesting the
network is generalising well to examples in the validation
dataset. to assess the performance of the trained network,
we generate a random test set of 1000 unseen examples. the
simulations for four randomly selected examples from this
test set are compared to the ground truth fd modelling simulation in fig. 4. we also compare the wavenet simulation
to 2-d ray tracing in fig. 5. for nearly all time samples, the
network is able to simulate the receiver pressure responses.
the wavenet is able to predict the normal moveout (nmo)
of the primary layer reflections with receiver offset, the direct arrivals at the start of each receiver recording and the
spherical spreading loss of the wavefield over time, though
the network struggles to accurately simulate the multiple reverberations at the end of the receiver recordings.
we plot the histogram of the average absolute amplitude
difference between the ground truth fd simulation and the
simulation from the wavenet and 2-d ray tracing over the
test set in fig. a1d in the appendix, and observe that the
wavenet simulation has a lower average amplitude difference than 2-d ray tracing. small differences in phase and
amplitude at larger offsets are the main source of discrepancy between the 2-d ray tracing and fd simulation, which
can be seen in fig. 5, and are likely due to errors both in the
ray tracing approximation and in using discretisation in the
fd simulation. the wavenet predictions are consistent and
stable across the test set, and their closer amplitude match
to the fd simulation is perhaps to be expected because the
network is trained to directly match the fd simulation rather
than the 2-d ray tracing.
we compare the sensitivity of the networkâ€™s accuracy to
two different convolutional network designs in fig. a1. their
main differences to the wavenet design is that both networks use standard rather than causal convolutional layers
and the second network uses exponential dilations whilst the
first does not. both networks have nine convolutional layers,
https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

1533

figure 4. wavenet simulations for four randomly selected examples in the test set. red shows the input velocity model, its corresponding reflectivity series and the ground truth pressure response from fd simulation at the 11 receiver locations. green shows the wavenet simulation
given the input reflectivity series for each example. a t 2.5 gain is applied to the receiver responses for display.

each with 256 hidden channels, filter sizes of 3, relu activations for all hidden layers and an identity activation function
for the output layer, with 1 387 531 free parameters in total.
we observe that the convolutional network without dilations
does not converge during training, whilst the dilated convolutional network has a higher average absolute amplitude difference over the test set from the ground truth fd simulation
than the wavenet network (fig. a1d).
the generalisation ability of the wavenet outside of its
training distribution is tested in fig. 6. we generate four velocity models with a much smaller average layer thickness
than the training set and compare the wavenet simulation to
https://doi.org/10.5194/se-11-1527-2020

the ground truth fd simulation. we find that the wavenet is
able to make an accurate prediction of the seismic response,
but it struggles to simulate the multiple reflections and sometimes the interference between the direct arrival and primary
reflections.
we compare the average time taken to generate 100 simulations to fd simulation and 2-d ray tracing in table 1. we
find that on a single cpu core, the wavenet is 19 times faster
than fd simulation, and using a gpu and the tensorflow library (abadi et al., 2015) it is 549 times faster. this speedup
is likely to be higher than if the gpu was used for accelerating existing numerical methods (rietmann et al., 2012). in
solid earth, 11, 1527â€“1549, 2020

1534

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

figure 5. comparison of wavenet simulation to 2-d ray tracing. we compare the wavenet simulation to 2-d ray tracing for two of the
examples in fig. 4. red shows the input velocity model, its corresponding reflectivity series and the ground truth pressure responses from fd
simulation. green shows the wavenet simulation (left) and 2-d ray tracing simulation (right). a t 2.5 gain is applied to the receiver responses
for display.

this case, the specialised 2-d ray tracing algorithm offers a
similar speedup to the wavenet network. the network takes
approximately 12 h to train on one nvidia tesla k80 gpu,
although this training step is only required once and subsequent simulation steps are fast.

solid earth, 11, 1527â€“1549, 2020

3

fast seismic simulation in 2-d faulted acoustic media
using a conditional autoencoder

the wavenet architecture we implemented above is limited
in that it is only able to simulate horizontally layered earth
models. in this section, we present a second network which is
significantly more general; it simulates seismic waves in 2-d
faulted acoustic media with arbitrary layers, fault properties
https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

1535

figure 6. generalisation ability of the wavenet. the wavenet simulations (green) for four velocity models with much smaller average layer
thicknesses than the training distribution are compared to ground truth fd simulation. red shows the input velocity model, its corresponding
reflectivity series and the ground truth pressure responses from fd simulation.

and an arbitrary location of the seismic source on the surface
of the media.
this is a much more challenging task to learn for multiple
reasons. firstly, the media varies along both dimensions and
the resulting seismic wavefield has more complex kinematics
than the wavefields in horizontally layered media. secondly,
we allow the output of the network to be conditioned on the
input source location which requires the network to learn the
effect of the source location. thirdly, we input the velocity
model directly into the network without conversion to a reflectivity series beforehand; the network must learn to carry
out its own depth to time conversion to simulate the receiver
https://doi.org/10.5194/se-11-1527-2020

responses. we chose this approach over our wavenet workflow because we note that for non-horizontally layered media
the pressure responses are not causally correlated to the normal incidence reflectivity series in general and our previous
causality assumption does not hold.
similar to sect. 2, we simulate the seismic response
recorded by a set of receivers horizontally offset from a point
source emitted within the earth model. an example simulation we wish to learn is shown in fig. 7. we will now discuss
the network architecture and training process in more detail
below.

solid earth, 11, 1527â€“1549, 2020

1536

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

figure 7. ground truth fd simulation example, with a 2-d faulted media. (a) the black circle shows the source location. overall, 32
receivers are placed at the same depth as the source with a horizontal spacing of 15 m (red triangles). the full wavefield pressure is overlain
for a single snapshot in time. (b) the earth velocity model. (c) the resulting ground truth pressure response recorded by each receiver, using
fd modelling. a t 2.5 gain is applied to the receiver responses for display.
table 1. speed comparison of simulation and inversion methods. the time shown is the average time taken to generate 100 simulations (or
100 velocity predictions for the inverse wavenet) on either a single core of a 2.2 ghz intel core i7 processor or a nvidia tesla k80 gpu.
for simulation methods, the speedup factor compared to fd simulation is shown in brackets. the inverse wavenet is faster than the forward
wavenet because it has fewer hidden channels in its architecture and therefore requires less computation.
method

average cpu time (s)

average gpu time (s)

training time (days)

73 Â± 1 (1Ã—)
2.2 Â± 0.1 (33Ã—)
3.79 Â± 0.03 (19Ã—)
3.3 Â± 0.1 (22Ã—)

â€“
â€“
0.133 Â± 0.001 (549Ã—)
0.180 Â± 0.003 (406Ã—)

â€“
â€“
0.5
4

1.27 Â± 0.02

0.051 Â± 0.001

0.5

2-d fd simulation
2-d ray tracing
wavenet (forward)
conditional autoencoder
wavenet (inverse)

3.1

conditional autoencoder architecture

our simulation workflow is shown in fig. 8. instead of preprocessing the input velocity model to its associated reflectivity model, we input the velocity model directly into the
network. the network is conditioned on the source position,
which is allowed to vary along the surface of the earth model.
the output of the network is a simulation of the pressure responses recorded at 32 fixed receiver locations in the model
shown in fig. 7.
we use a conditional autoencoder network design, shown
in fig. 8. the network is composed of 10 convolutional laysolid earth, 11, 1527â€“1549, 2020

ers which reduce the spatial dimensions of the input velocity
model until it has a 1 Ã— 1 shape with 1024 hidden channels.
we term this tensor the latent vector. the input source position is concatenated onto the latent vector and 14 convolutional layers are used to expand the size of the latent vector until its output shape is the same as the target receiver
gather. we choose this encoderâ€“decoder architecture to force
the network to compress the velocity model into a set of
salient features before expanding them to infer the receiver
responses. all hidden layers use relu activation functions
and the final output layer uses an identity activation function.

https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

1537

figure 8. our conditional autoencoder simulation workflow. given a 2-d velocity model and source location as input, a conditional autoencoder network outputs a simulation of the pressure responses at the receiver locations in fig. 7. the network is composed of 24 convolutional
layers and concatenates the input source location with its latent vector.

the resulting network has 18 382 296 free parameters. the
full parameterisation of the network is shown in table a1.
3.2

training process

we use the same training data generation process described
by sect. 2.3. when generating velocity models, we add a
fault to the model. we randomly sample the length, normal
or reverse direction, slip distance and orientation of the fault.
example velocity models drawn from this process are shown
in fig. 9. we generate 100 000 example velocity models and
for each model chose three random source locations along
the top of the model. this generates a total of 300 000 synthetic ground truth example simulations to use for training
the network. we withhold 60 000 of these examples to use as
a validation set during training.
we train using the same training process and loss function described in sect. 2.4, except that we employ a l1 norm
instead of a l2 norm in the loss function (eq. 3). we use
a learning rate of 1 Ã— 10âˆ’4 , a batch size of 100 examples
and run training over 3 000 000 gradient descent steps. we
use batch normalisation (ioffe and szegedy, 2015) after each
convolutional layer to help regularise the network during
training.
3.3

results

during training the losses over the training and validation
datasets converge to similar values and we test the performance of the trained network using a test set of 1000 unseen examples. the output simulations for eight randomly
selected velocity models and source positions from this set
are shown in fig. 9. we observe that the network is able
to simulate the kinematics of the primary reflections and in
most cases is able to capture their relative amplitudes. we
also plot the network simulation when varying the source location over two velocity models from the test set in fig. 10
and find that the network is able to generalise well over different source locations.
https://doi.org/10.5194/se-11-1527-2020

we test the accuracy of the simulation when using different network designs and training hyperparameters, shown in
fig. a2. we compare example simulations from the test set
when using our baseline conditional autoencoder network,
when halving the number of hidden channels for all layers,
when using an l2 loss function during training, when using
gain exponents of g = 0 and g = 5 in the loss function and
when removing two layers from the encoder and eight layers from the decoder. we plot the histogram of the average
absolute amplitude difference between the ground truth fd
simulation and the network simulation over the test set for
all of the cases above, and observe that in all cases the simulations are less accurate than our baseline approach. without the gain in the loss function, the network only learns to
simulate the direct arrival and the first few reflections in the
receiver responses. with a gain exponent of g = 5, the network simulation is unstable and it fails to simulate the first
0.2 s of the receiver responses. when using the network with
fewer layers, the simulations have edge artefacts, whilst the
network with half the number of hidden channels is closest to
the baseline accuracy. in testing, we find that training a network with the same number of layers but without using a bottleneck design to reduce the velocity model to a 1 Ã— 1 Ã— 1024
latent vector does not converge.
we compare the accuracy of the conditional autoencoder
to the wavenet network in fig. a3. we plot the simulation
from both networks for an example model in the horizontally
layered velocity model test set and the histogram of the average absolute amplitude difference between the ground truth
fd simulation and the wavenet and conditional autoencoder
simulations over this test set. both networks are able to accurately simulate the receiver responses, and the wavenet
simulation is slightly more accurate than the conditional autoencoder, though of course the latter is more general.
we test the generalisation ability of the conditional autoencoder outside of its training distribution by inputting randomly selected 640Ã—640 m boxes from the publicly available
2-d marmousi p-wave velocity model (martin et al., 2006)
into the network. this velocity model contains much more
solid earth, 11, 1527â€“1549, 2020

1538

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

figure 9. conditional autoencoder simulations for eight randomly selected examples in the test set. white circles show the input source
location. the left simulation plots show the network predictions, the middle simulation plots show the ground truth fd simulations and the
right simulation plots show the difference. a t 2.5 gain is applied for display.

complex faulting at multiple scales, higher dips and more
layer variability than our training dataset. the resulting network simulations are shown in fig. 11. we calculate the nearest neighbour to the input velocity model in the set of training
velocity models, defined as the training model with the lowest l1 difference summed over all velocity values from the
input velocity model and show this alongside each example.
we find that the network is not able to accurately simulate
the full seismic response from velocity models which have
large dips and/or complex faulting (fig. 11e, f, h) that are
absent in the training set. this observation is similar to most
studies which analyse the generalisability of deep neural networks outside their training set (e.g. zhang and lin, 2018
and earp and curtis, 2020). however, encouragingly, the net-

solid earth, 11, 1527â€“1549, 2020

work is able to mimic the response from velocity models with
small dips (fig. 11d, g), even though the nearest training-set
neighbour contains a fault, whereas the marmousi layers are
continuous.
we compare the average time taken to generate 100 simulations using the conditional autoencoder network to fd simulation in table 1. we find that on a single cpu core the
network is 22 times faster than fd simulation and when using a gpu and the pytorch library (pytorch, 2016), it is 406
times faster. this is comparable to the speedup obtained with
the wavenet. it is likely that 2-d ray tracing will not offer
the same speedup as observed in sect. 2.6, because computing ray paths through these models is likely to be more demanding. the network takes approximately 4 d to train on

https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

1539

figure 10. conditional autoencoder simulation accuracy when varying the source location. the network simulation is shown for six different
source locations whilst keeping the velocity model fixed. the source positions are regularly spaced across the surface of the velocity model
(white circles). example simulations for two different velocity models in the test set are shown, where each row corresponds to a different
velocity model. the pairs of simulation plots in each row from left to right correspond to the network prediction (left in the pair) and the
ground truth fd simulation (right in the pair), when varying the source location from left to right in the velocity model. a t 2.5 gain is applied
for display.

one nvidia titan v gpu. this is 8 times longer than training
the wavenet network, although we made little effort to optimise its training time. we find that when using only 50 000
training examples the validation loss increases and the network overfits to the training dataset.
4

discussion

both our deep neural networks accurately model the seismic response in horizontally layered and faulted 2-d acoustic media. the wavenet is able to carry out simulation of
horizontally layered velocity models, and the conditional autoencoder is able to generalise to faulted media with arbitrary
layers, fault properties and an arbitrary location of the seismic source on the surface of the media. this is a significantly
harder task than simulating horizontally layered media with
the wavenet network. furthermore, both networks are 1â€“2
orders of magnitude faster than fd modelling.
whilst these results are encouraging and suggest that deep
learning is valuable for simulation, there are further challenges when extending our methods to more complex, elastic and 3-d earth models required for practical simulation
tasks. we believe that further research will help to understand
whether deep learning can aid in these more general settings
and discuss these aspects in more detail below.
4.1

extension to elastic simulation

an important ability for practical geophysical applications
is to be able to simulate seismic waves in (visco)elastic media, rather than acoustic media. the architectures of our nethttps://doi.org/10.5194/se-11-1527-2020

works are readily extendable in this regard; s-wave velocity
and density models could be added as additional input channels to our networks and the number of output channels in the
networks could be increased so that multi-component particle velocity vectors are output. the same training scheme
could be used, with training data generated using elastic fd
simulation instead of acoustic simulation and a loss function
which compares vector fields instead of scalar fields. thus,
with some simple changes to our design, this challenge is at
least conceptually simple to address, though further research
is required to understand if it is feasible. the cost of traditional elastic simulation exceeds the cost of acoustic simulation by orders of magnitude and has prevented the seismic
industry from fully embracing this crucial step. we postulate
that the difference in simulation times between future elastic
and acoustic simulation networks might be smaller compared
to fully discretised methods such as fd, as a consequence of
the networks not needing to compute the entire discretised
wavefield. while this is speculative at this point, it is intriguing to investigate.
4.2

extension to 3-d simulation

another important extension is to move from 2-d to 3-d simulation. in terms of network design, our autoencoder could be
extended to 3-d simulation by increasing the dimensionality
of its input, hidden and output tensors. in this case, we would
expect a similar order of magnitude acceleration of simulation time to 2-d, because the network would still directly
estimate the seismic response without needing to iteratively
model the seismic wavefield through time. however, mulsolid earth, 11, 1527â€“1549, 2020

1540

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

figure 11. generalisation ability of the conditional autoencoder. the conditional autoencoder simulations for five velocity models taken
from different regions of the marmousi p-wave velocity model are shown (dâ€“h). for each example, the left plot shows the input velocity
model and source location, the middle simulation plots show the network prediction (left) and the ground truth fd simulation (right), and
the right plot shows the nearest neighbour in the training set to the input velocity model. simulations from three of the test velocity models
in fig. 9 are also shown with their nearest neighbours (aâ€“c). a t 2.5 gain is applied for display.

tiple challenges arise in this setting. firstly, increasing the
dimensionality would increase the size of the network and
therefore likely increase its training time. finding an alternative representation, such as meshes or oct-trees (ahmed
et al., 2018) to reduce the dimensionality of the problem,
or a way to exploit symmetry in the wave equation to reduce complexity, may be critical in this aspect. secondly, a
major challenge is likely to be the increased computational
cost of generating training data with conventional methods,
which, for instance, is significantly higher in 3-d when using
fd modelling. whilst we only used the subset of the wavefield at each receiver location to train our networks, finding a
way to use the entire wavefield from fd simulation to train
the network may help reduce the number of training simulations required. we note that generating training data are an
amortised cost because the network only needs to be trained

solid earth, 11, 1527â€“1549, 2020

once, and although large, in the case of seismic inversion
where millions of production runs are required the training
cost could become negligible. another intriguing aspect is
to investigate whether deep neural network simulation costs
scale more favourably with increasing frequency Ï‰ compared
to fully discrete methods which scale with Ï‰4 ; in this study,
we only consider simulation at a fixed frequency range.
4.3

generalisation to more complex earth models

perhaps the largest challenge in designing appropriate networks is to improve their generality so they can simulate
more complex earth models. we have shown that deep neural networks can move beyond simulating simple horizontally layered velocity models to more complex faulted models where, to the best of our knowledge, no analytical sohttps://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

1541

figure 12. inverse wavenet predictions for four examples in the test set. red shows the input pressure response at the zero-offset receiver
location, the ground truth reflectivity series and its corresponding velocity model. green shows the inverse wavenet reflectivity series
prediction and the resulting velocity prediction.

lutions exist, which we believe is a positive step. however,
both our networks performed worse on velocity models outside of their training distributions. furthermore, to be able
to generalise to more complex velocity models the conditional autoencoder required more free parameters, more time
to train and more training examples than the wavenet network. generalisation outside of the training distribution is a
well-known and common challenge of deep neural networks
in general (goodfellow et al., 2016).
a naive approach would be to increase the range of the
training data to improve the generality of the network; however, this would quickly become computationally intractable
when trying to simulate all possible earth models. we note
that for many practical applications it may be acceptable to

https://doi.org/10.5194/se-11-1527-2020

use a training distribution with a limited range; for example,
in many of the seismic applications such tomography, fwi
and seismic hazard assessment, a huge number of forward
simulations of comparatively few earth models are carried
out.
a promising research direction may be to better regularise
the networks by adding more physics-based constraints into
the workflow. we found that using causality in the wavenet
generated more accurate simulations than when using a standard convolutional network; this suggested that adding this
constraint helped the network simulate the seismic response,
although it is an open question how best to represent causality when simulating more arbitrary earth models. we also
found that a bottleneck design helped the conditional au-

solid earth, 11, 1527â€“1549, 2020

1542

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

toencoder to converge; our hypothesis is that this encouraged a depth-to-time conversion by slowly reducing the spatial dimensions of the velocity model before expanding them
into time. more advanced network designs, for example, using attention-like mechanisms (vaswani et al., 2017) to help
the network focus on relevant parts of the velocity model,
rather than using convolutional layers with full fields of view,
or using long short-term memory (lstm) cells to help the
network model multiple reverberations could be tested. another interesting direction would be to use the wave equation (eq. 1) to directly regularise the loss function, similar
to the physics-based machine learning approach proposed by
raissi et al. (2019).
we found that the nearest-neighbour test was a useful
way to understand if an input velocity model was close to
the training distribution and therefore if the networkâ€™s output simulation was likely to be accurate. probabilistic approaches, such as bayesian deep learning (gal, 2016), could
be investigated for their ability to provide quantitative uncertainty estimates on the networkâ€™s output simulation.
4.4

inversion with the wavenet

as an additional test, we were also able to retrain the
wavenet network to carry out fast seismic inversion in the
horizontally layered media, which offered a fast alternative to
existing inversion algorithms. we retrained the wavenet network with its inputs and output reversed; its input was then
a set of 11 recorded receiver responses and its output was a
prediction of the corresponding normal incidence reflectivity
series. we used the same wavenet architecture described in
sect. 2.2, except that we inverted its structure to maintain the
causal correlation between the receiver responses and reflectivity series, and we used 128 instead of 256 hidden channels
for each hidden layer. we used exactly the same training data
and training strategy described in sect. 2.3 and 2.4, except
that we used a loss function given by
l=

1
krÌ‚ âˆ’ rk22 ,
n

(4)

where r is the true reflectivity series and rÌ‚ is the predicted
reflectivity series. to recover a prediction of the velocity
model, we carried out a standard 1-d time-to-depth conversion of the output reflectivity values followed by integration.
predictions of the reflectivity series and velocity models
for four randomly selected examples from a test set of unseen examples are shown in fig. 12. the inverse wavenet
network was able to predict the underlying velocity model
for each example, although in some cases small velocity errors propagated with depth, which was likely a result of the
integration of the reflectivity series. the network was able to
produce velocity predictions in the same order of magnitude
time as the forward network (shown in table 1), which is
likely to be a fraction of the time needed for existing seismic
inversion algorithms which rely on forward simulation.
solid earth, 11, 1527â€“1549, 2020

we note that seismic inversion is typically an ill-defined
problem, and it is likely that the predictions of this network
are biased towards the velocity models it was trained on. we
expect the accuracy of the network to reduce when tested on
inputs outside of its training distribution and with real, noisy
seismic data. further research could try to quantify this uncertainty, for example, by using bayesian deep learning. we
have not yet compared our inverse wavenet network to existing seismic inversion techniques, such as posterior sampling
or fwi.
an alternative method for inversion is to use our forward networks in existing seismic inversion algorithms based
on optimisation, such as fwi. both the wavenet and conditional autoencoder networks are fully differentiable and
could therefore be used to generate fast approximate gradient
estimates in these methods. however, similar limitations on
their generality are likely to exist and one would need to be
careful to keep the inversion routine within the training distribution of the networks. furthermore, whilst fast, these approaches would still suffer from the curse of dimensionality
when moving to higher dimensions and require exponentially
more samples to fully explore the parameter space.
4.5

summary

given the potentially large training costs and the challenge
of generality, it may be that current deep learning techniques
are most advantageous to practical simulation tasks where
many similar simulations are required, such as inversion or
statistical seismic hazard analysis, and least useful for problems with a very small number of simulations per model family. in seismology, however, we suspect that most current
and future challenges fall into the former category, which
renders these initial results promising. deep learning approaches have different computational costs and benefits, and
accuracies that are less clearly understood compared to traditional approaches and these should be considered for each
application. further research is required to understand how
best to design the training set for a particular simulation application, as well as how to help deep neural networks generalise to unseen velocity models outside of their training distribution. finally, we note that we only tested two types of
deep neural networks (the wavenet and conditional autoencoders) and many other types exist which could prove more
effective.

5

conclusions

we have investigated the potential of deep learning for aiding seismic simulation in geophysics. we presented two deep
neural networks which are able to carry out fast and largely
accurate simulation of seismic waves. both networks are
20â€“500 times faster than fd modelling and simulate seismic waves in horizontally layered and faulted 2-d acoustic
https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

1543

media. the first network uses a wavenet architecture and
simulates seismic waves in horizontally layered media. we
showed that this network can also be used to carry out fast
seismic inversion of the same media. the second network is
significantly more general than the first; it simulates seismic
waves in faulted media with arbitrary layers, fault properties
and an arbitrary location of the seismic source on the surface of the media. our main contribution is to show that deep
neural networks can move beyond simulating simple horizontally layered velocity models to more complex faulted
models where, to the best of our knowledge, no analytical solutions exist, which we believe is a positive step towards understanding their practical potential. we discussed the challenges of extending our approaches to practical geophysical
applications and future research directions which could address them, noting where it may be favourable for using these
network architectures.

https://doi.org/10.5194/se-11-1527-2020

solid earth, 11, 1527â€“1549, 2020

1544

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

appendix a

figure a1. comparison of different network architectures on simulation accuracy. (a) the wavenet simulated pressure response for a
randomly selected example in the test set (green) compared to ground truth fd simulation (red). (b, c) the simulated response when using
two convolutional network designs with and without exponential dilations. (d) the histogram of the average absolute amplitude difference
between the ground truth fd simulation and the simulations from the wavenet, the dilated convolutional network and 2-d ray tracing over
the test set of 1000 examples. a t 2.5 gain is applied to the receiver responses for display.

solid earth, 11, 1527â€“1549, 2020

https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

1545

figure a2. comparison of different conditional autoencoder network designs and training hyperparameters on simulation accuracy. (a) a
randomly selected velocity model and source location from the test set and its corresponding ground truth fd simulation. (b) the histogram
of the average absolute amplitude difference between the ground truth fd simulation and the simulation from the different cases over the test
set. the histogram of the baseline network over the marmousi test dataset is also shown. (c) a comparison of simulations and their difference
to the ground truth when using our proposed conditional autoencoder (baseline), when halving the number of hidden channels for all layers
(thin), when using an l2 loss function during training (l2 loss), when using gain exponents of g = 0 and g = 5 in the loss function and when
removing two layers from the encoder and eight layers from the decoder (shallow). a t 2.5 gain is applied for display.

figure a3. comparison of wavenet and conditional autoencoder simulation accuracy. panel (a) shows a velocity model, reflectivity series
and ground truth fd simulation for a randomly selected example in the horizontally layered velocity model test set in red. green shows the
wavenet simulation. panel (b) shows the conditional autoencoder simulation for the same velocity model. panel (c) shows the histogram of
the average absolute amplitude difference between the ground truth fd simulation and wavenet and conditional autoencoder simulations
over this test set. a t 2.5 gain is applied for display.

https://doi.org/10.5194/se-11-1527-2020

solid earth, 11, 1527â€“1549, 2020

1546

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

table a1. conditional autoencoder layer parameters. each entry shows the parameterisation of each convolutional layer. the padding column
shows the padding on each side of the input tensor for each spatial dimension.
layer

type

1
2
3
4
5
6
7
8
9
10
11
12
13

conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
conv2d
concat
convt2d
convt2d

in, out channels

kernel size

stride

padding

(1,8)
(8,16)
(16,16)
(16,32)
(32,32)
(32,64)
(64,128)
(128,256)
(256,512)
(512,1024)
(1024,1025)
(1025,1025)
(1025,512)

(3,3)
(2,2)
(3,3)
(2,2)
(3,3)
(2,2)
(2,2)
(2,2)
(2,2)
(2,2)

(1,1)
(2,2)
(1,1)
(2,2)
(1,1)
(2,2)
(2,2)
(2,2)
(2,2)
(2,2)

(1,1)
0
(1,1)
0
(1,1)
0
0
0
0
0

(2,2)
(2,4)

(2,2)
(2,4)

0
0

solid earth, 11, 1527â€“1549, 2020

14
15
16
17
18
19
20
21
22
23
24
25

conv2d
conv2d
convt2d
conv2d
conv2d
convt2d
conv2d
conv2d
convt2d
conv2d
conv2d
conv2d

(512,512)
(512,512)
(512,256)
(256,256)
(256,256)
(256,64)
(64,64)
(64,64)
(64,8)
(8,8)
(8,8)
(8,1)

(3,3)
(3,3)
(2,4)
(3,3)
(3,3)
(2,4)
(3,3)
(3,3)
(2,4)
(3,3)
(3,3)
(1,1)

(1,1)
(1,1)
(2,4)
(1,1)
(1,1)
(2,4)
(1,1)
(1,1)
(2,4)
(1,1)
(1,1)
(1,1)

(1,1)
(1,1)
0
(1,1)
(1,1)
0
(1,1)
(1,1)
0
(1,1)
(1,1)
0

https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media
code and data availability. all our training data were generated
synthetically using the seismic_cpml fd modelling library.
the code to reproduce all of our data and results is available at
https://github.com/benmoseley/seismic-simulation-complex-media
(moseley, 2020).

author contributions. tnm and am were involved in the conceptualisation, supervision and review of the work. bm was involved
in the conceptualisation, data creation, methodology, investigation,
software, data analysis, validation and writing.

competing interests. tarje nissen-meyer is a topical editor for the
solid earth editorial board.

acknowledgements. the authors would like to thank the computational infrastructure for geodynamics (https://www.geodynamics.
org/, last access: 9 august 2020) for releasing the open-source
seismic_cpml fd modelling libraries. we would also like
to thank tom le paine for his fast wavenet implementation
on github which our code was based on (https://github.com/
tomlepaine/fast-wavenet/, last access: 9 august 2020), as well as
our reviewers andrew curtis and andrew valentine for their valuable and in-depth feedback.

financial support. this research has been supported by the centre for doctoral training in autonomous intelligent machines and
systems at the university of oxford, oxford, uk, and the uk engineering and physical sciences research council.

review statement. this paper was edited by caroline beghein and
reviewed by andrew curtis and andrew valentine.

references
abadi, m., agarwal, a., barham, p., brevdo, e., chen, z., citro,
c., corrado, g. s., davis, a., dean, j., devin, m., ghemawat,
s., goodfellow, i., harp, a., irving, g., isard, m., jia, y., jozefowicz, r., kaiser, l., kudlur, m., levenberg, j., manÃ©, d.,
monga, r., moore, s., murray, d., olah, c., schuster, m.,
shlens, j., steiner, b., sutskever, i., talwar, k., tucker, p., vanhoucke, v., vasudevan, v., viÃ©gas, f., vinyals, o., warden, p.,
wattenberg, m., wicke, m., yu, y., and zheng, x.: tensorflow: large-scale machine learning on heterogeneous systems, https://www.tensorflow.org, last access: 9 august 2020,
2015.
ahmed, e., saint, a., shabayek, a. e. r., cherenkova, k., das,
r., gusev, g., aouada, d., and ottersten, b.: a survey on deep
learning advances on different 3d data representations, arxiv
[preprint], https://arxiv.org/abs/1808.01462, 2018.
aki, k. and richards, p. g.: quantitative seismology, w. h. freeman and co., new york, new york, 1980.

https://doi.org/10.5194/se-11-1527-2020

1547

araya-polo, m., jennings, j., adler, a., and dahlke, t.: deeplearning tomography, the leading edge, 37, 58â€“66, 2018.
bergen, k. j., johnson, p. a., de hoop, m. v., and
beroza, g. c.: machine learning for data-driven discovery in solid earth geoscience, science, 363, eaau0323,
https://doi.org/10.1126/science.aau0323, 2019.
bohlen, t.: parallel 3-d viscoelastic finite difference seismic modelling, comput. geosci., 28, 887â€“899, 2002.
boore, d. m.: simulation of ground motion using the stochastic
method, pure appl. geophys., 160, 635â€“676, 2003.
bozdagÌ†, e., peter, d., lefebvre, m., komatitsch, d., tromp, j., hill,
j., podhorszki, n., and pugmire, d.: global adjoint tomography:
first-generation model, geophys. j. int., 207, 1739â€“1766, 2016.
chopra, s. and marfurt, k. j.: seismic attributes for prospect identification and reservoir characterization, society of exploration
geophysicists and european association of geoscientists and
engineers, 2007.
cui, y., olsen, k. b., jordan, t. h., lee, k., zhou, j., small,
p., roten, d., ely, g., panda, d. k., chourasia, a., levesque,
j., day, s. m., and maechling, p.: scalable earthquake simulation on petascale supercomputers, in: 2010 acm/ieee international conference for high performance computing, networking, storage and analysis, new orleans, la, usa, 13â€“19
november 2010, 1â€“20, 2010.
devilee, r. j. r., curtis, a., and roy-chowdhury, k.: an efficient,
probabilistic neural network approach to solving inverse problems: inverting surface wave velocities for eurasian crustal thickness, j. geophys. res.-sol. ea., 104, 28841â€“28857, 1999.
dowla, f. u., taylor, s. r., and anderson, r. w.: seismic discrimination with artificial neural networks: preliminary results with
regional spectral data, b. seismol. soc. am., 80, 1346â€“1373,
1990.
earp, s. and curtis, a.: probabilistic neural network-based 2d
travel-time tomography, neural comput. appl., 1â€“19, 2020.
fichtner, a.: full seismic waveform modelling and inversion,
springer, 2010.
gal, y.: uncertainty in deep learning, phd thesis, university of
cambridge, 2016.
goodfellow, i., bengio, y., and courville, a.: deep learning, mit
press, 2016.
gu, j., wang, z., kuen, j., ma, l., shahroudy, a., shuai, b., liu,
t., wang, x., wang, g., cai, j., and chen, t.: recent advances
in convolutional neural networks, pattern recogn., 77, 354â€“377,
2018.
guo, x., li, w., and iorio, f.: convolutional neural networks for
steady flow approximation, in: proceedings of the 22nd acm
sigkdd international conference on knowledge discovery and
data mining â€“ kdd â€™16, san francisco, ca, usa, august 2016,
481â€“490, 2016.
gutenberg, b.: the amplitudes of waves to be expected in seismic
prospecting, geophysics, 1, 252â€“256, 1936.
hosseini, k., sigloch, k., tsekhmistrenko, m., zaheri, a., nissenmeyer, t., and igel, h.: global mantle structure from multifrequency tomography using p, pp and p-diffracted waves, geophys. j. int., 220, 96â€“141, 2019.
igel, h.: computational seismology: a practical introduction, oxford university press, 2017.
ioffe, s. and szegedy, c.: batch normalization: accelerating deep
network training by reducing internal covariate shift, in: 32nd

solid earth, 11, 1527â€“1549, 2020

1548

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media

international conference on machine learning, icml 2015, 7â€“
9 july 2015, lille, france, 1, 448â€“456, 2015.
kingma, d. p. and ba, j.: adam: a method for stochastic optimization, arxiv [preprint], https://arxiv.org/abs/1412.6980, 2014.
komatitsch, d. and martin, r.: an unsplit convolutional perfectly
matched layer improved at grazing incidence for the seismic
wave equation, geophysics, 72, sm155â€“sm167, 2007.
komatitsch, d. and tromp, j.: introduction to the spectral element
method for three-dimensional seismic wave propagation, geophys. j. int., 139, 806â€“822, 1999.
kong, q., trugman, d. t., ross, z. e., bianco, m. j., meade, b. j.,
and gerstoft, p.: machine learning in seismology: turning data
into insights, seismol. res. lett., 90, 3â€“14, 2019.
krischer, l. and fichtner, a.: generating seismograms with deep
neural networks, agu fall meeting abstracts, 11â€“15 december 2017, new orleans, louisiana, usa, 2017.
krischer, l., hutko, a. r., van driel, m., stÃ¤hler, s., bahavar, m.,
trabant, c., and nissen-meyer, t.: on-demand custom broadband synthetic seismograms, seismol. res. lett., 88, 1127â€“
1140, 2017.
leng, k., nissen-meyer, t., and van driel, m.: efficient global
wave propagation adapted to 3-d structural complexity: a
pseudospectral/spectral-element approach, geophys. j. int., 207,
1700â€“1721, 2016.
leng, k., nissen-meyer, t., van driel, m., hosseini, k., and alattar, d.: axisem3d: broad-band seismic wavefields in 3-d
global earth models with undulating discontinuities, geophys. j.
int., 217, 2125â€“2146, 2019.
lerer, a., gross, s., and fergus, r.: learning physical intuition of
block towers by example, proceedings of the 33rd international
conference on international conference on machine learning,
20â€“22 june 2016, new york, ny, usa, 48, 430â€“438, 2016.
long, g., zhao, y., and zou, j.: a temporal fourth-order scheme
for the first-order acoustic wave equations, geophys. j. int., 194,
1473â€“1485, 2013.
lumley, d. e.: time-lapse seismic reservoir monitoring, geophysics, 66, 50â€“53, 2001.
margrave, g. f. and lamoureux, m. p.: numerical methods of exploration seismology, cambridge university press, 2018.
martin, g. s., wiley, r., and marfurt, k. j.: marmousi2: an elastic
upgrade for marmousi, leading edge, 25, 156â€“166, 2006.
moczo, p., robertsson, j. o., and eisner, l.: the finite-difference
time-domain method for modeling of seismic wave propagation, adv. geophys., 48, 421â€“516, 2007.
moseley, b.: code repository for deep learning for fast simulation
of seismic waves in complex media, available at: https://github.
com/benmoseley/seismic-simulation-complex-media, last access: 9 august 2020.
moseley, b., markham, a., and nissen-meyer, t.: fast approximate
simulation of seismic waves with deep learning, arxiv [preprint],
https://arxiv.org/abs/1807.06873, 2018.
murat, m. e. and rudman, a. j.: automated first arrival picking:
a neural network approach, geophys. prospect., 40, 587â€“604,
1992.
nair, v. and hinton, g.: rectified linear units improve restricted
boltzmann machines vinod nair, in: proceedings of icml, 21â€“
24 june 2010, haifa, israel, 27, 807â€“814, 2010.
nath, s. k., chakraborty, s., singh, s. k., and ganguly, n.: velocity inversion in cross-hole seismic tomography by counter-

solid earth, 11, 1527â€“1549, 2020

propagation neural network, genetic algorithm and evolutionary
programming techniques, geophys. j. int., 138, 108â€“124, 1999.
newman, p.: divergence effects in a layered earth, geophysics, 38,
481â€“488, 1973.
ni, s., tan, e., gurnis, m., and helmberger, d.: sharp sides to the
african superplume, science, 296, 1850â€“1852, 2002.
paganini, m., de oliveira, l., and nachman, b.: accelerating science with generative adversarial networks: an application to
3d particle showers in multilayer calorimeters, phys. rev. lett.,
120, 1â€“6, 2018.
perol, t., gharbi, m., and denolle, m.: convolutional neural network for earthquake detection and location, science advances,
4, e1700578, 2018.
poulton, m. m., sternberg, b. k., and glass, c. e.: location of subsurface targets in geophysical data using neural networks, geophysics, 57, 1534â€“1544, 1992.
pytorch: available at: https://www.pytorch.org (last access: 9 august 2020), 2016.
raissi, m., perdikaris, p., and karniadakis, g. e.: physics-informed
neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, j. comput. phys., 378, 686â€“707, 2019.
richardson, a.: seismic full-waveform inversion using deep
learning tools and techniques, arxiv [preprint], https://arxiv.
org/abs/1801.07232, 2018.
rietmann, m., messmer, p., nissen-meyer, t., peter, d., basini, p.,
komatitsch, d., schenk, o., tromp, j., boschi, l., and giardini,
d.: forward and adjoint simulations of seismic wave propagation
on emerging large-scale gpu architectures, international conference for high performance computing, networking, storage
and analysis, sc, november 2012, salt lake city, ut, 1â€“11,
2012.
rÃ¶th, g. and tarantola, a.: neural networks and inversion of seismic data, j. geophys. res., 99, 6753, 1994.
russell, b. h.: introduction to seismic inversion methods, society
of exploration geophysicists, 1988.
schuster, g. t.: seismic inversion, society of exploration geophysicists, 2017.
sun, h. and demanet, l.: low frequency extrapolation with deep
learning, 2018 seg international exposition and annual meeting, 14â€“19 october 2018, anaheim, ca, usa, 2011â€“2015,
2018.
tarantola, a.: inverse problem theory: methods for data fitting and
model parameter estimation, elsevier, 1987.
thorne, m. s., pachhai, s., leng, k., wicks, j. k., and nissenmeyer, t.: new candidate ultralow-velocity zone locations
from highly anomalous spdks waveforms, minerals, 10, 211,
2020.
valentine, a. p. and trampert, j.: data space reduction, quality assessment and searching of seismograms: autoencoder networks
for waveform data, geophys. j. int., 189, 1183â€“1202, 2012.
van den oord, a., dieleman, s., zen, h., simonyan, k., vinyals, o.,
graves, a., kalchbrenner, n., senior, a., and kavukcuoglu, k.:
wavenet: a generative model for raw audio, arxiv [preprint],
https://arxiv.org/abs/1609.03499, 2016.
van der baan, m., and jutten, c.: neural networks in geophysical
applications, geophysics, 65, 1032â€“1047, 2000.

https://doi.org/10.5194/se-11-1527-2020

b. moseley et al.: deep learning for fast simulation of seismic waves in complex media
van driel, m., and nissen-meyer, t.: optimized viscoelastic wave
propagation for weakly dissipative media, geophys. j. int., 199,
1078â€“1093, 2014a.
van driel, m., and nissen-meyer, t.: seismic wave propagation
in fully anisotropic axisymmetric media, geophys. j. int., 199,
880â€“893, 2014b.
van driel, m., ceylan, s., clinton, j. f., giardini, d., alemany,
h., allam, a., ambrois, d., balestra, j., banerdt, b., becker,
d., bÃ¶se, m., boxberg, m. s., brinkman, n., casademont, t.,
chÃ¨ze, j., daubar, i., deschamps, a., dethof, f., ditz, m., drilleau, m., essing, d., euchner, f., fernando, b., garcia, r., garth,
t., godwin, h., golombek, m. p., grunert, k., hadziioannou,
c., haindl, c., hammer, c., hochfeld, i., hosseini, k., hu,
h., kedar, s., kenda, b., khan, a., kilchling, t., knapmeyerendrun, b., lamert, a., li, j., lognonnÃ©, p., mader, s., marten,
l., mehrkens, f., mercerat, d., mimoun, d., mÃ¶ller, t., murdoch, n., neumann, p., neurath, r., paffrath, m., panning, m. p.,
peix, f., perrin, l., rolland, l., schimmel, m., schrÃ¶er, c.,
spiga, a., stÃ¤hler, s. c., steinmann, r., stutzmann, e., szenicer,
a., trumpik, n., tsekhmistrenko, m., twardzik, c., weber, r.,
werdenbach-jarklowski, p., zhang, s., and zheng, y.: preparing
for insight: evaluation of the blind test for martian seismicity,
seismol. res. lett., 90, 1518â€“1534, 2019.
vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l.,
gomez, a. n., kaiser, l., and polosukhin, i.: attention is
all you need, arxiv [preprint], https://arxiv.org/abs/1706.03762,
2017.

https://doi.org/10.5194/se-11-1527-2020

1549

vinje, v., iversen, e., and gjoystdal, h.: traveltime and amplitude
estimation using wavefront construction, geophysics, 58, 1157â€“
1166, 1993.
virieux, j. and operto, s.: an overview of full-waveform inversion
in exploration geophysics, geophysics, 74, 6, 2009.
wu, y. and lin, y.: inversionnet: a real-time and accurate full
waveform inversion with cnns and continuous crfs, arxiv
[preprint], https://arxiv.org/abs/1811.07875, 2018.
xie, x.-b., jin, s., and wu, r.-s.: wave-equation-based seismic illumination analysis, geophysics, 71, s169â€“s177, 2006.
yang, f. and ma, j.: deep-learning inversion: a next-generation
seismic velocity model building method, geophysics, 84, r583â€“
r599, 2019.
zhang, z. and lin, y.: data-driven seismic waveform inversion:
a study on the robustness and generalization,arxiv [preprint],
https://arxiv.org/abs/1809.10262, 2018.
zhu, w., sheng, y., and sun, y.: wave-dynamics simulation using deep neural networks, stanford report, stanford vision and
learning lab, stanford university, ca, usa, 2017.

solid earth, 11, 1527â€“1549, 2020



rapid seismic waveform modeling and inversion
with neural operators
yan yang, angela f. gao, kamyar azizzadenesheli, robert w. clayton, and zachary e. ross
abstractâ€” seismic waveform modeling is a powerful tool for
determining earth structure models and unraveling earthquake
rupture processes, but it is usually computationally expensive. we
introduce a scheme to vastly accelerate these calculations with a
recently developed machine learning paradigm called the neural
operator. once trained, these models can simulate a full wavefield
at negligible cost. we use a u-shaped neural operator to learn a
general solution operator to the 2d elastic wave equation from an
ensemble of numerical simulations performed with random
velocity models and source locations. we show that full waveform
modeling with neural operators is nearly two orders of magnitude
faster than conventional numerical methods, and more
importantly, the trained model enables accurate simulation for
velocity models, source locations, and mesh discretization
distinctly different from the training dataset. the method also
enables convenient full-waveform inversion with automatic
differentiation.
index terms â€” geophysics, machine learning, partial
differential equations (pdes), waveform modeling, full-waveform
inversion

i. introduction

t

he seismic wave equation relates displacement fields
to external forces and the density and elastic structure in
the earth. solutions to the wave equation form the basis
of ground shaking simulations of large earthquakes [1]â€“[3] and
full waveform inversion for earthâ€™s structure [4]â€“[6]. due to the
highly heterogeneous nature of the earth, as exemplified by
subduction zones and sedimentary basins, there are no exact
analytical solutions for these wavefields. instead, approximate
solutions are made possible by approximating derivatives
through discretized spatial and time or frequency domains.
finite difference methods (fdm) have been popular since the
early 80s due to their relatively straightforward formulation
[7]â€“[9]. the spectral-element method (sem), a particular case
of finite element methods (fem), which was introduced to
seismology in early 2000s, combined the flexibility of fems
with the accuracy of spectral approaches [10]â€“[13]. these
numerical solvers impose a tradeoff between resolution and
computation speed, with the computational cost proportional to
the fourth power of frequency [14]. thus, the cost of wave
simulation is a major barrier to using full-waveform techniques
for seismic inversion and updating models of the subsurface
with new data.
a number of machine learning-based methods have been
proposed in the past few years to provide a faster alternative for
tackling seismological problems such as signal denoising [15]â€“
[17], event detection [18]â€“[20], and phase association [21],
[22]. deep neural networks have also recently been used to
solve partial differential equations (pdes), such as the eikonal
equation and wave equation [23]â€“[27]. these approaches to

solving pdes offer not only speedup in computational
capabilities, but also low-memory overhead, differentiability,
and on-demand solutions. such advantages facilitate deep
learning being used for seismic inversion [28]â€“[32]. however,
one major limitation of these approaches is that the solutions
generated by these models are dependent on the specific spatial
and temporal discretization in the numerical simulation training
set.
recently, a paradigm named â€˜neural operatorâ€™ was developed
to address the mesh-dependent shortcoming of classical neural
networks by creating a single deep learning model that can be
applied to different discretizations [33]â€“[36]. this is made
possible because neural operators can provably learn mappings
between infinite-dimensional function spaces [37] and
therefore are suitable for learning general solution operators to
pdes, which are valid even when the pde coefficients (e.g.
elastic properties) are varied. since first introduced [33], a
variety of neural operator models have been developed. in
particular, the fourier neural operator (fno) is a model that
uses the fast fourier transform as an integral operator, and has
been shown to outperform other neural operators in terms of
efficiency and accuracy [38]. the fno has been applied to
many types of scientific problems including weather
forecasting [39], co2 sequestration [40], and coastal flooding
[41].
within the domain of seismology, neural operators were also
recently used to learn general solution operators to the 2-d
acoustic wave equation, a simplified case of the elastic wave
equation [42]. this pilot study demonstrated that it was possible
for a single fno model to predict a complete wavefield given
an arbitrary velocity model and mesh discretization. the
success of this limited case highlights the potential of these
methods, however, extending the method from the acoustic
wave equation to the elastodynamic case requires substantially
increased model complexity. by comparison with neural
networks, fno is not considered to be a deep architecture, and
is most analogous to the fully-connected neural networks
employed heavily until the 2010s. a u-shaped neural operator
(u-no) was recently proposed to enable very deep neural
operators and facilitate fast training, data efficiency, and
hyperparameter selection robustness [43].
in this paper, we apply the u-no architecture to full seismic
waveform modeling. we train a u-no model to learn a general
solution operator to the 2d elastic wave equation and
demonstrate that the trained model enables fast and accurate
simulation for source locations, velocity structures and mesh
discretization beyond the training dataset. the trained u-no
also allows for efficient full-waveform inversion with
automatic differentiation.

ii. methods
a. neural operator learning
operators are maps between function spaces, and the purpose
of operator learning is to learn the operator given a dataset of
input-output pairs. in seismology, it is common to write
solutions to the wave equation, ğ‘ˆ(ğ‘¥) in terms of a linear
integral operator acting on a source function, ğ´(ğ‘¥),
ğ‘ˆ(ğ‘¥) = âˆ« ğº(ğ‘¥, ğ‘¦)ğ´(ğ‘¦)ğ‘‘ğ‘¦.
(1)
where ğ‘¥ âˆˆ â„! is the physical domain and ğº is a so-called
greenâ€™s function defined for a particular velocity model.
equation 1 holds so long as the velocity model is not varied
because the wave equation remains a linear operator.
instead, if we consider the case where the input function,
ğ´(ğ‘¥), is a velocity model, the solution operator, â„’, relating this
to ğ‘ˆ(ğ‘¥) is nonlinear and cannot be written in the form of (1),
ğ‘ˆ(ğ‘¥) = (â„’ ğ´)(ğ‘¥).
the most general version of the nonlinear solution operator â„’
for the elastic wave equation is not known in closed form.
neural operators are a class of models that aim to solve this
problem, as they provably can learn a wide array of nonlinear
operators. their basic form consists of a composition of linear
operators with nonlinear activations. more specifically, a neural
operator with l layers can be written as:
ğ‘£" (ğ‘¥) = (ğ‘ƒ ğ´)(ğ‘¥),
(ğ‘¥)
(ğ‘¥)
ğ‘£#$%
= ğœ3ğ‘Š# ğ‘£#
+ âˆ« ğœ…# (ğ‘¥, ğ‘¦)ğ‘£# (ğ‘¦)ğ‘‘ğ‘¦7, ğ‘™ = 0, . . , ğ¿ âˆ’ 1
ğ‘ˆ(ğ‘¥) = (ğ‘„ ğ‘£& )(ğ‘¥).
(2)
where ğ‘£# is the input function at the ğ‘™'( layer, ğ‘ƒ is a pointwise
operator that lifts the input function to a higher dimensionality,
ğ‘„ is a pointwise operator that projects the function back to the
desired output dimensionality, ğ‘Š# is a linear pointwise
transformation that can keep track of non-periodic boundary
behavior, ğœ is a pointwise nonlinear activation operator, and ğœ…#
is a kernel function that acts along with the integral as a global
linear operator.
a neural operator is parameterized by ğ‘ƒ, ğ‘„, ğ‘Š# , and ğœ…# . a
critical aspect of this class of models is that these parameters
are independent of the numerical discretization of the physical
domain, i.e. they are shared across all possible discretizations
in a similar way that in convolutional networks, the parameters
are shared across neurons. it is this property that allows for the
learning of maps between infinite dimensional function spaces,
as the discretization can be chosen dynamically at inference
time independently of what was used for training.
if we are given a dataset of ğ‘ numerical simulations,
{ğ´) , ğ‘ˆ) }+
)*% , where the ğ´) are chosen to span the range of the
expected function space, we can train a neural operator in a
supervised fashion to map from arbitrary ğ´ into ğ‘ˆ.
due to the expense of evaluating integral operators, neural
operators may lack the efficiency of convolutional or recurrent
neural networks in finite-dimensional settings. the fno was

proposed to mitigate this difficulty through the fast fourier
transform [38]. the kernel integral operator in (2) can be
considered a convolution operator, defined in fourier space as:
âˆ« ğœ…# (ğ‘¥, ğ‘¦)ğ‘£# (ğ‘¦)ğ‘‘ğ‘¦ = â„± ,% (â„±(ğœ…# ) âˆ™ â„±(ğ‘£# )).
(3)
where â„± and â„± ,% denote fourier transform and its inverse,
respectively. however, fno imposes that each layer is a map
between functions spaces with identical domain spaces, which
may cause a large memory usage. the u-no, an analogy to the
u-net architectures, was proposed to allow progressively
transforming the input function space with respect to a sequence
of varying domains [43], [44]. after the lifting operator ğ‘ƒ, a
sequence of ğ¿% non-linear integral operators ğº) is applied to ğ‘£"
and map the input to a set of functions with decreasing
dimensional domain. then a sequence of ğ¿- non-linear integral
operators ğº) is applied to ğ‘£&!$% and map the input to a set of
functions with increasing dimensional domain before the
projection operator ğ‘„. skip connections [44] are included to
add vector-wise concatenation of ğ‘£&!$) and ğ‘£&!,) . the
contracting and expanding parts are symmetric. the
architecture of the u-no used in this study is illustrated in
figure 1, and we refer the interested readers to the references
[33]â€“[36], [38], [43] for more details.
b. numerical simulation
we set up a training dataset of random source locations, swave velocity (vs) models, and p- to s-wave velocity ratios
(vp/vs). we define the velocity model on a 64 Ã— 64 mesh with
0.16 km grid spacing. the source is set as an isotropic explosive
source randomly distributed on the mesh. the vs has an
average background of 3 km/s and perturbed by random fields
with a von kÃ¡rmÃ¡n covariance function with the following
parameters: hurst exponent ğœ…=0.5â , correlation length ğ‘. =ğ‘/ =8
grids, and the fractional magnitude of the fluctuation ğœ€=10%
background velocityâ â . the power spectral density function of the
von kÃ¡rmÃ¡n type random field follows a power law (fractal
randomness) and can accurately represent the distribution of
earthâ€™s heterogeneity [45]. the vp/vs is simplified to an
average background of 1.732 perturbed by a smooth gaussian
random field with the following parameters: correlation length
ğœ†=32 grids, standard deviation ğœ=2% background. this work,
as our very first experiment to evaluate the feasibility of solving
2d elastic wave equations, wants to focus on the parameters
that the wavefield is most sensitive to. therefore, we use the
empirical relation between density and vs to compute the
density [46]. other input parameters such as density and
attenuation may be explored in future work. a total of 20,000
random sets of models are generated and each of them is input
to a gpu based 2d finite difference code in cartesian
coordinates to simulate the 2-d displacement field [47]. for
simulation, the top boundary is set with a free-surface boundary
condition and the other three edges have absorbing boundary
conditions. a total of 4-sec wavefield with a time step of 0.01
sec and a major frequency content up to 6 hz is simulated. each
simulation takes about 1.23 sec with a gpu memory usage of
0.3 gb.

c. u-no model training
we developed a framework that applies u-no to the 2d
elastic wave equation. the architecture is depicted
schematically in figure 1. u-no takes the source location and
vp and vs as inputs, where vp is calculated from vs and vp/vs.
vp and vs are then passed through a point-wise lifting operator.
a sequence of non-linear integral operators (encoders) are
applied that gradually contract the physical domain size after
each inverse fourier transform step, while simultaneously
increasing the number of channels in the co-domain. these
operators are followed by a sequence of non-linear integral
operators (decoders) that progressively expand the physical
domain, and decrease the number of channels. finally, a pointwise projection operator leads to the output function [43]. the
output of the u-no model is the complete horizontal and
vertical displacement wavefield function over the medium
domain, which can be queried at any mesh points desired,
regardless of the input and output training mesh used.
we describe the detailed parameters used in u-no below
following the notations in figure 1. the goal is to learn an
operator mapping from the input function ğ‘ to the output
function ğ‘¢. the training is on an input mesh of ğ‘‹01 Ã— ğ‘Œ01 Ã— ğ‘‡01
and an output mesh of ğ‘‹23' Ã— ğ‘Œ23' Ã— ğ‘‡23' Ã— ğ¶23' , where ğ‘‹01 =
ğ‘Œ01 = ğ‘‹23' = ğ‘Œ23' = 64, ğ‘‡01 = 3 representing source, vs, and
vp/vs distribution on the mesh, ğ‘‡23' = 128 for 32 hz data
output, and ğ¶23' = 2 representing two displacement
components (horizontal and vertical). this work applies the uno architecture designed for mapping between 3-d spatiotemporal function domains (ğ‘¥, ğ‘¦, ğ‘¡) without any recurrent
composition in time [43]. the fourth dimension ğ¶23' of the
output function ğ‘¢ can be created in the last step through the
projection operator ğ‘„. constructing the operator to learn the
mapping between 3-d spatio-temporal function domains:
ğº: {ğ‘: [0,1]- Ã— [0, ğ‘‡01 ] â†’ â„4" }
â†’ {ğ‘¢: [0,1]- Ã— [0, ğ‘‡23' ] â†’ â„4# }.
(4)
the operators {ğº) }&)*" as shown in figure 1 that are used to
construct the u-no are defined as:
ğº) : {ğ‘£) : [0, ğ›¼) ]- Ã— ğ’¯) â†’ â„4$% }
&
â†’ yğ‘£)$% : [0, ğ‘)5 ğ›¼) ]- Ã— ğ‘)6 ğ’¯) â†’ â„7% 4$% [.
(5)
where [0, ğ›¼) ]- Ã— ğ’¯) is the domain of the input function ğ‘£) to the
operator ğº) , and ğ‘)5 , ğ‘)6 , and ğ‘)7 are the expansion or contraction
factors for the spatial domain, temporal domain, and co-domain
for ğ‘– '( operator, respectively. note that ğ’¯" = [0, ğ‘‡01 ] , ğ›¼" =
ğ›¼&$% = 1, and ğ’¯&$% = [0, ğ‘‡23' ]. in this work we set the number
of layers to ğ¿=8. the details of the expansion and contraction
factors ğ‘)5 , ğ‘)6 , and ğ‘)7 are in figure 1. the lifting operator ğ‘ƒ to
convert the input to a higher dimension channel space is a fullyconnected neural network with channel number ğ‘‘" = 16. the
projection operator ğ‘„ to the output domain is also a fully
connected neural network. the activation function used in each
fno block is the gaussian error linear unit (gelu) [48].
with the simulation dataset and the u-no design, we train the
u-no model in a supervised manner with the objective of
learning the general solution operator to the wave equation for
arbitrary inputs. we divide the training dataset into 90%

training and 10% validation. the model is trained with a batch
size of 8. after hyperparameter tuning, the loss function we use
in model training is the 90% relative l1 loss plus 10% relative
l2 loss. the incorporation of l1-norm loss is more resistant to
outliers. we use an adam optimizer [49] with a learning rate of
10,8 and a weight decay of 10,9 . we trained for 100 epochs,
which takes approximately 40 minutes per epoch using a single
nvidia tesla v100 gpu with 24gb memory usage. a 70%
of loss decrease is achieved in the first 10 epochs. once the uno model is trained, the model parameters require gpu usage
of 3.8 gb and the time for an evaluation on a new source and
velocity model takes only 0.02 sec with gpu usage of 0.9 gb.
iii. results
a. the number of simulations needed for training
once completely trained, the u-no model can be evaluated
on a new input with very little computational cost (0.02 sec
compared to the fdm runtime of 1.23 sec). the number of
training simulations is the main factor in the computational
cost. in the training process, we split the entire training dataset
to 90% for training and 10% for validation. we test the
performance of the model on the velocity models out of the
training data set. we can see that the u-no model trained on a
dataset of 5000 simulations can already predict the major phase
arrivals, while increasing the dataset size from 5000 simulations
to 20000 provides better fit to the amplitudes (figure 2). with
a training dataset of 20000 simulations, the validation and
training loss are very close, indicating there is no overfitting of
the training data.
b. generalizability to arbitrary velocity structure or
discretization
the u-no model is trained on random velocity models
generated with the von karman correlation function, which can
best mimic the earthâ€™s heterogenous velocity distribution [45],
[50]. we show by example that the u-no model, although
trained on random velocity models with some certain
parameters, is applicable to arbitrary velocity models. these
outcomes are in fact expected from theoretical grounds because
most physical functions can be approximated to arbitrary
accuracy by random fields.
our first example is with velocity models from a von karmantype random distribution, but with a different covariance
function than the one used for the training data. we increase the
roughness of the velocity structure by a factor of four by
decreasing the correlation length of vs and vp/vs to only onefourth that of the training data. as shown in figure 3, the
wavefield snapshot has more coda than with the smoother
models because of the scattering from increased heterogeneity.
however, the coda waves are well modeled by u-no when
compared to the ground truth simulation by fdm.
the velocity models used in the training data do not have
coherent structures with discontinuities as in the real earth, but
wavefields for such models can still be simulated with our
method. as mentioned before, this is because discontinuous
functions can be approximated to arbitrary accuracy by random

fields. figure 4 shows a simple model with a dipping â€˜slabâ€™
embedded in a homogeneous background. the slab has 20%
higher vs and 5% lower vp/vs. the wavefield snapshots show
that the reflections from the high velocity anomaly are clearly
predicted by u-no. a more complex example is shown in
figure 5, where a random subpanel of the marmousi model, a
2d velocity model with complex vertical and horizontal
structures used in exploration studies [51], is used. the
reflected and refracted waves are very complicated due to the
presence of folding and faulting, but the u-no predictions still
closely approximate the numerical solutions (figure 5).
one of the most important advantages of a neural operator
compared with a neural network is its mesh-free nature, since it
intrinsically learns the mapping between function spaces. a
model trained on a particular mesh can be evaluated on any
other mesh, even at finer spacing. the fourier layers may learn
from and evaluate functions on any discretization because
parameters are directly learned in fourier space and resolving
the functions in physical space is simply projecting on the basis
[38]. the example in figure 6 shows the u-no trained on a grid
of 64*64 nodes applied to an input velocity model with
160*160 nodes. here both the input velocity model and the
output wavefield can be seen at a much higher resolution, yet
u-no provides comparable prediction with the fdm solver.
note that if the resolution is increased by a factor of 2, a gridbased numerical solver like fdm takes about 6 times greater
computational time; however, the evaluation using u-no takes
only about 2.5 times longer, providing additional computational
efficiency.
we evaluate the overall generalization performance of the
trained u-no by performing a thousand random realizations on
each of these cases. the distribution of the relative l2-norm
misfit and cross-correlation coefficient are plotted in figure 7.
in the case of the marmousi model, the extended tail of the
histogram is attributed to the model's imbalanced complexity.
in general, however, we see a very high cross-correlation
coefficient (>0.95) between u-no prediction and ground truth,
confirming its robust generalizability.
c. application to full-waveform inversion
one of the most important applications of wavefield
simulations is in full-waveform inversion (fwi), which uses the
full recorded waveform to image the earthâ€™s interior. the
adjoint-state method is the traditional approach for computing
the gradients of an objective function with respect to parameters
of interest [4], [5]. neural operators are differentiable by
design, which enables gradient computation with reverse-mode
automatic differentiation. it has been shown that automatic
differentiation and the adjoint approach are mathematically
equivalent [28]. hence, the trained u-no model allows for
convenient fwi and the associated speed and accuracy should
depend only on the forward modeling part.
we demonstrate the inversion performance using the velocity
structure of random subpanels in the marmousi model [51]. the
synthetic waveform data are simulated with fdm [47] using 14
sources distributed in a ring shape. in figure 8, we use the true
source location, receivers on all 64*64 grids and noise-free

waveform data; the goal here is not to demonstrate resolution,
but rather the computational accuracy of the method. we then
invert for vp and vs simultaneously by starting with
homogeneous initial vp and vs models and forward
propagating the wavefield with the u-no for each source. the
misfit is defined by the mean square error between the forward
modeled and true wavefield. the gradient of the misfit with
respect to vp and vs can be computed through automatic
differentiation. vp and vs are then iteratively updated with
gradient descent for 100 iterations using the adam optimizer
[49] with a learning rate of 0.01. each iteration takes only about
1.4 sec by taking advantage of u-no forward computation. the
results in figure 8 show a relative l2-norm misfit between the
true and inverted model of only 3%. this successful inversion,
in turn, further validates the accuracy of forward modeling with
u-no.
besides the fact that the inversion target velocity model is
quite different from the smooth random fields in the training
dataset, this experiment itself is difficult due to conventional
problems in full-waveform inversion, such as cycle skipping
(multiple local maxima in the least-squares misfit function). we
also show that if we only use 64 receivers on the surface, the
inversion results in the region with ray path coverage are still
reasonably accurate (figure 9). inversion with a biased
homogenous initial model is also capable of producing
relatively accurate results (see supplementary materials).
iv. discussion and conclusion
we use the relative l2 loss between the fdm and u-no
predictions to evaluate the performance of the trained model for
generalization. the relative l2 loss is defined as the l2-norm
of the difference between the prediction and ground truth
divided by the l2-norm of the ground truth. this ratio is used
to evaluate the performance of the trained model. when using
the same mesh discretization as the training data, the relative l2
loss is around 10â€“20 percent, but this number rises to 30â€“40
percent when the tests are performed on finer grids (figure 7).
these values are misleading, however, because the relative l2
loss imposes equal weights to the entire sparse matrix of
waveforms that is dominated by small amplitudes close to 0.
alternatively, the cross-correlation coefficient is a quantity that
is more sensitive to the seismic phases with amplitudes larger
than background noise. a cross-correlation coefficient larger
than 0.95 suggests the coherence of the u-no prediction is
excellent, even for the scenarios with large relative l2 loss
(figure 7). in addition, the fwi results confirm that the large
l2 loss is not so important since even challenging models can
still be properly recovered (figure 8).
besides the more than an order of magnitude higher speed,
the most important advantage of the neural operator-based full
waveform modeling is its generalizability to arbitrary velocity
models or discretization. this is because the neural operator
learns a general solution operator to the wave equation instead
of a specific instance of input velocity models. once the neural
operator is trained, it can be used by the entire seismology
community for any region of a similar size without the need for
retraining. since the full waveform modeling with a neural

operator has easily accessible gradients for convenient fwi, we
anticipate that this approach will eventually make fwi as
affordable as travel time tomography.
one of the main limitations of the method is the domain
extent. for a trained neural operator, the function is defined on
a fixed domain extent (e.g. it could be a unit cube). we can
evaluate at a different grid size but cannot change the extent.
we are now working on an extension of the work, where we
recursively predict the wavefield. through this way, a trained
neural operator is essentially taking the first few time steps as
input and then output the next few time steps, and there will be
no need for retraining.
the scalability of evaluation using a trained neural operator
with respect to the grid size and the number of time steps is a
little different from conventional fdm. assuming the original
dimension is (ğ‘ğ‘¥% , ğ‘ğ‘¦% , ğ‘ğ‘¡% ), where ğ‘ğ‘¥% , ğ‘ğ‘¦% are the number
of grids in the x and y domain, respectively, and ğ‘ğ‘¡% is the
number of time steps. if the new dimension is (ğ‘ğ‘¥- , ğ‘ğ‘¦- , ğ‘ğ‘¡- ),
+. âˆ™+/ âˆ™+6
the memory becomes +.'âˆ™+/'âˆ™+6' times the original memory,
!

!

!

which is consistent with the fdm. in the example presented in
this paper, evaluation using u-no takes 3 times the gpu
memory of the fdm approach, and this scaling should be
consistent with increasing grid points. in terms of
computational cost, the majority of it for uno is on the fourier
transform and its inverse. the computational cost of fast
fourier transform with dimension (ğ‘ğ‘¥% , ğ‘ğ‘¦% , ğ‘ğ‘¡% ) is
proportional to ğ‘ğ‘¥% âˆ™ ğ‘ğ‘¦% âˆ™ ğ‘ğ‘¡% âˆ™ log(ğ‘ğ‘¥% âˆ™ ğ‘ğ‘¦% âˆ™ ğ‘ğ‘¡% ) , and
therefore, the new computational time becomes
+.' âˆ™+/' âˆ™+6' âˆ™;2<(+.' âˆ™+/' âˆ™+6' )
times the original computational
+. âˆ™+/ âˆ™+6 âˆ™;2<(+. âˆ™+/ âˆ™+6 )
!

!

!

!

!

!

time. this scaling is slightly higher than that of fdm, however,
considering the 60 times acceleration in the example presented
in this paper, uno evaluation on an increased dimension of
1024*1024*1024 should still have ~40 times the acceleration.
the most compute- and memory-intensive part of the uno
method is the one-time training process. the cost of training for
the 2d case is tractable on a single gpu. for the extension from
2d to 3d modeling, the computation and memory will increase
due to the larger dataset and the larger number of parameters to
learn. therefore, the next step is to enhance data compression
and parallelization to accelerate the training process and reduce
the storage. since this is a learning-based approach, the model
performance can be improved by fine-tuning the model
parameters and increasing the size of the training dataset. more
importantly, any future advancements made in neural operator
model architectures will be able to be directly incorporated into
the system as they occur. for example, the improvement from
linear layers of fno to u-no enables faster training
convergence. as a result, we should only take current
performance metrics as a starting point.
references
[1]

r. w. graves and a. pitarka, â€œbroadband groundmotion simulation using a hybrid approach,â€ bulletin of

the seismological society of america, vol. 100, no. 5 a,
pp. 2095â€“2123, 2010, doi: 10.1785/0120100057.
[2] a. j. rodgers, a. pitarka, n. anders petersson, b.
sjogreen, d. b. mccallen, and n. abrahamson,
â€œbroadband (0-5 hz) fully deterministic 3d groundmotion simulations of a magnitude 7.0 hayward fault
earthquake: comparison with empirical ground-motion
models and 3d path and site effects from source
normalized intensities,â€ seismological research letters,
vol. 90, no. 3, pp. 1268â€“1284, 2019, doi:
10.1785/0220180261.
[3] r. graves and a. pitarka, â€œkinematic ground-motion
simulations on rough faults including effects of 3d
stochastic velocity perturbations,â€ bulletin of the
seismological society of america, vol. 106, no. 5, pp.
2136â€“2153, oct. 2016, doi: 10.1785/0120160088.
[4] a. fichtner, b. l. n. kennett, h. igel, and h. p. bunge,
â€œfull seismic waveform tomography for upper-mantle
structure in the australasian region using adjoint
methods,â€ geophysical journal international, vol. 179,
no. 3, pp. 1703â€“1725, dec. 2009, doi: 10.1111/j.1365246x.2009.04368.x.
[5] c. tape, q. liu, a. maggi, and j. tromp, â€œadjoint
tomography of the southern california crust,â€ science,
vol. 325, no. 5943, pp. 988â€“992, 2009, doi:
10.1126/science.1175298.
[6] l. gebraad, c. boehm, and a. fichtner, â€œbayesian
elastic full-waveform inversion using hamiltonian
monte carlo,â€ journal of geophysical research: solid
earth, vol. 125, no. 3, mar. 2020, doi:
10.1029/2019jb018428.
[7] k. b. olsen, â€œsite amplification in the los angeles
basin from three-dimensional modeling of ground
motion,â€ bulletin of the seismological society of
america, vol. 90, no. 6b, pp. s77â€“s94, dec. 2000, doi:
10.1785/0120000506.
[8] k. r. kelly, r. w. ward, s. treitel, and r. m. alford,
â€œsynthetic seismograms: a finite â€difference approach,â€
geophysics, vol. 41, no. 1, pp. 2â€“27, feb. 1976, doi:
10.1190/1.1440605.
[9] h. igel, t. nissen-meyer, and g. jahnke, â€œwave
propagation in 3d spherical sections: effects of
subduction zones,â€ physics of the earth and planetary
interiors, vol. 132, no. 1â€“3, pp. 219â€“234, sep. 2002,
doi: 10.1016/s0031-9201(02)00053-5.
[10] a. fichtner, h. igel, h. p. bunge, and b. l. n. kennett,
â€œsimulation and inversion of seismic wave propagation
on continental scales based on a spectral-element
method,â€ journal of numerical analysis, industrial and
applied mathematics, vol. 4, no. 1â€“2, pp. 11â€“22, 2009.
[11] q. liu and y. j. gu, â€œseismic imaging: from classical
to adjoint tomography,â€ tectonophysics, vol. 566â€“567,
pp. 31â€“66, sep. 2012, doi:
10.1016/j.tecto.2012.07.006.
[12] d. komatitsch and j. tromp, â€œspectral-element
simulations of global seismic wave propagation-i.
validation,â€ geophysical journal international, vol.
149, no. 2, pp. 390â€“412, may 2002, doi:
10.1046/j.1365-246x.2002.01653.x.

[13] d. komatitsch and j. tromp, â€œspectral-element
simulations of global seismic wave propagation-ii.
three-dimensional models, oceans, rotation and selfgravitation,â€ geophysical journal international, vol.
150, no. 1, pp. 303â€“318, jul. 2002, doi: 10.1046/j.1365246x.2002.01716.x.
[14] o. pell, j. bower, r. dimond, o. mencer, and m. j.
flynn, â€œfinite-difference wave propagation modeling
on special-purpose dataflow machines,â€ ieee
transactions on parallel and distributed systems, vol.
24, no. 5, pp. 906â€“915, may 2013, doi:
10.1109/tpds.2012.198.
[15] w. zhu, s. m. mousavi, and g. c. beroza, â€œseismic
signal denoising and decomposition using deep
neural networks,â€ ieee transactions on geoscience
and remote sensing, vol. 57, no. 11, pp. 9476â€“9488,
nov. 2019, doi: 10.1109/tgrs.2019.2926772.
[16] l. yang, x. liu, w. zhu, l. zhao, and g. c. beroza,
â€œtoward improved urban earthquake monitoring
through deep-learning-based noise suppression,â€
science advances, vol. 8, no. 15, p. 3564, apr. 2022,
doi:
10.1126/sciadv.abl3564/suppl_file/sciadv.a
bl3564_movies_s1_and_s2.zip.
[17] c. birnie, m. ravasi, s. liu, and t. alkhalifah, â€œthe
potential of self-supervised networks for random noise
suppression in seismic data,â€ artificial intelligence in
geosciences, vol. 2, pp. 47â€“59, dec. 2021, doi:
10.1016/j.aiig.2021.11.001.
[18] w. zhu and g. c. beroza, â€œphasenet: a deep-neuralnetwork-based seismic arrival-time picking method,â€
geophysical journal international, vol. 216, no. 1, pp.
261â€“273, 2019, doi: 10.1093/gji/ggy423.
[19] s. m. mousavi, w. l. ellsworth, w. zhu, l. y. chuang,
and g. c. beroza, â€œearthquake transformerâ€”an
attentive deep-learning model for simultaneous
earthquake detection and phase picking,â€ nature
communications, vol. 11, no. 1, pp. 1â€“12, 2020, doi:
10.1038/s41467-020-17591-w.
[20] z. e. ross, m. a. meier, and e. hauksson, â€œp wave
arrival picking and first-motion polarity
determination with deep learning,â€ journal of
geophysical research: solid earth, vol. 123, no. 6, pp.
5120â€“5129, 2018, doi: 10.1029/2017jb015251.
[21] z. e. ross, y. yue, m. a. meier, e. hauksson, and t.
h. heaton, â€œphaselink: a deep learning approach to
seismic phase association,â€ journal of geophysical
research: solid earth, vol. 124, no. 1, pp. 856â€“869,
2019, doi: 10.1029/2018jb016674.
[22] w. zhu, k. s. tai, s. m. mousavi, p. bailis, and g. c.
beroza, â€œan end-to-end earthquake detection method
for joint phase picking and association using deep
learning,â€ journal of geophysical research: solid
earth, vol. 127, no. 3, pp. 1â€“13, 2022, doi:
10.1029/2021jb023283.
[23] a. siahkoohi, m. louboutin, and f. j. herrmann,
â€œneural network augmented wave-equation simulation,â€
sep. 2019, doi: 10.48550/arxiv.1910.00925.
[24] b. moseley, t. nissen-meyer, and a. markham, â€œdeep
learning for fast simulation of seismic waves in complex

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

media,â€ solid earth, vol. 11, no. 4, pp. 1527â€“1549,
aug. 2020, doi: 10.5194/se-11-1527-2020.
b. moseley, a. markham, and t. nissen-meyer, â€œfast
approximate simulation of seismic waves with deep
learning,â€ jul. 2018, [online]. available:
http://arxiv.org/abs/1807.06873
b. moseley, a. markham, and t. nissen-meyer, â€œfinite
basis physics-informed neural networks (fbpinns): a
scalable domain decomposition approach for solving
differential equations,â€ jul. 2021, [online]. available:
http://arxiv.org/abs/2107.07871
j. d. smith, k. azizzadenesheli, and z. e. ross,
â€œeikonet: solving the eikonal equation with deep
neural networks,â€ ieee transactions on geoscience
and remote sensing, vol. 59, no. 12, pp. 10685â€“10696,
dec. 2021, doi: 10.1109/tgrs.2020.3039165.
w. zhu, k. xu, e. darve, and g. c. beroza, â€œa general
approach to seismic inversion with automatic
differentiation,â€ computers & geosciences, vol. 151, p.
104751, jun. 2021, doi:
10.1016/j.cageo.2021.104751.
b. sun and t. alkhalifah, â€œml-descent: an
optimization algorithm for full-waveform inversion
using machine learning,â€ geophysics, vol. 85, no. 6,
pp. r477â€“r492, nov. 2020, doi: 10.1190/geo20190641.1.
x. zhang and a. curtis, â€œbayesian geophysical
inversion using invertible neural networks,â€ journal
of geophysical research: solid earth, vol. 126, no. 7,
p. e2021jb022320, jul. 2021, doi:
10.1029/2021jb022320.
m. rashtâ€behesht, c. huber, k. shukla, and g. e.
karniadakis, â€œphysicsâ€informed neural networks
(pinns) for wave propagation and full waveform
inversions,â€ journal of geophysical research: solid
earth, vol. 127, no. 5, pp. 1â€“21, 2022, doi:
10.1029/2021jb023120.
v. kazei, o. ovcharenko, p. plotnitskii, d. peter, x.
zhang, and t. alkhalifah, â€œmapping full seismic
waveforms to vertical velocity profiles by deep
learning,â€ geophysics, vol. 86, no. 5, pp. r711â€“
r721, sep. 2021, doi: 10.1190/geo2019-0473.1.
l. lu, p. jin, and g. e. karniadakis, â€œdeeponet:
learning nonlinear operators for identifying differential
equations based on the universal approximation theorem
of operators,â€ nature machine intelligence, vol. 3, no.
3, pp. 218â€“229, oct. 2019, doi: 10.1038/s42256-02100302-5.
k. bhattacharya, b. hosseini, n. b. kovachki, and a.
m. stuart, â€œmodel reduction and neural networks for
parametric pdes,â€ smai journal of computational
mathematics, vol. 7, pp. 121â€“157, may 2020, doi:
10.48550/arxiv.2005.03180.
z. li et al., â€œmultipole graph neural operator for
parametric partial differential equations,â€ advances in
neural information processing systems, vol. 2020december, jun. 2020, doi: 10.48550/arxiv.2006.09535.
z. li et al., â€œneural operator: graph kernel network
for partial differential equations,â€ mar. 2020, doi:
10.48550/arxiv.2003.03485.

[37] n. kovachki et al., â€œneural operator: learning maps
between function spaces,â€ arxiv, arxiv:2108.08481,
dec. 2021. accessed: aug. 19, 2022. [online].
available: http://arxiv.org/abs/2108.08481
[38] z. li et al., â€œfourier neural operator for parametric
partial differential equations,â€ oct. 2020, doi:
10.48550/arxiv.2010.08895.
[39] j. pathak et al., â€œfourcastnet: a global data-driven
high-resolution weather model using adaptive fourier
neural operators,â€ feb. 2022, doi:
10.48550/arxiv.2202.11214.
[40] g. wen, z. li, k. azizzadenesheli, a. anandkumar,
and s. m. benson, â€œu-fnoâ€”an enhanced fourier
neural operator-based deep-learning model for
multiphase flow,â€ advances in water resources, vol.
163, p. 104180, may 2022, doi:
10.1016/j.advwatres.2022.104180.
[41] p. jiang et al., â€œdigital twin earth â€“ coasts:
developing a fast and physics-informed surrogate
model for coastal floods via neural operators,â€ oct.
2021, doi: 10.48550/arxiv.2110.07100.
[42] y. yang, a. f. gao, j. c. castellanos, z. e. ross, k.
azizzadenesheli, and r. w. clayton, â€œseismic wave
propagation and inversion with neural operators,â€ the
seismic record, vol. 1, no. 3, pp. 126â€“134, oct. 2021,
doi: 10.1785/0320210026.
[43] m. a. rahman, z. e. ross, and k. azizzadenesheli, â€œuno: u-shaped neural operators,â€ apr. 2022, doi:
10.48550/arxiv.2204.11127.
[44] o. ronneberger, p. fischer, and t. brox, â€œu-net:
convolutional networks for biomedical image
segmentation,â€ lecture notes in computer science
(including subseries lecture notes in artificial
intelligence and lecture notes in bioinformatics), vol.
9351, pp. 234â€“241, 2015, doi: 10.1007/978-3-31924574-4_28/cover/.
[45] n. nakata and g. c. beroza, â€œstochastic
characterization of mesoscale seismic velocity
heterogeneity in long beach, california,â€ geophysical
journal international, vol. 203, no. 3, pp. 2049â€“2054,
2015, doi: 10.1093/gji/ggv421.
[46] t. m. brocher, â€œempirical relations between elastic
wavespeeds and density in the earthâ€™s crust,â€ bulletin
of the seismological society of america, vol. 95, no. 6,
pp. 2081â€“2092, dec. 2005, doi: 10.1785/0120050077.
[47] d. li, d. helmberger, r. w. clayton, and d. sun,
â€œglobal synthetic seismograms using a 2-d finitedifference method,â€ geophysical journal international,
vol. 197, no. 2, pp. 1166â€“1183, 2014, doi:
10.1093/gji/ggu050.
[48] d. hendrycks and k. gimpel, â€œgaussian error linear
units (gelus),â€ jun. 2016, doi:
10.48550/arxiv.1606.08415.
[49] d. p. kingma and j. l. ba, â€œadam: a method for
stochastic optimization,â€ 3rd international conference
on learning representations, iclr 2015 - conference
track proceedings, dec. 2014, doi:
10.48550/arxiv.1412.6980.
[50] p. m. mai and g. c. beroza, â€œa spatial random field
model to characterize complexity in earthquake slip,â€

journal of geophysical research: solid earth, vol. 107,
no. b11, p. ese 10-1-ese 10-21, 2002, doi:
10.1029/2001jb000588.
[51] r. versteeg, â€œthe marmousi experience: velocity
model determination on a synthetic complex data set,â€
https://doi.org/10.1190/1.1437051, vol. 13, no. 9, pp.
927â€“936, feb. 2012, doi: 10.1190/1.1437051.

yan yang is a ph.d. student with the
seismological laboratory, california
institute of technology, pasadena,
ca, usa. her research interests
focus on seismic imaging and
monitoring of subsurface.

angela gao is a ph.d. student with
the computing and mathematical
sciences
department
of
the
california institute of technology,
pasadena, ca, usa. she is interested
in inverse problems, computational
photography, generative models, and
deep learning, with applications in
scientific imaging problems.
kamyar azizzadenesheli is a senior research scientist at
nvidia. prior to his role at nvidia, he
was an assistant professor at purdue
university, department of computer
science. prior to his faculty position,
he was at the california institute of
technology as a postdoctoral
scholar in the department of
computing
and
mathematical
sciences.
robert w. clayton is a professor of
geophysics at california institute of technology, pasadena,
ca, usa, where he works in the
areas of seismic wave propagation,
earth structure, and tectonics. he
has applied imaging methods to the
los angeles region and to
subduction zones around the world.
zachary e. ross is an assistant
professor of geophysics with the
california institute of technology,
pasadena, ca, usa, where he uses
machine learning and signal
processing techniques to better
understand earthquakes and fault
zones. he is interested in the
dynamics of seismicity, earthquake
source properties, and fault zone
imaging.

figures

figure 1. overview of our method for solving the elastic wave equation with neural operators. the inputs, a, to
the u-no model are the p- and s-wave velocity (vp, vs) model and the source location (indicated by the white star).
vp is calculated from vs and vp/vs, examples of which are shown in the upper left panel. the outputs, u, are the
horizontal and vertical displacements at each time step, examples of which are shown in the upper right panel. in the
middle panel showing the u-no architecture, orange circles p and q denote point-wise operators, rectangles g denote
general operators, and smaller blue circles denote concatenations in function space. the lower panel shows the
architecture of each fno layer, where v is the input of the layer, f and f-1 are fourier transform and its inverse,
respectively, r and w are a linear transform, and ğœ is the nonlinear activation function. the expansion (or contraction)
8
%
!
8
%
8
5
5
5
5
6
6
factors in equation (5) are set as: ğ‘%,= ! , ğ‘8,!
= - , ğ‘9,@
= 2, ğ‘a,b
= 8 , ğ‘%6 = ! , ğ‘-6 = 8 , ğ‘8,!
= - , ğ‘9,@
= 2, ğ‘a6 = - , ğ‘b6 =
!
8

%

7
7
, ğ‘%,-,8,!
= 2, ğ‘9,@,a,b
= -.

figure 2 model performance as a function of the number of training samples. left panel: relative l2 loss curves
for the training and validation data. right panel: example of simulated waveform comparison between fdm (black
solid) and u-no (red dashed). n means the number of simulations in the training dataset, including 90% for training
and 10% for validation.

1

figure 3 model generalization experiments 1: random fields of vs and vp/vs model with 4-times roughness of the
training data. the top row shows the vs and vp/vs. from the second row to the fifth row, the wavefield snapshots at
0.5 s-2.0 s are shown. from left to right, the first three columns show the horizontal displacement of the fd simulation,
u-no prediction, and their misfit in the same color scale. the latter three columns show the vertical component. the
horizontal and vertical displacement waveforms at each grid are cross-correlated between fd and u-no, with the
maximum cross-correlation value and its associated time shift shown in the bottom row. for this case, the relative l2
loss of the u-no simulation is 0.182.

2

figure 4 model generalization experiments 2: similar as figure 3, but for a homogeneous background model
embedded with a â€˜slabâ€™ with 20% higher vs and 5% lower vp/vs. relative l2 loss of the u-no simulation is 0.090.

3

figure 5 model generalization experiments 3: similar as figure 3, but for a random subpanel from the marmousi
model. the velocity perturbation range is normalized to 30% of the average velocity. relative l2 loss of the u-no
simulation is 0.225.

4

figure 6 model generalization experiments 4: similar as figure 3, but vp and vs model mesh discretization is
increased from 64*64 to 160*160. relative l2 loss of the u-no simulation is 0.385.

5

figure 7 output evaluation. distribution of relative l2 loss (top) and correlation coefficient (bottom) between the
u-no predictions and ground truth. from left to right, the columns are corresponding to the experiments in figure 3,
5, 6. the red and black dashed lines mark the mean and standard deviation of the histograms.

figure 8 full waveform inversion. the inversion for a random subpanel from the marmousi model. 14 sources are
placed in a ring shape (black stars) and receivers are placed at every node of the 64*64 grid. from left to right, the
columns represent true velocity model, initial model for inversion, inverted model without regularization, and inverted
model with 0th and 1st order tikhonov regularization. the top and bottom rows are the models for vs and vp,
respectively.

6

figure 9 full waveform inversion. same as figure 8, but the receivers are only placed on the 64 grids on the surface
(blue line on the top of each subpanel). the gray shaded areas mask the areas without ray path coverage.

7



ieee transactions on geoscience and remote sensing, vol. 61, 2023

4500511

seismic inversion based on acoustic wave
equations using physics-informed
neural network
yijie zhang , xueyu zhu , and jinghuai gao , member, ieee

abstractâ€” seismic inversion is a significant tool for exploring
the structure and characteristics of the underground. however,
the conventional inversion strategy strongly depends on the initial
model. in this work, we employ the physics-informed neural
network (pinn) to estimate the velocity and density fields based
on acoustic wave equations. in contrast to the traditional purely
data-driven machine learning approaches, pinns leverage both
available data and the physical laws that govern the observed
data during the training stage. in this work, the first-order
acoustic wave equations are embedded in the loss function as a
regularization term for training the neural networks. in addition
to the limited amount of measurements about the state variables
available at the surface being used as the observational data,
the well logging data is also used as the direct observational
data about the model parameters. the numerical results from
several benchmark problems demonstrate that given noise-free or
noisy data, the proposed inversion strategy is not only capable of
predicting the seismograms, but also estimating the velocity and
density fields accurately. finally, we remark that although the
absorbing boundary conditions are not imposed in the proposed
method, the reflected waves do not appear from the artificial
boundary in the predicted seismograms.
index termsâ€” acoustic wave equations, data normalization,
physics-informed neural network (pinn), seismic inversion.

i. i ntroduction

s

eismic inversion is an important approach to exploring
the underground features of the earth, particularly for oil
and gas exploration and co2 sequestration. full waveform
inversion (fwi) is one of the most effective tools to estimate
the characteristic that includes velocity and density of the
subsurface [1]. the main idea of fwi is to minimize
the discrepancy between the modeled data and observed
data based on specific wave equations. conventionally, the
observed data are measured at the surface of the earth by
seismometers. nonetheless, fwi typically requires a good

manuscript received 11 july 2022; revised 27 november 2022;
accepted 11 january 2023. date of publication 16 january 2023; date of
current version 26 january 2023. the work of yijie zhang was supported
by the national natural science foundation of china under grant 42174137.
the work of xueyu zhu was supported by the simons foundation under
grant 504054. the work of jinghuai gao was supported by the national key
research and development program of china under grant 2020yfa0713400.
(corresponding authors: xueyu zhu; yijie zhang.)
yijie zhang and jinghuai gao are with the school of information and
communications engineering, xiâ€™an jiaotong university, xiâ€™an 710049,
china (e-mail: zhangyijie2016@mail.xjtu.edu.cn; jhgao@mail.xjtu.edu.cn).
xueyu zhu is with the department of mathematics, the university of iowa,
iowa city, ia 52246 usa (e-mail: xueyu-zhu@uiowa.edu).
digital object identifier 10.1109/tgrs.2023.3236973

initial model, otherwise it is trapped with local minima due to
cycle skipping [2].
with the successful application of machine learning in
many fields [3], there is a growing interest in exploiting
the approximation power of machine learning techniques for
seismic inversion [4], [5]. in [6] and [7], neural networks
are used to establish the relationship between the observed
seismograms and the subsurface structure. another is to adopt
machine learning as an alternative to replace one of the step in
fwi. for example, the neural network is adopted to represent
the velocity model in fwi, which overcomes the need for an
initial model [8]. furthermore, the neural network is applied
to extrapolate the missing low frequencies and help mitigate
the cycle skipping problem [9].
for the applications of machine learning in solving the
issues about partial differential equations (pdes), a popular
research direction is to encode physical laws in the learning framework. notably, physics-informed neural networks
(pinns) have received a lot of attention in many scientific
fields recently [10], [11]. pinn leverages the governing laws
that the observed data should be satisfied in the learning
framework, which has been shown effective on solving
various problems including elastodynamics [12], high speed
flows [13], fluid mechanics [14]. to increase the performance
of pinn, different strategies have been proposed. gradientenhanced pinn is proposed to improve the accuracy and
training efficiency [15]. parallel pinn based on domain
decomposition is studied to further improve the training
efficiency [16]. in order to quantify the uncertainty of the
prediction, bayesian pinn is proposed to solve both forward
and inverse nonlinear problems [17], [18].
recently, pinn is also used to tackle the forward and
inverse problems in the context of geophysics. the travel time
based on the eikonal equation is computed by pinn [19],
and the corresponding tomography is also studied [20], [21].
pinn is adopted to solve the helmholtz equation, which
prevents calculating the inverse of the impedance matrix to
reduce computational cost [22], [23], [24]. pinn-based wavefield reconstruction inversion (wri) method using helmholtz
equation is developed to invert the velocity field [25]. for
the acoustic wave equations, pinn is applied to simulate
the wavefields in acoustic media, which incorporates the
second-order wave equation in the loss function [26], [27].
recently, pinn is used to solve the seismic inversion of
the velocity field based on the second-order acoustic wave

1558-0644 Â© 2023 ieee. personal use is permitted, but republication/redistribution requires ieee permission.
see https://www.ieee.org/publications/rights/index.html for more information.
authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

4500511

ieee transactions on geoscience and remote sensing, vol. 61, 2023

equation, where the observation data is collected from the
snapshots [28]. the forward wave propagation and fwis are
handled simultaneously by pinn, where the two early time
snapshots and the seismograms of the pressure are used as the
observation data. however, only the inversion of the velocity
field was considered [29].
it should be noted that the previous works on pinn-based
fwi is mainly based on the constant-density second-order
acoustic wave equations, which only include pressure and
excludes density. for the inversions based on the coupled
acoustic-elastic model [30], [31], the first-order acoustic wave
equations are frequently employed, because it involves the
velocity and density simultaneously, and the particle velocities
are included in the equations. this is important for the offshore
exploration, as the elastic wave equations could be easily
reduced to the acoustic wave equations by setting the shear
modulus as zero. this motivates to develop pinn for seismic
inversion based on the first-order acoustic wave equations,
where the unknown model parameters include the velocity and
the density. the observational data are collected from the shot
gathers, which are only available at the surface of the model.
the contributions of this work include the following.
1) instead of considering the snapshot data, we can
consider the measurements of the state variables from
the seismograms observed at the surface. we also
include the well-log data as the observational data about
the model parameters to improve the performance of
inversion, which is available for seismic exploration in
practical applications.
2) we demonstrate that the velocity and density fields
can be estimated simultaneously by pinn, which is
independent of the initial model of the velocity and
density fields. the proposed scheme could potentially
provide a good initial model for fwi.
3) we reveal that although the absorbing boundary
conditions are not imposed in the proposed method,
the reflected waves from the artificial boundary do not
appear in the predicted seismograms.
this article is organized as follows. we describe the firstorder acoustic wave equations and review the basics of pinns
in section ii. in section iii, several numerical examples
are provided to demonstrate the effectiveness of the method.
finally, we conclude in section iv.
ii. s etup
a. acoustic wave equations
acoustic wave equations are widely used to describe the
underground wave propagation. in this work, we shall focus
on the first-order acoustic wave equations as follows:


âˆ‚v
âˆ‚p
âˆ‚u
= âˆ’c2 (x, z)Ï(x, z)
+
+ f (x, z, t)
âˆ‚t
âˆ‚x
âˆ‚z
1 âˆ‚p
âˆ‚u
=âˆ’
âˆ‚t
Ï(x, z) âˆ‚ x
1 âˆ‚p
âˆ‚v
=âˆ’
(1)
âˆ‚t
Ï(x, z) âˆ‚z
where p is the pressure, u, v are the x- and z-components of
the particle velocities, respectively, f (x, z, t) represents the

fig. 1.
schematic of the observed seismograms with different source
locations and the selected residual points (yellow dots).

source term. the velocity and density c, Ï are the model
parameters that need to be estimated based on available
measurements.
b. physics-informed neural network for seismic inversion
pinn was proposed by raissi et al. [10] and lu et al. [32],
which incorporates the physics laws into the loss function
as regularization term and therefore potentially can improve
generalization of the neural network approximation.
1) structure of neural networks: we begin approximating
the state variables u(x, x s , t) = ( p, u, v) by a feed forward
neural networks uÌ‚(x, x s , t) = n (x, x s , t; Î¸u ), whose input
features include the locations x = (x, z), time t and the source
locations x s . the parameter Î¸u represents the weights and bias
of the neural network. it is worth noting that we consider the
excitation and acquisition at the surface, then the depth of the
source is fixed, only the horizontal axis of the source x s is
presented. the output layer has three neurons representing the
state variables p, u, and v.
to estimate the model parameters, we shall approximate
model parameters Î»(x) = (c, Ï) by the same neural network
n (x; Î¸Î» ). here, Î¸Î» represents the weights and bias of the
network. our goal is to estimate the velocity and density fields
in the spatial domain, which is independent of the time and
source location, therefore the input of this neural network is
only related to x.
2) residual points and observed data: residual points: in
order to train the neural networks, the loss function should
be defined. first, we enforce uÌ‚ and Î»Ì‚ to satisfy the physical
laws as (1) over a set of selected points, which is referred
as residual points tr . the residual points could be randomly
chosen from the whole computational domain (x, z, x s , t).
in this work, we can observe seismograms u at the specific
locations, i.e., when the receivers are located at the earthâ€™s
surface, z = 0. in addition, as the propagating seismic waves
are sparsely distributed in the whole observed data domain

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

zhang et al.: seismic inversion based on acoustic wave equations using pinn

4500511

fig. 2. schematic of the pinn for seismic inversion based on the acoustic wave equations. the red dot rectangle is implemented by automatic differentiation.
the symbol nn represents the fully connected neural network, whose structure is listed in table i.
table i
t raining s trategies for d ifferent e xamples

d = (x, 0, x s , t), we adopt the effective sampling strategy
suggested in [33] and [34] to select the residual points:
randomly selecting 80% of the data points from the region
where the seismic waves locate (de = {x â‰¥ x t âˆˆ d : x t =
argmax |Î±u(x, 0, x s , t)|}, Î± = 0.001) and 20% of the points
from the rest of the domain. fig. 1 lists the schematic of
the selected residual points. we can observe that the selected
residual points are mainly located in the region with seismic
waves.
observed data: in addition, we shall employ the observed
seismograms u as the direct observational data for approximating the surrogate solution uÌ‚. these observed points are referred
to as data points td . due to a large amount of seismograms
data across the sensors, we shall only use partial observations
of the whole seismograms as the observed data, where the
locations of the observed data are also included in residual
points tr .
compared with the snapshot-based training strategy [28],
the seismograms are much less informative. therefore,
we consider leveraging well-logging data in the training stage.
well logging is the practice of making a detailed record
from a well log of the geologic formations penetrated by a
borehole [35], which can provide direct observations of the
model parameters, including velocity and density Î» = (c, Ï)

at specific locations. we refer those data points as well logging
data points tw .
3) loss function: with the above-mentioned notations,
we define the loss function as the weighted summation of the
l 2 norm of residuals for the pde and data misfit as follows:
l(Î¸ ; t ) = Ï‰r lr (Î¸ ; tr ) + Ï‰d ld (Î¸ ; td ) + Ï‰w lw (Î¸ ; tw ) (2)
where t = {tr , td , tw } and


2



f x; âˆ‚ uÌ‚ , âˆ‚ uÌ‚ ; Î»Ì‚ 


âˆ‚t âˆ‚ x
2
xâˆˆtr

2
uÌ‚ âˆ’ u2
ld (Î¸ ; td ) =
lr (Î¸ ; tr ) =

xâˆˆtd

lw (Î¸ ; tw ) =



Î»Ì‚ âˆ’ Î»2
2

(3)

xâˆˆtw

here f represents the residual of the wave equations (1),
Î¸ = (Î¸Î» , Î¸u ). the parameters Ï‰r , Ï‰d and Ï‰w represent the
adaptive weights, which can be computed based on the work
in [36] and [37]. the first term in (3) acts as the regularization
mechanism that penalizes solutions and estimated parameters
that do not satisfy the wave equations. we remark that this
term in pinn is an unsupervised loss, which is independent

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

4500511

ieee transactions on geoscience and remote sensing, vol. 61, 2023

fig. 4. comparison of the predicted and ground truth velocity c(x) for
the two-layered media when x = 0.5 km. (a) noise-free case. (b) 10% noise
included in the observed seismograms.

variables and model parameters, respectively. specifically,
we adopt the fully connected neural network, whose structure
is listed in table i. in order to handle the observed
seismograms efficiently, we encode the source location as the
input of the neural network n (x, x s , t; Î¸u ). with well-defined
neural networks, the pde loss, could be obtained by automatic
differentiation to constrain the outputs of the neural network
to satisfy the physical laws. in addition to the observed data,
we can minimize the total loss function as (2) to search the
optimal parameters Î¸u , Î¸Î» .
5) physics-based data normalization: in the practical
applications, the magnitude of the pressure p is o(107 ) larger
than those of particle velocities (u, v) in (1) due to the
international unit system. therefore, it is challenging to train
the neural network due to different scales of the state variables
and the corresponding equation residuals. here, we use the
dimension transformation to mitigate the issue.
first, to keep the orders of magnitude for velocity c and
density Ï as o(100 ), we change the unit of length and weight
10âˆ’3

10âˆ’12

as length : km âˆ’âˆ’â†’ m, weight : 1012 kg âˆ’âˆ’â†’ kg. then the
10âˆ’3

units of velocity c and density Ï become c : km/s âˆ’âˆ’â†’ m/s,
âˆ’3

10

fig. 3. (a) ground truth and (b) initial and predicted velocity c(x) (km/s,
first column) and density model Ï(x) (g/cm3 , second column) with (c) noisefree data and (d) noisy data for the two-layered media. the five sources are
located at a depth of 10 m and xs is in the range 10 to 810 m equally with a
spacing of 200 m. the 34 receivers are located at the same depth and placed
from 10 to 1000 m equally with a spacing of 30 m. (a) ground truth model
(velocity, density). (b) initial model (velocity, density). (c) predicted model
(velocity, density) with noise-free data. (d) predicted model (velocity, density)
with noisy data.

of the observed data. we can select the residual points tr
randomly from the whole computational domain.
4) training: with the well-defined loss function as (2),
the proper parameters Î¸u and Î¸Î» for the neural network
n (x, x s , t; Î¸u ) and n (x; Î¸Î» ) can be searched via minimizing
the following optimization problem by adam [38]:
{Î¸u , Î¸Î» } := argmin l(Î¸ ; t ).
Î¸u ,Î¸Î»

(4)

overall, the workflow of pinn for seismic inversion based
on the acoustic wave equations is shown in fig. 2. two
independent neural networks are selected to represent the state

Ï : 1012 kg/km3 âˆ’âˆ’â†’ kg/m3 . it is worth noting that
1012 kg/km3 = 1 g/cm3 actually. correspondingly, the units
of pressure p and particle velocities (u, v) are changed to
10âˆ’9

p : 1012 kg/km Â· s2 âˆ’âˆ’â†’ kg/m Â· s2
10âˆ’3

(u, v) : km/s âˆ’âˆ’â†’ m/s.

(5)

with the transformation of units, the magnitude of the pressure
p is o(101 ) bigger than those of particle velocities (u, v),
instead of o(107 ).
we remark that in contrast with the commonly used
normalization strategy, such as normalization by mean or
the maximum value of the state variable, the proposed one
maintains the physical relationships of the parameters and state
variables in the acoustic wave equations, which is important
for pinns.
iii. r esults
for seismic inversion, the observed seismic data are always
sparse, where the receivers and sources are only located at the
specific positions, i.e., the surface of the earth or the borehole.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

zhang et al.: seismic inversion based on acoustic wave equations using pinn

4500511

fig. 5. (a) ground truth seismograms, (b) predicted seismograms, and (c) corresponding differential profiles (first column: p, second column: u, third column:
v) when the observation data is noise-free and the source is located at xs = 410 m for the two-layered media.

to generate the seismogram for the next three examples,
we use the finite difference method [39] with grid spacing
10 m, time step 1 ms. the seismic source in the time domain
is represented by a ricker wavelet, which can be expressed as




r(t) = m0 1 âˆ’ 2(Ï€ f 0 (t âˆ’ t0 ))2 exp âˆ’(Ï€ f0 (t âˆ’ t0 ))2 (6)

fig. 6. comparison of the predicted and ground truth seismograms of the
state variable p for the two-layered media when x = 0.6 km. (a) noise-free
case. (b) 10% noise included in the observed seismograms.

therefore, it is challenging setting for inverse problems. in this
section, we shall test the feasibility and performance of the
pinns for seismic inversion based on the first-order acoustic
wave equations in the two-layered, complex three-layered, and
salt body media.

where the magnitude m0 = 100. the dominant frequency
f 0 is 10 hz, and the time delay t0 is 0.1 s. the source in
the spatial domain is a gaussian function as
 

 x âˆ’ xs 2


g(x; xs) = exp âˆ’
(7)
Î± 2
where the attenuation coefficient Î± is chosen to be 2.
in addition, we employ the perfect matched layer (pml)
absorbing boundary conditions [40] to handle the reflected
waves from the artificial boundary. the seismograms are
generated by five different sources, where their depth
(z-direction) is 10 m and x s ranges from 10 to 810 m equally
with a spacing of 200 m in x-direction.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

4500511

ieee transactions on geoscience and remote sensing, vol. 61, 2023

fig. 8. comparison of the predicted and ground truth velocity c(x) for the
complex three-layered media when x = 0.5 km. (a) noise-free case. (b) 10%
noise included in the observed seismograms.

are randomly generated on purpose to demonstrate that the
proposed inversion strategy is independent of the initial model.
we evaluate our approach on an nvidia rtx 2080ti gpu
with a memory of 11 gb.
we remark that it is well known that the solution of a
wave equation can be represented by the superposition of
plane simple harmonic waves [41]. in addition, a previous
study suggested that the sine activation functions are beneficial
due to the moment-preserving properties of intermediate
layers [42]. therefore, the activation functions in the neural
network n (x, x s , t; Î¸u ) that approximates the state variables
p, u and v are chosen as sin(x).

a. two-layered media

fig. 7. (a) ground truth and (b) initial and predicted velocity c(x) (km/s, first
column) and density model Ï(x) (g/cm3 , second column) with (c) noise-free
data and (d) noisy data for the complex three-layered media. the 100 receivers
are located at the same depth and placed from 10 to 1000 m equally with a
spacing of 10 m. (a) ground truth model (velocity, density). (b) initial model
(velocity, density). (c) predicted model (velocity, density) with noise-free data.
(d) predicted model (velocity, density) with noisy data.

two separated neural networks are used to approximate the
model parameters (c, Ï) and the state variables ( p, u, and
v), respectively. the corresponding hidden layers, neurons
per layer, and the activation function for different examples
are shown in table i. the learning rate, the batch size,
the number of epochs and the number of residual and data
points are also listed in table i. the learning rate decays
exponentially from 0.001 to 0.00001 in order to ensure
convergence. the number of residual points increases with the
increase in the complexity of the model structure. the width
and depth of the neural networks are chosen based on the best
performance on a validation set. it is worth to be noted that
the initialization of the neural networks used in the examples

first, we examine the performance of the pinn with a
simple two-layered model, which is shown in fig. 3(a). the
two well logs at x = 0.3 and 0.6 km are used to measure
the velocity c and density Ï. 200 well-logging data points are
used as the observation data for the velocity and density fields,
respectively.
the estimated velocity and density field by pinn based on
noise-free measurements is shown in fig. 3(b). it can be seen
that the profiles of predicted model parameters match well
with the ground truth ones. besides, we further compare the
predicted and ground truth velocities at x = 0.5 km, shown in
fig. 4(a), in which the predicted velocity is in good agreement
with the ground truth.
in fig. 5, we further plot the ground truth and predicted
seismograms as well as their corresponding differential profiles
when the source is located at x s = 410 m. it is worth noting
that the data points are collected from 34 receivers that are
located at the depth of 10 m and placed from 10 to 1000 m
equally with a spacing of 30 m. we can observe that the
seismograms predicted by pinn are accurate, where the direct
and the reflected waves from the interface are well captured.
in addition, from the differential profiles of p, u, and v,
we observe that the error of the reconstructed seismograms is
relatively small. it is worth noting that although the absorbing
boundary conditions are not imposed in pinns, the artificial
reflection is hardly observed from the predicted seismograms.
to further demonstrate the performance of pinns, we also
plot the time history of the predicted and the ground truth

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

zhang et al.: seismic inversion based on acoustic wave equations using pinn

4500511

fig. 9. (a) ground truth seismograms, (b) predicted seismograms, and (c) corresponding differential profiles (first column: p, second column: u, third column:
v) when the observation data is noise-free and the source is located at xs = 410 m for the complex three-layered media.

fig. 10. comparison of the predicted and ground truth seismograms of
the state variable p for the complex three-layered media when x = 0.6 km.
(a) noiseless observed seismograms as the observation data. (b) 10% noise
included in the observed seismograms as the observation data.

seismograms at x = 0.6 km in fig. 6(a), where the forward
solution by pinn also matches well with the ground truth.
in order to test the performance of the proposed method to
deal with the noisy measurements, we corrupt the observed

seismograms by the gaussian noise with the level of 10%.
in addition, we remark that the observed velocity and
density from the well-logging data are assumed to be noisefree, thanks to the accurate measurement process [35]. the
corresponding predicted velocity and density are shown in
fig. 3(c), where the predicted model parameters are relatively
robust to the noise included in the observed seismograms.
we further compare the predicted velocity at x = 0.5 km in
fig. 4(b), where the predicted velocity of the first layer differs
marginally with the ground truth one. we also compare the
seismograms as shown in fig. 6(b). notably, the predicted
seismogram by pinn indicates that the neural network tends
to filter out the noises and deliver smooth results. we point
out that the ground truth represents the observed seismogram
with 10% noise.
b. complex three-layered media
in this section, we consider a laterally inhomogeneous
media, which includes three layers and a slope between the

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

4500511

ieee transactions on geoscience and remote sensing, vol. 61, 2023

fig. 12. comparison of the predicted and ground truth velocity c(x) for
the salt body media when x = 0.5 km. (a) noiseless observed seismogram
as the observation data. (b) 10% noise included in the observed seismogram
as the observation data.

fig. 11. (a) ground truth and (b) initial and the predicted velocity c(x) (km/s,
first column) and density model Ï(x) (g/cm3 , second column) with (c) noisefree data and (d) noisy data for the salt body media. the 100 receivers are
located at the same depth and placed from 10 to 1000 m equally with a
spacing of 10 m. (a) ground truth model (velocity, density). (b) initial model
(velocity, density). (c) predicted model (velocity, density) with noise-free data.
(d) predicted model (velocity, density) with noisy data.

two deeper layers [28] as shown in fig. 7(a). the well logs at
x = 0.05, 0.18, 0.4, 0.61, 0.8, 0.99 km are used to measure
the velocity and density.
the predicted velocity and density fields are shown in
fig. 7(b). we can observe that the major features of the model
could be captured, especially the locations of two inflection
points are well predicted. we also compare the predicted
and ground truth velocities in fig. 8(a), where the predicted
velocity by pinns agrees well with the ground truth one.
in addition, when the observed seismograms are corrupted by
the gaussian noise with the level of 10%, the corresponding
result is shown in figs. 7(c) and 8(b). it is clear that the

inversion by pinns is less sensitive with the measurements
with mild level noises.
when the source is located at x s = 410 m, the ground truth
and predicted seismograms, and the corresponding differential
profiles are shown in fig. 9. we can observe that the direct
and reflected waves from the first interfaces are captured
clearly from the predicted seismograms by pinn, even the
reflected waves from the slope are also accurately predicted
in u (see the reflected waves around t = 0.5, 0.7 s for
example). the differential profiles demonstrate the errors of
the predicted results are relatively small, i.e., the errors of
p, u, and v are about 10%, which demonstrates the pinn
handles the forward problems effectively. notably, while the
absorbing boundary conditions are not formally incorporated
in the proposed method, the artificial reflections from the
boundaries do not appear in the predicted results by pinn.
in addition, the comparison between the predicted and
the ground truth seismograms of the state variable p at
x = 0.6 km is shown in fig. 10. for the noise-free case,
we observe that the predicted direct and reflected waves match
well with the ground truth ones, even for the reflected wave
with a small amplitude. moreover, the noisy case is also
shown here, which demonstrates that the suggested approach
could effectively mitigate the effect caused by mild noise on
predicted seismograms.
c. salt body media
the geometry of the complex model is typical for a
part from the north sea [43]. the model includes seven
different layers, and the velocity increases with depth from
2.0 to 5.0 km/s, the range of the density is from
1.4 to 3.0 g/cm3 . in each region, the velocity and density are
assumed to be constant. the inclusion in fig. 11(a) is a salt
diapir with a high velocity of 4.3 km/s. the inversion of the
salt body is quite challenging as insufficient illumination for
the subsalt sediments [44]. in this section, we shall examine
the performance of the pinn for the inversion of the salt body
model. the well logs at x = 0.02, 0.15, 0.26, 0.32, 0.37, 0.42,
0.45, 0.65, and 0.92 km are used to measure the velocity and
density.
the predicted velocity and density fields by pinn is shown
in fig. 11(b), where the shape of the salt body is clearly

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

zhang et al.: seismic inversion based on acoustic wave equations using pinn

4500511

fig. 13. (a) ground truth seismograms, (b) predicted seismograms, and (c) corresponding differential profiles (first column: p, second column: u, third
column: v) when the observation data is noise-free and the source is located at xs = 410 m for the salt body media.

fig. 14. comparison of the predicted and ground truth seismogram of the
state variable p for the salt body media when x = 0.6 km. (a) noise free
observed seismogram as the observation data. (b) 10% noise included in the
observed seismogram as the observation data.

captured by pinn. in addition, we compare the predicted and
the ground truth velocities in fig. 12(a). the key features of
the velocity fields are predicted accurately by pinns even
for discontinuities. when the observation data are collected

from the seismograms with 10% gaussian noise, the results
in figs. 11(c) and 12(b) demonstrate that the main trend
of predicted velocity is consistent with the ground truth
one. nonetheless, the predictions at several locations are less
accurate, i.e., the depth is 0.25 and 0.8 km as shown in
fig. 12(b) and the artifacts at depth of 0.8 km in fig. 11(c),
which indicates that the inversion problems for the complex
model based on the noisy data are challenging.
fig. 13 presents the ground truth, predicted seismograms,
and their differential profiles when the source is located at
x s = 410 m. it is clear that the main direct waves and
reflected waves are well reconstructed by pinn. the relatively
small errors as in the differential profiles demonstrate that
the proposed method works well in the forward problems.
as shown in fig. 13(b), the reflected waves from the artificial
boundary do not appear in the predicted results by pinn,
highlighting the capability of handling the scenario of wave
propagation in an infinite domain.
we also compare the time history from the predicted and
the ground truth seismograms as shown in fig. 14. the direct

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

4500511

ieee transactions on geoscience and remote sensing, vol. 61, 2023

wave and reflected wave from the first layer are well captured
by pinns. for the noise-free case, the reflected waves from
the deeper interfaces capture well with the ground truth
ones. when the observed seismograms are corrupted by 10%
noise, the predicted results and the noiseless ground truth
seismogram also agree well. the results demonstrate that the
proposed framework is capable to tackle the seismic inversion
for the complex model.
iv. c onclusion
we developed a seismic inversion strategy based on pinn
in which the first-order acoustic wave equations are encoded
into the loss function as a regularization term. besides the
partially observed seismograms at the surface, we also include
the well-logging data as the observation data about the
model parameters. the numerical results demonstrated that the
velocity and density fields could be accurately estimated by
pinn without requiring a good initial model as the standard
fwi does. in addition, we remarked that the reflected waves
from the artificial boundaries can be avoided without the
enforcement of the absorbing boundary conditions in our
method. therefore, the suggested method not only provides a
novel tool for seismic inversion, but also it potentially provides
a decent initial model about velocity and density for standard
fwi approaches, if the better inversion results are needed.
r eferences
[1] a. tarantola, â€œinversion of seismic reflection data in the acoustic
approximation,â€ geophysics, vol. 49, no. 8, pp. 1259â€“1266, 1984.
[2] d. kÃ¶hn, â€œtime domain 2d elastic full waveform tomography,â€ ph.d.
thesis, christian-albrechts universitÃ¤t kiel, kiel, germany, 2011.
[3] m. i. jordan and t. m. mitchell, â€œmachine learning: trends,
perspectives, and prospects,â€ science, vol. 349, no. 6245, pp. 255â€“260,
jul. 2015.
[4] k. j. bergen, p. a. johnson, m. v. de hoop, and g. c. beroza, â€œmachine
learning for data-driven discovery in solid earth geoscience,â€ science,
vol. 363, no. 6433, mar. 2019, art. no. eaau0323.
[5] w. zhu, k. xu, e. darve, b. biondi, and g. c. beroza, â€œintegrating
deep neural networks with full-waveform inversion: reparametrization,
regularization, and uncertainty quantification,â€ geophysics, vol. 87,
no. 1, pp. 1â€“103, 2021.
[6] f. yang and j. ma, â€œdeep-learning inversion: a next-generation
seismic velocity model building method,â€ geophysics, vol. 84, no. 4,
pp. r583â€“r599, jul. 2019.
[7] v. kazei, o. ovcharenko, p. plotnitskii, d. peter, x. zhang, and
t. alkhalifah, â€œmapping full seismic waveforms to vertical velocity
profiles by deep learning velocity model building by deep learning,â€
geophysics, vol. 86, no. 5, pp. r711â€“r721, 2021.
[8] a. dhara and m. k. sen, â€œphysics-guided deep autoencoder to overcome
the need for a starting model in full-waveform inversion,â€ lead. edge,
vol. 41, no. 6, pp. 375â€“381, jun. 2022.
[9] w. hu, y. jin, x. wu, and j. chen, â€œprogressive transfer learning for
low-frequency data prediction in full waveform inversion,â€ geophysics,
vol. 86, no. 4, pp. 1â€“82, 2021.
[10] m. raissi, p. perdikaris, and g. e. karniadakis, â€œphysics-informed
neural networks: a deep learning framework for solving forward and
inverse problems involving nonlinear partial differential equations,â€
j. comput. phys., vol. 378, pp. 686â€“707, mar. 2019.
[11] g. e. karniadakis, i. g. kevrekidis, l. lu, p. perdikaris, s. wang, and
l. yang, â€œphysics-informed machine learning,â€ nature rev. phys., vol. 3,
no. 6, pp. 422â€“440, 2021.
[12] c. rao, h. sun, and y. liu, â€œphysics-informed deep learning for
computational elastodynamics without labeled data,â€ j. eng. mech.,
vol. 147, no. 8, aug. 2021, art. no. 04021043.
[13] z. mao, a. d. jagtap, and g. e. karniadakis, â€œphysics-informed neural
networks for high-speed flows,â€ comput. methods appl. mech. eng.,
vol. 360, mar. 2020, art. no. 112789.

[14] m. raissi, a. yazdani, and g. e. karniadakis, â€œhidden fluid mechanics:
learning velocity and pressure fields from flow visualizations,â€ science,
vol. 367, no. 6481, pp. 1026â€“1030, feb. 2020.
[15] j. yu, l. lu, x. meng, and g. em karniadakis, â€œgradient-enhanced
physics-informed neural networks for forward and inverse pde
problems,â€ 2021, arxiv:2111.02801.
[16] k. shukla, a. d. jagtap, and g. e. karniadakis, â€œparallel
physics-informed neural networks via domain decomposition,â€ 2021,
arxiv:2104.10013.
[17] l. yang, x. meng, and g. e. karniadakis, â€œb-pinns: bayesian physicsinformed neural networks for forward and inverse pde problems with
noisy data,â€ j. comput. phys., vol. 425, jan. 2021, art. no. 109913.
[18] l. sun and j.-x. wang, â€œphysics-constrained bayesian neural network
for fluid flow reconstruction with sparse and noisy data,â€ theor. appl.
mech. lett., vol. 10, no. 3, pp. 161â€“169, mar. 2020.
[19] u. b. waheed, e. haghighat, t. alkhalifah, c. song, and q. hao,
â€œpinneik: eikonal solution using physics-informed neural networks,â€
comput. geosci., vol. 155, oct. 2021, art. no. 104833.
[20] u. b. waheed, t. alkhalifah, e. haghighat, c. song, and j. virieux,
â€œpinntomo: seismic tomography using physics-informed neural
networks,â€ 2021, arxiv:2104.01588.
[21] r. gou, y. zhang, x. zhu, and j. gao, â€œbayesian physics-informed
neural networks for the subsurface tomography based on the eikonal
equation,â€ 2022, arxiv:2203.12351.
[22] t. alkhalifah, c. song, u. b. waheed, and q. hao, â€œwavefield solutions
from machine learned functions constrained by the helmholtz equation,â€
in artificial intelligence in geosciences, vol. 2, 2021, pp. 11â€“19.
[23] c. song, t. alkhalifah, and u. b. waheed, â€œa versatile framework to
solve the helmholtz equation using physics-informed neural networks,â€
geophys. j. int., vol. 228, no. 3, pp. 1750â€“1762, nov. 2021.
[24] c. song, t. alkhalifah, and u. b. waheed, â€œsolving the frequencydomain acoustic vti wave equation using physics-informed neural
networks,â€ geophys. j. int., vol. 225, no. 2, pp. 846â€“859, feb. 2021.
[25] c. song and t. alkhalifah, â€œwavefield reconstruction inversion via
physics-informed neural networks,â€ 2021, arxiv:2104.06897.
[26] b. moseley, a. markham, and t. nissen-meyer, â€œsolving the wave
equation with physics-informed deep learning,â€ 2020, arxiv:2006.11894.
[27] d. voytan and m. k. sen, â€œwave propagation with physics informed
neural networks,â€ in proc. seg int. expo. annual meeting, sep. 2020,
p. 3887.
[28] y. xu, j. li, and x. chen, â€œphysics informed neural networks for
velocity inversion,â€ in proc. seg tech. program expanded abstr.,
aug. 2019, pp. 2584â€“2588.
[29] m. rasht-behesht, c. huber, k. shukla, and g. e. karniadakis,
â€œphysics-informed neural networks (pinns) for wave propagation and
full waveform inversions,â€ j. geophys. res., solid earth, vol. 127, no. 5,
2022, art. no. e2021jb023120.
[30] h. s. bae, c. shin, y. h. cha, y. choi, and d.-j. min, â€œ2d acousticelastic coupled waveform inversion in the laplace domain,â€ geophys.
prospecting, vol. 58, no. 6, pp. 997â€“1010, 2010.
[31] y. zhang, h. zhao, w. yan, and j. gao, â€œa unified numerical scheme
for coupled multiphysics model,â€ ieee trans. geosci. remote sens.,
vol. 59, no. 10, pp. 8228â€“8240, oct. 2021.
[32] l. lu, x. meng, z. mao, and g. e. karniadakis, â€œdeepxde: a deep
learning library for solving differential equations,â€ siam rev., vol. 63,
no. 1, pp. 208â€“228, 2021.
[33] y. zhang, x. zhu, and j. gao, â€œparameter estimation of acoustic wave
equations using hidden physics models,â€ ieee trans. geosci. remote
sens., vol. 58, no. 7, pp. 4629â€“4639, jul. 2020.
[34] y. zhang, x. zhu, and j. gao, â€œhidden physics model for parameter
estimation of elastic wave equations,â€ comput. methods appl. mech.
eng., vol. 381, aug. 2021, art. no. 113814.
[35] s. j. pirson, handbook of well log analysis for oil and gas formation
evaluation. houston, tx, usa: schlumberger limited, 1963.
[36] s. wang, x. yu, and p. perdikaris, â€œwhen and why pinns fail to train:
a neural tangent kernel perspective,â€ j. comput. phys., vol. 449,
jan. 2022, art. no. 110768.
[37] e. haghighat and r. juanes, â€œsciann: a keras/tensorflow wrapper
for scientific computations and physics-informed deep learning using
artificial neural networks,â€ comput. methods appl. mech. eng., vol. 373,
jan. 2021, art. no. 113552.
[38] d. p. kingma and j. ba, â€œadam: a method for stochastic optimization,â€
2014, arxiv:1412.6980.
[39] y. zhang, j. gao, and j. peng, â€œvariable-order finite difference scheme
for numerical simulation in 3-d poroelastic media,â€ ieee trans. geosci.
remote sens., vol. 56, no. 5, pp. 2991â€“3001, may 2018.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.

zhang et al.: seismic inversion based on acoustic wave equations using pinn

[40] f. d. hastings, j. b. schneider, and s. l. broschat, â€œapplication
of the perfectly matched layer (pml) absorbing boundary condition
to elastic wave propagation,â€ j. acoust. soc. amer., vol. 100, no. 5,
pp. 3061â€“3069, 1996.
[41] k. aki and p. g. richards, quantitative seismology. san francisco, ca,
usa: univ. science books, 2002.
[42] v. sitzmann, j. martel, a. bergman, d. lindell, and g. wetzstein,
â€œimplicit neural representations with periodic activation functions,â€ in
proc. adv. neural inf. process. syst., vol. 33, 2020, pp. 7462â€“7473.
[43] s. minisini, e. zhebel, a. kononov, and w. a. mulder, â€œlocal time
stepping with the discontinuous galerkin method for wave propagation
in 3d heterogeneous media,â€ geophysics, vol. 78, no. 3, pp. t67â€“t77,
may 2013.
[44] a. alali, v. kazei, m. kalita, and t. alkhalifah, â€œdeep learning unflooding for robust subsalt waveform inversion,â€ 2022, arxiv:2201.02947.

yijie zhang received the b.s. degree in information
engineering and the ph.d. degree in information
and communication engineering from xiâ€™an jiaotong
university, xiâ€™an, china, in 2010 and 2016,
respectively.
she was a visiting scholar with the department
of mathematics, the university of iowa, iowa city,
ia, usa. she is currently an associate professor
with the school of information and communications
engineering, xiâ€™an jiaotong university. her research
interests include machine learning in wave-fields
forward modeling and inversion, signal processing; wave propagation theory,
and numerical simulation in complex media.

4500511

xueyu zhu received the ph.d. degree in applied
mathematics from brown university, providence, ri,
usa, in 2013.
he is currently an associate professor with the
department of mathematics, university of iowa,
iowa city, ia, usa. he is also affiliated with an
interdisciplinary ph.d. program in applied mathematical and computational sciences. his research
interests include computational mathematics, scientific computing, uncertainty quantification, model
reduction, and machine learning.

jinghuai gao (member, ieee) received the
m.s. degree in applied geophysics from changâ€™an
university, xiâ€™an, china, in 1991, and the ph.d.
degree in electromagnetic field and microwave
technology from xiâ€™an jiaotong university, xiâ€™an,
in 1997.
from 1997 to 2000, he held a post-doctoral
position with the institute of geology and geophysics, chinese academy of sciences, beijing,
china. in 1999, he was a visiting scientist with
the modeling and imaging laboratory, university
of california at santa cruz, santa cruz, ca, usa. he is currently a
professor with the school of information and communications engineering
and the school of mathematics and statistics, xiâ€™an jiaotong university.
his research interests include seismic wave propagation and imaging theory,
seismic reservoir and fluid identification, and seismic inverse problem theory
and methods.

authorized licensed use limited to: universidad eafit. downloaded on august 24,2024 at 00:03:36 utc from ieee xplore. restrictions apply.



ore open research exeter

title
surrogate regression modelling for fast seismogram generation and detection of microseismic events in
heterogeneous velocity models
authors
das, s; chen, x; hobson, mp; et al.
journal
geophysical journal international
deposited in ore
27 july 2018

this version available at
http://hdl.handle.net/10871/33552

copyright and reuse
open research exeter makes this work available in accordance with publisher policies.

a note on versions
the version presented here may diï¬€er from the published version. if citing, you are advised to consult the published version for pagination, volume/issue and date of
publication

geophysical journal international

surrogate regression modelling for fast seismogram generation and
detection of microseismic events in heterogeneous velocity models
saptarshi das1, xi chen1, michael p. hobson1, suhas phadke2, bertwim van beest2, jeroen
goudswaard2, and detlef hohl3
1) cavendish astrophysics group, department of physics, university of cambridge, cambridge cb3
0he, united kingdom. (e-mail: {sd731, xc253, mph}@mrao.cam.ac.uk)
2) shell india markets pvt ltd, bangalore 562149, india. (email: {suhas.phadke, bertwim.vanbeest,
jeroen.goudswaard}@shell.com).
3) shell global solutions international bv, grasweg 31, 1031 hw amsterdam, the netherlands.
(email: detlef.hohl@shell.com)
summary
given a 3d heterogeneous velocity model with a few million voxels, fast generation of accurate seismic
responses at specified receiver positions from known microseismic event locations is a well-known
challenge in geophysics, since it typically involves numerical solution of the computationally expensive
elastic wave equation. thousands of such forward simulations are often a routine requirement for
parameter estimation of microseimsic events via a suitable source inversion process. parameter
estimation based on forward modelling is often advantageous over a direct regression-based inversion
approach when there are unknown number of parameters to be estimated and the seismic data has
complicated noise characteristics which may not always allow a stable and unique solution in a direct
inversion process. in this paper, starting from graphics processing unit (gpu) based synthetic
simulations of a few thousand forward seismic shots due to microseismic events via pseudo-spectral
solution of elastic wave equation, we develop a step-by-step process to generate a surrogate regression
modelling framework, using machine learning techniques that can produce accurate seismograms at
specified receiver locations. the trained surrogate models can then be used as a high-speed metamodel/emulator or proxy for the original full elastic wave propagator to generate seismic responses for
other microseismic event locations also. the accuracies of the surrogate models have been evaluated
1

geophysical journal international
using two independent sets of training and testing latin hypercube (lh) quasi-random samples, drawn
from a heterogeneous marine velocity model. the predicted seismograms have been used thereafter to
calculate batch likelihood functions, with specified noise characteristics. finally, the trained models on
23 receivers placed at the sea-bed in a marine velocity model are used to determine the maximum
likelihood estimate (mle) of the event locations which can in future be used in a bayesian analysis for
microseismic event detection.
keywords: synthetic seismogram generation, time domain compression, surrogate meta-model,
microseismic event detection, gaussian process regression
1. introduction
microseismic event detection has emerged as a significant field of research in computational
geosciences with an aim of studying the changing geological characteristics of a subsurface reservoir
during and after hydrocarbon production. these microseismic events are characterized by low
amplitude ground movements and are often indistinguishable from environmental seismic noise (leet
1949). a frequency band based quantification approach has been adopted in (groos & ritter 2009) to
grossly classify such events as microtremor (>1 hz), transitional (0.6-1 hz) and microseismic (<0.6
hz), although different sources and background noise in a marine environment (e.g. due to water waves,
storms, shipping and anthropogenic activities like drilling) share overlapping frequency bands, making
such a detection task quite challenging, using the real field datasets from marine seismic surveys. in
order to reliably detect these microseismic events, recent attempts have been made to simulate
approximate template seismic waves with known time-frequency domain characteristics using geomechanical modelling from first principles. amongst the available approaches for the forward
geophysical modelling given a heterogeneous velocity model, raytracing, acoustic wave and elastic
wave propagation modelling are widely used (chapman 2004). the elastic wave equation-based
modelling is the most detailed and accurate geophysical approach for microseismic events in marine
velocity models with a rock-water interface, whereas the raytracing method mostly relies on the high
frequency wave propagation approximation using separate compressional (p) and shear (s) wave
velocity models. in spite of the capabilities of accurate geophysical modeling, with mode conversion
2

geophysical journal international
between p-wave and s-waves in the boundaries between the rock layers, bulk scale simulation of the
3d elastic wave equation is often not a favorable solution as it suffers from extremely high
computational requirements to generate accurate synthetic seismograms from a given velocity model
(including density and p/s-wave velocity) with a few million grid points for detailed description of
subsurface heterogeneity. however, a gpu based generic elastic wave propagator like the k-wave
solver (treeby et al. 2014; treeby & cox 2010) can reduce the computational burden of bulk simulation
significantly and has been used in large scale geophysical wave propagation modelling before e.g. (guo
et al. 2016)(das et al. 2017). but gpu based forward simulation is still not fast enough to evaluate
thousands of batches of single shot elastic wave propagation simulations needed for fast computation
of the likelihood values at speculative locations of microseismic events, given recorded noisy
seismograms.
the outputs of the governing partial differential equation (pde) for elastic waves are more numerous
(i.e. multi-receiver seismogram time series) than the inputs to the pde solver (specified microseismic
source positions as 3d co-ordinates). therefore, such a high dimensional mapping from the
microseismic event locations to the full set of observables, i.e. the seismic waves recorded on multiple
receivers, make such a statistical regression modelling problem quite challenging. this becomes even
more challenging since the resulting wave-fields in response to unit strength microseismic events at
random locations are sparse in nature, with time localized information embedded in the time-series data
as multiple spikes. a direct regression modelling using a few thousands of sparse seismic wave-fields
would smear away the predicted seismograms, as the data samples can differ by a few order of
magnitude (depending on the distance from the receivers) and most of the information lies in the form
of localized spikes in time. a robust compression method is thus needed for predicting the simulated
seismic waves, before applying a regression framework as a â€œproxyâ€ for the elastic wave propagator.
the compression can be applied in many different ways (time, frequency or time-frequency domains
e.g. using fourier or wavelet bases). the mapping in the compressed domain needs to be smooth to
capture the short duration variable amplitude oscillations in the seismic waves. due to the timefrequency domain duality criteria, small errors incurred in the frequency domain may lead to wider and
sustained spurious oscillations in time domain, which suggests the use of time domain robust
3

geophysical journal international
compression methods over other frequency domain methods (wood 1974). this approach slightly
sacrifices the achievable compression performance and hence the number of observables in the
regression model and consequently the size of the trained surrogate model to predict accurate synthetic
seismograms.
with the aim of proxy construction, this paper first simulates synthetic elastic wave propagation using
gpus, from a few thousands of unit amplitude explosive microseismic events at random positions in
the subsurface and records the resulting seismograms at specified receiver locations at the seabed. these
synthetic data are then used to approximate, or â€˜statistically learnâ€™, the underlying physics of elastic
wave propagation, therefore generating a reduced physics model (wilson & durlofsky 2013; wilson et
al. 2012), for any random event location within the heterogeneous velocity model. here we also
compare the computational time of the full-scale forward model simulation vs. the trained surrogate
meta-models to obtain an estimate of the run-time saving. this can enable an iterative microseismic
source inversion process within a realistic time using standard computational resources. such a
statistical learning or approximation of physics in the form of pde solverâ€™s outputs has been widely
used in various surrogate meta-model assisted optimization methods before e.g. in (forrester et al. 2008;
forrester & keane 2009; forrester et al. 2007).
surrogate meta-models or proxy methods were traditionally developed for various optimization
problems e.g. constrained single or multi-objective optimization problems, missing data problems etc.
(forrester et al. 2008; forrester & keane 2009; forrester et al. 2007). similar machine learning
approaches have been adopted to approximate complicated likelihood functions within a bayesian
analysis framework in the blind accelerated multimodal bayesian inference (bambi) algorithm (graff
et al. 2013; graff et al. 2012)(hobson et al. 2014). surrogate meta-models are also used to learn
weighted multiple objective functions within single-objective (pan & das 2015), multi-objective (pan
et al. 2014b) and robust optimization frameworks (babaei, pan, et al. 2015)(babaei, alkhatib, et al.
2015), containing expensive function calls for the forward physics simulation. the trained surrogate
meta-models can be viewed as a â€˜proxyâ€™ for the expensive forward simulations, while it also acts as a
smooth interpolator in the parameter space of the forward model (i.e. microseismic event locations)
which can be verified using an independent testing dataset. such reduced physics or approximate
4

geophysical journal international
physics based proxy or surrogate models have been widely used in various other geophysical and
geological problems like shale gas production optimization (wilson & durlofsky 2013; kalantaridahaghi et al. 2015; wilson et al. 2012), geological co2 storage (babaei, pan, et al. 2015)(pan et al.
2014a; pan et al. 2014b), water injection in oil reservoirs (babaei & pan 2016; babaei, alkhatib, et al.
2015), and history matching (goodwin 2015; mohaghegh 2006; rodriguez et al. 2006; slotte &
smorgrav 2008; zubarev 2009), in the context of optimization or uncertainty quantification using
various monte carlo methods. to the best of our knowledge there isnâ€™t any study on surrogate metamodel or proxy development for microseismic response modelling via elastic wave propagation, using
the sparse spike time series which is difficult to learn unlike in many traditional areas of computational
geosciences, except few variable frequency decomposition methods for fixed receiver and source
position (modesto & de la puente 2016) and some not well-explored concepts of seismic inversion
(weglein et al. 2009).
seismic data driven geophysical parameter estimation and inverse problems often need a few
thousands of such likelihood or objective function calls where the forward geophysics simulation
produces a template seismic data to match with the noisy real recordings (aster et al. 2011)(mosegaard
& tarantola 2002; tarantola 2005; tarantola & valette 1982; mosegaard & tarantola 1995). previous
surrogate-based optimization and bayesian inference methods trained a proxy for the single valued
likelihood functions since the likelihood function is dependent on the data. in applications where the
data change frequently, such an approach needs retraining of the surrogate meta-model using the newly
recorded data, which may be a computationally wasteful approach. thus, we take a different approach
here of directly learning the raw observables obtained from the geophysical simulation model. this
poses mainly two challenges â€“ firstly, the observables (seismograms) recorded on multiple receivers
will produce too many parameters for a multivariate regression and secondly, the generated seismic
data are sparse which makes it difficult to predict via a standard regression framework. in other words,
for elastic wave propagation modelling, the simulated datasets are sparse in nature and also, they are
dense multivariate time series, the size of which massively increases with the number of receivers and
the sampling frequency. therefore, the contribution of this paper is to statistically learn the sparse
physical response of unit size microseismic activity, as a function of input parameter-set in the pde
5

geophysical journal international
(i.e. random event locations in this case) which is rather a harder problem than learning scalar valued
likelihood functions within an inference problem as shown in (graff et al. 2013; graff et al. 2012).
amongst previous approaches to supervised learning of physics based models i.e. fewer model
parameter to many observable mapping, the cosmonet algorithm (auld et al. 2007; auld et al. 2008)
employing multilayer perceptron neural networks is worth mentioning. the present paper extends this
idea for predicting sparse data using a robust compression technique. this paper also compares the
performance of various smooth interpolation methods available from a pool of supervised learning
techniques â€“ starting from robust polynomial regression to kernelized shrinkage regression, support
vector machine (svm), decision tree and ensemble regression, feedforward and cascaded forward
neural networks (nn) and gaussian process (gp) regression with various kernels and basis functions.
however the difference between the observable mapping, shown in cosmonet algorithm (auld et
al. 2007; auld et al. 2008) and our approach is that we here learn each compressed domain prediction
separately, rendering multiple partitioned regression models, without leveraging the underlying
correlation structure amongst the observations in the compressed domain. also, in surface seismic data
based microseismic activity monitoring, the gross geological characteristics given by the voxelized 3d
velocity model are not expected to change within a short span of time and can be considered constants,
hence leading to a deterministic mapping of the microsiseismic event parameters on to the observed
seismic profiles at various receivers. this motivates us to conceptually follow a similar route proposed
in physical measurement domain observable learning as shown in the cosmonet algorithm (auld et
al. 2007; auld et al. 2008), rather than the specific historical fixed dataset based likelihood learning as
reported in the bambi algorithm (graff et al. 2013; graff et al. 2012).
therefore, the goal of this paper is to develop a robust method to act as a proxy or surrogate metamodel or fast interpolator for mapping the input parameters in a sufficiently complex pde model with
material heterogeneity onto the sensor or measurement space to be used later in the likelihood calls for
fast parameter estimation and probabilistic inference problems. in other words, the broad objective here
is to teach the machine learning algorithms to rapidly predict the numerical solution of the elastic wave
propagation and then use these predictions to estimate the microseismic event locations in a simple
maximum likelihood or even more involved full posterior distribution estimation. amongst previous
6

geophysical journal international
efforts on such characterization of microseismic source activity in the subsurface from recorded
seismograms using spectral, spectrogram domain methods (eaton et al. 2014) and phase space domain
using polarization diagrams in (levy et al. 2011) are notable. also, (groos & ritter 2009) proposed a
scheme for classifying the sources in microtremor, transitional and microseismic events from the
observed seismograms using realistic field data.
the paper is divided in the following objectives to achieve this broader goal and presented in the
subsequent sections:
i)

fast gpu based synthetic seismogram simulation for training the supervised learning methods

ii)

a robust compression of the sparse seismic signals

iii)

learning a smooth mapping from event location on to the compressed domain seismograms

using various machine learning techniques
iv)

comparing accuracy, storage size, training time trade-offs for these supervised learning-based

surrogate meta-models and
v)

using the proxy-based fast predictions for calculating maximum likelihood estimates of

possible event locations
2. synthetic seismic trace generation for training machine learning algorithms
the aim of this work is to train machine learning algorithms to rapidly generate accurate seismograms
within each likelihood call. it needs to be trained using some example datasets to help statistically learn
the elastic wave propagation mechanism without numerically solving the expensive governing pdes.
to generate the synthetic seismograms, we have used the elastic wave equation solver k-wave, in a
specified 3d geometry using the pseudo-spectral method (treeby et al. 2014). the receivers and
microseismic source positions can be modelled using the given 3d voxelised heterogeneous velocity
model which can be run using general purpose gpus with a single precision (32-bit) number
representation (treeby & cox 2010; treeby et al. 2012). for synthetic trace generation, the medium
can be modelled as simple acoustic (with only p-waves), or elastic (having both p-wave and s-wave
sound velocities) or even as viscoelastic with frequency dependent absorption, which is considered as
zero in the present geophysical wave propagation modelling. in the simulation process, the stress/strain

7

geophysical journal international
tensors are iteratively updated using the specified 3d heterogeneous velocity model. during the
simulation, the 3-component particle velocity and the acoustic pressure are calculated from the
propagating waves at specified receiver locations. in most realistic 3d geophysical simulations, the
sound velocity and density model are used for solving the forward wave propagation in acoustic mode
(phadke et al. 2000) or elastic mode (igel et al. 1995) using a heterogeneous medium where the material
properties or the velocity model with ï»c p , c s , ï² ï½ being specified as 3d matrices with specified voxel
values. microseismic response simulation on gpus using the pseudo-spectral method has been explored
previously in (das et al. 2017). however the seismic data generation process using other numerical
schemes of pde discretizing methods are not the main focus here and a similar seismic wave
propagation method involving either finite difference, finite element, spectral element or finite volume
method can also be employed instead of the pseudo-spectral method (igel 2016). in our simulations, in
order to impose an absorbing boundary condition via the perfectly matched layer (pml), 10 grid points
were reserved along each direction before and after the regular grids of the velocity model. the elastic
wave propagation due to explosive microseismic sources were run on a 3d domain of 81Ã—81Ã—301 =
1.975Ã—106 grid points where the grid spacing in the three directions are given by
ï„ x ï€½ 12.5, ï„ y ï€½ 12.5, ï„ z ï€½ 10 m, therefore representing a geological model of dimension 1 kmÃ—1 kmÃ—3

km along the three directions as shown in figure 1. the elastic wave equation is solved with a sampling
time of 0.8 ms to guarantee numerical stability for this heterogeneous model over a total time interval
of 2 sec and then the recorded seismograms are down-sampled to ts = 4 ms. the strength of the sources
are considered as 1 mpa as many recent literature suggest that the typical range for microseismic
sources is around 1-10 mpa in sedimentary rocks and >20mpa in crystalline rocks (rutledge et al.
1998; collettini & barchi 2002) whereas for earthquakes it ranges between 5-100 mpa (dieterich et al.
2015).

8

geophysical journal international

figure 1: heterogeneous velocity model of 1 kmÃ—1 kmÃ—3 km, comprising of the density (kg/m3), compressional and shear
velocity (m/sec) at each grid-point. heterogeneity is higher in depth compared to the lateral directions.

figure 2: latin hypercube samples for event locations for training and testing dataset in the supervised learning. both training
and testing samples are selected almost uniformly from the whole volume.

in total 4000 random latin hypercube (lh) samples for speculative source positions were used here
for the forward simulations to generate the training and testing seismic data for different machine
learning algorithms. we have randomly split 2000 source positions for training the surrogate meta9

geophysical journal international
models and then use the remaining 2000 sources for testing the performance of the trained meta-model.
both these data-sets are uniformly distributed throughout the volume of the velocity model as shown in
figure 2. the forward simulations were run on the wilkes gpu cluster at the university of cambridge,
uk with non-interactive batch mode as separate matlab scripts. each batch contained 40 random event
locations and 10 such batches (400 sources) were run simultaneously. the generated seismic waves of
2 sec length were recorded at the surface receivers placed at the interface between the rock layers and
the water column in the velocity model in figure 1. the synthetic data after down-sampling to ts = 4
ms, becomes 182 gb for 4000 lh source locations. previous 3d elastic wave modelling attempts on
gpus e.g. (mu et al. 2013a; mu et al. 2013b) used 0.03Ã—106 and 0.3Ã—106 voxels respectively, whereas
our present model is significantly larger (65.8 times and 6.5 times respectively) than the results reported
there. on the 1 square kilometer surface at the sea-bed the 23 receivers are placed with the arrangement
shown in figure 3. this paper initially develops the proxy meta-model for the central receiver (r-12)
and then extends it to all the 23 receivers. we also show the effect of choosing different sub-sets of
these receivers on the final maximum likelihood detection performance of the events. amongst the 4000
forward simulations, 10 representative cases are shown in the supplementary material in map view of
the propagating acoustic pressure wave-field at a fixed time instant t = 1.4 s, where the respective
microseismic source positions in the volume are mentioned in the title of the subplots. the seismic
traces recorded at the 23 receiver locations can be seen in figure 4 where the corresponding map views
of the acoustic pressure wave-fields are shown in the supplementary material. on the seismogram
wiggle plots in figure 4, the appearance of multiple arrivals are actually an effect of strong p-wave,
followed by weak delayed s-waves and also the receiver arrangement where source to receiver distance
does not uniformly vary in different trace numbers, since the receivers are not placed along a line but
distributed all over the surface.
in the next section, we aim to learn a statistical mapping between the event locations and the resulting
seismograms at these 23 receivers without running the expensive forward simulations, for trained or
new test event positions. in order to achieve this goal, the recorded seismic data needs to be compressed
first in order to reduce the number of outputs of the surrogate regression meta-model i.e. 23 receiver Ã—
501 time samples = 11,523 data points per microseismic event location. this 3 to 11,523 dimensional
10

geophysical journal international
mapping is inherently a difficult learning problem because the output has complex correlation structures
and moreover are sparse in nature with time localized spikes. as discussed earlier, fewer observables
in the non-sparse and smooth cases can be statistically learned using multiple-input multiple-output
(mimo) regression frameworks e.g. using various neural network architectures (auld et al. 2007; auld
et al. 2008)(pandey et al. 2016). however most generic regression model involving nonlinear kernels
like svms, decision tree, polynomials and gaussian processes can mostly accommodate a many-toone mapping thus leading to a multiple input single output (miso) regression problem. in general,
neural networks, as universal function approximators, can accommodate both mimo and miso
regression framework, e.g. a comparison has been reported in (pandey et al. 2016), but in general nns
are sensitive to outliers, noise, and may not adequately learn sparse datasets, as it requires several
heuristics for choosing the right combination of hidden nodes, number of layers, activation functions
and optimizers. on the other hand the kernelized gaussian process models have been widely used in
geostatistical modelling and kriging that can naturally accommodate noisy data for regression and
outperformed many other family of algorithms especially on regression problems as shown in (mackay
1997)(sitharam et al. 2008; samui & sitharam 2010). in order to provide a fair comparison here we
have tested 9 different classes of regression models which can learn several many-to-one (miso)
mapping under the same framework i.e. given 3 event location parameters (x, y, z) the prediction of 100
compressed domain seismograms on the 23 receivers, instead of learning a many-to-many (mimo)
mapping that may capture the correlations between the data in the compressed domain components and
also between different receivers. learning this collection of many-to-one statistical mapping for the
compressed seismograms gives a smooth and robust method for predicting the seismic waves due to
microseismic sources, as explored in the next sections.

11

geophysical journal international

figure 3: receiver placement geometry in the sea-bed. receivers are placed at fixed depth of z = 244 in a 2.44 km deep
velocity model of rock layers.

figure 4: seismograms for the 23 receivers recording of the acoustic pressure in the forward simulation with fixed velocity
model. sample numbers and source locations are mentioned in the subplot titles. 500 samples represent 2 sec of seismic data
with Î´t = 0.004.

3. time domain compression of seismic traces and surrogate regression meta-modelling
3.1. compressed representation of seismograms for regression meta-modelling
this section first describes the robust time domain compression method for the time localised seismic
datasets shown in figure 4. each seismic trace at a specified receiver location (in the horizontal x, y
12

geophysical journal international
plane) has been first sorted in decreasing order of absolute amplitude and only the strongest 100 samples
(either capturing positive or negative pressure) are retained while the rest of the entries in the 501 sample
long time-series (equivalent to 2 sec of data) are set to zero. this helps in identifying the dominant time
instants within the sparse array of seismic traces, due to a smoothed delta-function like microseismic
event Î´(t) at different locations (x, y, z) in the heterogeneous volume. smoothing of the source level
spatial delta function is required and can be implemented using blackman window which may otherwise
create temporal oscillations, where more details on this can be found in (treeby et al. 2012)(das et al.
2017). this transforms the original long but sparse time series in two different components â€“ dominant
amplitude (si) and the corresponding index terms (idx) for these non-zero temporal instants, which are
extracted for all the event locations. such a simple time-domain compression technique is thus able to
reduce the number of observables to be predicted, to a lower value (from 501 to 200 i.e. for both si and
idx, only 100 values). upon reconstruction using the signal amplitude si and the index terms idx, the
2d correlation coefficient between the original and compressed images is r2d>0.99, indicating almost
lossless compression while also maintaining a smooth mapping of the observables in the event
parameter space. using frequency or time-frequency domain compression techniques involving fourier
or wavelet transforms can achieve a better compression ratio but often learning the idx term for such
representation need to be very accurate, otherwise the compressed signals upon reconstruction may get
shifted to different locations which needs further investigation. each time domain compressed seismic
dataset has been sorted in ascending order of idx, thus producing a smooth pattern in the location of the
dominant parts of the seismic traces. the first 500 realizations of the sorted data are shown in figure 5.
here the compression is done on the seismic response for a single unit amplitude microseismic event.
for 2 sec of data with 501 time samples, retaining only the strongest 100 samples gives us 99%
reconstructed accuracy for a single microseismic response with strong p-wave and then trailing swaves. it is worth noting here that the purpose of the compression here is to reduce the number of
regression outputs for noiseless template seismic responses for unit events and not noisy seismic traces
with multiple events. for other types of datasets like different source mechanisms or different size of
the velocity model, the length of the template noise free seismic trace may vary and under such a

13

geophysical journal international
scenario, the compression ratio might need to be retuned, but a similar method needs to be adopted to
reconstruct seismograms with an accuracy of r>0.99.

figure 5: sorted index terms (idx) and corresponding signal amplitudes (si) on the central receiverâ€™s seismic data for the first
100 dominant time instants of 500 random microseismic events.

the time-domain robust compression method can be described using the following three steps:
step 1: out of the 501 samples in each 2 sec of seismogram, sort and isolate first 100 strongest positive
or negative (absolute) amplitudes (si)
step 2: sort the strongest signal values with increasing idx (to get a monotonically increasing occurrence
of these dominant time instants)
step 3: sort the strongest signal values si, according to the respective time instants of idx
the aim is now to map each sample of these compressed domain sorted seismic data (both si and
idx) as a function of the event locations (x, y, z). through such a regression modelling, the seismic
traces can be accurately and smoothly interpolated within the heterogeneous medium without running
the computationally expensive pde solver for other event locations which have not been used while
training the surrogate meta-model. we found that apart from the three co-ordinates of the event
locations (x, y, z), in addition its distance from a fixed receiver location can also be an useful predictor
for modelling seismic data recorded on that particular receiver. here the distance (d) refers to the
14

geophysical journal international
euclidean norm of the differential co-ordinates between the source and the receiver. in the next
subsection, we explain with representative examples why compressing the seismic data in each receiver,
prior to statistical learning or the regression modelling is a necessity.
3.2. need for compressed domain representation of the seismic data within regression
the aim to predict each value in a 2d seismic snapshot at a fixed time slice is a regression problem
on a sparse image with important information embedded as time localised spikes as shown in figure 4.
therefore, a direct pixel by pixel regression approach fails to provide sufficient accuracy due to the
presence of too many zeros in the training dataset, because the informative spiky signals get smeared
away, under such direct regression framework. apparently it might seem that there is a clear linear
horizontal pattern for the seismic data amplitudes in figure 6 as a function of x ï€½ ï» x, y, z, d ï½ at the four
fixed time slices (t = 0.25, 0.5, 0.75, 1 sec), but actually the useful information lies only in the few
outliers above and below the baseline, containing mostly low amplitude fluctuations close to zero.
therefore, in order to predict the amplitude and temporal location of such spiky seismic signals, a
straight forward regression analysis cannot be applied, as any flexible machine learning algorithm will
pick up most of the frequently occurring zeros and not the few time-localised spikes or outliers in a
sparse seismic data. although the maximum amplitude shows a smoother variation with respect to z
and d (in the bottom row, last two entries of figure 6 as function of z and d), it can occur anywhere in
the long time trace of the seismic trace and cannot be used to recreate the full seismic wave. there is
another disadvantage of such direct prediction of the sparse seismograms as a function of event
parameters, apart from the computational burden of having more regression models i.e. 501 samples in
the case of a single seismic trace at a single receiver for 2 sec of data. our sparse prediction approach
essentially identifies the informative region in the seismic trace and predicts only the dominant values
at the respective temporal points (i.e. the two components â€“ si and idx), while considering the rest of
signal as sparse with zero values, whereas the voxel by voxel prediction generates small noise-like
fluctuations even at locations where there is not actually any significant information.

15

geophysical journal international

figure 6: variation in raw seismic signal amplitude as a function of microseismic source locations and its distance from the
central receiver {x, y, z, d}. the few outliers contain the most useful information of the seismograms as high/low amplitudes.

in order to remove any bias in the regression process, which may be introduced due to the different
ranges of input predictors or output observables ( x ) in the compressed domains, all the predictor and
compressed observables are standardized to zero mean and unit variance using (1) and the respective
standardization constants ( ï­ x , ï³ x ) are also stored apart from the trained proxy meta-models for
rescaling the new predictions to the actual physical scale:

ï€¨

x standard ï€½ x ï€­ ï­ x

ï€©ï³ .

(1)

x

compared to the uncompressed signal representation shown in figure 6, a smoother variation is
observed in the compressed domain, and can be seen from the 50th dominant time instant of the sorted
seismic data in the form of its two compressed components {si, idx} as a function of {x, y, z, d} in
figure 7. the patterns in the compressed domain are prominent and not sparse and hidden in the form
of outliers as in the previous case, as a function of these four covariates. in addition, the difference in
the signal amplitudes depending on the depth of the source are also an important factor as shown in
figure 5, since the response of the deep source may get smeared away as numerical noise under a
standard regression without any compression and normalization at each dominant time instant. from
figure 7, it is apparent that the two covariates {z, d} give rise to more correlated but complicated
16

geophysical journal international
patterns with few islanded regions which might be an effect of heterogeneous nature of the velocity
model and complex structure of the elastic waves with both p and s-waves in the two parts of
compressed domain.

figure 7: variation in compressed dominant amplitudes and their index terms as a function of source location and distance
from receiver {x, y, z, d}. depth and distance show prominent structure for predicting the two compressed components.

17

geophysical journal international
figure 8: schematic diagram of time domain compression, multiple regression learning in compressed domain and
decompression for predicted seismic trace generation. the unit event seismograms are compressed and learnt with parameter
selection of different regression models to achieve the best predictive accuracy.

next, we apply the same compression method on all the microseismic source locations as shown in
the lh samples in figure 2. therefore, using a few thousand microseismic event locations, each
containing 501 time-samples for a single receiver position, the time domain compression yields 100
dominant sorted time index (idx) and the corresponding signal amplitudes (si), i.e. 200 data points per
compressed seismogram. now, we aim to learn several regression meta-models between {x, y, z, d} as
predictors and the amplitudes of 100 si and the corresponding sorted time-index idx values as the
observables, in order to statistically learn the patterns represented in figure 7. after the compressed
representation of the seismograms, we choose a structure from a pool of regression models and
independently learn the compressed data at 100 dominant time-instants. the standardizing constants
computed before the regression are stored and then used to rescale the predictions to physical units. the
predicted si and idx values can be easily combined in the decompression step to generate the predicted
seismogram, as soon as a new input location for an event comes in. each regression model adjusts its
parameters by minimizing a mean squared error criterion between the ground truth vs. the predictions
in the two compressed domains in the normalized scale. upon reconstruction and rescaling the
predictions, the predictive accuracy of the two components along with the reconstructed signals are
calculated between the ground truth ( gij ) seismograms and the corresponding predicted ( pij ) versions
by different machine learning algorithms using the 2d pearson correlation coefficient in (2), for
choosing the best model parameters or suggesting a new model structure:

ïƒ¥ïƒ¥ ï€¨ g ï€­ g ï€©ï€¨ p ï€­ p ï€©
ij

r2 d ï€½

i

ij

j

2 ïƒ¶ïƒ¦
2ïƒ¶
ïƒ¦
ïƒ§ ïƒ¥ïƒ¥ gij ï€­ g ïƒ·ïƒ§ ïƒ¥ïƒ¥ pij ï€­ p ïƒ·
ïƒ¨ i j
ïƒ¸ïƒ¨ i j
ïƒ¸
1
1
gï€½
gij , p ï€½
ïƒ¥ïƒ¥
ïƒ¥ïƒ¥ pij ,
ni n j i j
ni n j i j

ï€¨

ï€©

ï€¨

ï€©

,

ï» ï½

(2)

where, g, p denote the 2d mean of the ground truth and predicted signals in either
compressed/reconstructed domains.
18

geophysical journal international
although the split regression models only see a smaller sub-problem with a goal of minimizing the
mean squared error (mse) between the grounds truth vs. the predictions, the combination of 200 such
predictions generate the full seismic traces for all the event locations in the training dataset. in order to
judge whether a structure is good enough from the pool of regression models or which parameters of
the models should be fine-tuned, a fixed threshold on 2d correlation coefficient (2) as r2d>0.9 has been
used for the initial screening. if the model structure is found to be less flexible to accommodate the
patterns in the compressed domain seismic data or a wrong control parameter is selected, a different
model and/or control parameter(s) are suggested. the proxy or surrogate meta-model training workflow
is schematically shown in figure 8, starting from the event locations, then finding the compressed
domain representation of seismic signals and then learning 100 split models for the dominant time
indices idx and the corresponding signal si, from the pool of regression models, along with fine tuning
of the associated control parameters. the next section briefly introduces the model structures in the pool
of regression models and discusses the control parameters to learn the patterns in the seismic waves in
the two-component compressed form as shown in figure 7.
4. machine learning techniques for multivariate compressed domain regression metamodelling
4.1. splitting high dimensional mimo regression as multiple miso regression problems
we here explore the predictive performance, computing requirements for training and also the
required storage for the trained surrogate models, using the following classes of regression techniques:
i)

robust polynomial regression

ii) gaussian process (gp) regression
iii) support vector machine (svm) regression
iv) decision tree regression
v) ensemble regression using tree method
vi) kernelized shrinkage regression using ïŒ1 ïŒ2 norm
vii) generalized linear model (glm) regression
viii) kernelized shrinkage based glm regression
19

geophysical journal international
ix) multilayer perceptron (mlp) neural network regression

there are also few hyper-parameters in each of the regression models that make a particular technique
more flexible for learning complex patterns in the data over other classes of models. a sufficiently
accurate surrogate regression meta-model can then be used in the inference or for optimization purposes,
hence a comparison of storage requirements for such models and the training time are also important
factors in such proxy design.
the regression models were trained in the matlab programming platform on a 64 bit 12-core linux
cpu with 32 gb memory and intel xeon e5, 2.5 ghz processor, while each of the 100 regression
problems for idx and si in figure 8 were parallelized independently over 12 cores using the parallel for
(parfor) loops in matlab. in the simplest implementation, the 100 dominant compressed time instants
are learned independently using a separate model without considering a correlation structure between
them. this could have been otherwise learned as a 3 to 200 parameter regression problem but only mlp
neural networks would be usable, with the possibility of accommodating a full mimo regression
instead of a combination of multiple miso regression problems (pandey et al. 2016). however, such
an approach here has not yielded a good prediction accuracy due to the large number of predictors (100
or 200) compared to the covariates (only 4) using moderate size nns and hence we here focus only on
splitting the mimo regression as a collection of multiple miso regression problems, since here the
main purpose is to get a good predictive accuracy without solving the full computationally expensive
elastic pde solver when called for fast likelihood calculation.
4.2. predicting compressed domain seismic data at a fixed receiver location as a function of
event location parameters
initially we explore the performance of various machine learning (regression) algorithms for
predicting the seismic traces, recorded at the central receiver (at nx/2, ny/2) for the sake of simplicity.
here we use all the four predictors i.e. position and distance {x, y, z, d}, as this has been found to yield
a better fitting performance over other subsets of covariates. the regression models have been assumed
to be different on the si and idx, since their patterns are found to be quite different in figure 7. in each

20

geophysical journal international
of the predictors we fit e.g. a higher order kernel with flexibility to adjust the order of the polynomial
using an exhaustive search that maximizes the 2d correlation coefficient between the compressed vs.
the regression model predicted image for both si and idx, hereafter called as rsi and ridx respectively.
the regression uses the robust option to reject outliers and fit a smoother response in its predictions.
also, the maximum order of polynomial kernel has been kept up to 4 in each predictor {x, y, z, d}, as
more complex models are prone to overfit inconsistent patterns, and higher order models with more
degrees of freedom have a larger number of parameters to estimate, hence producing more uncertainty
in the estimates and increased variance on the predictions. the highest accuracy achieved through
simple polynomial regression was insufficient because of the complexity of the data as shown in figure
7, a representative case for sorted 50th time instant, which may not be fitted well with simple quadratic,
cubic or quartic polynomial functions of the predictors. the polynomial case is used here as an example
from the pool of 9 classes of regression models in figure 8 and the different cases the polynomial order
replaces the respective free hyper-parameters of the algorithm to fine tune. also, some of the predictions
for idx may be negative or exceed the maximum time instant, yielding an unfeasible region (as the
signals cannot lie in negative time) or increased time series upon reconstruction. therefore, after the
predictions by the regression models, the idx<0 and idx>501 are thresholded at the lower and upper
bounds respectively and the corresponding signal amplitudes are set to zero. in both the predictions of
si, idx and the reconstructed seismic traces, the 2d correlation coefficient is used as reported in the
tables as {rsi, ridx, rrecon} using (2). it is to be noted that in all the cases, the machine learning algorithms
have been trained on the two compressed components of the signal to select the model with maximum
{rsi, ridx,}. the reconstructed accuracies on the seismograms are calculated outside the training process
to calculate rrecon.
4.3. robust polynomial regression
the robust regression method gives better estimates in the presence of outliers and noise, compared
to the commonly used ordinary least square (ols) method incorporating the moore-penrose pseudoinverse. let us consider the regression problem in (3), x i ï€½ ï› xi

21

yi

zi

di ï is the predictors and
t

geophysical journal international

yi ï€½ ï› idxi

sii ï is the observables with model weight ï¢ and the prediction error ( ï¥i ) being independent
t

and identically distributed (iid) with a scale factor ï³ for the modelling error:
(3)

yi ï€½ x it ï¢ ï€« ï³ï¥ i .

ï‚µ can be calculated from a given estimate of scaling factor ï³ï‚µ , considering
the estimate of the weight ï¢
the data and the weighted function (ï¹ ) of error to be uncorrelated i.e.

ï€¨1 n ï€© ïƒ¥ x iï¹ ï€¨ ï€¨yi ï€­ x it ï¢ï‚µ ï€© ï³ï‚µ ï€© ï€½ 0.
n

(4)

i ï€½1

ï» ï½

in the robust regression, starting from an initial estimate ï¢ï‚µ ,ï³ï‚µ , residuals of the estimates are calculated

ï€¨

as ri ï€½ yi ï€­ x it ï¢ï‚µ

ï€© ï³ï‚µ . the weights are defined as w ï€½ ï¹ ï€¨ r ï€© r and the estimates are updated with a
i

i

i

least square estimate with weight wi . the iterative update continues unless the algorithm converges
(street et al. 1988). there can be different choice of weight functions (holland & welsch 1977) for
robust regression as in (5):

andrews: w ï€½ ï€¨ r ï€¼ ï° ï€© sin ï€¨ r ï€© r , bisquare: w ï€½ ï€¨ r ï€¼ 1ï€© ï€¨1 ï€­ r 2 ï€© , cauchy: w ï€½ 1 ï€¨1 ï€« r 2 ï€© ,
2

ï€¨

ï€©

fair: w ï€½ 1 ï€¨1 ï€« r ï€© , huber: w ï€½ 1 1 ï€« max ï€¨1, r ï€© , logistic: w ï€½ tanh ï€¨ r ï€© r ,

(5)

welsch: w ï€½ exp ï€¨ ï€­r 2 ï€© , talwar: w ï€½ r ï€¼ 1.

ï€¨

ï€©

here the value of r is calculated as r ï€½ res consttune ï³ï‚µ 1 ï€­ h , with ï»r e s , h ï½ being the residual from the
previous iteration and leverage value from ols fit respectively. the standard deviation of error is
calculated as ï³ï‚µ ï€½ mad 0.6745 , using the median absolute deviation (mad) of the residuals,
considering it to be normally distributed.
in the present problem with robust polynomial regression, we first transform the input parameter
space i.e. microseismic event locations and distance from the central receiver ï» x , y , z , d ï½ using a
polynomial kernel function xnfx (ieong 2012) with a chosen order of 2 to 4 in order to form a design
matrix e.g. {1, x, y, z, xy, xz, yz, x2, y2, z2, â€¦} in the case of quadratic kernel, as an example. this high
dimensional transformed feature matrix is then used in the robust linear regression framework through
22

geophysical journal international
the weight functions in (5). the maximum order of the kernel has been chosen as 4 keeping in mind the
lower number of predictors (also 4) and to lower the possibility of overfitting. the results of robust
polynomial regression are reported in table 1.
4.4. gaussian process (gp) regression
starting from a linear model ( y ï€½ x t ï¢ ï€« ï¥ , ï¥ ï€º ï ï€¨ 0, ï³ 2 ï€© ) the gp explains the prediction using the
latent variables f ï€¨ x i ï€© , i ï€½ 1, 2 , ïŒ , n (for modelling the smoothness of the output) and the explicit basis
h (for projecting predictors in high dimensional space). if f ï€¨ x ï€© , x ïƒ ï‚¡ d be a gp having mean m ï€¨ x ï€©
and covariance k ï€¨ x i , x j ï€© , then given n observations ï» x 1 , x 2 , ïŒ , x n ï½ the joint distributions of the latent
variables ï» f ï€¨ x1 ï€© , f ï€¨ x 2 ï€© ,ïŒ, f ï€¨ x n ï€©ï½ are also gaussian. now let us consider the model as (6), with
h ï€¨ x ï€© , h : ï‚¡ d ï‚® ï‚¡ p being the basis and coefficients of the basis are ï¢ ïƒ ï‚¡

y ï€½ h ï€¨ x ï€© ï¢ ï€« f ï€¨ x ï€© , f ï€¨ x ï€© ï€º ï‡ï ï€¨ 0, k ( x i , x j ) ï€© .
t

p ï‚´1

:
(6)

the probabilistic predictions of gp regression is given by (7):

ï€¨

ï€©

p ï€¨yi f ï€¨ x i ï€© , x i ï€© ï€º ï yi h ï€¨ x i ï€© ï¢ ï€« f ï€¨ x i ï€© , ï³ 2 .
t

(7)

the gp regression utilizes the fact that two closely lying predictor values ï» x i , x j ï½ will have similar

ï»

ï½

response f ï€¨ x i ï€© , f ï€¨ x j ï€© and the similarity is represented by the kernel or covariance function

ï€¨

ï€©

k x i , x j ï± with the hyper-parameter vector ï± . the kernels vary mainly due to two parameters i.e. the

signal standard deviation ( ï³ f ) and characteristic length scale ( ï³ l ) which control how fast the
correlation between two points change. given a set of input-output data the gp algorithm estimates the
basis coefficients ï¢ , noise variance and the kernel hyper-parameters ï± . we used three different basis
functions where the model is extended by different basis matrix (h) by multiplying with the vector of
basis coefficients (Î²) i.e. the extended model becomes hÃ—Î². for the constant, linear and quadratic cases,
the basis matrix can be represented as (8)

23

geophysical journal international
constant ïƒ h ï€½ 1; linear ïƒ h ï€½ ï›1, x ï ;
ïƒ© x 112
ïƒª 2
x
ï‚¢ï‚¢
ï‚¢ï‚¢
quadratic ïƒ h ï€½ ï›1, x , x ï , x ï€½ ïƒª 21
ïƒª ï
ïƒª 2
ïƒªïƒ« x n1

x 122
x 222
ï
x n22

ïŒ x 12d ïƒ¹
ïƒº
ïŒ x 22d ïƒº .
ï ï ïƒº
ïƒº
ïŒ x nd2 ïƒºïƒ»

(8)

along with variation in the basis function we also explored six different kernel functions â€“ squared
exponential, matern 3/2, matern 5/2 and also their automatic relevance discovery (ard) versions (neal
1996; rasmussen & williams 2006):

ï€¨

ï€©

ï€¨

ï€©

ï€¨

ï€©

t
k squared ï€­ exp x i , x j ï± ï€½ ï³ 2f exp ïƒ©ïƒª ï€­ ï€¨ x i ï€­ x j ï€© ï€¨ x i ï€­ x j ï€© 2ï³ l2 ïƒ¹ïƒº
ïƒ«
ïƒ»
ïƒ¦
ïƒ¦
3r ïƒ¶
3r ïƒ¶
k matern ï€­3/2 x i , x j ï± ï€½ ï³ 2f ïƒ§ïƒ§ 1 ï€«
ïƒ·ïƒ· exp ïƒ§ïƒ§ ï€­
ïƒ·ïƒ·
ï³l ïƒ¸
ïƒ¨
ïƒ¨ ï³l ïƒ¸
ïƒ¦
ïƒ¦
5 r 5r 2 ïƒ¶
5r ïƒ¶
k matern ï€­5/ 2 x i , x j ï± ï€½ ï³ 2f ïƒ§ïƒ§1 ï€«
ï€« 2 ïƒ·ïƒ· exp ïƒ§ïƒ§ ï€­
ïƒ·ïƒ· ,
ï³ l 3ï³ l ïƒ¸
ïƒ¨
ïƒ¨ ï³l ïƒ¸

where, r ï€½

(9)

ï€¨ x ï€­ x ï€© ï€¨ x ï€­ x ï€© is the euclidean distance between the points ï» x , x ï½ .
t

i

j

i

j

i

j

considering separate length scale ( ï³ m ) for each of the predictors ( m ï€½ 1, 2,ïŒ , d ), the covariance
(kernel) function implementing the ard (neal 1996) takes the form in (10):
ïƒ© 1 d ï€¨ x im ï€­ x jm ï€© ïƒ¹
ïƒº
k ard ï€­ squared ï€­ exp x i , x j ï± ï€½ ï³ 2f exp ïƒª ï€­ ïƒ¥
ï³ m2
ïƒªïƒ« 2 m ï€½1
ïƒºïƒ»
k
x , x ï± ï€½ ï³ 2 1 ï€« 3 rï€¥ exp ï€­ 3 rï€¥

ï€¨

ard ï€­ matern ï€­ 3/ 2

k ard ï€­ matern ï€­5/ 2

where, rï€¥ ï€½

ï€©

ï€¨
ï€© ï€¨
ï€© ï€¨ ï€©
ï€¨ x , x ï± ï€© ï€½ ï³ ïƒ¦ïƒ§ïƒ¨1 ï€« 5 rï€¥ ï€« 53 rï€¥ ïƒ¶ïƒ·ïƒ¸ exp ï€¨ ï€­ 5 rï€¥ ï€©
i

j

f

i

j

2
f

(10)

2

ïƒ¥ ï€¨ï€¨ x ï€­ x ï€© ï³ ï€© .
d

m ï€½1

2

im

jm

2
m

the training of ard kernels in (10) e.g. ard materns are computationally more expensive than
their basic versions in (9) e.g. the simple matern kernels. the ard function automatically finds out
moving how far along a particular predictor will make the predictions uncorrelated. the inverse of the
length scale determines how relevant a predictor is, as also discussed in (rasmussen & williams 2006)
in a detailed manner. in all the regression models, the compact representation was adopted to store the

24

geophysical journal international
models as compact matlab objects that stores only the necessary information, instead of the full model
with information about training data etc. for a reduced storage requirement. also, while training the gp
models with constant basis, sometimes the cholesky decomposition of the covariance can be illconditioned, resulting in convergence failure of the gp algorithms. in such a case, a different initial
value of the kernel parameters, initial value and increased lower bound of the gp noise standard
deviation may improve the solution. for all the gp kernels, the computation for the log-likelihood and
gradient, the standard qr factorization and quasi-newton optimizer have been used for parameter
estimation. gaussian processes with certain kernels are known to have equivalent representation of
neural networks with infinite hidden nodes and are found to outperform many benchmark supervised
learning methods, especially complex regression problems due to its non-parametric bayesian nature
(mackay 1997)(sitharam et al. 2008; samui & sitharam 2010), albeit being computationally expensive
during the training process, compared to the nn learning. the gp based predictions in the compressed
as well as reconstructed domain signals along with the training time and storage requirements are shown
in table 2. here, the accuracies of the gp regression models are worth noticing in comparison with the
other classes of regression structures in figure 8.
4.5. support vector machine (svm) regression
in svm regression similar to the simple linear regression problem y ï€½ ï¢ t x ï€« b , the inputs (

ï€¨

ï€©

x i , i ï€½ 1,ïŒ , n ) can be mapped to a high dimensional space using a kernel ï† x i , x j . we used three

popular kernels viz. linear, polynomial and radial basis function (rbf) kernel in (11), with p being the
tuning parameters for the polynomial kernel (friedman et al. 2001)(rogers & girolami 2015):

ï† ï€¨ x i , x j ï€© ï€½ x it x j

: linear

ï€½ ï€¨1 ï€« x it x j ï€© , p ï€½ ï»2,3,4ï½ : polynomial .
p

ï€¨

ï€½ exp ï€­ x i ï€­ x j

2

ï€©

(11)

: rbf

the svm regression algorithm terminates using either of the three convergence criteria â€“ feasibility
gap (Î´), gradient difference ( ïƒ‘l ), or largest karush-kuhn-tucker (kkt) violation. the kkt criteria
act as constraints to the optimization problem which can be solved using the sequential minimal
25

geophysical journal international
optimization (smo) algorithm which is faster than the traditional quadratic programming approach
(huang et al. 2006). the comparative results of the svm regression models using the above three
kernels viz. linear, polynomial kernel of order 2-4 and the rbf have been reported in the supplementary
material as they do not yield high enough accuracy.
4.6. decision tree regression
in the decision tree regression, deep trees are grown first and then the optimal sequence of subtrees
are determined by pruning. firstly, the predictor space is divided into non-overlapping regions

rj , j ï€½ 1,2,ïŒ, j and then for every observation falling in a particular region, the prediction becomes
mean of the response values in rj . the regions are found out by minimizing the sum of squared error
(sse) in (12) between the real ( yi ) and the mean response of training samples within a particular box (

yï‚µ r j ) (james et al. 2013):
j

ï€¨

sse ï€½ ïƒ¥ïƒ¥ yi ï€­ yï‚µ r j
j ï€½1 iïƒr j

ï€©.
2

(12)

as the controlling parameters, the effects of varying the minimum number of leaf nodes (nleaf) and
minimum number of parent/branch nodes (nparent) are explored here and the accuracy vs. data storage
size trade-offs are also shown in the supplementary material. tree methods allow complicated nonlinear
and partitioned boundaries as non-overlapping regions, especially naturally modelling corners in the
input parameter space which are difficult to model with other regression models, thus often providing
good predictive accuracy, although generalization and storage size for large trees are inherent
challenges (james et al. 2013). also, the surrogate split option is used which is known to improve
predictive accuracy by randomly splitting the data at most 10 times in each leaf node. pruning of
decision trees is another option that has been used to produce a smaller tree with fewer splits. the results
of the tree methods with different leaf and parent size (3 to 20) have been reported in the supplementary
material.
4.7. ensemble regression with tree method

26

geophysical journal international
in ensemble regression, normally two algorithms are commonly used viz. least square boosting
(lsboost) and bootstrap aggregation (bag) (barutÃ§uouglu & alpaydin 2003). bagging grows multiple
weak learner trees on many resampled (bootstrap) replicas of the dataset and the predicted response is
the average prediction from all these trees. minimal leaf size of bagged regression tree is kept fixed at
5 and as the controlling parameter the number of learners (nlearn) are varied from 100 to 1000. in
lsboost every step fits a new learner using the difference between the observed and the aggregated
prediction of all the learners trained so far while minimizing the mse. while using the bagging method,
the size of the surrogate meta-models become huge (>1 gb) with just 100 learners and for just one
receiver location. therefore, bulk scale simulation using this approach is not recommended due to
unmanageable storage size of the trained proxy meta-models. the performance results of the ensemble
regression with several independently grown tree learners (100 to 1000) with bagging and boosting
method have been compared in the supplementary material.
4.8. shrinkage regression with polynomial kernel
when dealing with redundant or few less important predictors, the shrinkage methods give improved
performance over traditional regression methods that gives more priority on significant predictors over
the insignificant ones. in our four predictor ï» x , y , z , d ï½ based regression problem, the covariates are first
projected on to a higher dimensional space using a polynomial kernel of order 2-4 via the kernel function
xnfx (ieong 2012), since in many cases the kernel order for the design matrix exceeding the dimension
of the original inputs yield spurious results. as an example, a 3d event location ï» x , y , z ï½ under such a
3rd order polynomial kernel mapping would yield series of predictors like {1, x, y, z, xy, xz, yz, x2, y2, z2,
xyz, x3, y3, z3} etc. amongst these combinations, the shrinkage methods are expected to pick up the most
useful predictors from these new kernelized predictors, while pushing rest of the insignificant ones to
zero. the three variants of shrinkage regression i.e. least absolute shrinkage and selection operator
(lasso), elastic net and ridge regression solve the following minimization problem in (13) as weighted
sum of prediction error and penalty term on the coefficients (zou & hastie 2005):

27

geophysical journal international
2
ïƒ© 1 n
ïƒ¹
min ïƒª
yi ï€­ ï¢0 ï€­ x it ï¢ ï€© ï€« ï¬ pï¡ ï€¨ ï¢ ï€© ïƒº
ï€¨
ïƒ¥
ï¢0 , ï¢ 2 n
i ï€½1
ïƒ«
ïƒ»

(13)

where, pï¡ ï€¨ ï¢ ï€© ï€½ ï¡ ï¢ 1 ï€« ï€¨ ï€¨1 ï€­ ï¡ ï€© 2ï€© ï¢ 2 is the penalty term of elastic net interpolating between the
mixture of ïŒ1 ïŒ2 norm of the model coefficients and n is the number of samples for training. the above
elastic net problem approaches the lasso at ï¡ ï€½ 1, thus making lasso penalize the ïŒ1 norm only,
whereas the problem approaches ridge regression when ï¡ ï‚® 0 thus giving full penalty on the ïŒ2 norm
only (zou & hastie 2005; friedman et al. 2001). therefore, to implement the three shrinkage regression
methods ï¡ ï€½ ï»1, 0.5,10 ï€­6 ï½ have been considered using the four kernelized predictors ï» x , y , z , d ï½ with an
increasing polynomial order of 2-4. the regularization parameter (Î») in (13) controls the penalty
between the prediction error and a chosen norm ( ïŒ1 ïŒ2 ) of the model coefficients. a 10-fold cross
validation has been adopted to automatically choose the optimum Î» with minimum average error across
the folds of the training data and hence the best model is automatically chosen with optimum Î» for each
of the 100 multiple-regression problems. apart from the 10-fold cross-validation based optimum model
selection or kernel hyper-parameter tuning during the training phase, the best models on the training set
are also tested with a separate hold out dataset which is explored in the subsequent sections.
4.9. generalised linear model (glm) regression
glm is a special class of nonlinear models that still use linear methods for prediction. a linear model
( ï­ ï€½ x ï¢ ) based predictions can be interpreted as a normal distribution with mean ï­ where coefficients
ï¢ map each input on to the predictions linearly. in glm (14) the response can have a wide variety of

distributions f ï€¨ ïƒ— ï€© , known as the link function with mean ï­ :
f ï€¨ï­ ï€© ï€½ x ï¢

.

(14)

for normal distribution the link function becomes the mean i.e. f ï€¨ ï­ ï€© ï€½ ï­ , but for other complex
distributions, the canonical link functions and the mean inverse functions can be chosen in different
ways. for real valued outputs, choosing a normal distribution in glm is recommended which suits our

28

geophysical journal international
standardized outputs, whereas for positive/strictly integer values, other distributions like gamma,
inverse gamma, poisson or binomial can also be used.
4.10.

shrinkage based glm regression with polynomial kernel

these regression techniques have the advantages of both the shrinkage and glm methods, as
described in the earlier subsections. similar to the standard elastic net, norm based penalties are chosen

ï»

ï½

as ï¡ ï€½ 1, 0.5,10ï€­6 to implement lasso, elastic net and ridge regression respectively. in addition, a 10fold cross validation on the training data is also implemented to automatically select the regularization
parameter Î» in each of the multiple regression sub-problems. a normal distribution on the outputs and
an identity link function is considered in lasso-glm and other variants (friedman et al. 2010). for the
case of normal distribution as the link function, the predictions closely approach the base versions
without the glm, whereas glm enhancements are more popular in classification problems over the
regression problems. the kernelized shrinkage, glm and kernelized shrinkage glm based prediction
results have been compared in the supplementary material, where none of them attain a good predictive
accuracy.
4.11.

multi-layer perceptron (mlp) neural network (nn) regression

neural networks are widely used as universal function approximators and thus a popular choice in
many regression problems using a multiple inputs and multiple outputs (mimo) architecture. with an
aim of a fair comparison with other regression methods, we here employ a collection of multiple input
single output (miso) implementation of mlp neural networks with moderate size hidden nodes. often
neural networks are prone to pick up inconsistent patterns or outliers in the data, thus we used a
regularization constant of Î³ = 0.5 in the cost function (jreg), to keep an equal balance on both the penalties
due to the mse and the mean squared weight (msw) during the training process:
j reg ï€½ ï§ ï‚´ msw ï€« ï€¨1 ï€­ ï§ ï€© ï‚´ mse

.
ï€½ ï§ ï‚´ ï€¨1 m ï€© ïƒ¥ w2j ï€« ï€¨1 ï€­ ï§ ï€© ï‚´ ï€¨1 n ï€© ïƒ¥ yi ï€­ yï‚µ i
m

n

j ï€½1

i ï€½1

ï€¨

ï€©

29

(15)

geophysical journal international
although there have been recent developments on optimizers for fast training of large and deep
networks in classification problems, the traditional levenberg-marquardt (lm) backpropagation
algorithm has been shown to outperform on a wide variety of regression problems as it produces low
mse and high speed for training small to medium size networks with <1000 weights and bias terms
(plumb et al. 2005). as the size of the network grows, there are even efficient optimizers like scaled
conjugate gradient (scg) compared to traditional training algorithms like levenberg-marquardt. in the
present scenario, the whole dataset here during the nn training has been randomly divided in training
(70%), testing (15%) and validation (15%) set for each of the miso regression problems. the
hyperbolic tangent sigmoid (tansig) activation function in (16) is employed in the hidden layers and a
pure linear (purelin) activation function in the output layer which is commonly used for regression
modelling:
a ï€½ tansig ï€¨ n ï€© ï€½

2
1 ï€­ e ï€­2 n
ï€­
1
ï€½
, a ï€½ purelin ï€¨ n ï€© ï€½ n .
1 ï€« e ï€­2 n
1 ï€« e ï€­2 n

(16)

we also explored two different nn architectures â€“ feedforward and cascaded-forward networks
with single and double layer while the number of nodes is varied from 10 to 100 in each hidden layer
to keep the storage and training time comparable with other methods. the cascaded forward network
has similar architecture like feedforward networks except that it has an extra connection to the input
directly in each hidden layer, apart from the inputs from previous layer. the comparative performance
of these two nn architectures are shown in the supplementary material.
5. results and discussion
from the velocity model in figure 1, it is evident that along the y-direction, there is relatively small
variation in the rock properties, compared to the variation along the x-direction. also, the density and
p/s-wave velocities have rapid variation along the z-direction. therefore, the effect of the heterogeneity
will be different on different receivers placed at the sea-bed and thus finally affecting the likelihood
calculation in different ways. we here explore 6 different receiver arrangements for calculating the
likelihood by taking a subset of the 23 receivers, as shown in figure 3 (all 23, along principle diagonal,
anti-diagonal, central one, lower and upper triangular parts). in this section, the best regression model
30

geophysical journal international
from the previous section has been selected and the parameters are fine-tuned based on the seismic
responses recorded at the central receiver from the microseismic sources anywhere in the subsurface as
shown in figure 2.

5.1. prediction of a single seismogram at the central receiver
table 1: accuracy, computation time and size of the robust polynomial regression learning surrogates
polynomial

robustness

order

criteria

rsi

ridx

rrecon

andrews

0.7720

0.9626

0.3406

bisquare

0.7723

0.9626

0.3402

cauchy

0.7864

0.9661

0.3388

fair

0.7986

0.9679

0.3291

huber

0.7932

0.9672

0.3283

logistic

0.7946

0.9674

0.3322

ols

0.7913

0.9695

0.0951

talwar

0.7720

0.9629

0.3324

welsch

0.7756

0.9635

0.3410

andrews

0.8451

0.9665

0.7822

bisquare

0.8453

0.9665

0.7798

cauchy

0.8531

0.9700

0.5995

fair

0.8616

0.9720

0.6258

huber

0.8587

0.9713

0.6312

logistic

0.8593

0.9715

0.6254

2

3

31

training

model size for single

time (s)

receiver (kb)

3.70

16

3.52

16

3.45

16

3.44

16

3.43

16

3.85

16

3.13

15

3.30

16

3.54

16

4.32

21

4.62

21

4.00

21

3.83

21

3.88

21

4.08

21

geophysical journal international

4

ols

0.8706

0.9736

0.3289

talwar

0.8454

0.9667

0.7814

welsch

0.8465

0.9671

0.6816

andrews

0.8648

0.9725

0.7119

bisquare

0.8642

0.9725

0.7081

cauchy

0.8743

0.9759

0.7144

fair

0.8825

0.9776

0.7611

huber

0.8799

0.9770

0.7374

logistic

0.8803

0.9771

0.7414

ols

0.8827

0.9791

0.1963

talwar

0.8670

0.9729

0.6999

welsch

0.8660

0.9732

0.7091

3.31

21

3.58

21

4.22

21

4.68

25

4.65

25

4.44

25

4.14

25

4.26

25

4.25

25

3.54

25

4.06

25

4.54

25

table 2: accuracy, computation time and size of the gaussian process learning surrogates

basis

quadratic

linear

time in

compact model

kernel

rsi

ridx

rrecon

hour

size (in mb)

squared exponential

0.9575

0.9911

0.7851

1.96

5.017

matern 3/2

0.9728

0.9939

0.8729

2.53

5.024

matern 5/2

0.9647

0.9925

0.7136

2.42

5.023

ard squared exponential

0.9631

0.9945

0.9377

29.68

5.032

ard matern 3/2

0.9707

0.9963

0.9377

9.29

5.031

ard matern 5/2

0.9668

0.9956

0.9427

9.79

5.03

squared exponential

0.9522

0.9908

0.8125

2.04

5.022

matern 3/2

0.9535

0.9938

0.8434

3.25

5.02

matern 5/2

0.9624

0.9922

0.6873

3.16

5.02

32

geophysical journal international

constant

ard squared exponential

0.9625

0.9943

0.9186

4.65

5.032

ard matern 3/2

0.9696

0.9962

0.9467

8.12

5.027

ard matern 5/2

0.9664

0.9954

0.9431

9.64

5.027

squared exponential

0.9453

0.9902

0.8617

1.78

5.036

matern 3/2

0.9692

0.9916

0.8312

4.60

5.036

matern 5/2

0.9587

0.9919

0.6559

3.29

5.051

ard squared exponential

0.9621

0.9937

0.8812

36.19

4.995

ard matern 3/2

0.9703

0.9957

0.9428

7.08

4.941

ard matern 5/2

0.9409

0.9857

0.2389

1.53

4.991

here the central receiver (r-12 in figure 3) is considered to be seated at a fixed location (nx/2,
ny/2) at z = 244th grid point, whereas the sources can roam around anywhere in the rock volume
underneath. we first aim to predict the seismic traces at the central receiver using a regression metamodel, fitted using the 2000-unit amplitude microseismic events at different lh sample locations as
shown in figure 2. under an exhaustive search for the best polynomial kernel combining the right
polynomial order in the 4 different predictors, it is revealed from table 1 that using the random source
positions, a 3rd order polynomial with andrews robustness criterion yields the best prediction accuracy
upon reconstruction, with a 2d correlation coefficient of rsi = 0.8451, ridx = 0.9665 and rrecon = 0.7822
with respect to the original nsourceÃ—nt = 2000Ã—501 samples of seismic dataset. the other combinations
like 3rd and 4th order polynomials and different robustness criteria work fairly similarly, except the
ordinary least square as this is prone to outliers and non-normal datasets. the training time and the
storage of the robust regression coefficients are minimal amongst all the proxy meta-models, explored
in this section. other complex models can push the predictive accuracy to a higher value which are
explored next although they need more computational time for training. in this section, we show
comparison of different classes of regression models in terms of training accuracy (r2d) for the two
compressed parts having 100 dominant time instants, accuracy of the reconstructed seismograms,
training time and proxy storage size, utilizing the 2000 training samples, with 10-fold cross validation
to select hyper-parameters of different family of regression models. the best classes of models found

33

geophysical journal international
with this exploration as an initial screening has been further tested with 2000 independently held out
testing samples and reported in the following sections.

figure 9: ground truth vs. predicted variation in si and idx along depth (z-direction) using gaussian process surrogates.

figure 10: ground truth vs. predicted variation in si and idx as a function of distance d using gaussian process surrogates.

most variants of gaussian process surrogates with quadratic and linear basis are found to have a
high predictive accuracy particularly with ard covariance structures, although it is more
34

geophysical journal international
computationally expensive as compared in table 2. the squared exponential kernel produces inferior
results compared to the matern 3/2 and matern 5/2 versions with both ard and the basic kernels. the
best accuracy has been obtained using the ard matern 3/2 kernel using linear basis on the training
dataset in the initial screening. validation of the prediction performance on the 2000 testing datasets
and moreover on a sample by sample seismogram prediction using 1d pearson correlation coefficient,
instead of average 2d correlation coefficient has been shown in the subsequent sections, for the final
choice of best gp model structure.
the svm with polynomial and rbf kernel, glm and/or kernelized shrinkage regressions like
lasso, ridge, elastic nets and moderate size neural networks have produced a poor predictive
performance, particularly most of them fail to partition between the positive and negative pressures in
the scatter diagrams as a function of the predictors. the increased computation time in lasso and elastic
net is due to the cross-validation based automatic selection of hyper-parameter Î» by finding a balance
between penalizing prediction error and the model coefficients.
as described before, the decision tree method can produce high accuracy particularly with lesser
value of nparent thus producing large size of the tree and hence larger model size. in general, within the
ensemble methods, boosting trees produced better results than the bagging tree methods with the same
number of weak learners. with 1000 weak tree learners, the prediction accuracy reaches around r ï‚» 0.6
with the bagging method, while the sizes of the learned models become greater than few gbs and hence
not investigated further with higher number of ensemble learners. in each case of the tabulated results
using various family of regression models which can be found the supplementary material, the best
predictive accuracy and the associated tuned parameters has been highlighted as bold italics entries for
reconstructed accuracy rrecon.
from these exhaustive comparisons, it is found that the gp with linear basis function and ard
matern 3/2 kernel outperform all the rest of the model families to predict an accurate seismogram on
the central receiver and hence has been chosen for further analysis. however, although the accuracy of
ard family of kernels produce best results, they can be computationally expensive during the training
process and also depending on the number of data-points in the training set for the covariance
estimation. apart from the gp models, the decision tree with lower number of parents and in the family
35

geophysical journal international
of ensemble learning methods, bagging with higher number of weak learners also perform well but
these models have a larger storage requirement. therefore, as a compromise between the achievable
accuracy, training time, and storage requirement, the gp model with ard matern kernel is found to be
the best choice for this regression problem in seismology. however, it is important to note that the
regression models map each location in two compressed domains which are indeed smooth, as can be
seen from figure 9-figure 10. however, using the predicted samples in the compressed domain each
seismogram is reconstructed using the decompression steps that may not finally make the whole event
co-ordinate to seismic trace mapping to be smooth enough. a slight decrease in the index term may not
also yield smooth reconstructed seismograms which makes the final reconstructed correlation
coefficient (rrecon) to have a lower value for most of the learning algorithms compared to the gaussian
process regression models.
also, from the results with increasing number of layers and number of hidden nodes in both the
feedforward and cascaded neural networks, the compressed domain accuracies are found to be fair
(provided in the supplementary material). however, they take more time during the training process.
training of very large networks (>100 nodes in 3 layers) has not been attempted since they need more
time during the training process and since the alternative models are already giving better accuracy
within similar training time-frame. also, according to the â€œno free lunchâ€ theorem, for different
statistical learning scenario, there is no consensus that one single class of models whether neural
network or gaussian process would consistently outperform other family of learners and the best
recommendations are to try a pool of models amongst which a class of models wins for a specific
application. this fact is even more prominent in the context of difficult regression problems, as
discussed in (lattimore & hutter 2013; wolpert 2002; goutte 1997; domingos 2012), to achieve high
enough accuracy compared to the well-researched classification problems where nns are shown to
outperform in contemporary research.
previously in figure 7, the variation of the two compressed components of the original seismic
signal â€“ si and idx have been shown. here, the predictions of the machine learning algorithms have also
been shown on these two components as a function of 4 covariates {x, y, z, d} in figure 9 and figure
36

geophysical journal international
10 respectively showing variation with depth (z) and distance (d) and the two lateral directions in the
supplementary material. the ground truth of compressed domain data is presented as the circles and the
corresponding predictions are shown as square boxes. it is evident that the gaussian process metamodel is capable of learning the split predictions for the positive and negative pressure values and the
corresponding complicated shape for idx as found with respect to {z, d} in figure 9 and figure 10
respectively.

(a)

37

geophysical journal international
(b)
figure 11: (a) original, compressed and predicted reconstructed seismograms using gaussian process surrogates. (b)
zoomed seismic traces with predicted samples. the amplitude is in pascal and 500 samples represent 2 sec of seismic data.

it is understandable from the schematic diagram in figure 8 that the regression models are trained
to predict only the two components in the compressed domain and the respective accuracies have been
reported as rsi and ridx in the tables. the predictions in compressed domain are then used to decompress
and obtain the sparse predicted seismograms as shown in figure 11(b), corresponding to the
reconstructed accuracies rrecon in the tables. figure 11(a) compares the original (elastic pde
simulations), compressed and predicted compressed seismograms which show minimal loss of
information with the compressed representation (r = 0.999) and predicted compressed (r = 0.947) data.
in particular, the arrival times of the seismograms are accurately predicted which carry most of the
useful information in a source location inversion process (tarantola 2005), as also evident from the 10
representative samples from the 2000 data-points along with a zoomed version of three seismograms in
figure 11(b). the predictions of the surrogate regression models are most commonly visualized in the
form of cross plots as the deviation around the optimal least square line which are shown in the two
compressed as well as the reconstructed domains in figure 12, along with the achieved predictive
accuracies mentioned in the titles of the subplots.

figure 12: cross-plots between the ground truth and the best gp predictions in compressed and reconstructed domains.

38

geophysical journal international

figure 13: 2-d visualization of ground truth and the gp predictions in compressed and reconstructed domains.

the predictions can also be visualized in the form of a 2d image where the rows indicate different
trace numbers, sorted against the distance from the central receiver and the columns denote the
dominant time instant in the compressed domain and time samples in the reconstructed seismograms as
shown in figure 13, using the best found regression model i.e. gaussian process with linear basis and
ard matern 3/2 kernel. on the reconstructed image of the seismograms in the last column of figure
13, the power law behaviour in the dominant amplitude as a function of increasing distance from the
central receiver is evident. the first column of figure 13 clearly shows an oscillatory i.e. first increase
in pressure, followed by a pressure drop. the corresponding dominant time instants in figure 13 (also
incorporating the first arrival time) has more complexity due the heterogeneity, complex ray paths and
p/s-wave mode conversion of the seismic waves.
5.2. learning curve analysis and computation speed up using the gp regression models
the best found trained model structures reported in the previous subsection are now evaluated for
their predictive accuracy on both the 2000 training and 2000 testing lh samples shown in figure 2 with
a random choice of the subset of samples and multiple shuffles, commonly known as the learning curve
analysis. the learning curve analysis shows the accuracy vs. robustness trade-off for a trained model
39

geophysical journal international
and helps in selecting the minimum number of training samples required to get a fair predictive
accuracy. here the number of samples are gradually increased as shown in figure 14 while the average
prediction accuracy of 100 monte carlo shuffles are carried out to select a subset of samples from each
of the 2000 training and 2000 testing lh samples. both the training and testing datasets seem to
converge after 1000 samples. a smaller gap between training and testing dataset is reflected in the 2d
correlation coefficient of the data and indicates an improved performance over the other methods.
figure 14 also shows that the ard matern 3/2 kernel produces slightly better accuracy with the linear
basis over the quadratic basis on both training and testing dataset. however, the results seem to converge
closely using the ard matern 5/2 kernel. depending on the heterogeneity of the velocity model, the
learning curves on the training and testing datasets may vary, in other studies.

figure 14: learning curves of the gp models using the training and testing data using 100 monte carlo shuffles of the datasets.

in figure 15, we show the run time distributions of the 2000 synthetic seismogram simulations using
various trained gp proxy meta-models. it is evident from figure 15 that the time required to generate a
single seismogram is less than a sec using the surrogate model as compared to the gpu based full elastic
wave equation solving, as shown in earlier sections. however, there is an intermediate computationally
expensive step to train the surrogate meta-models as shown in table 2, which gradually increases with

40

geophysical journal international
the number of samples for gaussian process regression and particularly with ard family of kernels,
although they provide more accurate results than other methods. this has been investigated in the next
subsection.

figure 15: histogram of a single seismogram generation time at the central receiver using the trained surrogate model and
4000 data-points.

5.3. effect of training sample size on the regression modelling

41

geophysical journal international
figure 16: training time requirements and predictive accuracy with increased training samples size using ard matern 3/2
kernel with quadratic and linear basis function. saving in training is clear using 12 parallel cores (over 4 and 8 parallel cores)
and with linear basis function (over the quadratic one). predictive accuracies are comparable between linear and quadratic
basis with increased sample size.

here we explore the training time and predictive performances with increasing training sample size
for the best gp models which are found to outperform the other family of regression models. the gp
ard matern 3/2 model has been trained in parallel using 4, 8 and 12 core cpu via the parallel for
(parfor) loops in matlab on the 100 dominant time instants to learn the compressed seismograms. in
order to show the scalability of the training process, the number of samples have been increased from
20 to 500 in steps of 20 samples and the required training times along with the corresponding predictive
accuracies have also been shown in figure 16, as a function of the sample number (nsample). it is evident
that there is a steep increase in training time for over nsample>300 and even more with the quadratic basis
in the gp ard matern 3/2 kernel while the predictive accuracy fluctuates around 2d r2 = 0.93-0.95.
also the small fluctuations on the training accuracy can be observed in figure 16, as the random samples
come from different positions of the heterogeneous velocity model and thus introducing certain set of
samples may slightly reduce the overall accuracy but varies within a small range and finally settles
down. this is more evident in a finer resolution in the learning curve analysis in figure 14 on the trained
model using 2000 training/testing data with 100 monte carlo shuffles of increasing subset of samples,
as presented in the previous subsection.
5.4. prediction enhancement by using smoothing filter
since the gp models predict the dominant 100 time-instants and the corresponding signal values
independently, as a function of event spatial locations, without explicitly considering the temporal
correlation of the seismogram time series, sometimes the predicted signals may not be smooth in time.
especially in some cases, rapid positive and negative pressure fluctuation may be encountered i.e. with
reverse polarity with a small movement of the event locations as shown in figure 9 and figure 10.
therefore, a moving average (ma) smoothing filter is applied on the gp predicted seismic data while
varying its span size from 1-10 in order to select the best filter settings for ensuring the smoothness of
42

geophysical journal international
the seismogram time series. since the dominant signal values are predicted separately in the learning
process, without considering the temporal information between two consecutive time samples, the
predicted seismic signals may not vary smoothly in few cases. here, the purpose of the smoothing filter
is thus to introduce some amount of inertia against rapid fluctuation of the signals against changing
polarity within a short span of time. it is apparent that a larger span of the smoothing filter introduces a
delay in the seismograms and hence the performance degrades gradually, as evident from figure 17
showing a sweep over ma smoothing filter window size from 1-10 consecutive time samples. in the
smoothed versions of the seismograms, both the ard matern 3/2 and ard matern 5/2 kernels with
either linear or quadratic basis win over the other combinations, particularly on the test-set. figure 17
also suggests that these gp settings with a ma smoothing filter of span size of 3 samples are capable
of producing accurate predictions both in the training and testing set with a 2d r>0.91. representative
examples of the predicted reconstructed and smoothed seismograms are shown in figure 18 and figure
19 from the training and testing set respectively.

figure 17: performance of smoothed seismogram predictions using gp quadratic basis and six kernels. both ard matern
3/2 and 5/2 kernels for both linear and quadratic basis give good prediction performance on both the training and testing set.

43

geophysical journal international

figure 18: original simulated, gp predicted and smoothed reconstructed seismograms in the training dataset.

figure 19: original simulated, gp predicted and smoothed reconstructed seismograms in the testing dataset.

it is observed from figure 17 that the curves attain their maxima at ma filter span = 3 samples for
all the cases and also, the ard matern 3/2 and ard matern 5/2 kernels with both linear and quadratic
basis functions give comparable average accuracy on the training and testing dataset, in terms of 2d
correlation coefficient. individual accuracies of each seismogram may be different using these two
44

geophysical journal international
kernels. hence the 1d pearson correlation coefficient based on the individual smoothed seismograms
have also been calculated after the initial model screening, on both the training and testing dataset using
the ard matern 3/2 and 5/2 kernels with both the linear and quadratic basis functions. although the
overall predictive accuracy (in terms of 2d correlation) on the training and testing dataset are similar
for the ard matern 3/2 and 5/2 kernels with both linear and quadratic basis, as shown in figure 14,
the number of relatively poor predictions or outliers present in the predictions are actually different.
therefore, from the first stage screening from the pool of machine learning algorithms using 2d
correlation coefficient on all time instants and samples, following the schematic in figure 8, we carry
out a further second stage selection of the best algorithm that yields minimum number of outliers in its
predictions. we define a predicted data-point as outlier if the 1d pearson correlation coefficient between
a particular simulated and the corresponding predicted smoothed seismogram becomes negative i.e.
r1d<0. the goal here is to minimise such extreme predictions, although most of the predicted
seismograms show fairly high accuracy.

figure 20: histograms of 1d correlation coefficients between ground-truth vs. smoothed predicted seismograms for the
central receiver using the training and testing dataset. the percentage outliers in different models are mentioned in the titles.

45

geophysical journal international

figure 20 shows that in all the cases for both training and testing dataset, most of the samples give a
good predictive accuracy, as revealed from the peaks near r â‰ˆ 1. in all the cases there is a small decaying
left tail, indicating a drop in the predictive accuracy. counting these outliers as a fraction of the total
sample size below a fixed threshold r1d<0 can identify the best surrogate meta-model structure with
minimum number of outliers. it is apparent from figure 20 that the ard matern 3/2 kernel with both
linear and quadratic basis functions produce the same lowest number of outliers (0.7%), on the training
dataset. the same kernel with linear basis produces 0.05% less outliers having negative correlation,
over that with the quadratic basis and hence chosen in remainder of the paper for further analysis. the
presence of few predicted outliers can also be viewed from the cross-plots in figure 21, after applying
the tuned smoothing filter on the predicted seismograms. in general the ard matern 3/2 kernel with
linear basis gives a trade-off between high average predictive accuracy (as revealed from the cross-plots
on training/testing dataset in figure 21) and minimum number of outliers (represented by the left tail of
the histograms in figure 20).

figure 21: cross-plots of the training and testing dataset with and without smoothing using the gp quadratic basis matern
3/2 kernel. data is reshaped in 1d array to calculate the optimum least square line and correlation coefficient r.

6. prediction performance on all the 23 receivers
6.1. selection of the best regression model for all the 23 receivers

46

geophysical journal international

figure 22: histograms of 1d correlation coefficients of each predicted seismograms for all the 23 receivers using the training
and testing dataset. the percentage outliers in different models are mentioned in the titles of subplots.

the exploration reported in the previous section shows fine-tuning of the proxy models on the
central receiver when the event can roam around within the subsurface volume underneath. this
mapping for the event location to receiverâ€™s response is not necessarily similar for different receivers
(in figure 3), due to the heterogeneity of the velocity model. we now verify the performance of the
proxy or surrogate meta-model on all 23 receivers using the best set of models that produced good
predictive accuracy on the central receiver i.e. gaussian process regression with linear and quadratic
basis having ard matern 3/2 and ard matern 5/2 kernels. amongst these four class of models, the
smoothing filter-tuning and outlier detection has been carried out in the same way for the multiple
receiversâ€™ case, as shown in the earlier sections. the other choices of smoothing filter window size apart
from 3 samples are found to be worse, as also shown before for the central receiver. here, individual
receiver responses are analysed separately instead of comparing aggregated predictions on the 23
receivers. with the ma smoothing filter having a window size of 3 samples, the 1d pearsonâ€™s
correlation coefficient (r1d) between the predicted vs. original seismogram on training and testing data
for all 23 receivers are shown in figure 22, using the best four gp proxy meta-models. it is also evident
47

geophysical journal international
from figure 22 that although the ard matern 3/2 kernel with quadratic basis in gp produces 0.51%
fewer outliers with r1d<0 on the training dataset, on the testing dataset the same kernel with linear basis
produces 0.29% fewer outliers and is hence chosen for the rest of the analysis and the likelihood
calculation.

figure 23: box-plots of the receiver-wise correlation coefficient between the original vs. predicted seismograms on the
training and testing dataset using ard matern 3/2 kernel with linear (top panel) and quadratic basis (bottom panel). the red
crosses indicate outliers in the prediction on individual receivers.

48

geophysical journal international
figure 24: predicted seismogram wiggle plots using the ard matern 3/2 kernel on the training dataset (top) ground truth,
(middle) with linear basis, (bottom) with quadratic basis. corresponding r2d on 23 receivers are reported in the subplot titles.

receiver-wise prediction performances are shown in figure 23 on the 2000 training and 2000
testing datasets using the top 2 surrogate meta-models using ard matern 3/2 kernel with linear and
quadratic basis functions. it is apparent from figure 23 that the central receiver (r-12) produces the
best prediction accuracy amongst all the 23 receivers. also, amongst these 23 receivers in figure 3, r5, r-8, r-15, r-19 are found to have the next best responses whereas r-3, r-10, r-17, r-21 contain
relatively more outliers. the presence of outliers in certain channels does not necessarily represent
unusable predictions, as the 1d correlation coefficient r1d essentially compares the full morphology of
the spiky seismograms. in most cases, the arrival times and the polarity of first arrival of the seismic
waves are predicted accurately, containing most of the useful information (tarantola 2005). we have
also shown 5 representative examples of true vs. predicted seismograms on all the 23 receivers from
both the training and testing datasets in figure 24 and figure 25 respectively using the ard matern
3/2 kernel with linear/quadratic basis (in the two bottom rows), where the arrival times and morphology
of the seismic response using the proxy meta-models are identified almost accurately with the original
solutions of the expensive elastic pde solver (represented in the top row).

figure 25: predicted seismogram wiggle plots using the ard matern 3/2 kernel on the testing dataset (top) ground truth,
(middle) with linear basis, (bottom) with quadratic basis. corresponding r2d on 23 receivers are reported in the subplot titles.

49

geophysical journal international
6.2. comparison of the computation time for surrogate proxy meta-models vs. solving the full
elastic wave equation
in this subsection, we compare the run time saving due to the use of proxy or surrogate meta-models
instead of the full elastic pde solver for obtaining the seismic response at the 23 receivers at specified
locations due to explosive microseismic events at random locations in the subsurface. as discussed in
the introduction section, the purpose of surrogate meta-modelling is to reduce the computational time
for fast generation of approximate template seismic events and hence facilitate a likelihood-based
inversion approach where such fast noiseless template data generation is required in batches of
thousands of speculative event locations.
figure 26 shows that using the gp linear basis with both ard matern 3/2 and 5/2 kernel, the peak
of the run time distribution is around 2 sec and for the quadratic kernels the peak run time is around 2.8
sec on a standard 4-core 64-bit windows desktop pc with 16 gb memory and intel i5, 3.3 ghz
processor. whereas for a single shot seismic simulation, the original elastic wave propagation on a 12core linux pc with k20 gpu card with 5.5 gb memory and 1.1 ghz processor, the peak run time is
1063 sec â‰ˆ 17.7 min. therefore, to simultaneously compute the seismic response at the 23 receivers,
the surrogate regression meta-models produce a 531-fold acceleration using the linear kernel and 380fold acceleration using the quadratic kernel. this speed up for the forward simulation when called from
the likelihood function comes at the cost of initial simulation for training data generation and required
training efforts of gp regression meta-models, but this is needed only once for a fixed velocity model.
there is also a small inaccuracy incurred due to the compressed domain regression modelling in
comparison with the original elastic wave simulation. in many real microseismic monitoring
applications such approximate templates are sufficient for probabilistic event parameter estimation
problems, since the measurements are often buried under significant amounts of noise, thus making the
effect of such small modelling uncertainties due to the proxy negligible.

50

geophysical journal international

figure 26: comparison of run time distributions between elastic wave propagation on k20 gpu card and surrogate proxy
meta-model predictions on cpu for all the 23 receivers.

7. fast computation of the likelihood function using the trained surrogate meta-models
7.1. formulation of the likelihood function for detecting microseismic events
in this section, we use the best surrogate meta-model i.e. the gp with linear basis and ard matern
3/2 kernel, followed by a ma smoothing filter with a span-size of 3 samples, for fast computation of
template seismic response in the likelihood computation. in many geophysical inverse problems, fast
calculation of the likelihood is necessary in order to get the maximum likelihood (ml) or maximum aposteriori (map) estimates or calculation of the evidence to enable model comparison. representing
the true noiseless template seismic response due to a microseismic event as yï‚µ and a measured noisy
response as y , the gaussian likelihood function can be calculated as (17):

lï€½

ï€¨

ï€© ï€¨

ï€©

t
ïƒ© 1
ïƒ¹
exp ïƒªï€­ y ï€­ yï‚µ cï€­1 y ï€­ yï‚µ ïƒº .
n
ïƒ»
ï€¨ 2ï° ï€© c ïƒ« 2

1

(17)

here, c is the covariance matrix of the noise on the measured data y, and n is the number of observed
data points. often the likelihood is represented in log-scale for convenience in bayesian analysis and is
given by (18) considering either a full or only diagonal covariance matrix:

51

geophysical journal international

ï€¨

ï€©

ï€¨

ï€©

t
log c 1
n
log ï€¨ 2ï° ï€© ï€­
ï€­ y ï€­ yï‚µ c ï€­1 y ï€­ yï‚µ ,
2
2
2
2
ïƒ¬ n log ï€¨ï³ ï€©
for diagonal covariance .
ïƒ¯
log c ï€½ ïƒ­
cholesky
ï€© for full covariance
ïƒ¯ 2 ï‚´ ïƒ¥ log diag ï€¨ ci
i
ïƒ®

log l ï€½ ï€­

ï€¨

(18)

ï€©

in the log-likelihood calculation involving the full covariance matrix, the cholesky decomposition is
commonly used for numerical stability and increased speed, whereas for diagonal covariance the log
determinant of covariance (log|c|) can be easily computed using the common variance (Ïƒ2) as in (18).
for calculating the log-likelihood in (18), given some speculative microseismic event locations {x,
y, z} the noiseless predicted seismic data can be obtained using the trained proxy meta-model in (19)
following the steps shown in the schematic diagram figure 8:

y ï€½ fproxy ï€¨ x ï€© ï€½ fproxy ï€¨ x, y, z, d ï€© .

(19)

the covariance matrix c required in the likelihood (17) can be calculated from the measured noisy data
(y) using (20), considering a diagonal covariance or uncorrelated noise for the sake of simplicity:

ï€¨

ï€©ï€¨

ï€©

t
c ï€½ ï…ïƒ© y ï€­y
y ï€­ y ïƒ¹ , y ï€½ ï… ï›y ï ,
ïƒ«ïƒª
ïƒ»ïƒº
y ï€½ yï‚µ ï€« ï ï€¨ 0, c ï€© , c ï€½ ï³ 2 i .

(20)

here, Ïƒ2 is the common variance of the data, reshaped as 1d vector y in the multi-receiver case, with
the assumption of no correlation amongst them and y represent the mean of the measured data, while

ï… being the mathematical expectation operator. here in (20), the noise has been considered to have a
gaussian distribution with zero mean and a specified variance Ïƒ2, however any expert choice of the
noise covariance can also be incorporated in the likelihood function (17).
next we calculate and visualize the likelihood as an inverse problem for the microseismic event
locations (tarantola 2005)(aster et al. 2011), in different cases viz. using only the central receiverâ€™s
data, seismograms along the principal and anti-diagonals, in the upper/lower triangular parts or using
all the 23 receivers. for computation of the likelihood, template seismic responses corresponding to
single microseismic events at random positions are calculated first using the fast proxy/surrogate metamodels and independent white gaussian noise (wgn) of two different standard deviations Ïƒ = {100,
52

geophysical journal international
250} which are added on the noiseless seismic data to generate some realistic corrupted dataset. the
noise free data is assumed to be generated due to a microseismic event at the grid point (31, 25, 158),
as a representative example for the log-likelihood calculation. the signal to noise ratio (snr) has been
calculated on the single/multiple receivers using the ratio of average energy calculated through the sum
of squared signal amplitudes and represented in the decibel scale as in (21):

ïƒ¦
ïƒ¶
2
2
snr ï€½10log10 ïƒ§ ïƒ¥asignal
,i ïƒ¥anoise,i ïƒ· db .
i
ïƒ¨ i
ïƒ¸

(21)

it is understandable that in a relatively less noisy or high signal to noise ratio (snr) case, the
likelihood function will be manifested as a narrow delta function in the event parameter space which
may be harder to detect. in the case of higher noise or in other words low snr levels, the likelihood
function gets softened which may help navigating towards the maximum likelihood regions by standard
optimization or sampling algorithms like markov chain monte carlo (mcmc) etc. here we focus on
obtaining the ml estimates of the event parameters for a single microseismic source, with a specified
noise variance by gradually increasing the number of receivers.
the most likely event positions are visualized using the scatter diagrams for different receiver
combinations. for this purpose, here we use the top 90 percentile of all the log-likelihood values out of
the 4000 uniformly distributed lh samples. out of these 4000 samples one of them is the ground truth
voxel which is expected to have the highest likelihood value. in order to verify this, we calculate the
maximum likelihood estimate of the microseismic event location and the norm difference of the event
positions from the ground truth location (31, 25, 158), with two different snr levels using various
combinations of receiver positions which can be found in the supplementary material. in the next subsection, the joint distribution of the event parameters in the 2d scatter plots are shown using the top 90
percentile of the likelihood values where the higher likelihood values are represented by bubbles with
a darker shade. in all the cases the legends show the log-likelihood values.

53

geophysical journal international

(a)

(b)

(c)

(d)

figure 27: receiver subset selection for the likelihood calculation (a) principal diagonal, (b) anti-diagonal, (c) uppertriangular region, (d) lower triangular region.

7.2. different receiver subset selection and its effect on the likelihood function
here we explore 6 different cases of the receiver subset out of the 23, in order to calculate the
likelihood using the lh samples. the positions of the central receiver (r-12) and all the 23 receivers
have been shown in figure 3. out of the 23 receivers, we now select a subset of receivers along the
principal-diagonal (total 7) and anti-diagonals (total 5) as shown in the top row of figure 27 (a and b).
in order to show the effect of the heterogeneity in the velocity model of figure 1, here the 23 receivers
are divided in the upper and lower triangular parts as shown in the bottom row of figure 27 (c and d).
54

geophysical journal international
the corresponding noiseless and noisy seismograms in these different geometries are shown in figure
28. to better understand the spiky or narrow spatially localised nature of the likelihood function for
multiple receivers, the scatter diagrams of the randomly sampled likelihoods are provided in figure 29.
moreover, there are small errors incurred between the true pde simulated vs. the proxy predicted
seismic data in all the receivers which get combined within the likelihood calculation. the propagation
of these small modelling uncertainties due to the use of proxies or surrogates for different receivers on
the final parameter estimates in the inversion process may be explored more systematically in a future
work. as per the previous reports of geophysical inversion e.g. in (tarantola 2005), incorporating more
receiversâ€™ data should make the estimates more accurate and the non-vanishing high likely regions
should ideally shrink towards a smaller region within the volume under scanning which is also observed
here. however, addition of higher noise level decreases the snr and consequently softens the
likelihood in all the cases. the gross natures of the likelihood are not drastically altered for the same
receiver subsets but different noise levels.

figure 28: seismograms used for calculating the likelihoods and maximum likelihood estimate of event position (top) noiseless
(middle) wgn with Ïƒ = 100, (bottom) wgn with Ïƒ = 250.

it has been found that some of the regions have more high likelihood values where the data was
originally generated from whereas in certain cases some other regions have more non-vanishing
55

geophysical journal international
samples with high likelihood values. this is essentially a problem of resolution vs. smoothness tradeoff of the likelihood, whereas an accurate detection should locate towards the highest likelihood voxel
and its neighbouring regions. this may be an effect of the heterogeneity of the velocity model that
neighbouring samples not always yield a smooth variation of the likelihood values. alternatively, the
likelihood values could have been calculated using some derived features of the seismic traces like the
arrival times as shown in (tarantola 2005) or some other feature like the polarity of first arrival etc.
which may be pursued in a future research. the choice of the feature in such cases is crucial to yield a
smooth variation of the likelihood values in the neighbouring voxels whereas here we focus on the raw
seismic data based likelihood calculation only.
the non-vanishing most likely regions can also be summarized in terms of maximum likelihood
point estimates, by bulk likelihood calculation using the lh samples. a systematic exploration would
need a bayesian sampling of the posterior distribution using a chosen likelihood function involving the
raw seismic data itself or using some derived features (like arrival time or polarity) where the samples
will gather more towards the mode of the posterior probability distribution which may be pursued in a
future study. here we explore the maximum likelihood values for convenience, corresponding to the
bulk likelihood calculation at random locations. the detection error norm ( e ) for the event positions
has been calculated as the euclidean distance between the ground truth ( x o , y o , z o ) and estimated (

xï‚µo , ï‚µ
yo , zï‚µo ) locations via maximum likelihood using (22):
e ï€½

ï€¨ x ï€­ xï‚µ ï€© ï€« ï€¨ y ï€­ ï‚µy ï€© ï€« ï€¨ z ï€­ zï‚µ ï€© .
2

o

o

2

o

o

2

o

(22)

o

the noise levels, the corresponding maximum likelihood estimate based detected voxels and
detection error norm are reported in the supplementary material. it is evident that all the different cases
can essentially capable of identifying the ground truth voxel with highest likelihood value. depending
on the noise level and receiver arrangements, some other voxels may also spuriously show high
likelihood values which is explored next.

56

geophysical journal international

(a)

(b)
figure 29: scatter-diagram of top 10 percentile of likelihood values for two different noise levels and receiver combinations
(a) central, (b) all 23 receivers. similar plots with other receiver combinations are shown in the supplementary material.

as discussed before, the top 90 percentile of the likelihood values can also be visualised as scatter
diagrams between the event location parameters as shown in figure 29 for two different receiver
57

geophysical journal international
combinations. it is evident that using just the central receiver (figure 29a), although the ground truth
voxel is revealed in darker shade, there are other spurious voxels producing similar but slightly less
likelihood values. the number of these spurious voxels reduces in the case of 5 and 7 receivers along
the diagonals as shown in the supplementary material. the ground truth voxel becomes more prominent
when more receivers â€“ 15 (upper or lower triangular as in the supplementary material) or 23 (all of them
in figure 29b) are used in the inversion. use of more receivers shows the presence of many lower
likelihood values at a different depth instead of the ground truth which is expected to shrink with a
higher threshold on the likelihood values. in all the scatter diagrams in figure 29, the colour/shade of
the data points are proportional to its log-likelihood values shown in the legend.
the ml estimates of the event location parameters have been reported in the supplementary
material, using voxel by voxel batch likelihood evaluation with all the 4000 lh samples. here, the

yo , zï‚µo ) are obtained as the voxel returning the maximum logestimated location parameters ( xï‚µo , ï‚µ
likelihood value is found to be accurate in all the receiver combinations. a more systematic way could
be to maximize the likelihood function using an optimizer or using bayesian sampling methods with
accelerated likelihood calculation using the trained surrogate or proxy meta-model. the purpose of the
present work is to make the likelihood calculation faster and independent of the data under
consideration, as here the proxy directly predicts the raw observables i.e. template seismic patterns and
is different from the likelihood training approach in the bambi algorithm in (graff et al. 2013; graff
et al. 2012), that needs retraining the surrogate meta-model when the dataset and consequently the
nature of the likelihood changes.
a closer look at the scatter plots of the 2d joint distributions reveal that the high likelihood values
change rapidly with small variation in the event location, particularly with less number of receivers e.g.
only 1 receiver (figure 29a). incorporating more receivers reduces such variations as shown in the
supplementary material using 15 for the upper and lower triangular parts to all 23 receivers (figure
29b). even though in a binned histogram, it may show more number of non-vanishing higher likelihood
areas, the peak of the likelihood may lie in a different location i.e. the ground truth voxel for generating
the data.
58

geophysical journal international
it is also important to note that in all of the above likelihood scatter diagrams, many islanded regions
can be identified rendering such an event detection essentially a multi-modal inference problem. the
adopted lh samples drawn throughout the 3d volume of the velocity model smoothly interpolates the
true noiseless seismic responses in the forward problem but may be insufficient to accurately localise
the events using bulk-scale likelihood calculation at these prospective locations while using the noisy
seismic data in the likelihood calculation. hence the bayesian analysis techniques via mcmc or nested
sampling family of algorithms may be useful here with a suitable choice of likelihood function by
utilising the proposed method for fast data independent proxy meta-model to predict the observables,
in order to get the localised event posterior distributions along with calculation of the marginal
likelihood or evidence for comparing different models or carrying out hypothesis testing.
7.3. maximum likelihood estimate using various receiver geometries in the event detection
in the previous sub-section, the top 90 percentile likelihood values have been shown in the scatter
diagrams where some lower likelihood samples are gathered at a different depth compared to the ground
truth. now in this subsection, we show the maximum likelihood based most likely event location
estimation, using a much higher threshold of top 99 percentile of all the likelihood values amongst the
4000 lh samples. this helps in graphically understanding the accurate localisation of the microseismic
source using increasing number of receivers and different snr levels, corresponding to the location
estimates for the various receiver geometries. this also allows traditional â€˜dots in the boxâ€™ type
visualization of the most-likely microseismic event locations (kendall et al. 2011; eisner et al. 2010),
using the 6 different receiver sub-sets as explored in the earlier subsection.
here, figure 30 shows the event locations using the top 99 percentile of highest log-likelihood
values using a noise standard deviation of Ïƒ = 250, whereas the less noisy case with Ïƒ = 100 is shown
in the supplementary material. both figure 30 and its less noisy version in the supplementary material
(Ïƒ = 100) compares the top 99 percentile of likelihood values between the true likelihood (from the
elastic wave propagator) vs. the gp regression surrogate meta-model or proxy generated likelihoods. a
closer look at these figures will reveal that the introduction of the external noise manifests many
possible event locations with high likelihood values, particularly when the inversion is attempted using
59

geophysical journal international
just the central receiver or multiple receivers across the principal diagonal (7 receivers) and antidiagonals (5 receivers). with increasing number of receivers, the variation in the likelihood values for
other possible locations gradually decreases and the true event location becomes quite prominent, as
manifested in the form of a larger and darker bubble in the last 3 columns of figure 30, using 15 and
23 receivers respectively. it is also evident that with more number of receivers, the true vs. proxy
generated likelihood peaks are located at the same position, compared to that using less number of
receivers. this shows employing 15 or 23 receivers, the maximum likelihood detection is invariant
between the choice of expensive true likelihood vs. the cheap gp proxy-based likelihood.
during the proxy training it might seem that the near surface shallow sources introduce a bias due
to their higher amplitude compared to the deep sources. in order to show that the proxy-based
likelihoods are indeed unbiased, we have compared the true likelihood values vs. the proxy-based
likelihood values and their difference in figure 31. in the joint plane of depth vs. distance, the likelihood
differences are found to be low compared to the original likelihood values and are almost uniform with
variation in depth or distance from the central receiver. this indicates the efficacy of the proxy in
generating fast likelihood values close to the original ones. also, the likelihood surface is not smooth
owing to the fact that the medium is heterogeneous, and the seismograms containing complex structures
of both the p-wave and s-waves. the likelihood values from the full-physics simulation at the sampled
4000 locations vs. the likelihood obtained from the surrogate meta-model are compared in figure 31,
along with their differences. our simulations show that there is very small difference between these two
cases, due to the fact that the surrogate meta-model has learnt the data generation mechanism by the
elastic wave propagator rather than the likelihood surface itself. also, introduction of the surrogate does
not increase the complexity of the likelihood structure and indeed retains its shape intact.

60

geophysical journal international

(a)

(b)
figure 30: most likely event locations using various receiver arrangement with added noise std Ïƒ = 250. log-likelihood values
are shown in the color-bars and the size of the data-points are proportional to the likelihood values: (a) true likelihood, (b)
proxy-based likelihood.

61

geophysical journal international

figure 31: comparison of the true likelihood vs. the proxy-based likelihood and their difference for noise std Ïƒ = 250. the
legends represent the likelihood values or the difference.

8. discussions
it is important to note here that the likelihood calculation has become relatively cheaper using the
trained proxy meta-model, compared to the full elastic wave propagation solution, and therefore bulk
calculation of voxel by voxel likelihood or a subset of lh or uniformly sampled voxel values may also
help identifying the high likelihood regions for possible microseismic events. on larger velocity models
or in higher dimensions, this approach of bulk likelihood calculation may be wasteful. because using a
suitable sampler may easily identify the highly likely event locations within fewer likelihood calls.
using the raw seismic data makes a relatively wilder variation of the likelihood values between
neighbouring voxels for the microseismic source. several derived features like arrival times, polarity of
first arrival etc. can be used to calculate the likelihood instead as shown in (tarantola 2005), which may
produce a smoother likelihood function in the form of almost concentric circles (in 2d) or spheres (in
3d), for a single event. the trained proxy model can easily be used to derive any complex features out
of the raw predicted signals and use them in the likelihood function which may be explored in a future
research.

62

geophysical journal international
moreover, in our likelihood formulation the ground truth signal (y) has been generated from the
elastic wave equation solver and then corrupted with specified noise level (Ïƒ) whereas the template
seismic data ( yï‚µ ) has been generated from the surrogate or proxy meta-model. therefore, the likelihood
contains the misfit due to both the measurement noise on the receivers as well as the inaccuracy due to
the approximate seismic wave modelling with the proxy. we have shown through the above simulation
results that even with both these two components of possible inaccuracy, a single microseismic event
with known ground truth position can be reliably identified as the maximum likelihood point amongst
4000 randomly drawn source positions where the bulk likelihood calculation at possible source
locations can be made extremely fast compared to the full elastic wave solution. it is rational that the
variance of this likelihood analysis may be increased with the use of the surrogate/proxy model for fast
template data generation apart from the specified measurement noise that goes in to the likelihood
function. but the mode or the maximum likelihood point is unaffected by such an approximation due
to the accurate surrogate model which is shown by the zero detection-error for the test cases using all
the receiver combinations. a more elegant and accurate but massively computationally expensive
solution is to calculate both the ground truth data (y) and template data ( yï‚µ ) in each likelihood
evaluation by directly using the elastic pde solution which is explored here in brief and as a proof of
concept, on the sampled 4000 prospective locations which were used for training/testing of the surrogate
meta-model.
in previous literature on microseismic monitoring, there are abundant use of physically simplified
models instead of full-physics simulation with velocity model heterogeneity and elastic wave
propagation. this is one of the viable solutions to reduce the computational cost compared to the proxy
or surrogate meta-model based approach for fast likelihood calculation. however, for microseismic
simulation, the mode conversion between p-wave and s-wave are predominant at the layer edges, even
for explosive sources as described in this paper. therefore, approximate methods like ray-tracing etc.
that depends on separately calculating the p-wave and s-wave responses and then superimposing them
may miss these aspects of the geophysical modelling. rather we here took an alternative approach using
the full elastic solution of the wave equation and then using the surrogate regression meta-models. here,
63

geophysical journal international
the proxy meta-models are trained to produce close approximations of the full elastic solution which is
preferable than solving a reduced physics models for fast likelihood calculation.
this paper develops a methodology for learning wave propagation through heterogeneous medium.
given sufficient samples in the training process and from the convergence characteristics of the learning
curve in figure 14, it is apparent that the surrogate meta-model captures a close enough approximation
of the true seismic wave, obtained through the numerical solution of elastic wave propagation. the
velocity model we consider here is heterogenous compared to the layered ones and represented by
voxelized grids with different values of density and p/s-wave velocity in each voxel. however, the
vertical variations of the rock properties are larger than that in the lateral direction in our model. the
heterogeneity along different directions of similar models can be seen in (das et al. 2017). it is also
worth noting that the surrogate meta-model is trained on a fixed velocity model. for other complex
models, the same machine learning framework can be applied in principle but needs retraining using
thousands of independently simulated seismograms which are dependent on the structure of the velocity
model. in a more heterogenous case, the training process is likely to take more samples for the
convergence of the learning curve, as shown in figure 14. however, this paper aims at first developing
the generic methodology and testing on different velocity models may be addressed in a future research.

9. conclusion
starting from a heterogeneous velocity model, we propose a technique to teach machine learning
based surrogate regression meta-models to approximate elastic wave propagation solutions due to
microseismic events which is computationally expensive even using state of the art gpu computing
facilities. this allows calculation of thousands of batch evaluations of proxy-generated approximate
template seismic responses with reduced physics modelling for rapid calculation of likelihood
functions, for comparing with noisy dataset in a microseismic source inversion algorithm. the paper
first develops a robust time domain compression method to reduce the number of observables in a sparse
pressure wave-field generated by unit amplitude seismic events using a fixed heterogeneous velocity
model. then it compares 9 different families of surrogate regression models along with the details of
64

geophysical journal international
their parameter tuning to obtain sufficient predictive accuracy on the learned seismogram patterns on
multiple receivers. the machine learning algorithms essentially learn the mapping between the
compressed domain sparse and spiky time series of the seismic waves as a function of event location
parameters which can be decompressed next to get the full seismic waves with great saving of the
computational cost compared to solving the full elastic pdes with new event locations.
this paper also determines the achievable accuracy vs. the training time and storage requirement
trade-offs using different flexible regression structures for synthetic template seismogram generation.
the best results are achieved using the gaussian process regression by fine tuning of the kernels and
basis-functions, as it naturally incorporates a bayesian regression framework instead of yielding only
point estimates and hence provide superior performance as a smooth interpolator. seismic data
generation on 23 receivers using this proxy meta-model are found to be â“530 times faster than the gpu
simulations for full elastic wave equation, at the cost of negligible reduction in quality of the signals,
as revealed by the correlation analysis of the ground truth vs. predicted seismograms. however, the gp
proxy meta-models in spite of its high predictive accuracy on smooth regression problems need more
computational effort with growing sample size and number of receivers during the training period. for
fast likelihood calculation, it is not intended to online train the proxy models but to train it only once as
an offline process on a multi-core cpu, assuming the fact that in the real-fields the velocity model do
not change over shorter span of time. however, with uncertain velocity model the seismic patterns,
especially the arrival times, may be different, thus leading to inaccurate likelihood values which needs
further investigation in future. a combined approach of incorporating seismic measurement noise and
velocity model uncertainty together in the proxy models and hence in the likelihood function may also
be investigated in future.
as discussed before, a similar proxy based fast multi-modal bayesian inference technique has been
previously proposed in the bambi algorithm (graff et al. 2013; graff et al. 2012) by directly learning
complex likelihood functions which changes and need retraining for inference on different datasets.
however, the present paper extends this concept by learning the raw observables instead i.e. the multireceiver seismograms which does not need to be retrained if the data and consequently the likelihood
65

geophysical journal international
values had changed. in addition, mapping of the useful information in sparse observables buried under
few millions of data-points in the output i.e. the multi-receiver spiky time-series needed a robust
compression method which this paper develops first, to frame it as a non-sparse regression problem.
here we also show the predictive accuracy vs. training time and storage requirements using 9 different
family of regression models out of which gaussian process families with ard kernels outperform the
rest. future works may include extending the methodology for unknown number of microseismic events
in the presence of background noise of different spatio-temporal characteristics and comparing different
models using a bayesian analysis with evidence calculation for hypothesis testing. it may also be worth
exploring other compression methods e.g. wavelet compression, instead of the adopted time domain
method, considering the full seismic wavefield rather than individual seismograms and test for the best
regression model for this application. also, modelling stress tensor components along with the event
locations for non-explosive microseismic source mechanism is a challenging research topic and even
more in geological models with higher complexity and uncertainty. research in these directions are in
progress and will be reported in our future works.
acknowledgement
this work has been supported by the shell projects and technology. the wilkes high performance
gpu computing service at the university of cambridge has been used in this work.

appendix
additional analysis and high-resolution images for the simulation results are provided in the
supplementary material.

references
aster, r.c., borchers, b. & thurber, c.h., 2011. parameter estimation and inverse problems,
academic press.

66

geophysical journal international
auld, t. et al., 2007. fast cosmological parameter estimation using neural networks. monthly notices
of the royal astronomical society: letters, 376(1), pp.l11â€“l15.
auld, t., bridges, m. & hobson, m., 2008. cosmonet: fast cosmological parameter estimation in
non-flat models using neural networks. monthly notices of the royal astronomical society, 387(4),
pp.1575â€“1582.
babaei, m., alkhatib, a. & pan, i., 2015. robust optimization of subsurface flow using polynomial
chaos and response surface surrogates. computational geosciences, 19(5), pp.979â€“998.
babaei, m. & pan, i., 2016. performance comparison of several response surface surrogate models
and ensemble methods for water injection optimization under uncertainty. computers &
geosciences, 91, pp.19â€“32.
babaei, m., pan, i. & alkhatib, a., 2015. robust optimization of well location to enhance hysteretical
trapping of co2: assessment of various uncertainty quantification methods and utilization of
mixed response surface surrogates. water resources research, 51(12), pp.9402â€“9424.
barutÃ§uouglu, z. & alpaydin, e., 2003. a comparison of model aggregation methods for regression.
in artificial neural networks and neural information processingâ€”icann/iconip 2003.
springer, pp. 76â€“83.
chapman, c., 2004. fundamentals of seismic wave propagation, cambridge university press.
collettini, c. & barchi, m.r., 2002. a low-angle normal fault in the umbria region (central italy): a
mechanical model for the related microseismicity. tectonophysics, 359(1), pp.97â€“115.
das, s., chen, x. & hobson, m.p., 2017. fast gpu-based seismogram simulation from
microseismic events in marine environments using heterogeneous velocity models. ieee
transactions on computational imaging, 3(2), pp.316â€“329.
dieterich, j.h., richards-dinger, k.b. & kroll, k.a., 2015. modeling injection-induced seismicity
with the physics-based earthquake simulator rsqsim. seismological research letters, 86(4),
pp.1102â€“1109.
domingos, p., 2012. a few useful things to know about machine learning. communications of the
acm, 55(10), pp.78â€“87.

67

geophysical journal international
eaton, d.w. et al., 2014. scaling relations and spectral characteristics of tensile microseisms:
evidence for opening/closing cracks during hydraulic fracturing. geophysical journal
international, p.ggt498.
eisner, l. et al., 2010. beyond the dots in the box: microseismicity-constrained fracture models for
reservoir simulation. the leading edge, 29(3), pp.326â€“333.
forrester, a., sobester, a. & keane, a., 2008. engineering design via surrogate modelling: a
practical guide, john wiley & sons.
forrester, a.i. & keane, a.j., 2009. recent advances in surrogate-based optimization. progress in
aerospace sciences, 45(1), pp.50â€“79.
forrester, a.i., sÃ³bester, a. & keane, a.j., 2007. multi-fidelity optimization via surrogate modelling.
proceedings of the royal society of london a: mathematical, physical and engineering sciences,
463(2088), pp.3251â€“3269.
friedman, j., hastie, t. & tibshirani, r., 2010. regularization paths for generalized linear models via
coordinate descent. journal of statistical software, 33(1), p.1.
friedman, j., hastie, t. & tibshirani, r., 2001. the elements of statistical learning, springer, berlin.
goodwin, n., 2015. bridging the gap between deterministic and probabilistic uncertainty
quantification using advanced proxy based methods. in spe reservoir simulation symposium.
goutte, c., 1997. note on free lunches and cross-validation. neural computation, 9(6), pp.1245â€“
1249.
graff, p. et al., 2012. bambi: blind accelerated multimodal bayesian inference. monthly notices of
the royal astronomical society, 421(1), pp.169â€“180.
graff, p. et al., 2013. neural networks for astronomical data analysis and bayesian inference. in 2013
ieee 13th international conference on data mining workshops. pp. 16â€“23.
groos, j. & ritter, j., 2009. time domain classification and quantification of seismic noise in an
urban environment. geophysical journal international, 179(2), pp.1213â€“1231.
guo, p., mcmechan, g.a. & guan, h., 2016. comparison of two viscoacoustic propagators for qcompensated reverse time migration. geophysics, 81(5), pp.s281â€“s297.

68

geophysical journal international
hobson, m. et al., 2014. machine-learning in astronomy. proceedings of the international
astronomical union, 10(s306), pp.279â€“287.
holland, p.w. & welsch, r.e., 1977. robust regression using iteratively reweighted least-squares.
communications in statistics-theory and methods, 6(9), pp.813â€“827.
huang, t.-m., kecman, v. & kopriva, i., 2006. kernel based algorithms for mining huge data sets,
springer.
ieong, 2012. xnfx - high order predictor matrix for regression analysis. available at:
https://uk.mathworks.com/matlabcentral/fileexchange/39144-xnfx-high-order-predictor-matrix-forregression-analysis.
igel, h., 2016. computational seismology: a practical introduction, oxford university press.
igel, h., mora, p. & riollet, b., 1995. anisotropic wave propagation through finite-difference grids.
geophysics, 60(4), pp.1203â€“1216.
james, g. et al., 2013. an introduction to statistical learning, springer.
kalantari-dahaghi, a., mohaghegh, s. & esmaili, s., 2015. coupling numerical simulation and
machine learning to model shale gas production at different time resolutions. journal of natural
gas science and engineering, 25, pp.380â€“392.
kendall, m. et al., 2011. microseismicity: beyond dots in a boxâ€”introduction. geophysics, 76(6),
pp.wc1â€“wc3.
lattimore, t. & hutter, m., 2013. no free lunch versus occamâ€™s razor in supervised learning. in
algorithmic probability and friends. bayesian prediction and artificial intelligence. springer, pp.
223â€“235.
leet, l.d., 1949. microseisms. scientific american, 180, pp.42â€“45.
levy, c., jongmans, d. & baillet, l., 2011. analysis of seismic signals recorded on a prone-to-fall
rock column (vercors massif, french alps). geophysical journal international, 186(1), pp.296â€“
310.
mackay, d.j., 1997. gaussian processes-a replacement for supervised neural networks? lecture notes
for a tutorial at nips 1997.

69

geophysical journal international
modesto, d. & de la puente, j., 2016. exploring a priori reduced order models for fast seismic
simulations. in 78th eage conference and exhibition 2016.
mohaghegh, s.d., 2006. quantifying uncertainties associated with reservoir simulation studies using
a surrogate reservoir model. in spe annual technical conference and exhibition.
mosegaard, k. & tarantola, a., 2002. 16 probabilistic approach to inverse problems. international
geophysics, 81, pp.237â€“265.
mosegaard, k. & tarantola, a., 1995. monte carlo sampling of solutions to inverse problems.
journal of geophysical research: solid earth, 100(b7), pp.12431â€“12447.
mu, d., chen, p. & wang, l., 2013a. accelerating the discontinuous galerkin method for seismic
wave propagation simulations using multiple gpus with cuda and mpi. earthquake science,
26(6), pp.377â€“393.
mu, d., chen, p. & wang, l., 2013b. accelerating the discontinuous galerkin method for seismic
wave propagation simulations using the graphic processing unit (gpu)â€”single-gpu
implementation. computers & geosciences, 51, pp.282â€“292.
neal, r.m., 1996. bayesian learning for neural networks, springer science & business media.
pan, i. et al., 2014a. a multi-period injection strategy based optimisation approach using kriging
meta-models for co 2 storage technologies. energy procedia, 63, pp.3492â€“3499.
pan, i. et al., 2014b. artificial neural network based surrogate modelling for multi-objective
optimisation of geological co 2 storage operations. energy procedia, 63, pp.3483â€“3491.
pan, i. & das, s., 2015. kriging based surrogate modeling for fractional order control of microgrids.
ieee transactions on smart grid, 6(1), pp.36â€“44.
pandey, d.s. et al., 2016. artificial neural network based modelling approach for municipal solid
waste gasification in a fluidized bed reactor. waste management, 58, pp.202â€“213.
phadke, s., bhardwaj, d. & dey, s., 2000. an explicit predictor-corrector solver with application to
seismic wave modelling. computers & geosciences, 26(9), pp.1053â€“1058.
plumb, a.p. et al., 2005. optimisation of the predictive ability of artificial neural network (ann)
models: a comparison of three ann programs and four classes of training algorithm. european
journal of pharmaceutical sciences, 25(4), pp.395â€“405.
70

geophysical journal international
rasmussen, c.e. & williams, c.k., 2006. gaussian processes for machine learning, mit press.
rodriguez, a. et al., 2006. a multiscale and metamodel simulation-based method for history
matching. in ecmor x-10th european conference on the mathematics of oil recovery.
rogers, s. & girolami, m., 2015. a first course in machine learning, crc press.
rutledge, j.t., phillips, w.s. & schuessler, b.k., 1998. reservoir characterization using oilproduction-induced microseismicity, clinton county, kentucky. tectonophysics, 289(1), pp.129â€“
152.
samui, p. & sitharam, t., 2010. site characterization model using artificial neural network and
kriging. international journal of geomechanics, 10(5), pp.171â€“180.
sitharam, t., samui, p. & anbazhagan, p., 2008. spatial variability of rock depth in bangalore using
geostatistical, neural network and support vector machine models. geotechnical and geological
engineering, 26(5), pp.503â€“517.
slotte, p.a. & smorgrav, e., 2008. response surface methodology approach for history matching and
uncertainty assessment of reservoir simulation models. in europec/eage conference and
exhibition.
street, j.o., carroll, r.j. & ruppert, d., 1988. a note on computing robust regression estimates via
iteratively reweighted least squares. the american statistician, 42(2), pp.152â€“154.
tarantola, a., 2005. inverse problem theory and methods for model parameter estimation, siam.
tarantola, a. & valette, b., 1982. inverse problems= quest for information. journal of geophysics,
50(3), pp.150â€“170.
treeby, b., cox, b. & jaros, j., 2012. k-wave a matlab toolbox for the time domain simulation of
acoustic wave fields user manual.
treeby, b.e. et al., 2014. modelling elastic wave propagation using the k-wave matlab toolbox. in
ultrasonics symposium (ius), 2014 ieee international. pp. 146â€“149.
treeby, b.e. & cox, b.t., 2010. k-wave: matlab toolbox for the simulation and reconstruction of
photoacoustic wave fields. journal of biomedical optics, 15(2), pp.021314â€“021314.
weglein, a.b. et al., 2009. clarifying the underlying and fundamental meaning of the approximate
linear inversion of seismic data. geophysics, 74(6), pp.wcd1â€“wcd13.
71

geophysical journal international
wilson, k., durlofsky, l.j. & others, 2012. computational optimization of shale resource
development using reduced-physics surrogate models. in spe western regional meeting.
wilson, k.c. & durlofsky, l.j., 2013. optimization of shale gas field development using direct
search techniques and reduced-physics models. journal of petroleum science and engineering,
108, pp.304â€“315.
wolpert, d.h., 2002. the supervised learning no-free-lunch theorems. in soft computing and
industry. springer, pp. 25â€“42.
wood, l.c., 1974. seismic data compression methods. geophysics, 39(4), pp.499â€“525.
zou, h. & hastie, t., 2005. regularization and variable selection via the elastic net. journal of the
royal statistical society: series b (statistical methodology), 67(2), pp.301â€“320.
zubarev, d.i., 2009. pros and cons of applying proxy-models as a substitute for full reservoir
simulations. in spe annual technical conference and exhibition.

72



received may 14, 2020, accepted may 24, 2020, date of publication may 27, 2020, date of current version june 26, 2020.
digital object identifier 10.1109/access.2020.2997921

a physics-based neural-network way to perform
seismic full waveform inversion
yuxiao ren

1 , xinji xu

2 , senlin yang1 , lichao nie

1 school of qilu transportation, shandong university, jinan 250061, china

2 , and yangkang chen

3

2 geotechnical and structural engineering research center, shandong university, jinan 250061, china
3 school of earth sciences, zhejiang university, hangzhou 310027, china

corresponding authors: xinji xu (xuxinji1990@163.com) and lichao nie (lichaonie@163.com)
this work was supported in part by the national key research and development plan under grant 2016yfc0401801, in part by the
national natural science foundation of china under grant 51739007 and grant u1806226, in part by the fundamental research funds of
shandong university under grant 2017jc002 and grant 2018gn019. the work of yangkang chen was supported by the starting funds
from zhejiang university.

abstract seismic full waveform inversion is a common technique that is used in the investigation of
subsurface geology. its classic implementation involves forward modeling of seismic wavefield based on a
certain type of wave equation, which reflects the physics nature of subsurface seismic wavefield propagation.
however, obtaining a good inversion result using traditional seismic waveform inversion methods usually
comes with a high computational cost. recently, with the emerging popularity of deep learning techniques
in various computer vision tasks, deep neural network (dnn) has demonstrated an impressive ability in
dealing with complex nonlinear problems, including seismic velocity inversion. now, extensive efforts
have been made in developing a dnn architecture to tackle the problem of seismic velocity inversion,
and promising results have been achieved. however, due to the dependence of a labeled dataset, i.e., the
barely accessible true velocity model corresponding to real seismic data, the current supervised deep learning
inversion framework may suffer from limitations on generalization. one possible solution to mitigate this
issue is to impose the governing physics into this kind of purely data-driven method. thus, following
the procedures of traditional seismic full waveform inversion, we propose a seismic waveform inversion
network, namely swinet, based on wave-equation-based forward modeling network cells. by treating the
single-shot observation data and its corresponding shot position as training data pairs, the inverted velocity
model can be obtained as the trainable network parameters. moreover, since the proposed seismic waveform
inversion method is performed in a neural-network way, its implementation and inversion effect could benefit
from some built-in tools in pytorch, such as automatic differentiation, adam optimizer and mini-batch
strategy, etc. numerical examples indicate that the swinet method may possess great potential in resulting
a good velocity inversion effect with relatively fast convergence and lower computation cost.
index terms acoustic wavefield modeling, deep learning inversion, seismic waveform inversion.
i. introduction

seismic full waveform inversion plays an important role in
the estimation of subsurface properties, such as geology,
lithology, rock mass quality, etc., and it has been of significant interest to exploration geophysicists for decades [1]â€“[5].
waveform inversion is often achieved based on the forward
modeling of a particular wave equation to generate the synthetic observation data, which will be further compared with
the actual observation data to find the best description of the
subsurface properties.
the associate editor coordinating the review of this manuscript and
approving it for publication was wei liu.
112266

particularly, seismic full waveform inversion (fwi) in its
conventional form is to reconstruct the velocity model that is
capable of matching the actual recorded data by minimizing
the data residual between synthetic data and actual data in
a l2 norm [6]â€“[8]. this inverse problem is usually ill-posed
and sensitive to initial model. thus, the conventional fwi
often requires a good estimation of velocity as initial model
and utilizes an iterative optimization algorithm for updating
the model parameters. moreover, due to the large amount of
model parameters (usually between 104 in 2d and 1010 in
3d) to invert, the computation cost is another constraining
factor that requires consideration in the research and practical
application of this method [9].

this work is licensed under a creative commons attribution 4.0 license. for more information, see https://creativecommons.org/licenses/by/4.0/

volume 8, 2020

y. ren et al.: physics-based neural-network way to perform seismic fwi

in recent years, the successful applications of deep learning
in various fields like computer vision (e.g. [10]â€“[12]) and
natural language processing (e.g. [13]â€“[15]), have attracted
attention from scholars in exploration geophysics community. deep neural networks (dnns) have demonstrated
good application effects on almost all kinds of geophysical
data (e.g. [16]â€“[20]). particularly, some state-of-the-art deep
learning techniques have been widely applied in various seismic data problems, such as seismic noise attenuation (e.g.
[21]â€“[24]), automatic seismic event picking (e.g. [25]â€“[27])
and seismic structure interpretation (e.g. [28], [29]). all these
progresses indicate a great potential of dnn techniques and
collaboration between the deep learning community and the
geophysics community could lead to more achievements.
especially in the domain of seismic data inversion,
the remarkable ability of dnn to accurately simulate
a non-linear and complex mapping has demonstrated
some advantages over traditional methods. for example,
araya-polo et al. [30] proposed a velocity prediction method
using a dnn with the calculated velocity feature semblance
as an input and a k-means method to post-process the output. wu et al. [31] proposed a cnn-based network called
inversionnet to directly map the raw seismic data to the
corresponding seismic velocity model and it achieved good
inversion effect on simple fault models with flat or curved
subsurface layers. more recently, li et al. [32] deeply analyzed the features of mapping the time-series seismic data to
a velocity image and then developed a novel dnn framework called seisinvnet to perform the end-to-end velocity
inversion mapping with enhanced single-trace seismic data
as the input. in a word, various dnn frameworks have been
adopted into the task of seismic velocity inversion and some
of them have already outperformed the traditional fwi on
simple velocity models.
however, like all data-driven methods, the performance of
the aforementioned dnn-based seismic inversion methods
largely depends on the training dataset. currently, almost all
of the dnn-based applications in seismic velocity inversion
is to simplify the velocity models into some classes and then
design an algorithm to generate enough velocity models for
training the deep learning network. thus, the generalization
ability of the proposed deep learning inversion frameworks
to a more complex velocity model would be an issue. one
way to address this issue is to build or otherwise acquire
a set of realistic velocity models which is of a larger scale
and contain more complex structures such as salt bodies,
faults, several different curved layers with different velocity
distributions, etc. however, these kinds of improvements in
the aspects of velocity model size and complexity are usually
difficult, due to limitations of computation power such as
memories, cpu, gpu and etc. thus, the extension of current
deep learning inversion philosophy to practical application
may require extensive research work in the future.
on the other hand, the promising inversion performance
of traditional fwi in synthetic and actual complex models, indicates the great power of considering the governing
volume 8, 2020

wave equation in seismic inversion. thus, another possible
solution to generalize the seismic deep learning inversion to
large complex models is to incorporate the relevant physical
nature rules into the purely data-driven methods and form
a physics-based deep learning concept [33], [34]. in this
concept, a seismic forward modeling operator needs to be
designed and imposed into the deep learning inversion architecture, which will relate the basic generation rules and features of seismic observation data with the learning process,
and as a result, may lead the optimization of network parameters to a more stable and reasonable direction. in fact, since
the traditional method achieves seismic waveform inversion
based entirely on the physics nature of wavefield propagation, i.e., the wave equation, its implementation from the
perspective of deep learning can be considered as a special
case of the physics-based deep learning scheme. thus, the
physics-based seismic deep learning inversion may not only
have the ability of inverting a complex velocity model like
traditional fwi, but also maintain the potential of adopting
techniques from the deep learning community. for example,
the advanced tools like mini-batch strategy, various optimization algorithms and an easy access to gpu parallel computing
techniques will provide novel perspectives and convenience
for the study of seismic waveform inversion.
one key factor of the physics-based deep-learning inversion is to build a seismic forward-modeling operator that
can be incorporated into the deep-learning inversion network. by convolving with an angle-dependent wavelet,
biswas et al. [33] proposed a forward modeling data generation method for updating the classic convolutional neural
network (cnn) in the task of elastic pre-stack inversion.
richardson [35] achieved the simulation of acoustic wave
equation via a recurrent neural network (rnn) based on
tensorflow platform [36], and undertook an initial attempt
on seismic fwi based on some useful deep learning tools.
in this work, we simulate seismic wave equation based
on a general dnn architecture using pytorch [37], and the
implementation of a sponge absorbing boundary condition
in this framework is quite straightforward. then for seismic
inversion, a comparison of the gradient calculation between
the adjoint state method that is widely used in fwi and
automatic differentiation in deep learning is conducted for
the discrete form of acoustic wave equation. moreover, seismic waveform inversion is achieved in a neural network
optimization way, so that normal deep learning tools can
be adopted. as shown in the numerical examples, the proposed method demonstrates satisfactory inversion effect with
fast convergence and controllable computation cost. in the
end, we discuss the potential of the physics-based seismic
deep learning inversion and remark on some further possible
research directions along the line.
ii. network formulation for seismic wavefield
extrapolation

as stated in the introduction section, the seismic forward modeling operator simulated via a neural network is
112267

y. ren et al.: physics-based neural-network way to perform seismic fwi

an essential component for the physics-based seismic deep
learning inversion. currently, one of the most popular and
accurate seismic wavefield extrapolation methods is based
on wave equation, which can be modified according to the
physical property of the wave propagation medium. usually, for a homogeneous medium with constant density,
acoustic wave equation in time domain is used based on a
finite-difference discretization approach. thus, for the convenience of demonstrating our work, we will start with the
basic one-dimensional acoustic wave equation and its corresponding network formulation. similar implementations can
be extended to higher dimensions and other types of wave
equation.
a. problem definition and background theory

figure 1. the network architecture of seismic forward modeling (fm)
cells. the hexagon denotes a convolution operation with the laplacian
operator l and the circles represent element-wise operations like
addition or multiplication with a number or matrix. especially, matrix a(v)
contains the trainable parameter v of the network.

the acoustic wave equation in 1d time-space domain with a
source term s can be described as
âˆ‚ 2p
âˆ‚t 2

= v2 (z)

âˆ‚ 2p
âˆ‚z2

(1)

+ s(t, z),

where t and z denote time and depth, respectively, p(t, z)
usually denotes the pressure field and v(z) represents the
acoustic wave velocity. the finite-difference discretization in
the second order gives
pn+1
âˆ’ (2 + Î±v2i âˆ‡ 2 )pni + pnâˆ’1
= sn+1
.
i
i
i

(2)

1t
here, Î± = 1z
2 , vi denotes the acoustic velocity at all
nz model gird points and âˆ‡ 2 denotes the discrete laplace
operator that conducts spatial discretization of the acoustic
wavefield p(t, z). by letting pn = [pn1 , pn2 , Â· Â· Â· , pnnz ]t and
sn = [sn1 , sn2 , Â· Â· Â· , snnz ]t represent the wavefield and source
at the time step n, respectively, the discrete acoustic wave
equation can be further rewritten in matrices as
2

pn+1 = gpn âˆ’ pnâˆ’1 + sn+1 ,

(3)

where g = 2i + al and a = diag{Î±v2i }, l is the laplacian
matrix of the form
ï£®âˆ’2
1
âˆ’2
ï£¯1
..
l=ï£¯
.
ï£°

0

ï£¹
1
..
.
1

..

.
âˆ’2
1

ï£º
ï£º
ï£»

1
âˆ’2

.

(4)

nzÃ—nz

notice that both matrix a and l are symmetric, while the
matrix g is not, which means that this discretization form
fails to maintain the self-adjoint property of acoustic wave
equation in the continuous form and extra attention is required
when calculating the inversion gradient based on the corresponding adjoint wave equation.
furthermore, by denoting p = [p1 , p2 , . . . , pnt ]t , s =
1
[s , s2 , . . . , snt ]t , we can obtain the acoustic equation (3) in
a more compact form as bp = s. here
ï£® i
ï£¹
âˆ’g
i
ï£¯ i
ï£º
âˆ’g
i
ï£º.
b=ï£¯
(5)
ï£°
ï£»
..
..
..
.
.
.
i
âˆ’g i
112268

as for the seismic acquisition geometry, we introduce the
matrix ds to distribute the source wavelet w on every wavefield grids for all time steps, that is, s = ds w. moreover, data
collection process is denoted by the observation matrix dr ,
which will extract the wavefield value at the predetermined
receiver position to an abstract data vector, i.e., d = dr p =
dr bâˆ’1 s. especially, when the seismic sources and receivers
are set on the same grids, we will have ds = dtr . actually, for
seismic data recorded on ground
surface, the

 corresponding
observation matrix is dr = d1 d2 Â· Â· Â· dnt with submatrix
dn of the size nt Ã— nz in the following form
ï£¹
ï£®
1 0 Â·Â·Â· 0
0
0
Â·
Â·
Â·
0
ï£º
ï£¯
d1 = ï£° ..
..
ï£»,
..
. 0
.
.
0 0 Â·Â·Â· 0
ï£¹
ï£®
0 0 Â·Â·Â· 0
ï£¯1 0 Â· Â· Â· 0ï£º
ï£º
ï£¯
ï£º
ï£¯
(6)
d2 = ï£¯0 0 Â· Â· Â· 0ï£º , Â· Â· Â· .
ï£º
ï£¯ ..
..
..
ï£°.
. 0ï£»
.
0

Â·Â·Â·

0

b. forward modeling network formulation

generally speaking, seismic wavefiled extrapolation is an
iterative process in time domain based on forward modeling
operators. in our work, considering the acoustic wave equation denoted in (3), we design a neural network with nt cells
and in each cell, wave extrapolation within one time step is
performed. that is, seismic wavefield extrapolation for nt
time steps can be achieved via the forward calculation of the
network. moreover, the network parameters can be regarded
as the velocity values at every grid points and they can be
trained to arrive at their optimal points when provided with
suitable training setup and dataset.
more specially, the detailed network architecture in each
cell is designed by following the operations in the wave
equation (3). as shown in fig. 1, the future wavefield pn+1
is calculated based on the wavefield on the previous two time
steps pnâˆ’1 and pn as well as the future source sn+1 . here, the
volume 8, 2020

y. ren et al.: physics-based neural-network way to perform seismic fwi

figure 2. illustration map for the (a) padding and (b) stacking
operations in absorbing boundary layers. the middle blue area denotes
the original velocity model.

circles represent element-wise operations. for example, the
circle with a number in it means element-wise multiplication
with that number and the circle a means element-wise multiplication between wavefield pn and the velocity model Î±v.
the circle with a â€˜â€˜plusâ€™â€™ sign denotes element-wise addition.
moreover, the hexagon denotes the convolution operation of
wavefield pn with the discrete laplace kernel of the corresponding dimension. for example, the vector [1, âˆ’2, 1] is
for 1d problems and 2d problems will adopt the matrix
filter
ï£®
0
ï£°1
0

1
âˆ’4
1

ï£¹
0
1ï£» .
0

(7)

the calculation within each cell can be accelerated by parallel
computing of the laplace transform, which can be easily
achieved by calling a ready-made function in the pytorch
platform.
c. absorbing boundary conditions

usually, an absorbing boundary condition is considered in
seismic wavefield simulation to improve the calculation
efficiency by reducing the wavefield reflection from the
numerical boundaries. on the other hand, the addition of an
absorbing boundary layers should avoid changing the basic
architecture of the proposed forward modeling network and
maintain its differentiability, which is essential for accurately
computing the gradient when updating network parameters.
one of the most efficient and convenient ways to mitigate the
boundary reflection is to add several sponge layers outside
the original velocity models [38], [39].
in fact, adding sponge absorbing boundary layers includes
the padding of the outer boundary of the original velocity
model (see fig. 2a) and then element-wise times with an
exponentially decaying factor to reduce the wavefield amplitudes in the extended layers. thus, adding sponge layers can
be considered as putting a â€˜â€˜maskâ€™â€™ over the whole wavefield,
and the â€˜â€˜maskâ€™â€™ is a matrix whose element is one in the
original velocity area and the decaying factor in the extended
sponge area. with the absorbing boundary matrix m, the
governing matrix b for the acoustic forward modeling can
volume 8, 2020

figure 3. the network architecture of seismic forward modeling (fm)
cells with a sponge absorbing boundary condition. the circle m denotes
the sponge absorbing boundary condition, which indicates the â€˜â€˜maskâ€™â€™
matrix is element-wise multiplying with the corresponding wavefield.

be expressed as
ï£®
i
ï£¯âˆ’gm
2
ï£¯
b=ï£¯ m
ï£°

ï£¹
i
âˆ’gm
..
.

i
..
.
m2

ï£º
ï£º
ï£º.
ï£»

..

.
âˆ’gm

(8)

i

fortunately, the realization of the sponge absorbing boundary condition is quite straightforward. as shown in fig. 3, the
two input previous wavefields are multiplied by the â€˜â€˜maskâ€™â€™
m in an element-wise form, which easily achieves the goal
of reducing wavefield amplitude without compromising the
differentiability of the proposed forward modeling network.
iii. seismic waveform inversion network

in this section, we build a neural network based on the proposed forward modeling (fm) cells and the velocity model
can be updated during the training process. it will be evident
to see that the way of optimizing velocity model is similar
to the traditional seismic inversion except for some adapted
concepts and techniques from the deep learning community.
a. network architecture

the aim of training a deep neural network is to find a set
of optimal dnn parameters that best fits the corresponding true observation data, and it is usually measured via a
loss function. in our case, based on the proposed forward
modeling cells, we can build a seismic waveform inversion
network, namely swinet, whose trainable parameters are
the subsurface velocity distribution. the swinet output is
the synthetic observation data that will be further compared
with the actual observation data to determine the velocity
update. thus for a certain set of seismic data receivers, with
the shot positions acting as the network input, we can train
the swinet to fit the actual observation data and obtain a
best guess of the subsurface velocity model.
moreover, as shown in the network architecture illustration
map in fig. 4, the input information about shot position is
transformed into seismic wavefield via the matrix ds . in addition, nt forward modeling cells are adopted sequentially to
output the synthetic observation data dsyn for nt time steps.
112269

y. ren et al.: physics-based neural-network way to perform seismic fwi

figure 4. the network architecture of the seismic waveform inversion
network swinet based on nt forward modeling cells.

considering the fact that seismic data and its corresponding
shot position naturally come in pairs, it is not difficult to
acquire a training dataset containing seismic data generated
by thousands of or even millions of seismic shots in practice.
similarly to the traditional seismic waveform inversion,
for every input data (i.e., single-shot seismic data), the loss
function can be defined as
1 xx
(dobs âˆ’ dsyn )2 .
(9)
l=
2 nt nr
here, dobs and dsyn are the observed data and the synthetic
data modeled by the neural network, respectively. nr and
nt denote the number of receivers and time steps, respectively. in this way, we can optimize the velocity model
by training the swinet using all sorts of deep learning
advances, such as the adam optimizer and mini-batch strategy in the numerical examples. it is worth noticing that
the requirement of a relatively good initial velocity model
in traditional fwi also applies to the training process of
swinet, which means a good network initialization. it will
provide essential low-frequency information for the reconstruction of the velocity model and plays a vital role in
mitigating the cycle-skipping problem in seismic waveform
inversion [40]â€“[42].
b. gradient calculation

in the standard training process of dnn, network parameters
are usually optimized using gradient-based methods, which
can be found very convenient to use in the deep learning
platforms pytorch and tensorflow. the gradient can be calculated easily via the built-in automatic differentiation tools,
which are based on the backward propagation (bp) theory.
on the other hand, the gradient calculation in traditional
seismic inversion is based on the adjoint state method and
it has been theoretically proved to be coincident with bp
gradient in continuous form. considering the discrete nature
of the finite-difference method, we demonstrate our proof in
the appendices vi and vi from the perspective of matrix
calculation.
as shown in the appendix a, the gradient calculation
involves computing the adjoint wavefield bâˆ’t  with the
112270

data residual  as source. normally, the adjoint wavefield is
obtained via the operator bâˆ’t , and the adjointness between
them should be numerically verified by the dot-product test
[43]. when considering the absorbing boundary condition,
in addition to adopting the forward modeling operator shown
in (8), one should also consider the corresponding adjoint
operation of padding the boundary of the original velocity
model, which is the stacking of the absorbing layers onto
the corresponding boundary of the original velocity model
(see fig. 2b for illustration). this can be understood via
matrix transposition where padding corresponds to a vertical vector a = [1, 1, Â· Â· Â· , 1]t and stacking corresponds to
at = [1, 1, Â· Â· Â· , 1]. for example, when padding a boundary
velocity value vi for m times, the extended layers will have
a velocity value of vi a = p
[vi , vi , Â· Â· Â· , vi ]t and the stacking
t
operation means a vi a = m vi = mvi . thus, there will be
abnormal values on the outmost layers of the gradient and
the updated velocity model and this influence can be easily
neglected by cutting these abnormal layers during the training
process.
as mentioned before, using a handcraft code to calculate
the gradient for seismic velocity inversion usually requires
an adjoint test on the code [44], which is not easy to pass.
however, automatic differentiation calculates the gradient
based on the chain rule and its algorithm passes the adjoint
test automatically. thus, considering the convenience of
implementing gradient calculation in deep learning platform,
we choose to use the automatic differentiation way for the
gradient computation in our work.
c. gpu parallel computation

another advantage of using a neural network way to perform seismic waveform inversion is the easy implementation of gpu parallelization based on modern deep learning
platforms. in our case, the parallel computation is mainly
included in two processes: (a) parallel computing of the
laplacian in acoustic extrapolation via convolution and
(b) parallel computing of the wavefield with different sources
on different gpus. both can be easily implemented by calling
the built-in functions in pytorch. however, using gpu parallel computation also poses restrictions on the size of velocity
model due to the limited gpu memory. thus, the effective
utilization of multiple gpus in large 2d or even 3d velocity
models is worth further investigation.
in our work, we achieve the inversion of small 2d velocity models by implementing the swinet on several gpus
with 24g memory. as for a larger velocity, swinet can be
implemented via cpu with larger memory. moreover, experience from deep learning indicates that seismic inversion
can benefit from the gradient oscillations of the mini-batch
strategy and the adaptive gradient momentum of the adam
optimizer, thereby obtaining a good inversion convergence.
actually, the randomness in the gradient calculation is conducive for the gradient trajectory to escape from saddle points
or local minima during the optimization process. considering
that adam optimizer utilizes historical gradients and works
volume 8, 2020

y. ren et al.: physics-based neural-network way to perform seismic fwi

well with mini-batch strategy in various deep learning tasks,
we would like to expend their application scope to the field of
seismic inversion and impose them in the training process of
swinet. thus, the basic algorithm for training the swinet
can be summarized in alg. 1.
algorithm 1 training process of swinet
inputs:
{dÌ‚n }ns
n=1 : a set of real observation data; v0 : initial velocity
model; s: seismic source wavefield.
notations:
p: seismic wavefield; p(Â·): boundary padding of the
velocity model; l(Â·): laplacian of wavefield;
:
hadamard product; m: absorbing boundary condition
mask.
training process:
1: init: set trainable parameter v = v0 and p = 0;
2: for j = 1 : 1 : batchsize in gpus do
3:
v â† p(v)
4:
for t = 1 : 1 : nt do
dt 2
2
5:
a â† dz
2 Â·v
6:
ptemp â† m p(t âˆ’ 1)
7:
p(t) â† m p(t)
8:
p(t + 1) â† 2p(t) + a l(p(t)) âˆ’ ptemp
9:
p(t + 1) â† p(t + 1) + s(j, t + 1)
10:
end for
11:
dj â† p(receiver position)
12:
loss â† l(dj , dÌ‚j )
13: end for
14: v â† adam(v, loss, lr)

iv. numerical examples

one major goal of building the swinet is to introduce some
popular strategies and techniques from the deep learning
community which can be experimented conveniently on seismic waveform inversion via deep learning platforms. in this
work, we focus on the approaches of adam optimizer and
mini-batch strategy, and evaluate their application effect with
the proposed swinet on the task of inverting the complex marmousi model. this experiment is conducted on ten
nvidia titan rtx with 24g memory.
a. dataset and training parameter setup

the training dataset is constructed based on the synthetic data
simulated from the marmousi model shown in fig. 5a. this
model is down-sampled to a grid size of 112 Ã— 384 in order to
fit the gpu memory. the smoothed velocity model in fig. 5b
will be used for the initialization of the swinet parameters.
moreover, the grid spacing in both horizontal x and vertical
z directions is 10m and time stepping interval is 1ms. we set
20 shots and 384 receivers evenly deployed on the ground
surface. the dominant frequency of seismic source ricker
wavelet is 10hz.
volume 8, 2020

figure 5. the marmousi model. (a) true velocity and (b) smoothed
velocity. the scale-bar indicates the velocity value in m/s.

as for the network hyper-parameters, we follow the discussion in [34] and set the learning rate lr = 40 for the adam
optimizer with the number of epochs equal to 50. note that
there is no activation function or normalization applied in the
swinet, a larger learning rate will benefit the convergence of
the adam optimizer. to test the influence of the mini-batch
strategy in seismic waveform inversion, we design a series of
comparison experiments with different batch sizes and batch
iteration steps per epoch. firstly, we fix the total computation
costs similar to the traditional full waveform inversion and
conduct the velocity inversion using swinet with batch sizes
set as 2, 3, 4, 5, 6, 7, 10 and 20. the total computation cost
is defined as the number of gradient computations, which is
evaluated as batchsize Ã— step Ã— nepoch. considering that in
every batch iteration step, the traditional fwi usually utilizes
all of the observation data to update the velocity parameters,
its process is coincident with the trial with one batch iteration
step per epoch and batch size of 20. in other neural network
based waveform inversion, for batch size equal to 3, 6, 7,
we set batch step be 6, 3 and 2, respectively. in addition,
we remove the restriction regrading computation cost and set
batch steps equal to 2, 3, 4 and 5 to explore the inversion
effect of swinet with more or limited computation power.
in order to obtain a reliable outcome of swinet inversion
on marmousi model, we repeat the aforementioned experiments for four times and their mean losses are summarized
in table 1. here, the data loss is computed via the l2 norm
of the difference between the forward modeling data and the
real observation data and the model loss is defined based on
the l2 norm of the difference between the inverted velocity
model and the truth velocity model corresponding to the real
observation data. according to the data loss, we mark the the
traditional fwi loss, the overall lowest loss, the lowest loss
with the fixed computation cost and the similar loss with lowest computation cost, respectively with color blue, red, green
and magenta. the model losses are also marked accordingly
112271

y. ren et al.: physics-based neural-network way to perform seismic fwi

figure 6. swinet inversion results on marmousi model corresponding to (a) traditional fwi (color blue in the table), (b) the overall lowest loss (color
red in the table), (c) the lowest loss with the fixed computation cost (color green in the table) and (d) the similar loss with lowest computation cost
(color magenta in the table).

table 1. statistics of the swinet inversion loss with respect to different batch size (bs) and different batch iteration steps per epoch. according to the
data loss, we mark the the traditional fwi loss, the overall lowest loss, the lowest loss with the fixed computation cost and the similar loss with lowest
computation cost, respectively with color blue, red, green and magenta.

and their corresponding inversion results randomly selected
from the four repeated experiments are shown in fig. 6.
b. result analysis

here, we use the conventional fwi result as a benchmark
to compare the swinet inversion results in the view of data
and model misfit as well as computation costs. by comparing
the fwi result with the results of swinet inversion based
on other combinations of batch size and batch iteration step,
we can see that the mini-batch strategy plays an essential role
in obtaining a good inversion outcome with fast convergence
and less computation power consumption.
112272

as shown in fig. 6, the best inversion result (fig. 6b)
comes from a batch size of ten and five batch iteration steps
per epoch, which gives a total of 2500 times of gradient
computations. instead of 50 iterations, when conducting the
conventional fwi for 250 iterations, which corresponds to
a batch size of 20 and five batch iteration steps, one still
cannot obtain an inversion result as good as fig.6b. on the
other hand, for a fixed computation power consumption (see
the first row of data and model loss in table 1), the best
inversion result (fig. 6c) comes from the case with batch size
of five, which also demonstrates the advantage of using a
mini-batch strategy. moreover, it spends only 200 times of
volume 8, 2020

y. ren et al.: physics-based neural-network way to perform seismic fwi

figure 7. swinet inversion losses corresponding to the four figures in
fig. 6, respectively. (a) data loss and (b) model loss. the â€˜â€˜sccâ€™â€™ and â€˜â€˜lccâ€™â€™
in legend correspond to the inversion results using a similar computation
cost (scc) to the conventional fwi and the lowest computation cost
(lcc), respectively.

gradient computations to get an inversion result (see fig. 6d)
whose inversion effect is similar to the conventional fwi.
noticing that the conventional fwi requires 1000 times gradient computations, which is another proof of the fast convergence when using mini-batch strategy in swinet inversion.
it is worth mentioning that the data loss curve for cases of
setting batch size as two presents relatively severe oscillation
in fig. 7a. although it is not ideal in the perspective of loss
decaying, it implies a flexibility in updating the network
parameters and demonstrates a great potential in refining the
velocity model in the last stage of inversion. thus, one may
consider using a very small batch size to further invert the
obtained inversion results in order to get an even better result.
generally speaking, a smaller batch size leads to various
gradient direction with smaller memory storage requirement.
for a fixed batch size, larger batch iteration steps per epoch
usually results in a better convergence, but requires larger
computation cost. in our case, in order to determine an optimal batch size, we plot the losses in fig. 8, where each curve
represents each row in the loss table 1. close observation of
these curves indicates that for a fixed batch step, both data
loss and model loss have a decreasing trend. however, the
downward trend seems to slow down at the batch size of 5.
moreover, for a fixed computation cost, i.e., when the batch
size is defined as â€˜â€˜fullâ€™â€™, the data loss curve started to rise
after a turn point at batch size of 5. thus, for a scenario
with limited computation resources, especially a limited gpu
memory, batch size set as 5 is potentially an efficient choice
to receive a good inversion result and it is empirical to adopt
four or five batch iteration steps in each epoch.
c. comparison with seisinvnet

in order to demonstrate the potential of the physics-based
deep learning scheme, we conduct a comparison experiment
to show the inversion effect of the seisinvnet and the proposed swinet. a dataset containing 18000 pairs of velocity
models and the corresponding observation data is numerically
generated. 15000 of them will be used to train the seisinvnet
and we use another 1500 pairs of data for validation and 1500
for test. to make the comparison in a more realistic situation,
volume 8, 2020

figure 8. swinet inversion losses with respect to different batch size.
(a) data loss and (b) model loss.

we design the velocity models to have some complex geology structures like folding layers and faults, and 20 seismic
sources and 32 receivers are evenly placed on the top layer
to generate the corresponding observation data. by following
the setup parameters shown in the reference [32], we train
the seisinvnet for 200 epochs and its inversion effect on the
test dataset is compared with the swinet, which is trained on
the observation data of each test velocity model based on the
same network setup as the color green indicates in table 1.
an example of the comparison experiment result is shown
in fig. 9. by comparing figs. 9b and 9c, one can see that
swinet seems to have a better inversion effect on the shallow
layers and the faulting structure while the inverted velocity
value within each layers seems more consistent in the seisinvnet result than the swinet result. however, we believe this
drawback of swinet can be addressed via incorporating a
smooth regularization constraint in the network architecture.
generally speaking, both methods have certain advantages.
unlike the seisinvnet, swinet does not rely on the availability of the true velocity models as training labels. on the other
hand, the training process of swinet requires a good estimated velocity model for network parameter initialization,
while seisinvnet can be initialized randomly. as will be discussed later, a combination of both swinet and seisinvnet
could be studied to weaken the requirement of labelled dataset
(for seisinvnet) and good network parameter initialization
(for swinet) in the future.
v. discussion

the goal of this paper is to reformulate the traditional
fwi from a different perspective, e.g., in the format of
training a neural network, where the network parameters
become the subsurface velocity distribution. thus in this
work, we demonstrate a physics-based neural network way
to perform seismic waveform inversion by proposing the
swinet. after theoretically proving the equivalence of two
gradient calculation methods, i.e., the adjoint state method in
traditional seismic waveform inversion and the automatic differentiation widely used in deep learning, in a discrete matrix
form, we choose the latter due to its guarantee of adjointness and the convenient implementation in pytorch. numerical experiments with the adam optimizer and mini-batch
strategy indicate that popular ideas from the deep learning
112273

y. ren et al.: physics-based neural-network way to perform seismic fwi

figure 9. a comparison experiment example. (a) true velocity model, (b) inversion result of seisinvnet and (c) inversion result of swinet.

community can also be beneficial to seismic full waveform
inversion and result in an improvement in the perspective of
convergence speed and computation cost.
moreover, the numerical experiment results could be
further analyzed in terms of the possibility to obtain an uncertainty estimation [45] in the proposed framework. in a nutshell, the uncertainty estimation based on the adam optimizer
is highly worth investigating, but we havenâ€™t reached that
step. from the perspective of a general inverse problem, the
uncertainty estimation can be easily obtained by a bootstrapping method, i.e., we randomly select a fraction of the whole
input data for the inversion, and then calculate the standard
derivation of all the inverted result we have tested to be the
uncertainty of the inversion process.
compared with the traditional fwi, the innovation of this
work lies in the sense of the implementation form and optimization strategy. this not only grants geophysicist an easy
access to latest advancements arised in all sorts of deep learning tasks, but also have a potential in solving the deep learning
problems with inspiring seismic approaches. for example,
when facing a high memory demand problem like seismic
inversion, where calculating the gradient using automatic
differentiation method requires the storage of the wavefield
from both forward modeling and backward propagation. one
possible way to greatly reduce the memory cost is to incorporate a wavefield reconstruction method [46]â€“[48] into the
automatic differentiation, which can rebuild the forward modeling wavefield and calculate the gradient while backward
propagation. therefore, this work can be seen as a stepping
stone to deepen the cooperation among the researchers from
both geophysics and deep learning.
in addition, comparing the proposed swinet with the
classic multi-layer perceptron (mlp) neural network, one can
conclude with two major differences in the network architecture design: in swinet, (a) there is no need for activation
functions like the sigmoid function in accurately simulating
the wave equation, which plays an essential role in seismic
waveform inversion problem and (b) network parameters
(i.e., velocity model) are the same for all fm cells. that is,
there seems to be a very strong constraint on network parameters. in addition, instead of the single outcome of the last
layers, the loss function in seismic inversion problem involves
112274

the outcome of every layers. thus, differently from the classic
deep learning network, where random initialization of network parameters works well [49], swinet inversion depends
on a good initial model. how to modify the network architecture and tackle this issue may potentially lead to a major
breakthrough in seismic full waveform inversion.
another potential of swinet is to incorporate the fm cells
into the current purely data-driven seismic inversion network
such as the inversionnet [31] and the seisinvnet [32]. in this
way, the physics nature of wave propagation can provide
guidance for the deep learning network to capture the true pattern between the training data pairs, thereby improve the generalization ability of seismic inversion network. furthermore,
since the forward modeling data can be generated within the
network, how to construct an unsupervised seismic inversion
network and ease the requirement of true velocity model as a
label would be a hot topic worth further investigation.
vi. conclusion

in this work, we constructed a seismic forward modeling
neural network and proposed the swinet for seismic waveform inversion. with the velocity model acting as learnable
parameters, a set of single-shot observation data will be used
to train the swinet. numerical examples indicate that the
novel way to perform fwi provides an easy access to the
useful tools in deep learning society and the incorporation of
deep learning approaches, such as the adam optimizer and
mini-batch strategy, can lead to an improvement on seismic
inversion like fast convergence and lower computation costs.
as illustrated in the discussion, there are some topics related
to this line of work that may be worth investigating.
appendix a
gradient derivation via matrix calculus

in this appendix, we will derive the velocity gradient in a
matrix calculus way, whose outcome is identical to the result
of adjoint state method that is massively used in traditional
seismic full waveform inversion. following the notations in
the background acoustic wave equation theory, we have the
wavefield solver in the discretized form as
p = bâˆ’1 s,

(10)
volume 8, 2020

y. ren et al.: physics-based neural-network way to perform seismic fwi

and the least-squares loss function is
1
(11)
l = (dr p âˆ’ dr pÌ„)t (dr p âˆ’ dr pÌ„) .
2
here, p and pÌ„ denote the forward modeling wavefield and
the real wavefield, respectively. the goal is to calculate the
gradient of the loss function with respect to the velocity v.
firstly, we can easily compute
dl(p)
= dr t dr (p âˆ’ pÌ„) := ,
(12)
dp
which is in fact the data residual distributed at all receiver
points. then, based on the chain rule and considering that b
is a function of the velocity model v, for each vi we have

t
dp dl
dl
âˆ’1 db âˆ’1
=
=âˆ’ b
b s 
dvi
dvi dp
dvi
t

db âˆ’1
b s bâˆ’t .
(13)
=âˆ’
dvi
here,
ï£®0
ï£¹
0
g
0
ï£¯ i
ï£º
db
ï£¯
ï£º
g0i
0
(14)
= âˆ’ï£¯
ï£º,
ï£°
ï£»
..
..
dvi
.
.
g0i 0
and
ï£®

..

..

ï£¯ .
ï£¯

0

dg
g0i =
= 2Î±vi ï£¯
ï£¯
dvi
ï£°

.

..
0
1

.

ï£¹
0
âˆ’2
0

1
0
..
.

0
..
.

..

ï£º
ï£º
ï£º.
ï£º
ï£»

.
(15)

notice that, the subdiagonal matrix in (14) corresponds to
the one-time-step delay in forward modeling wavefield bâˆ’1 s
when it cross-correlates with the backward propagated wavefield bâˆ’t . the matrix g0 relates to the ith row of the laplacian matrix, which can be further proved to be coincident
with the second-order time derivative of the forward model
wavefield in the conventional born approximation theory.
appendix b
gradient calculation via auto-differentiation

here, we will calculate the gradient of network parameters
following the steps of automatic differentiation. in this case,
the least-squares loss function (11) can be rewritten as
nt

l=

1x n n
(dr p âˆ’ dnr pÌ„n )2 .
2

(16)

n=1

here, pn and pÌ„n are the forward modeling wavefield and
the real wavefield at the nth time step. according to the
back-propagation method, we need to firstly define a series of
middle variables according to the forward modeling equation
(3) as
âˆ‚l
t
Î´ nt =
= dnt (dnt pnt âˆ’ dnt pÌ„nt ) =  nt ,
âˆ‚pnt
volume 8, 2020

âˆ‚l âˆ‚pnt
âˆ‚l
+ ntâˆ’1
âˆ‚pnt âˆ‚pntâˆ’1
âˆ‚p
= gt Î´ nt +  ntâˆ’1 ,
âˆ‚l âˆ‚pntâˆ’1
âˆ‚l
âˆ‚l âˆ‚pnt
+
+ ntâˆ’2
Î´ ntâˆ’2 =
nt
ntâˆ’2
ntâˆ’1
ntâˆ’2
âˆ‚p âˆ‚p
âˆ‚p
âˆ‚p
âˆ‚p
= gt Î´ ntâˆ’1 âˆ’ Î´ nt +  ntâˆ’2 ,
Â·Â·Â·Â·Â·Â·
n
Î´ = gt Î´ n+1 âˆ’ Î´ n+2 +  n .
(17)
Î´ ntâˆ’1 =

similarly to appendix a, we obtain the data residual at the
âˆ‚l
n
nth time step by  n = âˆ‚p
n . Î´ corresponds to the data residual
propagation backward in time based on the future wavefield
and it is in fact the adjoint wavefield bâˆ’t  at the nth time
step.
then based on the chain rule, the gradient of loss function
with respect to the ith network parameter vi can be calculated
as

nt 
x
dl dpn dg
dl
=
dvi
dpn dg dvi
n=0

=

nt 
x


(g0i pnâˆ’1 )t Î´ n .

(18)

n=1

here g0i is the same as the (15) in the appendix a, pnâˆ’1
denotes the forwarding modeling wavefield bâˆ’1 s at the
(n âˆ’ 1)th time step, which agrees with the one-time-step
delay caused by the subdiagonal matrix (14). in other
words, forward-propagating through the swinet gives bâˆ’1 s
and backward-propagating the data error gives bâˆ’t , then
cross-correlating both wavefields according (18) is identical
to its theorical value shown in (13). therefore, adopting the
automatic differentiation steps to calculate the gradient leads
to the same results as in the traditional seismic waveform
inversion using the adjoint state method.
references
[1] c. bunks, f. m. saleck, s. zaleski, and g. chavent, â€˜â€˜multiscale seismic waveform inversion,â€™â€™ geophysics, vol. 60, no. 5, pp. 1457â€“1473,
sep. 1995.
[2] r. g. pratt, â€˜â€˜seismic waveform inversion in the frequency domain, part 1:
theory and verification in a physical scale model,â€™â€™ geophysics, vol. 64,
no. 3, pp. 888â€“901, may 1999.
[3] z. pan, j. wu, z. gao, and j. gao, â€˜â€˜adaptive differential evolution by
adjusting subcomponent crossover rate for high-dimensional waveform
inversion,â€™â€™ ieee geosci. remote sens. lett., vol. 12, no. 6, pp. 1327â€“1331,
jun. 2015.
[4] s. qu, e. verschuur, and y. chen, â€˜â€˜fwi/jmi with an automatic directional
total variation constraint,â€™â€™ geophysics, vol. 84, no. 2, pp. 175â€“183, 2019.
[5] l. mosser, o. dubrule, and m. j. blunt, â€˜â€˜stochastic seismic waveform
inversion using generative adversarial networks as a geological prior,â€™â€™
math. geosci., vol. 52, no. 1, pp. 53â€“79, jan. 2020.
[6] p. lailly, â€˜â€˜the seismic inverse problem as a sequence of before stack
migrations,â€™â€™ in conference on inverse scattering: theory and application.
philadelphia, pa, usa: siam, 1983, pp. 206â€“220.
[7] a. tarantola, â€˜â€˜inversion of seismic reflection data in the acoustic approximation,â€™â€™ geophysics, vol. 49, no. 8, pp. 1259â€“1266, aug. 1984.
[8] r.-e. plessix, â€˜â€˜a review of the adjoint-state method for computing the
gradient of a functional with geophysical applications,â€™â€™ geophys. j. int.,
vol. 167, no. 2, pp. 495â€“503, nov. 2006.
112275

y. ren et al.: physics-based neural-network way to perform seismic fwi

[9] q. zhang, w. mao, h. zhou, h. zhang, and y. chen, â€˜â€˜hybrid-domain
simultaneous-source full waveform inversion without crosstalk noise,â€™â€™
geophys. j. int., vol. 215, no. 3, pp. 1659â€“1681, dec. 2018.
[10] y. lecun, y. bengio, and g. hinton, â€˜â€˜deep learning,â€™â€™ nature, vol. 521,
pp. 436â€“444, may 2015.
[11] a. kendall and y. gal, â€˜â€˜what uncertainties do we need in bayesian deep
learning for computer vision?â€™â€™ in proc. adv. neural inf. process. syst.,
2017, pp. 5574â€“5584.
[12] a. voulodimos, n. doulamis, a. doulamis, and e. protopapadakis, â€˜â€˜deep
learning for computer vision: a brief review,â€™â€™ comput. intell. neurosci.,
vol. 2018, pp. 1â€“13, 2018.
[13] r. collobert and j. weston, â€˜â€˜a unified architecture for natural language
processing: deep neural networks with multitask learning,â€™â€™ in proc. 25th
int. conf. mach. learn. (icml), 2008, pp. 160â€“167.
[14] w. yin, k. kann, m. yu, and h. schÃ¤tze, â€˜â€˜comparative study of cnn and
rnn for natural language processing,â€™â€™ 2017, arxiv:1702.01923. [online].
available: http://arxiv.org/abs/1702.01923
[15] t. young, d. hazarika, s. poria, and e. cambria, â€˜â€˜recent trends in deep
learning based natural language processing,â€™â€™ ieee comput. intell. mag.,
vol. 13, no. 3, pp. 55â€“75, aug. 2018.
[16] l. huang, x. dong, and t. e. clee, â€˜â€˜a scalable deep learning platform for
identifying geologic features from seismic attributes,â€™â€™ lead. edge, vol. 36,
no. 3, pp. 249â€“256, mar. 2017.
[17] g. zhang, z. wang, and y. chen, â€˜â€˜deep learning for seismic lithology
prediction,â€™â€™ geophys. j. int., pp. 1368â€“1387, aug. 2018.
[18] s. lameri, f. lombardi, p. bestagini, m. lualdi, and s. tubaro, â€˜â€˜landmine
detection from gpr data using convolutional neural networks,â€™â€™ in proc.
25th eur. signal process. conf. (eusipco), aug. 2017, pp. 508â€“512.
[19] b. liu, q. guo, s. li, b. liu, y. ren, y. pang, x. guo, l. liu,
and p. jiang, â€˜â€˜deep learning inversion of electrical resistivity data,â€™â€™
ieee trans. geosci. remote sens., early access, feb. 11, 2020, doi:
10.1109/tgrs.2020.2969040.
[20] f. wang, s. chen, and y. liu, â€˜â€˜deep learning for gravity and magnetic data
interpolation,â€™â€™ in proc. seg tech. program expanded abstr., aug. 2019,
pp. 2533â€“2537.
[21] h. li, w. yang, and x. yong, â€˜â€˜deep learning for ground-roll noise
attenuation,â€™â€™ in proc. seg tech. program expanded abstr., aug. 2018,
pp. 1981â€“1985.
[22] d. liu, w. wang, w. chen, x. wang, y. zhou, and z. shi, â€˜â€˜random-noise
suppression in seismic data: what can deep learning do?â€™â€™ in proc. seg
tech. program expanded abstr., aug. 2018, pp. 2016â€“2020.
[23] s. yu, j. ma, and w. wang, â€˜â€˜deep learning for denoising,â€™â€™ geophysics,
vol. 84, no. 6, pp. v333â€“v350, nov. 2019.
[24] o. saad and y. chen, â€˜â€˜deep denoising autoencoder for seismic random
noise attenuation,â€™â€™ geophysics, vol. 85, no. 4, doi: 10.1190/geo2019â€“
0468.1.2020.
[25] y. chen, â€˜â€˜automatic microseismic event picking via unsupervised
machine learning,â€™â€™ geophys. j. int., vol. 212, no. 1, pp. 88â€“102, jan. 2018.
[26] h. wu, b. zhang, f. li, and n. liu, â€˜â€˜semiautomatic first-arrival picking of
microseismic events by using the pixel-wise convolutional image segmentation method,â€™â€™ geophysics, vol. 84, no. 3, pp. v143â€“v155, may 2019.
[27] w. zhu and g. c. beroza, â€˜â€˜phasenet: a deep-neural-network-based
seismic arrival time picking method,â€™â€™ geophys. j. int., pp. 261â€“273,
oct. 2018.
[28] a. u. waldeland, a. c. jensen, l.-j. gelius, and a. h. s. solberg, â€˜â€˜convolutional neural networks for automated seismic interpretation,â€™â€™ lead.
edge, vol. 37, no. 7, pp. 529â€“537, jul. 2018.
[29] h. di, d. gao, and g. alregib, â€˜â€˜developing a seismic texture analysis
neural network for machine-aided seismic pattern recognition and classification,â€™â€™ geophys. j. int., vol. 218, no. 2, pp. 1262â€“1275, aug. 2019.
[30] m. araya-polo, j. jennings, a. adler, and t. dahlke, â€˜â€˜deep-learning
tomography,â€™â€™ lead. edge, vol. 37, no. 1, pp. 58â€“66, 2018.
[31] y. wu, y. lin, and z. zhou, â€˜â€˜inversionnet: accurate and efficient seismic
waveform inversion with convolutional neural networks,â€™â€™ in proc. seg
tech. program expanded abstr., 2018, pp. 2096â€“2100.
[32] s. li, b. liu, y. ren, y. chen, s. yang, y. wang, and p. jiang, â€˜â€˜deeplearning inversion of seismic data,â€™â€™ ieee trans. geosci. remote sens.,
vol. 58, no. 3, pp. 2135â€“2149, oct. 2020.
[33] r. biswas, m. k. sen, v. das, and t. mukerji, â€˜â€˜pre-stack inversion
using a physics-guided convolutional neural network,â€™â€™ in proc. seg tech.
program expanded abstr., aug. 2019, pp. 4967â€“4971.
[34] j. sun, z. niu, k. a. innanen, j. li, and d. o. trad, â€˜â€˜a theory-guided deeplearning formulation and optimization of seismic waveform inversion,â€™â€™
geophysics, vol. 85, no. 2, pp. r87â€“r99, mar. 2020.
112276

[35] a. richardson, â€˜â€˜seismic full-waveform inversion using deep learning
tools and techniques,â€™â€™ 2018, arxiv:1801.07232. [online]. available:
http://arxiv.org/abs/1801.07232
[36] m. abadi, â€˜â€˜tensorflow: large-scale machine learning on heterogeneous distributed systems,â€™â€™ 2016, arxiv:1603.04467. [online]. available:
http://arxiv.org/abs/1603.04467
[37] a. paszke, â€˜â€˜pytorch: an imperative style, high-performance deep learning
library,â€™â€™ in proc. adv. neural inf. process. syst., 2019, pp. 8024â€“8035.
[38] c. cerjan, d. kosloff, r. kosloff, and m. reshef, â€˜â€˜a nonreflecting
boundary condition for discrete acoustic and elastic wave equations,â€™â€™
geophysics, vol. 50, no. 4, pp. 705â€“708, apr. 1985.
[39] c. shin, â€˜â€˜sponge boundary condition for frequencyâ€“domain modeling,â€™â€™
geophysics, vol. 60, no. 6, pp. 1870â€“1874, nov. 1995.
[40] n. shah, m. warner, t. nangoo, a. umpleby, i. stekl, j. morgan, and
l. guasch, â€˜â€˜quality assured full-waveform inversion: ensuring starting model adequacy,â€™â€™ in proc. seg tech. program expanded abstr.,
sep. 2012, pp. 1â€“5.
[41] k. jiao, d. sun, x. cheng, and d. vigh, â€˜â€˜adjustive full waveform
inversion,â€™â€™ in proc. seg tech. program expanded abstr., aug. 2015,
pp. 1091â€“1095.
[42] y. chen, h. chen, k. xiang, and x. chen, â€˜â€˜geological structure guided
well log interpolation for high-fidelity full waveform inversion,â€™â€™ geophys.
j. int., vol. 207, no. 2, pp. 1313â€“1331, nov. 2016.
[43] j. f. claerbout, earth soundings analysis: processing versus inversion,
vol. 6. london, u.k.: blackwell scientific, 1992.
[44] m. louboutin, p. witte, m. lange, n. kukreja, f. luporini, g. gorman,
and f. j. herrmann, â€˜â€˜full-waveform inversion, part 2: adjoint modeling,â€™â€™
lead. edge, vol. 37, no. 1, pp. 69â€“72, jan. 2018.
[45] z.-d. zhang and t. alkhalifah, â€˜â€˜regularized elastic full-waveform inversion using deep learning,â€™â€™ geophysics, vol. 84, no. 5, pp. r741â€“r751,
sep. 2019.
[46] h. liu, r. ding, l. liu, and h. liu, â€˜â€˜wavefield reconstruction methods
for reverse time migration,â€™â€™ j. geophys. eng., vol. 10, no. 1, feb. 2013,
art. no. 015004.
[47] e. b. raknes and w. weibull, â€˜â€˜efficient 3d elastic full-waveform inversion using wavefield reconstruction methods,â€™â€™ geophysics, vol. 81, no. 2,
pp. 45â€“55, mar. 2016.
[48] m. vasmel and j. o. a. robertsson, â€˜â€˜exact wavefield reconstruction on
finite-difference grids with minimal memory requirements,â€™â€™ geophysics,
vol. 81, no. 6, pp. 303â€“309, nov. 2016.
[49] x. glorot and y. bengio, â€˜â€˜understanding the difficulty of training deep
feedforward neural networks,â€™â€™ in proc. 13th int. conf. artif. intell. statist.,
2010, pp. 249â€“256.

yuxiao ren received the bachelorâ€™s degree in
mathematics from shandong university, china,
in 2014, and the masterâ€™s degree in mathematics
from loughborough university, u.k., in 2015.
he is currently pursuing the ph.d. degree in civil
engineering with shandong university. he is also
a visiting scholar with the georgia institute of
technology under the supervision of prof. felix
herrmann. his research interests include seismic
modeling and imaging, full-waveform inversion,
and deep-learning-based geophysical inversion.

xinji xu was born in jining, shandong, china,
in 1990. he received the b.s. degree in city underground engineering and the ph.d. degree from
shandong university, jinan, shandong, in 2012
and 2017, respectively. since 2017, he has been
working as an assistant research fellow with the
geotechnical and structural engineering research
center, shandong university. his research interests include advance geological forecast of tunnel
and underground engineering, seismic data processing, and seismic imaging and inversion.
volume 8, 2020

y. ren et al.: physics-based neural-network way to perform seismic fwi

senlin yang received the bachelorâ€™s degree
in engineering from shandong university, china,
in 2017, where he is currently pursuing the
ph.d. degree. his research interests include deeplearning-based geophysical data processing and
inversion.

lichao nie received the ph.d. degree from the
geotechnical and structural engineering research
center, shandong university, china, in 2014.
he is currently an associate professor with the
school of civil engineering, shandong university. he is mainly engaged in geophysical forward and inversion theory and method, geological forward-prospecting method and technology in tunnels, and tunnel boring machine
with forward-prospecting system as well as its
engineering application, and so on.

volume 8, 2020

yangkang chen received the b.s. degree in
exploration geophysics from the china university of petroleum, beijing, in 2012, and the ph.d.
degree in geophysics from the university of texas
at austin, in 2015, under the supervision of prof.
sergey fomel. he joined with the oak ridge
national laboratory, as a distinguished postdoctoral research associate, where he conducted
research on global adjoint tomography. he is currently an assistant professor with zhejiang university. he has published more than 100 internationally renowned journal
articles and more than 80 international conference papers. his research
interests include seismic signal analysis, seismic modeling and inversion,
simultaneous-source data deblending and imaging, global adjoint tomography, and machine learning.

112277



robust f ull waveform i nversion with deep h essian
deblurring

arxiv:2403.17518v1 [physics.geo-ph] 26 mar 2024

a p reprint
mustafa alfarhan
kaust
thuwal, kingdom of saudi arabia
mustafa.alfarhan@kaust.edu.sa

matteo ravasi
kaust
thuwal, kingdom of saudi arabia
matteo.ravasi@kaust.edu.sa

fuqiang chen
kaust
thuwal, kingdom of saudi arabia
fuqiang.chen@kaust.edu.sa

tariq alkhalifah
kaust
thuwal, kingdom of saudi arabia
tariq.alkhalifah@kaust.edu.sa

march 27, 2024

a bstract
full waveform inversion (fwi) is a technique widely used in geophysics to obtain high-resolution
subsurface velocity models from waveform seismic data. due to its large computation cost, most
flavors of fwi rely only on the computation of the gradient of the loss function to estimate the update
direction, therefore ignoring the contribution of the hessian. depending on the level of computational
resources one can afford, an approximate of the inverse of the hessian can be calculated and used to
speed up the convergence of fwi towards the global (or a plausible local) minimum. in this work,
we propose to use an approximate hessian computed from a linearization of the wave-equation as
commonly done in least-squares migration (lsm). more precisely, we rely on the link between a
migrated image and a doubly migrated image (i.e., an image obtained by demigration-migration of
the migrated image) to estimate the inverse of the hessian. however, instead of using non-stationary
compact filters to link the two images and approximate the hessian, we propose to use a deep neural
network to directly learn the mapping between the fwi gradient (output) and its hessian (blurred)
counterpart (input). by doing so, the network learns to act as an approximate inverse hessian: as such,
when the trained network is applied to the fwi gradient, an enhanced update direction is obtained,
which is shown to be beneficial for the convergence of fwi. the weights of the trained (deblurring)
network are then transferred to the next fwi iteration to expedite convergence. we demonstrate the
effectiveness of the proposed approach on two synthetic datasets and a field dataset.
keywords full waveform inversion Â· imaging Â· born modeling Â· deep learning Â· neural networks

1

introduction

seismic data, commonly acquired at the surface of the earth, contain useful information about the earthâ€™s structure.
seismic waves travel in the subsurface, interact with it, and some of them eventually return to the surface to be recorded
by an array of receivers. however, inferring the subsurface structure and its medium properties from seismic data
is a challenging task. this difficulty arises mostly from the fact that the subsurface is accessed from only one side,
and therefore some areas are not covered well by the recorded waves, or in other words, they are poorly illuminated.
moreover, multiply scattered waves (i.e., free-surface and internal multiples) interfere with primary arrivals, which
present a direct linear link with the subsurface properties of interest. as a consequence, geophysical research over
the years has been focused on developing methods that can separate different parts of the seismic data and optimally
utilize them to image the subsurface. full waveform inversion (fwi) was introduced by lailly and bednar [1983],

a deep inverse hessian for fwi

a p reprint

tarantola [1984] as an alternative approach that aims to utilize the entire seismic waveform data at once to construct
a high-resolution subsurface model. more precisely, fwi casts the problem of transforming seismic records into a
subsurface model as an optimization problem, where a measure of the difference between modeled and observed data is
minimized. given an initial subsurface model, obtained, for example, through traveltime tomography bording et al.
[1987] or migration velocity analysis symes [2008], synthetic seismic data are calculated with a wave-equation based
forward modeling engine. the data misfit between modeled and observed data is then utilized to update the model
parameters and the process is repeated until convergence.
as a consequence of its pursuit in utilizing the entire seismic data content at once, fwi is known to be a highly non-linear
inverse problem. whether or not fwi converges to the right model depends on the initial model: more specifically,
if the travel-time difference between the modeled and observed data is more than half a cycle, we will converge to a
local minimum model, a problem known as cycle-skipping. to mitigate cycle-skipping, fwi is usually performed in a
multi-scale fashion bunks et al. [1995], miller and willsky [1996], fichtner et al. [2013], alkhalifah [2016], zhang
et al. [2019] where the inversion is applied to the low-frequency components of the data first. only after the kinematic
part of the subsurface model is correctly inverted for to ensure that we are within the half-cycle difference for the
higher frequencies that the high-frequency components of the data are incorporated to add high-resolution features
to the inverted model. although there exist both local lailly and bednar [1983], tarantola [1984] and global jin and
madariaga [1994] approaches to solving fwi, the former is usually preferred: each model update involves calculating
the first and (possibly) second derivatives of the data-misfit with respect to the current model, which are called the
gradient and the hessian, respectively. whilst the gradient provides the direction of the model update, the inverse of the
hessian alters the direction slightly in compliance with the curvature information and introduces a parameter-dependent
scaling to the gradient. from a physical standpoint, the inverse of the hessian accounts for losses in amplitude due to
geometrical spreading and transmission losses occurring in the wave equation, as well as it provides a trade-off between
different model parameters in multi-parameter fwi alkhalifah [2016]. however, calculating the inverse of the hessian
for large subsurface models is computationally expensive.
traditionally, gradient-based methods have been popularized in fwi applications as they only rely on the gradient
for the model update calculation. among gradient-based methods, the most popular ones are the steepest-descent
method, whose update is given by a scaled version of the negative of the gradient, and the non-linear conjugate-gradient
method, whose update is a combination of the gradient at the current iteration and the previous update. on the other
hand, second-order methods such as the newton method rely on the hessian in addition to the gradient to define the
model update. a special case, when second-order derivative terms in the hessian (which account for second-order
scattering) are neglected, is represented by the gauss-newton (gn) method pratt et al. [1998]. different strategies
have emerged over the years to approximate the gn hessian. for example, the gradient can be preconditioned by the
diagonal elements of the hessian or the pseudo-hessian as proposed by shin et al. [2001]. on the other hand, the model
and gradient from previous iterations can be used to iteratively update the inverse of the hessian, as done in the family
of quasi-newton methods such as the limited-memory bfgs algorithm liu and nocedal [1989]. another alternative to
estimating the hessian is the family of truncated-newton methods, which do not need the hessian explicitly but rather
compute the hessian-vector product mÃ©tivier et al. [2014], altheyab et al. [2013]. an equivalently accurate approach
to approximate the gn hessian, and is the inspiration to the approach developed here, is a least-squares optimization of
the gradient wu and alkhalifah [2015]. in general, the inclusion of the approximate inverse of the hessian provides
faster convergence than gradient-based methods.
seismic imaging is another area where approximating the hessian is of great importance as the resolution of the
resulting seismic images can be enhanced and uneven illumination resulting from imperfect acquisition geometries can
be mitigated. more specifically, least-squares migration (lsm) is a technique that aims to mitigate the degrading
effects of the hessian. in the image domain, the hessian effect can be considered a form of blurring operator. therefore,
lsm can be formulated as an image deblurring problem where the inverse of the hessian is the deblurring operator.
using a migrated and doubly migrated image, the inverse of the hessian can be approximated with non-stationary
matching filters as done in guitton [2004]. similarly, aoki and schuster [2009] use deblurring filters to accelerate lsm
in a regularized or preconditioning fashion. alternatively, as initially proposed by valenciano [2008], one can rely on
point spread functions (psfs) to construct the deblurring filters. in recent years, various deep learning methods have
been proposed for image deblurring (which are equivalently applicable to the problem of approximating the action of
the inverse of the hessian matrix). liu et al. [2022] suggested to employ a convolutional neural network to learn the
deconvolution operator of the psfs in least-squares reverse time migration (lsrtm). kumar et al. [2022] uses a neural
network to learn the inverse of the hessian using the original migration image and a remigrated image. finally, peng
et al. [2023] suggest to use a loop-unrolled gradient descent method to include the contribution of the known psf in the
learning process. in all studies, it was concluded that using a network to approximate the action of the inverse of the
hessian is faster than other state-of-the-art methods and can more stably deconvolve seismic images.

2

a deep inverse hessian for fwi

a p reprint

in this work, we propose to embed a similar approach to that of kumar et al. [2022] within fwi. more specifically,
born modeling and its adjoint are used to create our hessian blurred gradient. in this case, the hessian blurred gradient
is approximately related to the fwi gradient through the inverse of the hessian, whose mapping is learned using a
deep neural network. after training, the network is applied to the gradient to obtain a hessian corrected update that we
demonstrate can speed up fwi convergence. we apply our approach to two synthetic data to showcase its benefits, and
to the volve field data to validate its applicability under field conditions.

2

theoretical framework

2.1

full waveform inversion

the constant-density acoustic wave equation is given by
1 âˆ‚ 2 u(x, t)
âˆ’ âˆ‡2 u(x, t) = f (x, t)
m2 (x) âˆ‚t2

(1)

where m(x) is the medium velocity, u(x, t) is the wavefield, f (x, t) is the source function, all defined in space x and
time t, and âˆ‡2 is the laplacian operator. starting from an initial velocity model, the wave equation is solved using a
forward modeling scheme such as the finite difference method to obtain the simulated data, ds . then, the simulated
data are compared to the observed data, do , using a misfit function such as the l2 norm:
j(m) =

1
âˆ†dt âˆ†d
2

(2)

where âˆ†d = do âˆ’ ds . note that in the above equation we have expressed both datasets and the model in vector form.
from now on, the use of regular or bold symbols will discriminate whether we are considering a scalar or a vector for
our equations.
using the generalized taylor series expansion around a background model mo , the misfit function can be approximated
by

t
1
âˆ‚j(mo )
âˆ‚ 2 j(mo )
j(m) â‰ˆ j(mo ) +
(m âˆ’ mo ) + (m âˆ’ mo )t
(m âˆ’ mo )
(3)
âˆ‚m
2
âˆ‚m2
taking the derivative of equation (3) with respect to Î´m = m âˆ’ mo and setting it to zero yields
 2
âˆ’1
âˆ‚ j(mo )
âˆ‚j(mo )
Î´m = âˆ’
(4)
âˆ‚m2
âˆ‚m
âˆ’1
where Î´m is the model update, âˆ‚j(mo )/âˆ‚m is the gradient, and âˆ‚ 2 j(mo )/âˆ‚m2
is the inverse of the hessian.
fwi is an optimization process where the current model m is iteratively updated by generating the data ds and
comparing them against the observed data to compute the model update Î´m accordingly until the data residual is
sufficiently small.
2.2

born modeling

in born modeling, the model m(x) is commonly split into two parts: background model and perturbation. the
background model is often smooth, while the perturbation may lead to scattering in the wavefield. therefore, the
velocity m(x) can be written as a combination of a background velocity mo (x) and a perturbation Î´m(x)
m(x) = mo (x) + Î´m((x)

(5)

in addition, we can approximate 1/m2 (x) using taylor series as
1
1
Î´m(x)
â‰ˆ 2
âˆ’2 3
m2 (x)
mo (x)
mo (x)

(6)

similarly, the seismic wavefield u(x, t) can be split into a background wavefield uo (x, t), satisfying the wave equation
for mo (x) and a scattering wavefiled Î´u(x, t) such that
u(x, t) = uo (x, t) + Î´u(x, t)
3

(7)

a deep inverse hessian for fwi

a p reprint

substituting equations (6) and (7) into equation (1) yields
1
m2o (x)

1 âˆ‚ 2 Î´u(x, t)
âˆ‚ 2 uo (x, t)
âˆ’ âˆ‡uo (x, t) + 2
2
âˆ‚t
mo (x)
âˆ‚t2

Î´m(x) âˆ‚ 2 (uo (x, t) + Î´u(x, t))
= f (x, t) (8)
m3o (x)
âˆ‚t2
which can be split into the background and the scattering parts, respectively:
âˆ’ âˆ‡Î´u(x, t) âˆ’ 2

1 âˆ‚ 2 uo (x, t)
âˆ’ âˆ‡uo (x, t) = f (x, t)
m2o (x)
âˆ‚t2

(9)

1 âˆ‚ 2 Î´u(x, t)
Î´m(x) âˆ‚ 2 (uo (x, t) + Î´u(x, t))
âˆ’
âˆ‡Î´u(x,
t)
=
2
(10)
m2o (x)
âˆ‚t2
m3o (x)
âˆ‚t2
equation (10) is implicit since both the left- and right-hand sides depends on the scattering wavefield. to make it explicit,
we use the born approximation, which assumes that the scattered wavefield is small compared to the background
wavefield,
1 âˆ‚ 2 Î´u(x, t)
Î´m(x) âˆ‚ 2 uo (x, t)
âˆ’
âˆ‡Î´u(x,
t)
=
2
(11)
m2o (x)
âˆ‚t2
m3o (x)
âˆ‚t2
consequently, born modeling can be performed by first modeling the background wavefield uo (x, t) in the background
model mo (x) with source function f (x, t) using equation (9). this wavefield is then used to ignite a secondary source
in equation (10); in other words, a second step of modeling is therefore performed in the background model to compute
the scattered wavefield using this secondary source. note that, in practice, equations (9) and (10) are jointly solved so
that there is no need to store the entire background wavefield from the first modeling to be injected in the second one.
in compact notation, we can define the linear born modeling as
Î´u = lÎ´m

(12)

with Î´m = Î´m/mo . finally, an additional sampling operator p is used to extract the scattered wavefield at the location
of the receivers and produce the surface seismic data:
d = plÎ´m = lÌƒÎ´m

(13)

where, for simplicity, we define lÌƒ from here onwards as the born modeling operator.
2.3

approximating the inverse of the hessian for fwi with deep learning

the full model update for fwi as shown in equation (4) requires the calculation of the gradient and the inverse of the
hessian. however, the calculation of the inverse of the hessian is highly expensive: this has led to the development
of methods to estimate an approximate version of such an inverse with affordable computational cost. in this work,
we propose a method to approximate the hessian that relies on deep learning. the full hessian is composed of two
terms: a single-scattering and a double-scattering terms. in the gauss-newton method, the second term is neglected
and only the first term is used. multiplying the gradient by the inverse of the hessian can be viewed as a deblurring
operation that corrects the gradient for the effects of the earth such as geometric spreading. we present an approach for
approximating the inverse of the hessian for fwi in the following sub-section.
2.3.1

the born modeling and its adjoint approach

one way to approximate the hessian is via a migration/demigration approach, or, after linearizing fwi, born modeling
and its adjoint. the gradient of fwi can be summarized as follows
g = lÌƒt âˆ†d

(14)

where lÌƒt is the adjoint of the born modeling operator, and âˆ†d is the data residual. moreover, considering the true
earth perturbation is Î´m, âˆ†d = lÌƒÎ´m, equation (14) can be equivalently written as
g = lÌƒt lÌƒÎ´m

(15)

which reveals that the gradient g and the true perturbation Î´m are linked via a pair of born modeling and its adjoint
(demigration/migration) operations. a blurred gradient (referred in this work as doubly migrated image) can be obtained
from the gradient by applying once again born modeling and its adjoint
Î´m1 = lÌƒt lÌƒg
4

(16)

a deep inverse hessian for fwi

a p reprint

figure 1: proposed method to approximate the inverse of the hessian using deep learning.

given availability of both g and Î´m1 , one can approximate (lÌƒt lÌƒ)âˆ’1 (the inverse of the hessian) and apply it to m to
obtain an estimate of Î´m, which is the objective of lsm. in our approach, m is the fwi gradient, g. in other words, by
estimating and applying the inverse of the hessian to the fwi gradient, we aim to retrieve an approximation of the
scattering potential as used in born modeling.
in this work, we suggest to train a neural network to learn the mapping from Î´m1 to g, which ultimately represents
the action of the inverse of the hessian on a vector. also, since the fwi update is related to the gradient through the
hessian (see equation (4)), the trained network can be applied to the gradient to obtain an improved fwi update; this
can ultimately lead to faster convergence of the fwi process. the same process is repeated at every iteration, except for
the fact that the neural network is not trained from scratch: instead, we leverage transfer learning and start from the
weights learned in the previous iteration. the proposed method is depicted in figure 1.
2.4

neural-network based inverse hessian approximation

in this work, we suggest to train a neural network to approximate the inverse of the hessian in fwi. given that learning
the action of the inverse of the hessian can be framed as a domain translation process (from the doubly migrated to
the migrated image), our network presents a unet-like architecture ronneberger et al. [2015]. more specifically, the
network architecture is composed of four blocks in both the encoder and decoder parts. the input is passed to an
input block which consists of a double convolution layer which itself contains two sequences of operations. each
sequence involves a 2d convolution with a kernel size of 3 Ã— 3, stride of 1, and padding of 1, followed by a leaky
relu activation function with a negative slope of 0.2. the first convolutional layer transforms the input channel to
128 channels, and the second maintains these 128 channels. there are four downsampling blocks in the encoder, each
consisting of an average pooling operation with a 3 Ã— 3 kernel and a stride of 2 followed by a double convolution block
(similar to the input block). these blocks progressively increase the number of channels while reducing the spatial
dimensions of the feature maps. the channel sizes double at each block, going from 128 to 256, then to 512, 1024, and
finally 2048. corresponding to the downsampling blocks, there are four upsampling blocks in the decoder. each block
starts with a transposed 2d convolution which halves the number of channels, and is followed by a double convolution
block (similar structure as the encoder). these blocks incrementally reduce the number of channels while increasing the
spatial dimensions of the feature maps, effectively recovering the spatial resolution that was lost during downsampling.
the final part of the network is an output convolution layer. it consists of a convolution layer with a kernel size of 1 Ã— 1,
used to transform the 128 channels from the last upsampling block to the desired number of output channels (in this
case, 1 channel). the network architecture is summarized in table 1 in appendix a. the adam optimizer is used for
optimizing the networkâ€™s parameters with a cosine annealing scheduler for the learning rate. the mse loss function is
used to measure the misfit between the networkâ€™s output and the fwi gradient. the training process can be carried out
5

a deep inverse hessian for fwi

a p reprint

using patches or the full image at once. whilst both approaches have similar performance, we chose the latter because
it makes training faster.

3

numerical examples

in this section, we test the approach on two synthetic examples and one field data used to validate the proposed
methodology.
3.1

marmousi model

due to its intricate geological composition, the synthetic marmousi model brougois et al. [1990] serves as a benchmark
for assessing algorithms in seismic imaging and inversion. the model used in this example spans 601 grid points
horizontally and 221 grid points vertically, with a grid size of 15 m in both directions. figure 2 illustrates both the
marmousi model and its smoothed counterpart, utilized as the starting model for fwi. the observed data is modeled
using a variable-velocity, constant-density acoustic finite-difference modeling engine, using 30 sources and 300 receivers
equally spaced at zero depth. a ricker wavelet of 5 hz peak frequency is used for the source signature.
to begin with, we consider the first iteration of the fwi process to detail the proposed approach. starting from the fwi

figure 2: a) the true marmousi model and b) its smoothed version.

figure 3: the components of our approach: a) the fwi gradient, b) Î´m1 , c) the predicted gradient by the network, and
d) Î´m.
gradient (figure 3a), we apply born modeling and its adjoint to obtain the doubly-migrated image Î´m1 in figure 3b.
comparing the gradient to the doubly migrated image, we can see that the latter is of lower resolution and presents a
stronger amplitude decay over depth; this is expected as the effect of geometrical spreading is accumulated multiple
times as part of the born modeling and its adjoint. figure 3c shows the network output after training. this result
highlights that the model has learned to map Î´m1 to the gradient. also, we show the normalized training loss for the
6

a deep inverse hessian for fwi

a p reprint

figure 4: the normalized training loss for the first three iterations of fwi.
first three iterations of fwi in figure 4 which highlights that the network is learning to map Î´m1 to the gradient. note
that in the first fwi iteration the network is trained for 1000 iterations, but we only show the first 300 iterations just
to make the plot more presentable. in addition, we should point out that the fitting of the gradient is not perfect (e.g.,
there is an amplitude loss) which could be a result of not having enough information to go from Î´m1 to the gradient.
comparing the networkâ€™s gradient to the fwi gradient we can say that the network has learned an approximation of the
inverse hessian. thus, if we apply the network to the fwi gradient, we get a hessian corrected update as shown in
figure 3d. for certain, we do not expect the network to learn the exact inverse hessian due to the linear link between
the gradient and Î´m1 whilst the fwi inverse hessian is non-linear. not surprisingly, the update presents more balanced
contributions throughout the entire depth axis and at the edges of the horizontal axis compared to the gradient; in other
words, the effect of the acquisition geometry and geometrical spreading has been compensated for by the network.
to demonstrate the effectiveness of our approach, we compare its speed of convergence with that of a gradient-based
method with barzilai-borwein (bb) update barzilai and borwein [1988] and a quasi-newton method, namely lbfgs liu and nocedal [1989]. the bb method is a two-point step size gradient method used to approximate the
quasi-newton method. more precisely, the hessian is approximated with a scalar using the difference between two
evaluations of the gradient and two solutions
Î±klong =

âˆ†x Â· âˆ†x
âˆ†x Â· âˆ†g

(17)

Î±kshort =

âˆ†x Â· âˆ†g
âˆ†g Â· âˆ†g

(18)

where âˆ†x and âˆ†g are the differences between two solutions and two gradients, respectively. in our implementation we
used Î±short . for nonlinear problems, the bb method tends to converge faster than the conjugate-gradient methods as
demonstrated in fletcher [2005].
in order to provide a fair comparison in terms of computational cost for the different algorithms, the data misfit loss is
plotted as function of the the number of wave equation solves (instead of the iteration itself). in our approach, in fact,
every fwi iteration requires solving the wave equation twice more than in the conventional approaches. alongside
inspecting the data misfit loss, we also assess the quality of the updated model with respect to the true model (whenever
possible) using the structural similarity index measure (ssim) wang et al. [2004]; this choice is dictated by the fact that
a small vertical (or horizontal) shift in the predicted model could lead to a small element-wise loss (e.g., mean squared
error), even if the model closely resembles the true one.
as shown in figure 5 for the data loss and the ssim index, the rate of convergence as function of the number of wave
equations solved is higher for the proposed approach than for the bb and l-bfgs methods. looking at the models
after the first iteration of fwi, we observe that the predicted model from our method (figure 6b) is already of higher
quality than that obtained from the bb method (figure 6a). the shallow layers are better defined, and even the deeper
parts of the model (at a depth of 3000 m or greater) are already updated. moreover, looking at the final predicted models
for the bb, l-bfgs, and Î´m methods, respectively (figure 7), it is clear that our method managed to better update
the deeper part of the subsurface. this is expected when the hessian is included in the model update as of the case
with l-bfgs and Î´m, but Î´m resolved the deeper part better when comparing the two approaches to the true model.
7

a deep inverse hessian for fwi

a p reprint

figure 5: data loss and ssim index for fwi on marmousi using the bb method, l-bfgs, and Î´m.

figure 6: first model update using a) the bb method, and b) Î´m.
moreover, the update in the deeper part with Î´m did not overestimate the width of the high velocity layers looking at
the two ridges in the right and left of the deeper part of the model (depicted in dark red), for example.
3.2

volve synthetic

in our second example, we consider a synthetic dataset created to closely resemble the openly available volve field data.
this dataset has been created in ravasi et al. [2022] as follows: first, a high-resolution acoustic impedance model is
obtained by applying post-stack seismic inversion to a 2d section of the migrated volve field data. next, the inverted
acoustic impedance model is converted into a velocity model with the aid of well logs and used to simulate a synthetic
dataset with a 20 hz ricker wavelet. an array of 110 sources, spaced 50 m apart and positioned at a depth of 6 m,
along with an array of 180 receivers, spaced 25 m apart at depths ranging from 86 m to 99 m, are used to model the
data. figure 8 shows the inverted velocity model, which we refer to here as the true volve synthetic velocity model and
its smoothed version used as an initial model for fwi.
for the volve synthetic, which is more challenging given its more realistic data and model, and larger model size, we
utilized the data up to 7 hz and we estimated the wavelet from it. inspecting the data loss curves in figure 9a, our
approach converges at a similar rate as the bb method; however the ssim curves show that our methodâ€™s estimated
model is of a superior quality compared to that of the bb method (figure 9b); this is especially the case for later
iterations where the bb model seems to be stagnant, whilst our model continues to improve. even though l-bfgs
reached a similar data residual to the bb and Î´m methods at the end, the ssim shows that the inverted model started to
degrade half-way through the iterations. also, looking at the updated models at the third (we chose the third iteration
rather than the first because the difference is easier to spot in the third iteration.) and last iterations (figures 10 and 11)
using both approaches confirms that our approach resulted in a better inversion since in our approach the effect of the
limited aperture is less pronounced (if any), and between 2200 to 2700 m depth the velocity estimation is better in
general. in addition, our approach is not just a scalar approximation for the hessian (like the bb step size) which is
evident by the correction for limited aperture in the acquisition as shown in figure 11c.
8

a deep inverse hessian for fwi

a p reprint

figure 7: final updated models: a) using the bb method, b) using l-bfgs, c) using Î´m
, and d) the true model for comparison.

figure 8: a) the true volve synthetic model and b) its smoothed version. the arrays of sources and receivers are shown
in white and red, respectively.

figure 9: data loss and ssim index for fwi on the volve synthetic dataset using the bb method, l-bfgs, and Î´m.
9

a deep inverse hessian for fwi

figure 10: third model update using a) the bb method, and b) Î´m for volve synthetic.

figure 11: final model update: a) using the bb method, b) using l-bfgs, and c) using Î´m.

10

a p reprint

a deep inverse hessian for fwi

a p reprint

figure 12: a) the velocity model obtained for volve using tomography, and b) its smoothed version. the arrays of
sources and receivers are shown in red.

figure 13: data loss for fwi on the volve dataset a) using the bb method and Î´m, and b) using l-bfgs.

3.3

volve field data

in 2008, the production of oil commenced in the volve oil field located in the norwegian north sea and continued until
its shutdown in 2016. from the 3d obc survey acquired by statoil in 2010 for volve, we selected a 2d line to test
our algorithm. since we removed multiples by multi-dimensional deconvolution (mdd), the processed data has now
co-located sources and receivers at the seafloor. thus, this 2d line now contains 180 sources and receivers, positioned
at depths ranging from 86.8 m to 99.2 m, with a spacing of 25 m. in addition, we applied
other processing methods
âˆš
such as denoising, vector-fidelity corrections, anti-aliasing filtering, and a scaling by t to manage the conversion from
3d to 2d due to geometrical spreading. for a detailed description of the processing sequence, the reader is referred
to ravasi et al. [2015, 2016]. figure 12 shows a 2d section of the tomographic velocity model provided as part of the
open volve dataset along the line of sources and receivers selected for this study in red, and its smoothed version used
as starting model for fwi.
working with field data is more challenging due to factors such as unknown wavelet, insufficient frequency content in
the data, different physics from the one used in the numerical modeling which leads to events in the data that cannot
be matched/reproduced, etc. thus, after pre-processing the data to be suitable for seismic processing and imaging as
described earlier, we approximated the shape and scale of the wavelet from the data and ran multi-scale fwi for 4, 7,
and 10 hz maximum frequency as commonly done in practical applications of fwi. figure 13a shows the normalized
data loss for the three frequencies from lowest to highest for the bb method and Î´m while figure 13b is for l-bfgs.
since we are plotting three different runs of fwi and we have less control on the number of iteration when running the
l-bfgs optimization, we plotted the curves for l-bfgs in a separate plot. for 4hz and 10 hz our approach converges
faster than the bb method, whilst for 7hz it seems to be slightly slower. in contrast, l-bfgs takes more time to catch
11

a deep inverse hessian for fwi

a p reprint

figure 14: first model update: a) using the bb method, and b) using Î´m for volve.

figure 15: final model update: a) using the bb method, b) using l-bfgs, and c) using Î´m for volve.

figure 16: rtm images computed using: a) the initial model, b) the updated model with the bb method, c) the updated
model with l-bfgs, and d) the updated model with Î´m for volve.

12

a deep inverse hessian for fwi

a p reprint

figure 17: adcigs computed using the initial model at 5.0, 5.5, 6.0, 6.5, and 7.0 km in the x direction for volve.

figure 18: adcigs computed using the updated model with the bb method at 5.0, 5.5, 6.0, 6.5, and 7.0 km in the x
directionfor volve.
13

a deep inverse hessian for fwi

a p reprint

figure 19: adcigs computed using the updated model with l-bfgs at 5.0, 5.5, 6.0, 6.5, and 7.0 km in the x direction
for volve.

figure 20: adcigs computed using the updated model with Î´m at 5.0, 5.5, 6.0, 6.5, and 7.0 km in the x direction for
volve.
14

a deep inverse hessian for fwi

a p reprint

figure 21: observed data compared to simulated data computed with the model inverted a) using the bb method, b)
using l-bfgs, and c) using Î´m. first and third quarters of each panel are the simulated data while the second and
fourth quarters are the observed data.

up with the other two methods for 4 hz and does not perform as well in the other two frequency scales. note that
large increases in data loss when the frequency range is changed are expected because there is a jump in the frequency
content of the data which is not represented in the updated velocity model. figure 14 shows the first updated model for
the bb and Î´m methods: it is clear that our approach did not only provide a better update for the shallow part of the
model, but it also was able to update the deep layers. similarly, when looking at the final updated models in figure 15
for the three approaches, we can conclude that our approach has led to more significant changes in the deeper layers,
whilst at the same time the velocities of the shallower layers are not overestimated as in the bb method and l-bfgs.
to validate our observations, we finally performed imaging using both the initial and the three estimated velocity
models (figure 16). the image produced using the velocity model from the bb method presents an artifact between
around 2 âˆ’ 2.5 km depth (red arrow in figure 16b), which does not appear in the other three images. also, the image
produced from l-bfgs (figure 16c) is of lower quality as the reflectors in the deeper part are less focused. most likely,
this is because l-bfgs was more aggressive in its velocity update. lastly, comparing the image produced using our
velocity model with the one obtained from the initial velocity model, we notice that the former presents more continuous
reflectors (red arrow in figure 16d); moreover, the reflector at around 2.5 km has been slightly shifted downward. in
addition, figures 17, 18, 19, and 20 show the angle-domain common image gathers (adcigs) produced using the
same three velocity models used for imaging. the image gathers produced from the initial model are in general flat;
this suggests that the initial model is already good in terms of kinematic which is because the tomographic model is
highly accurate j. szydlik et al. [2006], but we smoothed it to mimic the scenario where such a model is not available.
however, the angle gathers for the bb method are curving down as pointed out by the red arrows in figure 18 (i.e., the
updated velocity model is not better). on the other hand, the angle gathers produced with the velocity model estimated
by our approach show similar flatness as the ones from the initial model but with more focused reflectors and better
continuity, indicated by red arrows in figure 20. as for l-bfgs, like the rtm image, the gathers are not focused
and not flat, see the red arrow in figure 19 for example. finally, figures 21a-c compares the observed data with the
simulated data using the inverted models with the bb methos, l-bfgs, and Î´m, respectively. in each panel, the first
and third quarters are the simulated data, and the second and the fourth quarters are the observed data. in general, the
bb method, and Î´m data align more accurately and resemble better the field data than l-bfgs data. however, Î´m data
has stronger events in the deeper part.
15

a deep inverse hessian for fwi

4

a p reprint

discussion

the novelty of this work stems from the fact that we embed the concept of approximating the inverse of the hessian
into a neural network, which is trained to map a hessian-adjusted gradient to the fwi gradient. the effect of the
gauss-newton hessian is added to the fwi gradient by means of born modeling and its adjoint, and the role of the
network is that of approximating the inverse of the hessian, with the goal of producing an improved update vector
at each iteration of fwi. since we expect the inverse of the hessian to slowly change from iteration to iteration, we
leverage the ability of neural networks to learn a task and adapt quickly to a similar, but slightly different task via
transfer learning. this would not be the case for standard approaches with matching filters, as they would have to be
performed from scratch at every fwi iteration. furthermore, since the inverse of the hessian can be considered as a
deblurring operator that can increase the resolution of images, the use of neural networks to approximate it is attractive
due to their success in super-resolution applications.
despite the aforementioned advantages, our method comes also with some shortcomings. first, the hessian that
links the gradient with the doubly migrated image based on born modeling and its adjoint is an approximation of the
fwi hessian. thus, the resulting approximation of the inverse of the hessian most likely does not provide as fast
convergence (which is, also, the case with most matching filters methods) as the actual inverse of the fwi hessian,
even though we expect this to be better than having no hessian at all. second, the bandwidth of the signal we are
attempting to recover (the full fwi update) is larger than what the network is provided within training. that is due to
the fact that the fwi gradient and the doubly migrated image are â€™convolvedâ€˜ by one and two wavelets, respectively.
thus, the doubly migrated image has the same frequency support of the fwi gradient or even less, and both have less
support than the sought after scattering potential. thus, even though the network has learned to produce an enhanced
version of the gradient, there is no guarantee that this can extend the frequency content beyond that present in the
training data. third, our approach presents a computational bottleneck as it requires to perform an additional step of
modeling-migration at each fwi iteration, as well as training or fine-tuning the network. applying the network to the
gradient once it is trained is fast, however the training cost is not negligible.
we have applied our method to three examples, two with synthetic datasets and one with the volve field dataset. in
general, our approach converges faster (or has a similar convergence rate) than the quasi-newton methods (bb and
l-bfgs). however, as discussed in the two volve examples, our approach resulted in a better inverted model as
assessed by the ssim index in the synthetic case or by the different imaging products in the field case. field data are
contaminated by noise and unwanted events, and thus, we do not wish to perfectly fit the data. thus, having a neural
network embedded within the fwi iteration can have a regularization effect, and thus, we might avoid fitting the noise
in the data as supported by our volve example.
finally, we have observed experimentally that our approach works best when inverting low frequency data; however,
when we start fwi from higher frequencies, our method does not perform as well as when starting from low frequencies.
nevertheless, our method is still applicable to the multi-scale fwi, provided that we train the network from scratch at
the start of each new frequency band. on the other hand, even though our approximation of the hessian entails a cost
equals to one conventional fwi iteration to obtain the remigrated image, we have a better model update from the start
as illustrated in the previous examples. thus, we can use our approximation of the hessian in the early stage of fwi to
steer the optimization problem into a better direction, and then, continue with the conventional fwi. this approach
will not only reduce the cost, but it is also more practical taking into account that the role of inverse of the hessian is
generally more important in the early iterations of fwi.

5

conclusions

we have proposed a deep learning-based approach to approximate the inverse of the hessian in the context of fwi.
our method is based on obtaining a remigrated image that is related to the gradient through the inverse of the hessian
by means of born modeling and its adjoint; subsequently, a neural network is trained to learn the mapping from the
remigrated image to the gradient; the trained network is then applied to the gradient to enhance its resolution and
compensate for illumination effects. numerical results on three different datasets show that the enhanced gradients can
lead to an fwi algorithm with faster convergence and improved inverted models.

acknowledgments
the author thanks kaust and the deepwave consortium sponsors for supporting this research, as well as equinor and
the volve license partners for releasing the field dataset.
16

a deep inverse hessian for fwi

a p reprint

references
patrick lailly and j bednar. the seismic inverse problem as a sequence of before stack migrations. 1983.
albert tarantola. inversion of seismic reflection data in the acoustic approximation. geophysics, 49(8):1259â€“1266,
1984.
r phillip bording, adam gersztenkorn, larry r lines, john a scales, and sven treitel. applications of seismic
travel-time tomography. geophysical journal international, 90(2):285â€“303, 1987.
william w symes. migration velocity analysis and waveform inversion. geophysical prospecting, 56(6):765â€“790,
2008.
carey bunks, fatimetou m saleck, s zaleski, and g chavent. multiscale seismic waveform inversion. geophysics, 60
(5):1457â€“1473, 1995.
eric lawrence miller and alan s willsky. a multiscale, statistically based inversion scheme for linearized inverse
scattering problems. ieee transactions on geoscience and remote sensing, 34(2):346â€“357, 1996.
andreas fichtner, jeannot trampert, paul cupillard, erdinc saygin, tuncay taymaz, yann capdeville, and antonio
villasenor. multiscale full waveform inversion. geophysical journal international, 194(1):534â€“556, 2013.
tariq alkhalifah. full-model wavenumber inversion: an emphasis on the appropriate wavenumber continuation.
geophysics, 81(3):r89â€“r98, 2016.
pan zhang, ru-shan wu, and liguo han. seismic envelope inversion based on hybrid scale separation for data with
strong noises. pure and applied geophysics, 176:165â€“188, 2019.
side jin and raul madariaga. nonlinear velocity inversion by a two-step monte carlo method. geophysics, 59(4):
577â€“590, 1994.
r gerhard pratt, changsoo shin, and gj hick. gaussâ€“newton and full newton methods in frequencyâ€“space seismic
waveform inversion. geophysical journal international, 133(2):341â€“362, 1998.
changsoo shin, seonghyung jang, and dong-joo min. improved amplitude preservation for prestack depth migration
by inverse scattering theory. geophysical prospecting, 49(5):592â€“606, 2001.
dong c liu and jorge nocedal. on the limited memory bfgs method for large scale optimization. mathematical
programming, 45(1-3):503â€“528, 1989.
ludovic mÃ©tivier, franÃ§ois bretaudeau, romain brossier, stÃ©phane operto, and jean virieux. full waveform inversion
and the truncated newton method: quantitative imaging of complex subsurface structures. geophysical prospecting,
62(6):1353â€“1375, 2014.
abdullah altheyab, xin wang, and gerard t schuster. time-domain incomplete gauss-newton full-waveform inversion
of gulf of mexico data. in 2013 seg annual meeting. onepetro, 2013.
zedong wu and tariq alkhalifah. simultaneous inversion of the background velocity and the perturbation in fullwaveform inversion. geophysics, 80(6):r317â€“r329, 2015. doi:10.1190/geo2014-0365.1. url https://doi.
org/10.1190/geo2014-0365.1.
antoine guitton. amplitude and kinematic corrections of migrated images for nonunitary imaging operators. geophysics, 69(4):1017â€“1024, 2004.
naoshi aoki and gerard t schuster. fast least-squares migration with a deblurring filter. geophysics, 74(6):wca83â€“
wca93, 2009.
alejandro a valenciano. imaging by wave-equation inversion. phd thesis, citeseer, 2008.
cewen liu, mengyao sun, nanxun dai, wei wu, yanwen wei, mingjie guo, and haohuan fu. deep learning-based
point-spread function deconvolution for migration image deblurring. geophysics, 87(4):s249â€“s265, 2022.
a kumar, g hampson, t rayment, and t burgess. a deep learning inverse hessian for least-squares migration. in 83rd
eage annual conference & exhibition, volume 2022, pages 1â€“5. eage publications bv, 2022.
h. peng, i. vasconcelos, and m. ravasi. image-domain seismic inversion by deblurring with invertible recurrent
inference machines. geophysics, 89(2):1â€“62, 2023.
olaf ronneberger, philipp fischer, and thomas brox. u-net: convolutional networks for biomedical image segmentation. in medical image computing and computer-assisted interventionâ€“miccai 2015: 18th international
conference, munich, germany, october 5-9, 2015, proceedings, part iii 18, pages 234â€“241. springer, 2015.
a brougois, marielle bourget, patriek lailly, michel poulet, patrice ricarte, and roelof versteeg. marmousi, model
and data. in eaeg workshop-practical aspects of seismic data inversion, pages cpâ€“108. european association of
geoscientists & engineers, 1990.
17

a deep inverse hessian for fwi

a p reprint

jonathan barzilai and jonathan m. borwein. two-point step size gradient methods. ima journal of numerical
analysis, 8(1):141â€“148, 01 1988. issn 0272-4979. doi:10.1093/imanum/8.1.141. url https://doi.org/10.
1093/imanum/8.1.141.
roger fletcher. on the barzilai-borwein method. in liqun qi, koklay teo, and xiaoqi yang, editors, optimization and
control with applications, pages 235â€“256, boston, ma, 2005. springer us. isbn 978-0-387-24255-2.
zhou wang, a.c. bovik, h.r. sheikh, and e.p. simoncelli. image quality assessment: from error visibility to structural
similarity. ieee transactions on image processing, 13(4):600â€“612, 2004. doi:10.1109/tip.2003.819861.
matteo ravasi, tamin selvan, and nick luiken. stochastic multi-dimensional deconvolution. ieee transactions on
geoscience and remote sensing, 60:1â€“14, 2022. doi:10.1109/tgrs.2022.3179626.
m. ravasi, i. vasconcelos, a. curtis, and a. kritski. multi-dimensional free-surface multiple elimination and source
deblending of volve obc data. 2015(1):cp-451-00802, 2015. issn 2214-4609. doi:https://doi.org/10.3997/22144609.201413355. url https://www.earthdoc.org/content/papers/10.3997/2214-4609.201413355.
matteo ravasi, ivan vasconcelos, alexander kritski, andrew curtis, carlos alberto da costa filho, and giovanni angelo meles. target-oriented marchenko imaging of a north sea field. geophysical journal international, 205(1):
99â€“104, 02 2016. issn 0956-540x. doi:10.1093/gji/ggv528. url https://doi.org/10.1093/gji/ggv528.
t. j. szydlik, s. way, p. smith, l. aamodt, and c. friedrich. 3d pp/ps prestack depth migration on the volve
field. art. cp-2-00185, 2006. issn 2214-4609. doi:https://doi.org/10.3997/2214-4609.201402177. url https:
//www.earthdoc.org/content/papers/10.3997/2214-4609.201402177.
matteo ravasi and ivan vasconcelos. pylopsâ€”a linear-operator python library for scalable algebra and optimization.
softwarex, 11:100361, 2020.
alan richardson. deepwave, september 2023. url https://doi.org/10.5281/zenodo.8381177.

a ppendix : n etwork architecture
table 1: summary of the network architecture
layer
input conv
downsampling 1
downsampling 2
downsampling 3
downsampling 4
upsampling 1
upsampling 2
upsampling 3
upsampling 4
output conv

operation
2 Ã— conv2d + leakyrelu
avgpool2d + 2 Ã— conv2d + leakyrelu
avgpool2d + 2 Ã— conv2d + leakyrelu
avgpool2d + 2 Ã— conv2d + leakyrelu
avgpool2d + 2 Ã— conv2d + leakyrelu
convtranspose2d + 2 Ã— conv2d + leakyrelu
convtranspose2d + 2 Ã— conv2d + leakyrelu
convtranspose2d + 2 Ã— conv2d + leakyrelu
convtranspose2d + 2 Ã— conv2d + leakyrelu
conv2d

18

channels
128
256
512
1024
2048
1024
512
256
128
1



sensors
article

encoderâ€“decoder architecture for 3d seismic inversion
maayan gelboim 1 , amir adler 1, *, yen sun 2 and mauricio araya-polo 2
1
2

*

electrical engineering department, braude college of engineering, karmiel 2161002, israel
totalenergies, ep r&t, houston, tx 77002, usa
correspondence: adleram@braude.ac.il

abstract: inverting seismic data to build 3d geological structures is a challenging task due to the
overwhelming amount of acquired seismic data, and the very-high computational load due to
iterative numerical solutions of the wave equation, as required by industry-standard tools such as
full waveform inversion (fwi). for example, in an area with surface dimensions of 4.5 km Ã— 4.5 km,
hundreds of seismic shot-gather cubes are required for 3d model reconstruction, leading to terabytes
of recorded data. this paper presents a deep learning solution for the reconstruction of realistic
3d models in the presence of field noise recorded in seismic surveys. we implement and analyze a
convolutional encoderâ€“decoder architecture that efficiently processes the entire collection of hundreds
of seismic shot-gather cubes. the proposed solution demonstrates that realistic 3d models can be
reconstructed with a structural similarity index measure (ssim) of 0.9143 (out of 1.0) in the presence
of field noise at 10 db signal-to-noise ratio.
keywords: 3d reconstruction; seismic inversion; seismic velocity; inverse problems; deep learning;
transfer learning; encoderâ€“decoder

1. introduction

citation: gelboim, m.; adler, a.; sun,
y.; araya-polo, m. encoderâ€“decoder
architecture for 3d seismic inversion.
sensors 2023, 23, 61. https://doi.org/
10.3390/s23010061
academic editor: aiguo song
received: 31 october 2022
revised: 12 december 2022
accepted: 19 december 2022
published: 21 december 2022

copyright: Â© 2022 by the authors.
licensee mdpi, basel, switzerland.
this article is an open access article
distributed under the terms and
conditions of the creative commons
attribution (cc by) license (https://
creativecommons.org/licenses/by/
4.0/).

a key step into understanding the subsurface by remote sensing, is the acquisition of
seismic data, which consists of the recorded response of the subsurface when mechanical
perturbations are introduced. after data has been collected, several disciplines of geoscience are involved towards the common objective of producing a reliable subsurface
model(s). earth models can be used for many purposes, such as: seismology studies,
hydrocarbon exploration and co2 sequestration. when used for the later purpose, models
are critical inputs to drilling decisions. the problem at hand is daunting, involving too
many variables and huge datasets. an example of great societal importance is injecting
co2 from industrial processes into specially reconditioned reservoirs. to that end, having
high quality subsurface models is crucial. the solution of 3d seismic inverse problems
using deep learning (dl) [1,2] is an emerging field of research, motivated by state-of-theart results obtained by dl for the 2d case [3,4]. in particular, dl has been applied for
velocity inversion [5â€“13], impedance inversion [14â€“16], reflectivity inversion [17â€“19] and
low-frequency extrapolation [20â€“24]. in this study, we address the problem of 3d velocity
inversion in large scale areas with hundreds of seismic shot-gather cubes, as required in
realistic seismic surveys. the proposed solution is demonstrated in an area with surface
dimensions of 4.5 km Ã— 4.5 km, which requires over 500 seismic shot-gather cubes for
3d model reconstruction. nevertheless, our solution is scalable to larger area dimensions
and higher numbers of shot-gather cubes, facilitated by the utilization of a dimensionality
reduction approach. the contributions of this paper are threefold: (1) a convolutional
encoderâ€“decoder network is proposed with an efficient input data dimensionality reduction
and time boosting of all seismic traces, to reconstruct complex 3d models at an average inference time of 0.165 s (on one nvidia a100 gpu), which is a fraction of the time required
by any iterative global optimization solver. (2) the proposed approach is demonstrated to
provide inherent robustness against noise in the recorded seismic data. (3) the proposed
approach is successfully evaluated with realistic 3d geological models and field noise.

sensors 2023, 23, 61. https://doi.org/10.3390/s23010061

https://www.mdpi.com/journal/sensors

sensors 2023, 23, 61

2 of 12

2. problem formulation
direct reconstruction of models of solid earth is not possible, this renders the following
forward model:
d = f(m) + e,
(1)
only practical when synthetic seismic data (d) is to be generated from a forward operator
f acting on a artificial model m, under ambient noise e. f approximates the behavior of
seismic waves propagating through the mechanical medium (m), and it is represented by
the following expression:
âˆ‚2 u
âˆ’ vp 2 âˆ‡(âˆ‡ Â· u) + vs 2 âˆ‡ Ã— (âˆ‡ Ã— u) = f,
âˆ‚t2

(2)

where u = u( x, y, z, t) is the seismic wave displacement, vp is p-wave velocity (compression/rarefaction), vs is s-wave velocity (shear stress), and f is the source function. while
the elastic (in the presence of p-wave and s-wave attenuation the viscoelastic wave equation
is utilized instead of the elastic equation) wave equation describes faithfully seismic waves
propagation, it is often preferred (as in this work) to approximate it by the acoustic wave
equation [25], which assumes only p-waves and requires less computational resources and
parameters, as compared to solving the elastic equation. the acoustic wave equation for a
medium without density variations is given by:
âˆ‚2 u
âˆ’ v2 âˆ‡2 u = f,
âˆ‚t2

(3)

where u is the wave displacement, v is the p-wave velocity model and f is the perturbation
source (i.e., shot) function.
since the direct formulation is not tractable, it is common to use the inverse approach.
seismic velocity inversion computes a complete 3d velocity model (mÌ‚) of a certain target
area, from recorded seismic data dr , and it can be summarized as:
mÌ‚ = fâˆ’1 (dr ),

(4)

where fâˆ’1 is the inversion operator. seismic inversion problems [26] are ill-posed, e.g., the
solution is non-unique and unstable in the sense that small noise variations may alter the
solution significantly. the dl formulation for solving inverse problems is detailed in the
next section.
3. the deep learning approach
3.1. encoderâ€“decoder architecture
deep learning (dl) is a powerful class of data-driven machine learning algorithms,
built using deep neural networks (dnns), which are formed by a hierarchical composition
of non-linear functions (layers). the main reason for the success of dl is the ability to train
very high capacity networks using very large datasets, often leading to good generalization
capabilities in numerous problem domains. generalization is defined as the ability of an
algorithm to perform well on unseen examples. in statistical learning terms an algorithm
a : x â†’ y is learned using a training dataset s = {( x1 , y1 ), ..., ( x n , y n )} of size n, where
xi âˆˆ x is a data sample (in this work, a seismic shot-gather) and yi âˆˆ y is the corresponding
label (in this work, a 3d velocity model). let p (x , y ) be the true distribution of the data,
then the expected risk is defined by: r(a) = ex,yâˆ¼p (x ,y ) [l(a( x ), y)], where l is a
loss function that measures the misfit between the algorithm output and the data label.
the goal of dl is to find an algorithm a within a given capacity (i.e., function space)
that minimizes the expected risk; however, the expected risk cannot be computed since
the true distribution is unavailable. therefore, the empirical risk is minimized instead:
re (a) = n1 âˆ‘in=1 l(a( xi ), yi ), which approximates the statistical expectation with an
empirical mean computed using the training dataset.

sensors 2023, 23, 61

3 of 12

in this work we implemented and trained a 3d convolutional encoderâ€“decoder, inspired by the 2d u-net architecture [27], to learn the mapping from seismic data space to
3d models space (i.e., inversion). the complete network architecture is depicted in figure 1,
and the details of each block are provided in table 1, with a total of 99m parameters.

figure 1. the proposed 3d encode-decoder architecture, based on the u-net architecture. the skip
connections perform a replication of the encoder feature maps, which are further concatenated with
the corresponding decoder feature maps. additional details are provided in table 1.
table 1. proposed encoderâ€“decoder architecture.
block

layer

unit

comments

input

0

seismic cube

96 Ã— 96 Ã— 224 grid points

enc1

1
2
3

conv3d(32, (5 Ã— 5 Ã— 5), relu)
conv3d(32, (5 Ã— 5 Ã— 5), relu)
maxpool3d

+ instancenormalization
+ instancenormalization
+ dropout(0.2)

enc2

4â€“6

enc1(64)

enc3

7â€“9

enc1(128)

enc4

10â€“12

enc1(256)

enc5

13â€“14

enc1(512)

without maxpool3d

dec1

15
16
17

convtrans3d(256, (2 Ã— 2 Ã— 2), relu)
conv3d(256, (5 Ã— 5 Ã— 5), relu)
conv3d(256, (5 Ã— 5 Ã— 5), relu)

+ instancenormalization
+ instancenormalization
+ instancenormalization

dec2

18â€“20

dec1(128)

dec3

21â€“23

dec1(64)

dec4

24â€“26

dec1(32)

27

conv3d(1, (1 Ã— 1 Ã— 1), relu)

final reconstruction layer

28

velocity model

96 Ã— 96 Ã— 224 grid points

output

3.2. computational considerations
the main challenge in training such a deep convolutional neural network (dcnn)
for real-life inversion tasks lies in the demanding gpu ram size and external storage
access requirements due to the large number of input channels and large size of each input
channel: each sample in our training data was composed of nx Ã— ny = 529 seismic data
cubes (i.e., dcnn input channels), where nx , ny are the total numbers of shots in the lateral
and longitudinal axes, respectively. therefore, a total storage size of 42gb per sample
(after decimation to dimensions 96 Ã— 96 Ã— 224). a modest training dataset of 800 samples
occupies â‰ˆ 4tb storage size, which requires very high-speed storage access to facilitate
dcnn training in reasonable duration. thus, the problem belongs to a high-performance

sensors 2023, 23, 61

4 of 12

computing class [28]. to overcome these challenging requirements, we propose a simple
yet highly-effective dimensionality reduction scheme: let d(sx , sy , r x , ry , t) denote the
5d tensor that represents a single data sample, i.e., the collection of seismic data cubes
(shot-gathers), where sx , sy are the indices of the shot position, r x , ry are the indices of the
receiver position, and t is time. we define the time-boosted and dimensionality-reduced
data cube dÌ„ by spatial averaging along the shots dimensions:
n

dÌ„( r x , ry , t) =

y
nx
b(t)
d ( s x , sy , r x , r y , t ),
âˆ‘
âˆ‘
nx Ã— ny s =1 s =1
x

(5)

y

where b(t) is a monotonically-increasing time-boosting function that compensates the
attenuation of wave reflections from the lowest geological layers, by amplifying latearrival time samples. therefore, dÌ„ forms a single 3d input channel, thus significantly
mitigating the memory and computational requirements for training and inference of the
proposed dcnn.
in the next section, we describe the performance of the proposed architecture for
noiseless seismic data, as well as data contaminated by synthetic and field noise.
4. performance evaluation
4.1. data preparation
we created 800 3d velocity models using the gempy (https://www.gempy.org/,
accessed on 1 october 2020) tool that creates 3d geologically-feasible models with realistic
combinations of features. the selection of gempy as subsurface modeler is not arbitrary, and
obeys to the intention of solving a more realistic problem than just flat layer-cake models.
a subset of 300 models were augmented with random 3d geometries that resemble salt
structures, as illustrated in figure 2. the physical dimensions of each model were 4.5 km Ã—
4.5 km Ã— 4.0 km (lateral Ã— longitudinal Ã— depth), represented by a 3d tensor of dimensions
300 Ã— 300 Ã— 800 grid points, which was down-sampled for dcnn training to dimensions
of 96 Ã— 96 Ã— 224. to generate the synthetic seismic data, through forward modeling, we
use an acoustic isotropic wave equation propagator with a 15 hz peak frequency ricker
wavelet as a source. shots and receivers are evenly spaced on the top surface of the 3d
model (200 m between shots and 25 m between receivers). to avoid reflections from the
boundaries and free surface multiples, convolutional perfectly matched layer (cpml) [29]
boundaries are imposed all around the model. each generated seismic data cube was
computed on a grid of dimensions 180 Ã— 180 Ã— 500 (lateral Ã— longitudinal Ã— time) points,
which was down-sampled for dcnn training to dimensions of 96 Ã— 96 Ã— 224.
the 800 3d models were split to disjoint training and testing sets by a 90%/10% ratio,
respectively. the proportions of models without and with salt structures were identical
in the training and testing sets, namely, 450 models without salt and 270 with salt in the
training set, and 50 models without salt and 30 with salt in the testing set. the proposed
dcnn was implemented in pytorch and trained using the nvidia a100 tensor core
gpu card (40 gb ram). training was performed using the adam optimizer with early
stopping regularization, by minimizing the mean absolute error (mae) loss function,
defined by:

sensors 2023, 23, 61

5 of 12

figure 2. 3d inversion results of four velocity models with salt geometry, from the held-out testing set.
each row presents one model: the two left columns display the ground-truth and the corresponding
inverted model. in the two right columns, the shallow layers (slow velocity) are removed to enable
the visualization of the embedded salt bodies.

sensors 2023, 23, 61

6 of 12

mae(x, y) =

1 n
| x i âˆ’ y i |,
n iâˆ‘
=1

(6)

where xi , yi are the grid point entries of the ground truth 3d model x and inverted 3d
model y, respectively (each model with n grid points). training was initially performed for
the noiseless data case, and subsequently using transfer learning, for the six noisy cases. for
each type of noise training started from the learned weights of the noiseless case (we found
this approach to provide a significant advantage as compared to training from random
weights, for each noisy data case). this process resulted in seven different trained dcnns:
noiseless data, data contaminated with white gaussian noise at signal-to-noise (snr) levels
of 20 db, 10 db and 0 db, and data contaminated with noise extracted from field data (from
recordings of an onshore field) at snr levels of 20 db, 10 db and 0 db. figure 2 presents
3d inversion results from noiseless data of four velocity models with salt geometry, from
the held-out testing set. the results clearly indicate high-quality 3d reconstruction of the
geological layers and salt bodies. examples of the clean and noisy data are provided in
figure 3, demonstrating the highly correlated patterns in space- and time-domains of the
field noise.

figure 3. a 2d slice of a 3d shot-gather: (a) noiseless. (b) distorted by additive white gaussian noise
(snr = 10 db). (c) distorted by additive colored field noise (snr = 10 db).

sensors 2023, 23, 61

7 of 12

4.2. evaluation metrics
ssim [30] results were computed per 3d model by first averaging ssim values along
the three 2d planes: 96 along the xz plane, 96 along the yz plane, and 224 along the xy
plane. finally, the three results were averaged to obtain the single ssim(3d) result. the
distribution of ssim values, as computed along the three 2d planes (xy, yz, xz), for the
entire testing set is presented in figure 4, demonstrating accurate reconstruction along the
three 2d planes, with slightly lower ssim values for the vertical planes (yz, xz), which can
be explained by the difficulty to reconstruct the deepest layers. table 2, details ssim and
mae results, averaged on the testing set, clearly demonstrating that the proposed dcnn
is capable to reconstruct 3d velocity models from noiseless data (figure 5eâ€“h), as well as
with additive white noise (figure 5iâ€“p) or field noise (figure 5qâ€“x). importantly, results
for seismic data contaminated by field noise, indicate close similarity to the ground truth
models at a snr of 20 db, but the ssim metric slightly deteriorated at snr of 10 db.

(a)

(b)

(c)
figure 4. ssim values histograms of all testing set samples, computed separately along the three
2d planes: (a) noiseless data. (b) data contaminated by white noise at snr = 20 db; and (c) data
contaminated by field noise at snr = 20 db. the histograms clearly indicate that most ssim values
are distributed between 0.80 and 0.99. the average 3d ssim values are 0.9335 for the noiseless case,
0.9316 with white noise (snr = 20 db) and 0.9271 with field noise (snr = 20 db).

sensors 2023, 23, 61

8 of 12

figure 5. 2d cross-sections of reconstructed 3d models from unseen data: (aâ€“d) ground truth. (eâ€“h) reconstruction from noiseless data. (iâ€“l) reconstruction from noisy data: white noise, snr = 20 db; and
(mâ€“p) white noise, snr = 10 db. (qâ€“t) reconstruction from noisy data: field noise, snr = 20 db; and
(uâ€“x) field noise, snr = 10 db.

the results in the noiseless data case are surprising, indicating that the dimensionalityreduced data cube (5) contains sufficient information for practical reconstruction given
the measured metric, achieving an average ssim(3d) of 0.9003 by the dcnn. in addition,

sensors 2023, 23, 61

9 of 12

examples of the structure of the prediction error are presented in figure 6. we next discuss
the noisy data case.

figure 6. three testing samples were selected and a 2d (central inline) cut is shown, a comparison
between ground-truth and prediction is presented in terms of error. as expected, the greater error
(right most bar, in percentage) occurs around the salt geometry and mostly over estimating the model
velocity, nevertheless the background velocity is overall correct.

sensors 2023, 23, 61

10 of 12

table 2. 3d velocity model building quality comparison. all values are reported as: mean(std), mae
results are in [km/s]. results with salt geometry augmented models are included in this table.
metric

noiseless
data

white noise
(snr = 20 db)

white noise
(snr = 10 db)

white noise
(snr = 0 db)

field noise
(snr = 20 db)

field noise
(snr = 10 db)

field noise
(snr = 0 db)

ssim(3d)
ssim(xz)
ssim(xy)
ssim(yz)
mae(3d)

0.9335 (0.0449)
0.9294 (0.0458)
0.9433 (0.0388)
0.9280 (0.0471)
0.0380 (0.0349)

0.9316 (0.0440)
0.9272 (0.0446)
0.9417 (0.0394)
0.9257 (0.0460)
0.0394 (0.0343)

0.9192 (0.0465)
0.9135 (0.0467)
0.9329 (0.0409)
0.9113 (0.0484)
0.0490 (0.0428)

0.8621 (0.0528)
0.8475 (0.0484)
0.8960 (0.0425)
0.8426 (0.0497)
0.1214 (0.0716)

0.9271 (0.0461)
0.9222 (0.0470)
0.9385 (0.0407)
0.9207 (0.0481)
0.0426 (0.0375)

0.9143 (0.0493)
0.9078 (0.0495)
0.9296 (0.0427)
0.9054 (0.0515)
0.0550 (0.0510)

0.8605 (0.0488)
0.8455 (0.0425)
0.8953 (0.0401)
0.8407 (0.0438)
0.1206 (0.0667)

4.3. noisy data analysis
in the presence of additive white noise that is spatially (and temporally) independent
and identically distributed (iid), the spatial averaging along the shots dimensions results
in a reduction of the noise variance, as explained in the following analysis. denoting by
n( r x , ry , t) the noise random variable resulting from the spatial averaging of noise samples,
corresponding to receiver coordinates ( r x , ry ) and time t:
n

y
nx
1
n ( s x , sy , r x , r y , t ),
nÌ„( r x , ry , t) =
âˆ‘
âˆ‘
nx Ã— ny s =1 s =1
x

(7)

y

where n(sx , sy , r x , ry , t) are iid random variables with zero mean and variance Ïƒn2 . by
using the iid property, the variance of nÌ„( r x , ry , t) is independent of r x , ry , t and given
1
2
by ÏƒnÌ„2 ( r x , ry , t) = ÏƒnÌ„2 = nx Ã—
ny Ïƒn (in our study nx Ã— ny = 529). the time-boosting
function b(t) is omitted from (7), since in the presence of additive noise, both the signal
and noise components are multiplied by b(t), according to (5), thus the contribution of
b(t) is cancelled in a snr analysis. therefore, the variance of the noise component in the
dimensionality-reduced data cube is effectively reduced by nx Ã— ny , for iid white noise.
however, the field noise is clearly not iid, therefore a smaller reduction in the noise variance
is achieved.
5. conclusions
seismic inversion based on dl effectively reconstruct 3d subsurface models from
synthetic seismic data and synthetic seismic data contaminated by either white or field
noise. once training is settled, the inference step is fastâ€”a fraction of a secondâ€”to a
point that allows many different experiments to be carried out with marginal cost. to
allow feasible training time a dimensionality reduction technique is deployed. also, the
robustness to noise was demonstrated for practical snr levels. the next steps for this
approach are: training and testing with more complex and larger scale structures, and to
estimate the sensitivity with respect to acquisition towards direct use of field data.
author contributions: conceptualization, m.g., a.a., y.s. and m.a.-p.; methodology, m.g., a.a.,
y.s. and m.a.-p.; software, m.g., m.a.-p. and y.s.; validation, m.g.; formal analysis, m.g., a.a., y.s.
and m.a.-p.; investigation, m.g.; data curation, m.g.; writingâ€”original draft preparation, m.g., a.a.,
y.s. and m.a.-p.; writingâ€”review and editing, a.a., y.s. and m.a.-p.; visualization, m.g. and m.a.-p.;
supervision, a.a., m.a.-p. and y.s.; all authors have read and agreed to the published version of
the manuscript.
funding: this research received funding from totalenergies ep r&t usa.
institutional review board statement: not applicable.
informed consent statement: not applicable.
data availability statement: not applicable.

sensors 2023, 23, 61

11 of 12

acknowledgments: the authors acknowledge totalenergies ep research & technology usa, for
supporting this work and allowing its publication.
conflicts of interest: the authors declare no conflict of interest.

references
1.
2.
3.
4.
5.
6.
7.

8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.

28.

goodfellow, i.; bengio, y.; courville, a. deep learning; mit press: cambridge, ma, usa, 2016.
jin, k.h.; mccann, m.t.; froustey, e.; unser, m. deep convolutional neural network for inverse problems in imaging. ieee
trans. image process. 2017, 26, 4509â€“4522. [crossref] [pubmed]
adler, a.; araya-polo, m.; poggio, t. deep learning for seismic inverse problems: toward the acceleration of geophysical
analysis workflows. ieee signal process. mag. 2021, 38, 89â€“119. [crossref]
mousavi, s.m.; beroza, g.c. deep-learning seismology. science 2022, 377, eabm4470. [crossref] [pubmed]
araya-polo, m.; jennings, j.; adler, a.; dahlke, t. deep-learning tomography. lead. edge 2018, 37, 58â€“66. [crossref]
yang, f.; ma, j. deep-learning inversion: a next generation seismic velocity-model building method. geophysics 2019, 84,
r583â€“r599. [crossref]
adler, a.; araya-polo, m.; poggio, t. deep recurrent architectures for seismic tomography. in proceedings of the 81st eage
conference and exhibition 2019, london, uk, 3â€“6 june 2019; european association of geoscientists & engineers: bunnik, the
netherlands, 2019; volume 2019, pp. 1â€“5.
araya-polo, m.; adler, a.; farris, s.; jennings, j. fast and accurate seismic tomography via deep learning. in deep learning:
algorithms and applications; springer: berlin/heidelberg, germany, 2020; pp. 129â€“156.
park, m.j.; sacchi, m.d. automatic velocity analysis using convolutional neural network and transfer learning. geophysics 2020,
85, v33â€“v43. [crossref]
zhang, z.; alkhalifah, t. regularized elastic full-waveform inversion using deep learning. in advances in subsurface data analytics;
elsevier: amsterdam, the netherlands, 2022; pp. 219â€“250.
li, y.; alkhalifah, t. target-oriented time-lapse elastic full-waveform inversion constrained by deep learning-based prior
model. ieee trans. geosci. remote sens. 2022, 60, 1â€“12. [crossref]
fabien-ouellet, g.; sarkar, r. seismic velocity estimation: a deep recurrent neural-network approach. geophysics 2020, 85,
u21â€“u29. [crossref]
zhu, w.; xu, k.; darve, e.; biondi, b.; beroza, g.c. integrating deep neural networks with full-waveform inversion: reparameterization, regularization, and uncertainty quantification. geophysics 2022, 87, r93â€“r109. [crossref]
das, v.; pollack, a.; wollner, u.; mukerji, t. convolutional neural network for seismic impedance inversion. geophysics 2019, 84,
r869â€“r880. [crossref]
wang, y.; wang, q.; lu, w.; li, h. physics-constrained seismic impedance inversion based on deep learning. ieee geosci. remote
sens. lett. 2021, 19, 1â€“5. [crossref]
wang, y.q.; wang, q.; lu, w.k.; ge, q.; yan, x.f. seismic impedance inversion based on cycle-consistent generative adversarial
network. pet. sci. 2022, 19, 147â€“161. [crossref]
kim, y.; nakata, n. geophysical inversion versus machine learning in inverse problems. lead. edge 2018, 37, 894â€“901. [crossref]
biswas, r.; sen, m.k.; das, v.; mukerji, t. prestack and poststack inversion using a physics-guided convolutional neural network.
interpretation 2019, 7, se161â€“se174. [crossref]
russell, b. machine learning and geophysical inversionâ€”a numerical study. lead. edge 2019, 38, 512â€“519. [crossref]
sun, h.; demanet, l. low frequency extrapolation with deep learning. in seg technical program expanded abstracts 2018; society
of exploration geophysicists: houston, tx, usa, 2018; pp. 2011â€“2015.
ovcharenko, o.; kazei, v.; kalita, m.; peter, d.; alkhalifah, t. deep learning for low-frequency extrapolation from multioffset
seismic data. geophysics 2019, 84, r989â€“r1001. [crossref]
ovcharenko, o.; kazei, v.; alkhalifah, t.a.; peter, d.b. multi-task learning for low-frequency extrapolation and elastic model
building from seismic data. ieee trans. geosci. remote sens. 2022, 60, 1â€“17. [crossref]
sun, h.; demanet, l. extrapolated full-waveform inversion with deep learning. geophysics 2020, 85, r275â€“r288. [crossref]
sun, h.; demanet, l. deep learning for low-frequency extrapolation of multicomponent data in elastic fwi. ieee trans. geosci.
remote sens. 2021, 60, 1â€“11. [crossref]
tarantola, a. inversion of seismic reflection data in the acoustic approximation. geophysics 1984, 49, 1259â€“1266. [crossref]
schuster, g.t. seismic inversion; society of exploration geophysicists: houston, tx, usa, 2017.
ronneberger, o.; fischer, p.; brox, t. u-net: convolutional networks for biomedical image segmentation. in proceedings of
the medical image computing and computer-assisted interventionâ€”miccai 2015: 18th international conference, munich,
germany, 5â€“9 october 2015; springer international publishing: cham, switzerland, 2015; pp. 234â€“241.
akhiyarov, d.; gherbi, a.; araya-polo, m. machine learning scalability requires high performance computing strategies.
in proceedings of the first eage conference on machine learning in americas, online, 22â€“24 september 2020; european
association of geoscientists and engineers: bunnik, the netherlands, 2020; pp. 1â€“5. [crossref]

sensors 2023, 23, 61

29.
30.

12 of 12

li, y.f.; bou matar, o. convolutional perfectly matched layer for elastic second-order wave equation. j. acoust. soc. am. 2010,
127, 1318â€“1327. [crossref] [pubmed]
wang, z.; bovik, a.; sheikh, h.; simoncelli, e. image quality assessment: from error visibility to structural similarity. ieee trans.
image process. 2004, 13, 600â€“612. [crossref] [pubmed]

disclaimer/publisherâ€™s note: the statements, opinions and data contained in all publications are solely those of the individual
author(s) and contributor(s) and not of mdpi and/or the editor(s). mdpi and/or the editor(s) disclaim responsibility for any injury to
people or property resulting from any ideas, methods, instructions or products referred to in the content.



240 (2024) 213028

contents lists available at sciencedirect

geoenergy science and engineering
journal homepage: www.sciencedirect.com/journal/geoenergy-science-and-engineering

shear-wave velocity prediction of tight reservoirs based on poroelasticity
theory: a comparative study of deep neural network and rock
physics model
zhijian fang a, jing ba a, *, qiang guo b, fansheng xiong c
a
b
c

school of earth sciences and engineering, hohai university, nanjing, china
school of resources and geosciences, china university of mining and technology, xuzhou, china
beijing institute of mathematical sciences and applications, beijing, china

a r t i c l e i n f o

a b s t r a c t

keywords:
deep neural network
rock physics model
wave propagation equation
shear-wave velocity prediction
shale oil formation

the absence of shear-wave (s-wave) velocity in well log data limits the effective seismic characterization of liÂ­
thology, petrophysical properties, and fluid distribution of tight reservoirs in the chang 7 shale oil formations of
ordos basin, west china. however, conventional rock physics models (rpms), e.g., the xu-white model, struggle
to accurately obtain the key parameters such as pore aspect ratio in the process of s-wave velocity prediction.
this work performs a comparative study of s-wave velocity prediction based on deep neural network (dnn) and
rpm for such reservoirs. by employing the poroelasticity equations which are capable of describing elastic wave
propagation in the complex reservoirs saturated with viscous fluid, the relation between elastic and petrophysical
properties is established with a dnn-based method. this approach quantifies the connection between p-/s-wave
velocities and rock properties. on the other hand, we reformulate the traditional xu-white model by incorpoÂ­
rating the decoupled kuster-toksoÌˆz (k-t) model and the poroelasticity equations with viscoelastic fluid in the
modeling process. the variation of pore aspect ratio with depth is also considered to characterize the complex
pore structures, which is appropriate for improving the accuracy of prediction. log data from the three wells are
considered for verification, of which results show that the s-wave velocity prediction methods help improve the
final result. aided by the dnn-based method, the prediction exhibits a better performance if the training data is
sufficient. in particular, provided a small amount of measured s-wave data (even ten data points), the trained
dnn model can also provide reasonable predictions.

1. introduction
the shear wave (s-wave) velocity, in addition to the compressional
wave (p-wave) velocity and bulk density, plays an important role as the
geophysical well log data in practical applications such as pre-stack
seismic inversion, reservoir-property prediction, and fluid identificaÂ­
tion (downton, 2005; ba et al., 2011; rabbani et al., 2017; oloruntobi
et al., 2019; pang et al., 2021; zhang et al., 2021; guo et al., 2022; qing
et al., 2022; luo et al., 2023). particularly, s-wave velocity of log data is
essential when a pre-stack seismic inversion is performed to extract
elastic parameters (vernik et al., 2018; guo et al., 2021). however, due
to the actual technical and economical concerns, s-wave logging data is
usually absent (tan et al., 2015; mehrgini et al., 2017; wang et al.,
2020). therefore, it is quite meaningful to develop the prediction

methods of s-wave velocity for petroleum reservoirs.
in absence of s-wave velocity logging data, a great number of studies
have been performed on the methods for s-wave velocity prediction.
these conventional methods can be classified into two categories:
empirical formula-based estimation and rock physics model (rpm)based prediction. the empirical formula-based method is a rapid and
straightforward approach which is commonly employed for estimating
s-wave velocity. it involves conducting a statistical analysis of both pand s-wave velocities, to achieve the derivation of empirical relations
between p- and s-wave velocities through a fitting process (pickett,
1963; castagna et al., 1985). han et al. (1986), castagna et al. (1985),
and eskandari et al. (2004) showed that based on a great number of
experimental studies, a quasi-linear or nonlinear empirical relationship
between p- and s-wave velocities can be derived for reservoir rocks.

* corresponding author.
e-mail address: jba@hhu.edu.cn (j. ba).
https://doi.org/10.1016/j.geoen.2024.213028
received 2 december 2023; received in revised form 28 april 2024; accepted 8 june 2024
available online 12 june 2024
2949-8910/Â© 2024 elsevier b.v. all rights are reserved, including those for text and data mining, ai training, and similar technologies.

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

fig. 1. location of the study area in the ordos basin.

physics-informed ml for overpressure prediction by embedding a
frequency-dependent quality factor (q)-pressure petrophysical model
(guo and fu, 2007) into the activation function of convolution neurons.
in particular, various ml technologies have been applied for preÂ­
dicting s-wave velocity logging data, such as support vector regression
(svr) (brereton and lloyd, 2010; ni et al., 2017), long-short-term
memory (lstm) (mehrgini et al., 2017; wang et al., 2020; zhang
et al., 2020b), gated recurrent units (grus) (sun and liu, 2020; arunÂ­
kumar et al., 2021), and gradient boosting (guelman, 2012; benteÌjac
et al., 2021; zhong et al., 2021), etc. by constructing a neural network
(nn), it is possible to establish a multivariate function that relates
s-wave velocity to petrophysical properties. however, the nn-based
approach for s-wave velocity prediction suffers from inadequate
generalization and a scarcity of labeled data to construct the predictive
networks, which presents challenges in the practical applications (feng
et al., 2023). in addition, most of the ml-based methods of s-wave veÂ­
locity prediction are purely based on the data-driven concepts. the pure
data-driven ml method might lack the possibility for revealing the
intrinsic physical insights (wang et al., 2023b,c).
this work proposes a s-wave velocity prediction method based on
dnns which assumes the mathematical formulation (theoretical model)
describing the elastic wave propagation within the reservoirs is known.
the elastic parameters are considered as unknowns and are learned
through a nn on the basis of well log data. subsequently, the correÂ­
sponding p- and s-wave velocities are obtained with the plane-wave
analysis method by combining the nn approach and the theoretical
model. in this process, we first verify that ml can learn the wave
propagation equations from the labeled data. then, we analyze the erÂ­
rors between the s-wave velocity obtained by solving the wave equation
learned by the network and the actual data. we also consider two cases
for the network training, i.e., all s-wave velocity labels are available and
only part of them are available. the proposed dnn-based model is
compared to rpms (the traditional xu-white model and the reformuÂ­
lated model which incorporates the depth-variation pore aspect ratio).
to evaluate the performance, the log data from the three wells that
provide abundant information in the study area are adopted. the
established dnn model and the reformulated rpm are separately
applied for s-wave velocity prediction, and the results are compared and
analyzed with those by the traditional xu-white model.

however, reliable empirical relations require the statistical analysis of
extensive data from the specific area or target layer. although it is
efficient and practical, the approach may only reflect general trends and
can be heavily influenced by regional variations (either horizontally or
vertically), which potentially reduces the degree of accuracy in estiÂ­
mation. the rpm-based method primarily utilizes different types of
theories or models according to the real geological conditions (jian
et al., 2020; liu et al., 2020; rao et al., 2021), and computes the
equivalent elastic moduli of rocks to obtain the p- and s-wave velocities.
xu and white (1995, 1996) proposed a rock physics modeling approach
for sand and mudstone formations, by combining the differential
effective medium (dem) theory (zimmerman, 1984; norris, 1985),
kuster-toksoÌˆz (k-t) model (kuster and toksoÌˆz, 1974) and gassmannâ€™s
equation of fluid substitution (gassmann, 1951). subsequently, based on
rock physics models and theories, a series of s-wave prediction models
are presented, which can be suitable for different lithological reservoirs
(keys and xu, 2002; qi et al., 2019; azadpour et al., 2020; ren et al.,
2020; zhang et al., 2020a; wang et al., 2021; han et al., 2022; ba et al.,
2023a). in practice, for the process of rock physics modeling, it is
necessary to accurately determine the various influencing factors, such
as rock mineral compositions and geometries, pore structure and conÂ­
nectivity, and pore fluid properties, etc., so as to develop an appropriate
rpm according to the specific reservoir conditions and achieve the acÂ­
curate prediction.
the ability of machine learning (ml), such as dnn, to establish
complex nonlinear mapping relationships between inputs and outputs
has led to its extensive applications in the fields such as facial recogniÂ­
tion, machine translation, image classification, etc. (zhao et al., 2003;
zhang et al., 2014; young et al., 2018; boateng and fu, 2018; boateng
et al., 2020; li et al., 2023). in recent years, ml was introduced into the
field of geosciences with significant results in the areas such as rock
physics modeling, lithology identification, and reservoir parameter
inversion (fu, 2004; wu et al., 2008; rajabi et al., 2010; anemangely
et al., 2017; alfarraj and alregib, 2019; you et al., 2020, 2021; xiong
et al., 2021a, 2021b; zou et al., 2021; wang et al., 2023a; botterill and
mcmillan, 2023; yang et al., 2023). most ml techniques are purely data
driven, which does not require any prior physical information. such
purely data-driven learning requires massive training data. it is worth
mentioning that physics-informed learning can be obtained by embedÂ­
ding relevant rpms into the ml algorithms (qin and fu, 2013; boateng
et al., 2017). for example, cheng and fu (2022) propose a
2

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

fig. 2. thin sections of the tight sandstone samples.

fig. 3. one well log data of (a) porosity, (b) water saturation, (c) clay content, (d) density, (e) permeability, (f) p-wave velocity, and (g) s-wave velocity in the
study area.

2. materials

are particularly abundant in the changqing oilfield. in recent years, the
chang 7 member has emerged as a key exploration target. fig. 1 shows
the lithological distribution in this area. chang 71 is composed of
mudstone interbedded with siltstone/sandstone, as well as oil shale.
chang 72 consists of siltstone and fine sandstone, locally interbedded
with sandy mudstone, and additionally, thin layers of shale are develÂ­
oped. chang 73 is mainly composed of black oil shale and siltstones (fu
et al., 2020a,b).
fig. 2 shows the cast thin sections of tight sandstone rocks observed
in the laboratory, which indicates the rock has a fine-silty sandy strucÂ­
ture with directional clastic particles, belonging to the fine-silty strucÂ­
ture lithic feldspathic sandstone. the major mineral components of the
rocks are quartz, feldspar, and clay, with lower volumetric content of
other minerals. the ratio of quartz to feldspar is approximately 1.85.
after diagenetic transformation, the rock is relatively tight, and only
intergranular residual pores and feldspar dissolution pores can be
observed. complex pore structures bring challenges to rock physics
modeling. pore aspect ratio is commonly used in rock physics modeling
to characterize pore morphology and structure. however, in practical
applications, pore aspect ratio is difficult to be directly measured.

2.1. overview of the study area
the ordos basin located in western china is a vast and stable polyÂ­
genetic craton basin featuring a gently inclined monocline towards the
west (guo et al., 2020). it can be divided into six distinct tectonic units,
with the tianhuan depression and the shanbei slope being the most
prominent units (as illustrated in fig. 1). these units also correspond to
regions with significant hydrocarbon deposits. notably, the shanbei
slope is the primary area for paleozoic natural gas and mesozoic oil
distribution. over time, starting from the late triassic era, the ordos
basin has been transitioned into an inland lacustrine sedimentary
environment, where the upper triassic yanchang formation contains a
series of fluvial-deltaic-lacustrine clastic rocks. the yanchang formation
can be further subdivided into ten members, including the chang 7
member (bai and ma, 2020). the study area is positioned at the
convergence point of sediment sources for the southwest and northeast
of the basin. the hydrocarbons presented in the tight sandstones and
source rocks have not undergone long-distance migration and accumuÂ­
lation, which are defined as a variety of high-quality shale/tight oil, and
3

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

2.2. data preparation

3. theory and methodology

the dataset comprises well log measurements obtained from 16 wells
located within the study area. following the preprocessing of the data,
we acquired a total of 21,215 data points from the wells. these data
points are suitable for training and testing our models, and they
encompass all the necessary variables. the quantity of data points may
differ across different wells. for evaluating the model performance, we
selected three wells with sufficient data points as an independent test
data set. the remaining wells, comprising 12,415 data points, were used
to train the dnn model. fig. 3 shows the data information from one of
the wells, including porosity (Ï•), water saturation (sw), clay content
(vcl), density (Ï), permeability (Îº), and p-wave and s-wave velocities (vp
and vs). these data were processed and interpreted by the oilfield
companies. porosity, water saturation, clay content, and density were
calculated with the conventional well logging curves. permeability was
calculated based on a regional model established through fitting with
parameters such as porosity, and using core calibration logging. p- and
s-wave velocities were extracted from array sonic logs.

the elastic parameters of the wave propagation equations in the
study area are determined by using the dnn and rpm, respectively,
based on which the corresponding s-wave velocity can be obtained by
plane wave analysis.

nâˆ‡2 u1 + (a + n)
(

âˆ‚

âˆ‚x1

e+q

âˆ‚

s+

âˆ‚x1

)

(

(

âˆ‚2
Î·k Î·Î¼
u3 +
uÌ‡3
âˆ’
âˆ‚x1 âˆ‚x3
2kf Î¼f

)

+

âˆ’

3.1. poroelasticity wave equations with viscoelastic fluid
ba et al. (2023b) derived the wave propagation equations of a porous
media saturated with a viscoelastic fluid, where the stress-strain relaÂ­
tionship of the fluid is characterized by a kelvin-voigt mechanical model
(kelvin, 1875; voigt, 1892). by invoking hamiltonâ€™s principle and the
lagrangian equation of l = t-w (where t and w are the kinetic energy
and strain energy, respectively. all of this can be determined according
to ba et al. (2023b)), the governing equations of wave propagation are as
follows,

(
) )
(
(
)
qÊ¹
âˆ‚2
Î·k Î·Î¼
u2 +
uÌ‡2
sÌ‡ âˆ’
âˆ’
2kf
2 âˆ‚x1 âˆ‚x2
2kf Î¼f
(
)
(
))

Î·k

Î·Î¼
âˆ‚2
u1 +
uÌ‡1
âˆ‚x22
2Î¼f

âˆ’

Î·Î¼
âˆ‚2
u1 +
uÌ‡1
âˆ‚x23
2Î¼f

(1a)

= Ï11 uÌˆ1 + Ï12 uÌˆ1 + b(uÌ‡1 âˆ’ uÌ‡1 ),

(
(
)
(
))
(
)
)
Î·Î¼
Î·Î¼
âˆ‚2
Î·k
âˆ‚2
âˆ‚2
sÌ‡ + nÊ¹
u
+
u
+
+
+
uÌ‡
uÌ‡
uÌ‡
u
+
âˆ‚x1
2kf
âˆ‚x21 1 2kf 1
âˆ‚x22 1 2Î¼f 1
âˆ‚x23 1 2Î¼f 1
(
(
)
(
))
)
)
(
2 (
2
2
Î·Î¼
Î·Î¼
âˆ‚
Î·
âˆ‚
âˆ‚
âˆ‚
qÊ¹
âˆ‚2
âˆ‚2
âˆ‚2
âˆ‚2
+nÊ¹
u1 + k uÌ‡1 +
eâˆ’
u2 +
u3 âˆ’
u
âˆ’
u
u2 +
u3 +
+q
uÌ‡2 +
uÌ‡3
1
1
2
âˆ‚x1
âˆ‚x1
2kf
âˆ‚x1 âˆ‚x2
2Î¼f
âˆ‚x1 âˆ‚x3
2Î¼f
2 âˆ‚x1 âˆ‚x2
âˆ‚x1 âˆ‚x3
âˆ‚x2 2
âˆ‚x3 2
aÊ¹

(

âˆ‚

Î·k

s+

(1b)

= Ï12 uÌˆ1 + Ï22 uÌˆ1 âˆ’ b(uÌ‡1 âˆ’ uÌ‡1 ),

nâˆ‡2 u2 + (a + n)
(

âˆ‚

âˆ‚x2

e+q

âˆ‚

s+

âˆ‚x2

)

(

(

âˆ‚2
Î·k Î·Î¼
u3 +
uÌ‡3
âˆ’
âˆ‚x2 âˆ‚x3
2kf Î¼f

)

+

âˆ’

(
) )
(
(
)
qÊ¹
âˆ‚2
Î·k Î·Î¼
u1 +
uÌ‡1
sÌ‡ âˆ’
âˆ’
2kf
2 âˆ‚x1 âˆ‚x2
2kf Î¼f
(
)
(
))

Î·k

Î·Î¼
âˆ‚2
u2 +
uÌ‡2
âˆ‚x21
2Î¼f

âˆ’

Î·Î¼
âˆ‚2
u2 +
uÌ‡2
âˆ‚x23
2Î¼f

(1c)

= Ï11 uÌˆ2 + Ï12 uÌˆ2 + b(uÌ‡2 âˆ’ uÌ‡2 ),

(
(
)
(
))
(
)
)
Î·Î¼
Î·Î¼
âˆ‚2
âˆ‚2
Î·k
âˆ‚2
+
+
+
uÌ‡
uÌ‡
uÌ‡
u
+
u
+
u
sÌ‡ + nÊ¹
2
2
2
2
2
2
âˆ‚x2
2kf
âˆ‚x21
2Î¼f
âˆ‚x22
2kf
âˆ‚x23
2Î¼f
(
(
)
(
))
)
)
( 2
2
2 (
2
Ê¹
Î·Î¼
Î·Î¼
âˆ‚
âˆ‚
Î·
âˆ‚
âˆ‚
q
âˆ‚
âˆ‚2
âˆ‚2
âˆ‚2
+nÊ¹
u1 +
u3 +
+q
uÌ‡1 + 2 u2 + k uÌ‡2 +
uÌ‡3
eâˆ’
u1 +
u3 âˆ’
u2 âˆ’
u2
2
2
âˆ‚x2
âˆ‚x1 âˆ‚x2
2Î¼f
âˆ‚x2
2kf
âˆ‚x2 âˆ‚x3
2Î¼f
2 âˆ‚x1 âˆ‚x2
âˆ‚x2 âˆ‚x3
âˆ‚x1
âˆ‚x3
aÊ¹

âˆ‚

(

s+

Î·k

= Ï12 uÌˆ2 + Ï22 uÌˆ2 âˆ’ b(uÌ‡2 âˆ’ uÌ‡2 ),

4

(1d)

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

(
(
(
) )
)
qÊ¹
âˆ‚2
Î·k Î·Î¼
sÌ‡ âˆ’
nâˆ‡ u3 + (a + n)
e+q
âˆ’
s+
u1 +
uÌ‡1
âˆ‚x3
âˆ‚x3
2kf
2 âˆ‚x1 âˆ‚x3
2kf Î¼f
(
) )
(
)
(
))
(
2

âˆ‚

âˆ‚

(

âˆ‚2
Î·k Î·Î¼
u2 +
uÌ‡2
âˆ’
âˆ‚x2 âˆ‚x3
2kf Î¼f

+

Î·k

âˆ’

Î·Î¼
âˆ‚2
u3 +
uÌ‡3
âˆ‚x21
2Î¼f

âˆ’

Î·Î¼
âˆ‚2
u3 +
uÌ‡3
âˆ‚x22
2Î¼f

(1e)

= Ï11 uÌˆ3 + Ï12 uÌˆ3 + b(uÌ‡3 âˆ’ uÌ‡3 ),

(
(
)
(
)
(
))
)
Î·Î¼
Î·Î¼
âˆ‚2
âˆ‚2
âˆ‚2
Î·k
u
+
u
+
sÌ‡ + nÊ¹
+
+
+
uÌ‡
uÌ‡
uÌ‡
u
3
3
3
3
3
3
âˆ‚x3
2kf
âˆ‚x21
2Î¼f
âˆ‚x22
2Î¼f
âˆ‚x23
2kf
(
(
)
(
)
))
)
(
2
2
2 (
2
Î·Î¼
Î·Î¼
âˆ‚
âˆ‚
âˆ‚
Î·
âˆ‚
qÊ¹
âˆ‚
âˆ‚2
âˆ‚2
âˆ‚2
uÌ‡1 +
uÌ‡2 + 2 u3 + k uÌ‡3
+q
u1 +
u2 +
+nÊ¹
eâˆ’
u1 +
u2 âˆ’
u
âˆ’
u
3
3
âˆ‚x3
âˆ‚x1 âˆ‚x3
2Î¼f
âˆ‚x2 âˆ‚x3
2Î¼f
âˆ‚x3
2kf
2 âˆ‚x1 âˆ‚x3
âˆ‚x2 âˆ‚x3
âˆ‚x1 2
âˆ‚x2 2
aÊ¹

âˆ‚

(

s+

Î·k

= Ï12 uÌˆ3 + Ï22 uÌˆ3 âˆ’ b(uÌ‡3 âˆ’ uÌ‡3 ).

fig. 4. workflow of the dnn-based s-wave velocity prediction.

fig. 5. the influence of different pore aspect ratios on the bulk (a) and shear (b) moduli of dry rock.
5

(1f)

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

fig. 7. training loss curve of the dnn model.

components of the solid and the bulk strain of the fluid, respectively.
in this work, we adopt the wave propagation equations (ba et al.,
2023b) as the theoretical model that can describe elastic wave propaÂ­
gating in the complex reservoirs of the study area. however, there are six
unknown elastic coefficients in these equations that need to be
determined.
we then obtain the equations of wave velocities as follows,

fig. 6. the workflow of the rpm-based s-wave velocity prediction.

table 1
physical properties of minerals and fluids (mavko et al., 2020).
k (gpa)
Î¼ (gpa)
Ï (g/cm3)

clay

quartz

feldspar

water

oil

21
7
2.6

37
44
2.65

46.7
23.63
2.54

2.25
\
1

1.02
\
0.8

vp,s =

Ï‰

)
(
re kp,s

(3)

where Ï‰ and k are the angular frequency and the complex wave number,
respectively (see appendix a).
3.2. workflow of the dnn-based s-wave velocity prediction

where
n = Î¼s (1 âˆ’ Ï•) âˆ’

/

Ï•Î¼
/ s

Î¼f Î¼s + Ï•Î¼s (Î¼s (1 âˆ’ Ï•) âˆ’ Î¼b )

/
Ï• Î¼f 2 Î¼s
/
/
nâ€™ = Ï•Î¼f âˆ’
Î¼f Î¼s + Ï•Î¼s (Î¼s (1 âˆ’ Ï•) âˆ’ Î¼b )
qâ€™ =

/

Ï•Î¼
/ f

Î¼f Î¼s + Ï•Î¼s Î¼s (1 âˆ’ Ï•) âˆ’ Î¼b

Ï•ks (1 âˆ’ kb /ks âˆ’ Ï•)
2
/ + qâ€™
)
q= ( /
Ï• ks kf âˆ’ 1 + 1 âˆ’ kb ks 3
a=

(2a)

(2b)

(2c)

(2d)

/
(1 âˆ’ Ï•)(1 âˆ’ kb /ks âˆ’ Ï•)ks + Ï•ks kf kb 2
/
( /
)
âˆ’ n
3
1 + Ï• ks kf âˆ’ 1 âˆ’ kb ks

(2e)

Ï•2 ks
2
/ âˆ’ nâ€™
( /
)
1 + Ï• ks kf âˆ’ 1 âˆ’ kb ks 3

(2f)

aâ€™ =

in this section, we consider the assumption that elastic wave propÂ­
agation in the oil reservoirs can be appropriately described with eq. (1),
while the six unknown elastic parameters in eq. (2) can be determined
with the dnn training from the actual log data. the workflow of the
dnn-based method is outlined in fig. 4. the dnn is designed with input
and output layers, between which there are five fully-connected hidden
layers (each layer has fifty units). in the nn, we select the rectified linear
unit (relu) as the activation function and utilize the adaptive moment
estimation (adam) algorithm as the optimization technique (kingma
and ba, 2014). in the training, we set 0.01 as the initial learning rate and
choose the learning rate decay strategy. the learning process is based on
the â€™lr_scheduleâ€™ function, i.e., an integrated feature in pytorch that
facilitates learning rate adjustment as the number of epochs progresses
(paszke et al., 2017). besides, weights and biases are continuously
adjusted to minimize the loss function. the loss function is the mean
square error (mse), quantifying the discrepancy between the dnn
predictions and the provided labels. in the process of the nn training, all
the data in the training set are utilized in each epoch.
through eq. (2), it is known that the six elastic parameters are deÂ­
pends on Ï•, ks , Î¼s , and kf . in this case, we assume that the bulk and shear
moduli of fluid-saturated rock (k and Î¼) are related to ks and Î¼s , and kf is
Ìƒ = (Ï•, k, Î¼, sw ), and the dnn models
related to sw . therefore, the input z

where a, aâ€², n, nâ€², q and qâ€² are the six extended biotâ€™s elastic coÂ­
efficients. kf , ks and kb are the bulk moduli of the fluid, mineral and dry
rock, respectively, Î¼f , Î¼s and Î¼b are the shear moduli of the fluid, minÂ­
eral, and dry rock, respectively, Î·k and Î·Î¼ are the bulk viscosity and
shear viscosity of the fluid, respectively, Ï11 , Ï12 and Ï22 are the three
density coefficients, b is the dissipation coefficient, ui and ui (i = 1, 2, 3)
are the components of displacements of solid and fluid, respectively. dot
over a variable implies partial derivative with time. eij =
(
)
(1 /2) âˆ‚j ui +âˆ‚i uj and s = âˆ‚u1 /âˆ‚x1 + âˆ‚u2 /âˆ‚x2 + âˆ‚u3 /âˆ‚x3 are the strain

are introduced to obtain the elastic parameters as,

6

Ì‚ z;
Ìƒ â‰ˆ a(
Ìƒ Ï‰1 ) = Î¸(z;
Ìƒ Ï‰1 )
a(z)

(4a)

Ìƒ â‰ˆ n(
Ì‚ z;
Ìƒ Ï‰2 ) = Î¸(z;
Ìƒ Ï‰2 )
n(z)

(4b)

Ì‚ z;
Ìƒ â‰ˆ q(
Ìƒ Ï‰3 ) = Î¸(z;
Ìƒ Ï‰3 )
q(z)

(4c)

Ì‚ z;
Ìƒ â‰ˆ aâ€™(
Ìƒ Ï‰4 ) = Î¸(z;
Ìƒ Ï‰4 )
aâ€™(z)

(4d)

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

fig. 8. predicted p- and s-wave velocities of well #1 (a), well #2 (b), and well #3 (c) by dnn model.

predictions of p- and s-wave velocities have to be computed by using eq.
(1) to establish the loss function.
then we have,

table 2
r2 and relative rmse between the log data and prediction results in fig. 8.
well
#1
#2
#3

attribute
vp
vs
vp
vs
vp
vs

Ìƒ â‰ˆ nâ€™(
Ì‚ z;
Ìƒ Ï‰5 ) = Î¸(z;
Ìƒ Ï‰5 )
nâ€™(z)
Ì‚ z;
Ìƒ â‰ˆ qâ€™(
Ìƒ Ï‰6 ) = Î¸(z;
Ìƒ Ï‰6 )
qâ€™(z)

evaluation metrics
r2

relative rmse

0.9867
0.9917
0.9652
0.9685
0.9743
0.9822

0.0083
0.0081
0.0157
0.0169
0.0128
0.0128

Ì‚ p (z; Ï‰)
vp â‰ˆ v

(5a)

Ì‚ s (z; Ï‰)
vs â‰ˆ v

(5b)

Ì‚s
Ì‚ p and v
where Ï‰ = |Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 , Ï‰5 , Ï‰6 |, z = (Ï•, k, Î¼, Ï, Îº, sw ), and v
represent the p- and s-wave velocities obtained through the plane wave
analysis to the wave propagation equations trained by the dnn model.
thus, the loss function can be defined as mse,
(4e)

loss =

(4f)

n (âƒ’
âˆ‘
âƒ’
âƒ’ )
âƒ’
âƒ’vp,i âˆ’ v
Ì‚ p,i (z; Ï‰)âƒ’2 + âƒ’vs,i âˆ’ v
Ì‚ s,i (z; Ï‰)âƒ’2
i=1

(6)

where vp,i and vs,i are the i-th components of p- and s-wave velocities
Ì‚ p,i and v
Ì‚ s,i are the i-th components
from well log data, respectively, and v

Ì‚ a
Ì‚ Ê¹, n,
Ì‚ and q
Ì‚ Ê¹ are the elastic parameters output by the
Ì‚ n
Ì‚ Ê¹, q,
where a,
dnn model, Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 , Ï‰5 and Ï‰6 are nn parameters, updated by
minimizing the loss function, and Î¸ represents the dnn model. the

of p- and s-wave velocities from dnn model prediction, respectively.
the workflow of the dnn-based method for predicting s-wave ve

fig. 9. crossplot of the measured and predicted (a) p- and (b) s-wave velocities by dnn model.
7

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

fig. 10. same as fig. 9 but for well #2.

fig. 11. same as fig. 9 but for well #3.

fig. 12. predicted p- and s-wave velocities of well #1 (a), well #2 (b), and well #3 (c) by re-trained dnn model.
8

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

table 3
r2 and relative rmse between the log data and prediction results in fig. 12.
well

attribute

evaluation metrics
2

#1

vp
vs
vp
vs
vp
vs

#2
#3

table 4
r2 and relative rmse between the log data and prediction results in fig. 13.
wells

r

relative rmse

0.9842
0.9873
0.9792
0.9787
0.9253
0.9421

0.0091
0.0101
0.0121
0.0139
0.0219
0.0231

#1
#2
#3

step 1 utilizing the parameters Ï•, Ï, Îº, and sw from z to calculate the
three density parameters Ï11 , Ï12 , and Ï22 , as well as the dissiÂ­
pation coefficient b, which are required to solve the wave
propagation equations with eq. (a-4).
Ìƒ into the dnn model, initial values for the six
step 2 by inputting z

r =1âˆ’

)2
2

y)

,y=

âˆ‘n

i=1 yi

n

well

relative
rmse

r2

relative
rmse

r2

relative
rmse

0.9489
0.9104
0.9553

0.0202
0.0286
0.0203

0.9428
0.9345
0.9494

0.0214
0.0244
0.0215

0.9238
0.8141
0.8898

0.0247
0.0412
0.0319

kb = (k* âˆ’ km )

the following two indicators of root-mean-squared error (rmse) and
r-squared (r2) are used to quantitatively evaluate the accuracy of the
predicted results,
âˆšÌ…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…
n
1âˆ‘
2
rmse =
(yi well âˆ’ Ì‚
yi)
(7)
n i=1
Ì‚
yi

r2

3.3.1. the k-t model
the estimation of elastic moduli of dry rock is an important aspect in
rock physics modeling process (keys and xu, 2002; tan et al., 2015).
kuster and toksoÌˆz (1974) utilized the pore aspect ratio (Î±) to quantify
pore geometry. based on the scattering theory, the relation between
elastic moduli, porosity, and pore geometry of rock is established (the
so-called k-t model). then, xu and white (1995) developed the
xu-white model based on the k-t model to compute elastic moduli of
dry rock for predicting s-wave velocity. the equations of k-t model are,

Ì‚ p and v
Ì‚ s obtained in step 2 and the values vp
step 3 using the values v
and vs from the actual log data, the loss function (eq. (6)) is
constructed. during the training process, the nn parameter Ï‰ is
iteratively updated by minimizing the loss function, to support
ultimately establishing the final form of the wave propagation
Ì‚ p and v
Ì‚ s.
equation and solution for the final values of v

well
âˆ’
i=1 yi
âˆ‘n
well âˆ’
i=1 (yi

training with partial
values of vs

3.3. workflow of the rpm-based s-wave velocity prediction

elastic parameters are obtained, followed by establishing the
preliminary wave propagation equation and obtaining initial
Ì‚ p and v
Ì‚ s with eq. (3).
values for v

âˆ‘n (

re-trained dnn model

velocities of the actual log data, Ì‚
y i denotes the i-th set of wave velocities
of the dnn-based prediction, y is the mean of yi well (i = 1, 2, 3, â€¦, n), and
r2 = 1 indicates the dnn model is perfect.

locity is shown in fig. 4, which can be summarized as follows.

2

training with full
values of vs

3kb + 4Î¼m âˆ‘
Î½l tiijj (Î±) + km
9km + 12Î¼m l=s,c

6Î¼b (2Î¼m + km ) + Î¼m (8Î¼m + 9km )
25Î¼m (4Î¼m + 3km )
)
âˆ‘ (
tiijj (Î±)
+ Î¼m
Î½l tijij (Î±) âˆ’
3
l=s,c

(9a)

Î¼b = (Î¼* âˆ’ Î¼m )

(9b)

where km and k* are the bulk moduli of rock matrix and inclusions,
respectively, Î¼m and Î¼* are the shear moduli of rock matrix and inÂ­
clusions, respectively, Î½ is the fractional volume (l = s, c respect the sandand clay-related fractions, respectively), and tiijj and tijij represents the
tensor functions, which are related to pore aspect ratio (berryman,

(8)

where n is the number of data points, yi well denotes the i-th set of wave

fig. 13. predicted s-wave velocities of well #1 (a), well #2 (b), and well #3 (c) from three dnn-based models.
9

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

Î±s can induce apparent elastic fluctuations for both bulk and shear

table 5
commonly-used empirical formulas for s-wave velocity prediction.
model

empirical formulas

han et al. (1986)
castagna et al. (1993)
eskandari et al. (2004)

vs = 0.794vp-0.849
vs = âˆ’ 0.05509v2p+1.0168vp-1.0305
vs = âˆ’ 0.1236v2p+1.612vp-2.3057

moduli (by assuming Ï• = 0.1 and vcl = 0.45), e.g., the shear modulus
increases about 10 gpa from Î±s = 0.2 to Î±s = 0.6 given Î±c = 0.04.
therefore, we should take into account the impact of the aspect ratio on
the elastic properties of rocks with complex pore structures in the
modeling process.
3.3.3. rock physics modeling
in this section, we propose a new rpm-based s-wave prediction
method by combining the v-r-h (voigt-reuss-hill) average, decoupled
k-t model, and poroelasticity theory with viscoelastic fluid. the major
differences from the conventional xu-white model are that the pore
aspect ratio varies with depth and the intrinsic viscous characteristics of
fluid is incorporated in the rock physics modeling. fig. 6 outlines the
workflow of the proposed rpm-based s-wave prediction as follows.

1980).
when the k-t model is applied to compute the elastic moduli of dry
rock, the bulk and shear moduli are coupled. to simplify the modeling
process, keys and xu (2002) assume that poissonâ€™s ratio of the dry rock
frame does not change with porosity, and the equations are transformed
into a system of linear ordinary differential equations. the elastic
moduli of dry rock are
kb = km (1 âˆ’ Ï•)p

(10a)

Î¼b = Î¼m (1 âˆ’ Ï•)q

(10b)

step 1 estimate the bulk and shear moduli of rock matrix by using the vr-h average,
(
)
n
âˆ‘
ki fi 2 + 1
km =
(11a)
2fi
i=1

where p and q are geometric factors associated with pore geometry (see
appendix b).
3.3.2. analysis of pore aspect ratio
when the xu-white model is used to predict s-wave velocity, the
pore aspect ratio assigned to clay- and sand-related pores are set as
0.035 and 0.10, respectively. but there may exist other factors that can
influence the pore aspect ratio, including the pressure, fluid composiÂ­
tion, lithology, diagenetic history, and sedimentary environment (guo
et al., 2021). therefore, if it is assumed that the pore aspect ratio of
reservoirs is fixed within a large depth range, it could potentially lead to
inaccuracies or limitations in the prediction of elastic properties.
fig. 5 shows the influence of the pore aspect ratio on the elastic
moduli of dry rock. it can be observed that the different values of Î±c and

Î¼m =

(
)
n
âˆ‘
Î¼i fi 2 + 1
i=1

2fi

(11b)

where ki, Î¼i, and fi are the bulk modulus, shear modulus and volume
ratio of the i-th component, respectively. the thin section and mineral
analysis results of samples from the target reservoir formation indicate
that the major mineral components of the rocks are quartz, feldspar, and
clay, with minor volumetric content of other minerals. the ratio of
quartz to feldspar is approximately 1.85. in the process of calculating the
matrix moduli, the minerals are considered with quartz, feldspar, and
clay, and the clay content is determined based on the clay content curve

fig. 14. predicted s-wave velocities with the dnn model and three empirical formulas for well #1.
10

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

fig. 15. same as fig. 14 but for well #2.

fig. 16. same as fig. 14 but for well #3.

11

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

table 6
average relative errors between the log data and predictions in figs. 14â€“16.
wells

dnn
model

han et al.
(1986)

castagna et al.
(1993)

eskandari et al.
(2004)

#1
#2
#3

2.06%
3.39%
2.78%

4.69%
5.11%
4.85%

6.28%
5.83%
6.25%

5.90%
5.57%
5.97%

step 4 update petrophysical parameters and re-estimate s-wave
velocity.
4. results
4.1. results with the dnn model
for training the dnn-based s-wave velocity prediction model, the
data set of training model composed of 21,215 data points from 16 wells
are selected. the data points of wells #1-#3 (8800 data points in total)
are used to verify the dnn model performance and analyze the preÂ­
diction accuracy. the loss value during the training process is shown in
fig. 7. it is apparent that the training loss gradually decreases, reaching a
minimum value after 1656 epochs. the convergence indicates that
overfitting is absent, and the trained dnn model possesses reliable
generalization capabilities. the comparison between the predicted pand s-wave velocities obtained from the dnn model (blue curves) and
the actual log data (black curves) are illustrated in fig. 8. it is showed
that the prediction obtained by the dnn model exhibits a good agreeÂ­
ment with the log data.
for the constructed dnn model, the relative rmse and r2 are
computed with the velocity predictions of the three wells, which are
listed in table 2. the relative rmse for the p- and s-wave velocity
predictions based on the dnn model falls within the range from 0.0081
to 0.0169, with r2 values ranging from 0.9652 to 0.9917. overall, the
errors are small. a correlation analysis is conducted to compare the
results of dnn model prediction with the actual data. as is shown in
figs. 9â€“11, the prediction results exhibit a strong correlation with the
actual data, with data scatters are mostly distributed nearby the stanÂ­
dard line.
using the same dnn as is shown in fig. 4 (with all hyperparameters
being set the same), where the input and output are z and (vp, vs),
respectively. in addition, a normalization of the input and output

from the log data. the physical properties of minerals and fluids are
given in table 1.
step 2 estimate the bulk and shear moduli of dry rock frame by using
the decoupled k-t model (eq. (10)). then, the p- and s-wave
velocities of a saturated rock are obtained with eq. (3).
step 3 to address the complex pore structures, we incorporate the
variation of pore aspect ratios with respect to depth in the preÂ­
diction of s-wave velocity. by considering the aspect ratios of
clay- and sand-related pores as unknown variables, they can be
determined by solving the optimization problem calibrated by
real log data, i.e.,
âƒ¦2
âƒ¦
Î±c , Ì‚
Î± s ) = argminâƒ¦vpâˆ’ pre (Î±c , Î±s ) âˆ’ vpâˆ’ well âƒ¦
(12)
(Ì‚
where vpâˆ’ pre is the p- wave velocity predicted by rpm, and vpâˆ’ well is the
p-wave velocity from the log data. in practice, the ranges of Î±c and Î±s are
set as 0.001â€“0.1 and 0.1â€“0.4, respectively.
in addition, according to nur and simmons (1969), there may exist a
statistical correlation between porosity and sand-related pore aspect
ratio, therefore an extra prior constraint can be provided for estimating
pore aspect ratio, which is

Î±s = 0.1762 expâˆ’ 2.22Ï•

(13)

fig. 17. predicted s-wave velocity by rpm method (a) with the relative error (b) and estimated clay-related (c) and sand-related (d) pore aspect ratios of well #1.
12

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

fig. 18. same as fig. 17 but for well #2.

quantities are needed before training because the magnitudes differ
(xiong et al., 2021b). subsequently, the prediction performance is
evaluated with the same test dataset (wells #1-#3). the comparison
between the predicted p- and s-wave velocities obtained from the
re-trained dnn model (green curves) and the actual log data (black
curves) are illustrated in fig. 12. it is showed that the direct prediction

obtained by the re-trained dnn model also exhibits a good agreement
with the log data.
table 3 lists r2 and relative rmse of velocity predictions for the retrained dnn model. r2 for the p- and s-wave velocity predictions based
on the re-trained dnn model falls within the range from 0.9253 to
0.9873, with relative rmse values ranging from 0.0091 to 0.0231. by

fig. 19. same as fig. 17 but for well #3.
13

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

fig. 20. comparison of the relative errors in well #1(a), well #2 (b), and well #3(c) corresponding to the dnn model, rpm, and xu-white model.

table 4 lists r2 and relative rmse of vs predictions for the three
dnn-based models. in three cases, the errors of relative rmse are less
5%. when the s-wave velocity in actual log data is available, both the
dnn-based poroelasticity model and the re-trained dnn model can
predict s-wave velocity with a good precision. when the s-wave velocity
is absent or only a few data points are available, it is impossible to utilize
dnn to establish the relationship between z1 and vs. the dnn-based
poroelasticity model can predict the s-wave velocity when vs labels
are available or only a few data points are available, compared to the
dnn model trained with full data, that with partial data gives a relaÂ­
tively lower r2, but it still maintains a good level of precision. the
analysis indicates that, even under the situations with limited vs inforÂ­
mation, the proposed dnn-based poroelasticity method can predict vs
with acceptable precision.
the dnn model trained with partial vs data is compared to the three
commonly used empirical formulas (as is shown in table 5). the
empirical formulas are derived from the calibration of a large dataset but
may not be universally applicable for the different areas and target
formations. consequently, in predicting vs of wells #1-#3, the empirical
formulas exhibit significant errors (as are depicted in figs. 14â€“16). the
dnn model yields improved prediction accuracy with the better
agreement with the real data. the average relative errors between the
log data and predictions in figs. 14â€“16 are summarized in table 6.

comparing tables 2 and 3 and it is showed that the re-trained dnn
model has a slightly higher prediction accuracy for well # 2 compared to
the dnn-based poroelasticity model. however, for wells # 1 and #3, the
prediction results are lower than those with the dnn-based poroelasÂ­
ticity model. especially for well # 3, r2 between the log data and preÂ­
diction results are lower by 0.049 and 0.0401 for p- and s-wave
velocities, respectively, which could be attributed to the relatively poor
quality of well # 3 data. overall, the errors of the re-trained dnn and
dnn-based poroelasticity models are small, but the dnn-based poroeÂ­
lasticity model shows lower requirements for data quantity and quality
and is physically clear in describing the wave propagations.
by considering the possible absence of s-wave velocity in actual log
data, where k and Î¼ cannot be determined based on p- and s-wave veÂ­
locities and density, we introduce the p-wave modulus. the process
illustrated in fig. 4 is repeated with the parameters z1 = (Ï•, Ï’, Ï, Îº, sw )
Ìƒ1 = (Ï•, Ï’, Îº, sw ), and a new dnn model is trained with the same
and z
dataset. subsequently, the prediction performance is evaluated with the
same test dataset (wells #1-#3).
in the process of constructing a poroelasticity model based on dnn,
the entire vp data are used, while different vs data are considered in the
process of dnn training. additionally, we use the same network archiÂ­
tecture, with inputs and outputs as z1 and vs, respectively. another dnn
model is retrained (re-trained dnn model). fig. 13 shows the preÂ­
dictions of s-wave velocity for the three specific cases, in which the
black curve denotes the log data, the blue curve is the prediction result
of the dnn-based poroelasticity model trained with all vs data, the
brown curve is the prediction result of the re-trained dnn model, and
the orange curve is the prediction result of dnn-based poroelasticity
model trained with partial vs data (ten vs labels). all of the three dnnbased models can provide reasonable predictions.

4.2. results with the rpm
the rpm proposed in section 3.3.3 is employed to predict vs for
wells #1-#3, and is compared with the classical xu-white model. the
physical parameters of fluid and minerals for the prediction procedure
are given in table 1. figs. 17â€“19 shows the comparisons between the
14

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

predictions by the two rpms and the actual log data (the black curve
gives the log data, the red curve denotes the reformulated rpm, and the
blue curve denotes the xu-white model). the relative errors of preÂ­
dictions of the two models are computed. it is showed that the rpm
predictions are better consistent with the log data. the error is less than
10% (mainly within 5%) for the proposed model, while the error is
generally more than 10% for the xu-white model. the average relative
errors of prediction results for the two models are computed for the three
wells, which are 3.57%, 4.76%, and 3.34%, and 5.25%, 13.1%, and
8.01%, for the rpm and xu-white model, respectively. the proposed
model improves the prediction precision of vs.
fig. 20 displays the relative errors of predictions with the dnn model
(trained with partial vs data), rpm, and xu-white model with histoÂ­
grams. as is shown in fig. 20, both the proposed dnn model and rpm
improve the prediction result. the prediction errors of the dnn model
exhibit a relatively narrow distribution, with the average relative errors
of 2.06%, 3.39% and 2.78% for the three wells, which indicates more
reliable and accurate predictions. in contrast, rpm gives the relative
errors mostly within 10%, while the xu-white model exhibits larger
errors. therefore, provided a sufficient data set, the dnn-based
approach yields better vs predictions compared to the traditional rock
physics modeling. notably, although the traditional modeling is indeÂ­
pendent of data quantity, the reformulated rpm for vs prediction can
still provide valuable references under the limited data condition.

method. however, the dnn model is dependent on data quantity and
usually relies on sufficient training data to achieve more accurate preÂ­
dictions, especially for the dnn model that directly outputs wave veÂ­
locities. on the other hand, compared to the xu-white model, the
reformulated rpm better characterizes the pore structures within comÂ­
plex reservoirs and thereby predicts s-wave velocity with higher preciÂ­
sion. this work employs a fully-connected nn, while different nns
might be considered for a comparative analysis in future studies.
credit authorship contribution statement
zhijian fang: writing â€“ original draft, visualization, validation,
resources, methodology, investigation, formal analysis, conceptualiÂ­
zation. jing ba: writing â€“ review & editing, writing â€“ original draft,
validation, supervision, resources, project administration, methodolÂ­
ogy, investigation, funding acquisition, formal analysis, data curation,
conceptualization. qiang guo: writing â€“ review & editing, validation,
methodology, formal analysis. fansheng xiong: validation, software,
investigation, formal analysis.
declaration of competing interest
the authors declare that they have no known competing financial
interests or personal relationships that could have appeared to influence
the work reported in this paper.

5. conclusion

data availability

this study focuses on the comparisons of s-wave velocity prediction
with the dnn- and rpm-based methods in the tight reservoirs of the
chang 7 shale oil formation, ordos basin. based on the assumption that
the mathematical formulation that describes the elastic wave propagaÂ­
tion within the reservoirs is known (the poroelasticity equations with
viscoelastic fluid), dnn model is trained to obtain elastic parameters
and subsequently predict s-wave velocity. in contrast to the classic xuwhite model, a reformulated rpm for predicting s-wave velocity is also
developed by considering the variation of pore aspect ratio with depth.
the two methods are tested with log data from the three wells in the
work area and achieve good precision. in general, the dnn-based
approach shows a better prediction performance compared to the rpm

data will be made available on request.
acknowledgments
we would like to appreciate the support of the national natural
science foundation of china (42174161), the fundamental research
funds for the central universities (b240205030), the postgraduate
research & practice innovation program of jiangsu province
(kycx24_0886), and the natural science foundation of jiangsu provÂ­
ince (bk20220995).

appendix a. plane-wave solution to the poroelasticity wave equations with viscoelastic fluid
by substituting a plane-wave kernel into eq. (1), we can obtain the characteristic equations for wave velocities, and that for the p-waves is
âƒ’
âƒ’
âƒ’ a11 k2 + b11 a12 k2 + b12 âƒ’
âƒ’
âƒ’
(a-1)
âƒ’ a21 k2 + b21 a22 k2 + b22 âƒ’ = 0
where
/
/
(
)
(
)
a11 = a + 2n, a12 = q iÏ‰Î·k 2kf + 1 , a21 = q, a22 = (aÊ¹ + 2nÊ¹) iÏ‰Î·k 2kf + 1

(a-2)

b11 = âˆ’ Ï11 Ï‰2 + ibÏ‰, b12 = âˆ’ Ï12 Ï‰2 âˆ’ ibÏ‰, b21 = b12 , b22 = âˆ’ Ï22 Ï‰2 + ibÏ‰

(a-3)

Ï12 = (1 âˆ’ Î²)Ï•Ïf , Ï11 = (1 âˆ’ Ï•)Ïs âˆ’ Ï12 , Ï22 = Ï•Ïf âˆ’ Ï12 , Î² = (1/4)(1/Ï• + 1)

(a-4)

where the dissipation coefficient b = (Î·/Îº)Ï•2 , and Ïs and Ïf are the solid and fluid densities, respectively. according to carcione et al. (2006), Ïf =

Ïw sw + Ïo (1 âˆ’ sw ) and Î· = Î·w (Î·o /Î·w )sw , where Ïw = 1 g/cm3 and Ïo = 0.8 g/cm3 are the water and oil densities, respectively, Î·w = 1 mpaâ‹… s and Î·o =
1.44 mpaâ‹…s are the viscosity of water and oil, respectively, and Ïs is inferred with Ï = Ï•Ïf + (1 âˆ’ Ï•)Ïs . the angular frequency Ï‰ = 2Ï€f. in this work, we
set f = 104 hz.
similarly, we obtain the equation for s-waves as
âƒ’
âƒ’
âƒ’ c11 k2 + d11 c12 k2 + d12 âƒ’
âƒ’
âƒ’
âƒ’ c21 k2 + d21 c22 k2 + d22 âƒ’ = 0

(a-5)

15

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

where

/
)
/
)
(
/
(
c11 = n, c12 = (qÊ¹ / 2) 1 + iÏ‰Î·Î¼ 2Î¼f , c21 = qâ€™ 2, c22 = nâ€™ 1 + iÏ‰Î·Î¼ 2Î¼f

(a-6)

d11 = âˆ’ Ï11 Ï‰2 + ibÏ‰, d12 = âˆ’ Ï12 Ï‰2 âˆ’ ibÏ‰, d21 = d12 , d22 = âˆ’ Ï22 Ï‰2 + ibÏ‰

(a-7)

then the p- and s-wave velocities are determined as,
vp,s =

Ï‰

(a-8)

)
(
re kp,s

appendix b. the geometric factors p and q
p=

1âˆ‘
Î½l tiijj (Î±l )
3 l=s,c

(b-1)

q=

(
)
tiijj (Î±l )
1âˆ‘
Î½l tijij (Î±l ) âˆ’
5 l=s,c
3

(b-2)

where
tiijj (Î±) =

3f1
f2

(b-3)

tijij (Î±) =

f2 + f1 f4 + f4 f5 + f6 f7 âˆ’ f8 f9 2
âˆ’
f3
f2 f4

(b-4)

with

[
(
)]
3
3
5
4
f1 = 1 + s (g + z) âˆ’ d g + z âˆ’
2
2
2
3

(b-5)

]
[
3
d
f2 = 1 + s (g + z) âˆ’
(3g + 5z) + c(3 âˆ’ 4d)+
2
2

(b-6)

[
(
)]
s
(s + 3c)(s âˆ’ 4d) g + z âˆ’ d g âˆ’ z + 2z2
2
]
[
s
1 + Î±2
f3 = 1 + d(2 âˆ’ z) +
(d âˆ’ 1)g
2
2
Î±

(b-7)

s
f4 = 1 + [3z + g âˆ’ d(g âˆ’ z)]
4
[ (
)
]
4
âˆ’ g + cz(3 âˆ’ 4d)
f5 = s d g + z âˆ’
3

(b-8)
(b-9)

f6 = 1 + s[1 + g âˆ’ d(g + z)] + c(1 âˆ’ z)(3 âˆ’ 4d)

(b-10)

s
f7 = 2 + [9z + 3g âˆ’ d(5z + 3g) + cz(3 âˆ’ 4d)]
4
]
[
g
z
f8 = s 1 âˆ’ 2d + (d âˆ’ 1) + (5d âˆ’ 3) + c(1 âˆ’ z)(3 âˆ’ 4d)
2
2
)
(
Î¼*
1 k*
Î¼*
3Î¼m
s=
, and d =
âˆ’ 1, c =
âˆ’
3 km Î¼m
Î¼m
3km + 4Î¼m
g=

Î±2

1 âˆ’ Î±2

(3z âˆ’ 2) and z =

(b-11)
(b-12)
(b-13)

[
âˆšÌ…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì… ]
Î±2
cosâˆ’ 1 (Î±) âˆ’ Î± 1 âˆ’ Î±2
3/2
2
(1 âˆ’ Î± )

(b-14)

16

z. fang et al.

geoenergy science and engineering 240 (2024) 213028

references

guo, q.l., chen, x.m., liuzhuang, x.x., yang, z., zheng, m., chen, n.s., mi, j.k., 2020.
evaluation method for resource potential of shale oil in the triassic yanchang
formation of the ordos basin, china. energy explor. exploit. 38 (4), 841â€“866.
https://doi.org/10.1177/0144598720903394.
guo, q., ba, j., luo, c., pang, m.q., 2021. seismic rock physics inversion with varying
pore aspect ratio in tight sandstone reservoirs. j. petrol. sci. eng. 207, 109131
https://doi.org/10.1016/j.petrol.2021.109131.
guo, j.x., gurevich, b., chen, x.f., 2022. dynamic sv-wave signatures of fluid-saturated
porous rocks containing intersecting fractures. j. geophys. res. solid earth 127,
e2022jb024745. https://doi.org/10.1029/2022jb024745.
han, d., nur, a., morgan, d., 1986. effects of porosity and clay content on wave
velocities in sandstones. geophysics 51 (11), 2093â€“2197. https://doi.org/10.1190/
1.1442062.
han, t.c., yan, h., fu, l.-y., xu, d.h., 2022. effective medium modeling of the joint
elastic-electrical properties of sandstones with partial water saturation. geophysics
87 (3), mr129â€“mr137. https://doi.org/10.6038/cjg2020o0021.
jian, s.k., fu, l.-y., wang, z.w., han, t.c., liu, j.l., 2020. elastic equivalent numerical
modeling based on the dynamic method of longmaxi formation shale digital core.
chinese. j. geophys. 63 (7), 2786â€“2799 in chinese.
kelvin, t.w., 1875. mathematical and physical papers: volume3: elasticity, heat,
electro-magnetism. cambridge univ. press.
keys, r.g., xu, s., 2002. an approximation for the xu-white velocity model. geophysics
67 (5), 1406â€“1414. https://doi.org/10.1190/1.1512786.
kingma, d.p., ba, j., 2014. adam: a method for stochastic optimization arxiv preprint
arxiv:1412.6980.
kuster, g.t., toksoÌˆz, m.n., 1974. velocity and attenuation of seismic waves in two-phase
media: part i. theoretical formulations. geophysics 39, 587â€“618. https://doi.org/
10.1190/1.1440450.
li, y.e., oâ€™malley, d., beroza, g., curtis, a., johnson, p., 2023. machine learning
developments and applications in solid-earth geosciences: fad or future?
j. geophys. res. solid earth 128, e2022jb026310. https://doi.org/10.1029/
2022jb026310.
liu, n., fu, l.-y., cao, c.h., liu, j.l., 2020. research on numerical modeling method of
lsm-rvm and toc content influence for digital core from longmaxi formation
shale. chin. j. geophys. 63 (7), 2774â€“2785 in chinese.
luo, c., ba, j., guo, q., 2023. probabilistic seismic petrophysical inversion with
statistical double-porosity biot-rayleigh model. geophysics 88 (3), m157â€“m171.
https://doi.org/10.1190/geo2022-0288.1.
mavko, g., mukerji, t., dvorkin, j., 2020. the rock physics handbook. cambridge
university press.
mehrgini, b., izadi, h., memarian, h., 2017. shear wave velocity prediction using elman
artificial neural network. carbonates evaporites 34 (4), 1281â€“1291. https://doi.org/
10.1007/s13146-017-0406-x.
ni, w., qi, l., tao, f., 2017. prediction of shear wave velocity in shale reservoir based on
logging data and machine learning. in: proc. int. conf. softw. eng. knowl. eng. ieee,
london, uk, pp. 231â€“234, 21â€“23 october 2017.
norris, a.n., 1985. a differential scheme for the effective moduli of composites. mech.
mater. 4 (1), 1â€“16. https://doi.org/10.1016/0167-6636(85)90002-x.
nur, a., simmons, g., 1969. stress-induced velocity anisotropy in rock: an experimental
study. j. geophys. res. 74 (27), 6667â€“6675. https://doi.org/10.1029/
jb074i027p06667.
oloruntobi, o., onalo, d., adedigba, s., james, l., chunduru, r., butt, s., 2019. datadriven shear wave velocity prediction model for siliciclastic rocks. j. petrol. sci. eng.
183, 106293 https://doi.org/10.1016/j.petrol.2019.106293.
pang, m.q., ba, j., carcione, j.m., zhang, l., ma, r.p., wei, y.j., 2021. seismic
identification of tight-oil reservoirs by using 3d rock-physics templates. j. petrol.
sci. eng. 201, 108476 https://doi.org/10.1016/j.petrol.2021.108476.
paszke, a., et al., 2017. automatic differentiation in pytorch. in: von luxburg, ulrike,
guyon, isabelle, bengio, samy, wallach, hanna, fergus, rob (eds.), proceedings of
the 31st conference on neural information processing systems (nips 2017),
december 4â€“9, 2017. long beach, ca.
pickett, g.r., 1963. acoustic character logs and their applications in formation
evaluation. j. petrol. technol. 15, 650â€“667. https://doi.org/10.2118/452-pa.
qi, q., cheng, a.c.h., li, y.e., 2019. determination of formation shear attenuation from
dipole sonic log data. geophysics 84 (3), d73â€“d79. https://doi.org/10.1190/
geo2018-0006.1.
qin, n., fu, l.-y., 2013. reliability measurement of joint seismic inversion based on
seismic-to-well correlation. explor. geophys. 44, 85â€“98. https://doi.org/10.1071/
eg12043.
qing, x., zhao, l.x., cai, z.j., wang, y., xu, m.h., zhang, f.s., han, d.h., geng, j.h.,
2022. compressional and shear wave velocities relationship in anisotropic organic
shales. j. petrol. sci. eng. 219, 111070 https://doi.org/10.1016/j.
petrol.2022.111070.
rabbani, a., schmitt, d.r., nycz, j., gray, k., 2017. pressure and temperature
dependence of acoustic wave speeds in bitumen-saturated carbonates: implications
for seismic monitoring of the grosmont formation. geophysics 82 (5),
mr133â€“mr151. https://doi.org/10.1190/geo2016-0667.1.
rajabi, m., bohloli, b., gholampour ahangar, e., 2010. intelligent approaches for
prediction of compressional, shear and stoneley wave velocities from conventional
well log data: a case study from the sarvak carbonate reservoir in the abadan plain
(southwestern iran). comput. geosci. 36 (5), 647â€“664. https://doi.org/10.1016/j.
cageo.2009.09.008.
rao, y., fu, l.-y., wang, z.w., fu, b.y., 2021. multiscale reconstructions, effective elastic
properties, and ultrasonic responses of kerogen matter based on digital organic
shales. ieee access 9, 43785â€“43798. https://doi.org/10.1109/
access.2021.3058944.

alfarraj, m., alregib, g., 2019. semisupervised sequence modeling for elastic impedance
inversion. interpretation 7 (3), se237â€“se249. https://doi.org/10.1190/int-20180250.1.
anemangely, m., ramezanzadeh, a., tokhmechi, b., 2017. shear wave travel time
estimation from petrophysical logs using anfis-pso algorithm: a case study from
ab-teymour oilfield. j. nat. gas sci. eng. 38, 373â€“387. https://doi.org/10.1016/j.
jngse.2017.01.003.
arunkumar, k.e., kalaga, d.v., kumar, c.m.s., kawaji, m., brenza, t.m., 2021.
forecasting of covid-19 using deep layer recurrent neural networks (rnns) with
gated recurrent units (grus) and long short-term memory (lstm) cells. chaos, solit.
fractals 146, 110861. https://doi.org/10.1016/j.chaos.2021.110861.
azadpour, m., saberi, m.r., javaherian, a., shabani, m., 2020. rock physics modelbased prediction of shear wave velocity utilizing machine learning technique for a
carbonate reservoir, southwest iran. j. petrol. sci. eng. 195, 107864 https://doi.org/
10.1016/j.petrol.2020.107864.
ba, j., carcione, j.m., nie, j.x., 2011. biot-rayleigh theory of wave propagation in
double-porosity media. j. geophys. res. solid earth 116, b06202. https://doi.org/
10.1029/2010jb008185.
ba, j., fang, z.j., fu, l.y., guo, q., 2023a. seismic inversion method for tight sandstone
reservoir properties based on a variable critical porosity model. chin. j. geophys. 6
(66), 2576â€“2591. https://doi.org/10.6038/cjg2022q0106 in chinese.
ba, j., fang, z.j., fu, l.y., xu, w.h., zhang, l., 2023b. acoustic wave propagation in a
porous medium saturated with a kelvin-voigt non-newtonian fluid. geophys. j. int.
235 (3), 2056â€“2077. https://doi.org/10.1093/gji/ggad355.
bai, y.l., ma, y.h., 2020. geology of the chang 7 member oil shale of the yanchang
formation of the ordos basin in central north china. pet. geosci. 26 (2), 355â€“371.
https://doi.org/10.1144/petgeo2018-091.
benteÌjac, c., csoÌˆrgoÌ‹, a., martÃ­nez-munÌƒoz, g., 2021. a comparative analysis of gradient
boosting algorithms. artif. intell. rev. 54, 1937â€“1967.
berryman, j.g., 1980. long wavelength propagation in composite elastic media i.
ellipsoidal inclusions. j. acoust. soc. am. 68, 1820â€“1831. https://doi.org/10.1121/
1.385172.
boateng, c.d., fu, l.-y., 2018. analysis of reservoir heterogeneities and depositional
environments: a new method. explor. geophys. 49 (6), 868â€“880. https://doi.org/
10.1071/eg17072.
boateng, c.d., fu, l.-y., wu, y., guan, x.z., 2017. porosity inversion by caianiello neural
networks with levenberg-marquardt optimization. interpretation 5 (3), sl33â€“sl42.
https://doi.org/10.1190/int-2016-0119.1.
boateng, c.d., fu, l.-y., danuor, s.k., 2020. characterization of complex fluvioâ€“deltaic
deposits in northeast china using multi-modal machine learning fusion. sci. rep. 10
(1), 13357 https://doi.org/10.1038/s41598-020-70382-7.
botterill, t.e., mcmillan, h.k., 2023. using machine learning to identify hydrologic
signatures with an encoderâ€“decoder framework. water resour. res. 59,
e2022wr033091 https://doi.org/10.1029/2022wr033091.
brereton, r.g., lloyd, g.r., 2010. support vector machines for classification and
regression. analyst 135 (2), 230â€“267. https://doi.org/10.1039/b918972f.
carcione, j.m., picotti, s., gei, d., rossi, g., 2006. physics and seismic modeling for
monitoring co2 storage. pure appl. geophys. 163, 175â€“207. https://doi.org/
10.1007/s00024-005-0002-1.
castagna, j.p., batzle, m.i., eastwood, r.i., 1985. relationship between compressional
wave and shear wave velocities in clastic silicate rock. geophysics 50 (5), 571â€“581.
https://doi.org/10.1190/1.1441933.
castagna, j.p., batzle, m.l., kan, t.k., 1993. rock physics: the link between rock
properties and avo response, offset-dependent reflectivity - theory and practice of
avo analysis. seg 135â€“171.
cheng, y.f., fu, l.-y., 2022. nonlinear seismic inversion by physics-informed caianiello
convolutional neural networks for overpressure prediction of source rocks in the
offshore xihu depression, east china. j. petrol. sci. eng. 215, 110654 https://doi.
org/10.1016/j.petrol.2022.110654.
downton, j.e., 2005. seismic parameter estimation from avo inversion. university of
calgary, department of geology and geophysics.
eskandari, h., rezaee, m.r., mohammadnia, m., 2004. application of multiple
regression and artificial neural network techniques to predict shear wave velocity
from wireline log data for a carbonate reservoir. south-west iran: cseg recorder 42,
40â€“48.
feng, g., zeng, h.h., xu, x.r., tang, g.y., wang, y.x., 2023. shear wave velocity
prediction based on deep neural network and theoretical rock physics modeling.
front. earth sci. 10, 1025635 https://doi.org/10.3389/feart.2022.1025635.
fu, l.-y., 2004. joint inversions of seismic data for acoustic impedance. geophysics 69
(4), 994â€“1004. https://doi.org/10.1190/1.1778242.
fu, j.h., li, s.x., niu, x.b., deng, x.q., zhou, x.p., 2020a. geological characteristics and
exploration of shale oil in chang 7 member of triassic yanchang formation, ordos
basin, nw china. pet. explor. dev. 47 (5), 931â€“945. https://doi.org/10.1016/
s1876-3804(20)60107-0.
fu, s., fu, j., niu, x., li, s., wu, z., zhou, x., liu, j., 2020b. accumulation conditions and
key exploration and development technologies in qingcheng oilfield. acta pet. sin.
41 (7), 777â€“795.
gassmann, f., 1951. uber die elastic poroser media. vierteljahrschrift der
naturforschenden gesellschaft in zÃ¼rich 96, 1â€“23.
guelman, l., 2012. gradient boosting trees for auto insurance loss cost modeling and
prediction. expert syst. appl. 39 (3), 3659â€“3667. https://doi.org/10.1016/j.
eswa.2011.09.058.
guo, m.q., fu, l.-y., 2007. stress associated coda attenuation from ultrasonic waveform
measurements. geophys. res. lett. 34 (9), 1â€“5.

17

z. fang et al.

geoenergy science and engineering 240 (2024) 213028
xu, s., white, r.e., 1996. a physical model for shear wave velocity prediction. geophys.
prospect. 44, 687â€“717. https://doi.org/10.1111/j.1365-2478.1996.tb00170.x.
yang, j.q., lin, n.t., zhang, k., jia, l.y., zhang, d., 2023. an improved integration
strategy for prediction of shear wave velocity using petrophysical logs: integration of
spatiotemporal and small sample nonlinear feature. geoenergy science and
engineering 230, 212270. https://doi.org/10.1016/j.geoen.2023.212270.
you, n., li, y.e., cheng, a., 2020. shale anisotropy model building based on deep neural
networks. j. geophys. res. solid earth 125, e2019jb019042. https://doi.org/
10.1029/2019jb019042.
you, n., li, y.e., cheng, a., 2021. 3d carbonate digital rock reconstruction using
progressive growing gan. j. geophys. res. solid earth 126, e2021jb021687.
https://doi.org/10.1029/2021jb021687.
young, t., hazarika, d., poria, s., cambria, e., 2018. recent trends in deep learning
based natural language processing. ieee comput. intell. mag. 13 (3), 55â€“75. https://
doi.org/10.48550/arxiv.1708.02709.
zhang, l., ba, j., carcione, j.m., 2021. wave propagation in infinituple-porosity media.
j. geophys. res. solid earth 126. https://doi.org/10.1029/2020jb021266
e2020jb021266.
zhang, b., fu, l.-y., wei, w., guan, x.z., 2014. acoustic wave attenuation analysis of the
abnormal geopressure and seismic extrapolation inversion of the formation pressure.
chinese. j. geophys. 57 (10), 3373â€“3388 in chinese.
zhang, b., jin, s., liu, c., guo, z., liu, x., 2020a. prediction of shear wave velocity based
on a statistical rock-physics model and bayesian theory. j. petrol. sci. eng. 195,
107710 https://doi.org/10.1016/j.petrol.2020.107710.
zhang, y., zhong, h., wu, z., zhou, h., ma, q., 2020b. improvement of petrophysical
workflow for shear wave velocity prediction based on machine learning methods for
complex carbonate reservoirs. j. petrol. sci. eng. 192, 107234 https://doi.org/
10.1016/j.petrol.2020.107234.
zhao, w., chellappa, r., phillips, p.j., rosenfeld, a., 2003. face recognition: a literature
survey. acm comput. surv. 35 (4), 399â€“458. https://doi.org/10.1145/
954339.954342.
zhong, c., geng, f., zhang, x., zhang, z., wu, z., jiang, y., 2021. shear wave velocity
prediction of carbonate reservoirs based on cat boost. in: 2021 4th international
conference on artificial intelligence and big data (icaibd), pp. 622â€“626. chengdu,
china.
zimmerman, r.w., 1984. elastic moduli of a solid with spherical pores: new selfconsistent method. int. j. rock mech. min. sci. geomech. abstr. 21 (6), 339â€“343.
https://doi.org/10.1016/0148-9062(84)90366-8.
zou, c.f., zhao, l.x., xu, m.h., chen, y.y., geng, j.h., 2021. porosity prediction with
uncertainty quantification from multiple seismic attributes using random forest.
j. geophys. res. solid earth 126, e2021jb021826. https://doi.org/10.1029/
2021jb021826.

ren, s.b., han, t.c., fu, l.-y., 2020. theoretical and experimental study of p-wave
attenuation in partially saturated sandstones under different pressures. chin. j.
geophys. 63 (7), 2722â€“2736. https://doi.org/10.1190/geo2021-0412.1.
sun, y., liu, y., 2020. prediction of s-wave velocity based on gru neural network. oil
geophys. prospect. 55 (3), 484â€“492.
tan, m., peng, x., cao, h., wang, s.x., yuan, y.j., 2015. estimation of shear wave
velocity from wireline logs in gas-bearing shale. j. petrol. sci. eng. 133, 352â€“366.
https://doi.org/10.1016/j.petrol.2015.05.020.
vernik, l., castagna, j., omovie, s.j., 2018. s-wave velocity prediction in
unconventional shale reservoirs. geophysics 83 (1), mr35â€“mr45. https://doi.org/
10.1190/geo2017-0349.1.
voigt, w., 1892. ueber innere reibung fester koÌˆrper, insbesondere der metalle. ann.
phys. 283, 671â€“693.
wang, j., cao, j.x., yuan, s., 2020. shear wave velocity prediction based on adaptive
particle swarm optimization optimized recurrent neural network. j. petrol. sci. eng.
194, 107466 https://doi.org/10.1016/j.petrol.2020.107466.
wang, z.d., zhang, q., liu, j.l., fu, l.-y., 2021. effective moduli of rocks predicted by
the kuster-toksoÌˆz and mori-tanaka models. j. geophys. eng. 18 (4), 539â€“557.
https://doi.org/10.1093/jge/gxab034.
wang, l.y., zhu, s.p., luo, c.q., liao, d., wang, q.y., 2023a. physics-guided machine
learning frameworks for fatigue life prediction of am materials. int. j. fatig. 172,
107658 https://doi.org/10.1016/j.ijfatigue.2023.107658.
wang, y., wang, w.k., ma, z.t., zhao, m., li, w.x., hou, x.y., li, j., ye, f., ma, w.j.,
2023b. a deep learning approach based on physical constraints for predicting soil
moisture in unsaturated zones. water resour. res. 59, e2023wr035194 https://doi.
org/10.1029/2023wr035194.
wang, z.w., fu, l.-y., jian, s.k., deng, w.b., 2023c. application of a model-driven
simultaneous prestack inversion of rock physical properties in ultra-deep ordovician
carbonate reservoirs in the shunbei area. front. earth sci. 10, 1035735 https://doi.
org/10.3389/feart.2022.1035735.
wu, m., fu, l.-y., li, w.x., 2008. ahigh-resolution nonlinear inversion method of
reservoir parameters and its application to oil/gas exploration. chin. j. geophys. 51,
255â€“266 in chinese.
xiong, f.s., ba, j., gei, d., carcione, j.m., 2021a. data-driven design of wavepropagation models for shale-oil reservoirs based on machine learning. j. geophys.
res. solid earth 126, e2021jb022665. https://doi.org/10.1029/2021jb022665.
xiong, f.s., yong, h., chen, h., wang, h., shen, w.d., 2021b. biotâ€™s equations-based
reservoir parameter inversion using deep neural networks. j. geophys. eng. 18 (6),
862â€“874. https://doi.org/10.1093/jge/gxab057.
xu, s., white, r.e., 1995. a new velocity model for clay-sand mixtures. geophys.
prospect. 43 (1), 91â€“118. https://doi.org/10.1111/j.1365-2478.1995.tb00126.x.

18



downloaded 06/16/20 to 130.238.7.40. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/

physics informed neural networks for velocity inversion

yiran xu1,2, jingye li1, xiaohong chen1
1
state key laboratory of petroleum resources and prospecting, china university of petroleum, beijing, 102249, china
2
division of applied mathematics, brown university, providence, ri 02912, usa
summary
in this abstract, a new neural network, physics-informed
neural networks (pinns) (m. raissi, 2019) are introduced
and implemented to solve the inversion problems of wave
equations. pinns employ standard feedforward neural
networks (nns) with the partial differential equations (pdes)
explicitly encoded into the nn using automatic
differentiation, while the sum of the mean-squared error in
initial/boundary conditions is minimized with respect to the
nn parameters. specifically, here we use this network
structure to produce an accurate velocity model from seismic
data. our approach relies on training deep neural networks
that are extended to encode the acoustic wave equation. in
the first case, given analytical solution to an initial boundary
value problem (ibvp) to infer very accurately the velocity
parameter. in the second case, given seismic wavefield data
in space-time, we use several coupled deep neural networks
to infer vary accuracy the velocity field. after compared the
results with full waveform inversion (fwi), the promising
results for synthetic 2d data demonstrate a new way of using
seismic data to identify key structures in the subsurface from
machine learning approaches.
introduction
seismic inversion aims to reconstruct an earth subsurface
model based on seismic measurements. such a subsurface
model is quantitatively represented by spatially variable
physical parameters, and is extracted from seismic data by
solving an inverse problem (wang, 2016). while inverse
problem is well understood, it often leads to cumbersome
numerical methods which generally work very differently
from human interpreters(roth and tarantola, 1994). recent
advances in machine learning to new data analyzing have the
potential to revolutionize our understanding to of the
physical world in modern application areas such as image
recognition(krizhevsky et al., 2012), neuroscience(lake et
al.,
2015),
cognitive
science,
finance,
and
genomics(alipanahi et al., 2015). in the recent research
about geophysics, the explosive growth of literatures
concentrates on the following aspects.
â€¢
â€¢
â€¢
â€¢
â€¢

compressive sensing by feature learning
structured dictionary learning for seismic data
fault imaging in 3d seismic using machine
learning
segmentation of shale sem by machine learning
deep learning prior models for full waveform
inversion (fwi)

Â© 2019 seg
seg international exposition and 89th annual meeting

however, few articles use machine learning to explore the
inherent constitutive relations of seismic data. in other
words, less well studied is how to discover the underlying
physical laws expressed by partial differential equations
from scattered data collected in space and time.
in this work, inspired by recent development in physicsinformed deep learning, which take a different approach by
employing deep neural networks and leverage their wellknown capability as universal function approximation. we
focus on the construction of nonlinear regression models that
can uncover the dynamic dependencies in a given set of
spatio-temporal dataset. more specifically, for the acoustic
wave equation, using the seismic wavefield dataset, we
return a closed form model that can be subsequently used to
discover the parameters in the equations.
formulation of the problem
in this work, we consider parametrized and nonlinear formed
acoustic wave equation case.
ğ‘¢"" + ğ‘¢$$ âˆ’ ğœ†ğ‘¢'' = ğ‘†(ğ’“, ğ‘¡), (ğ‘¥, ğ‘¦) âˆˆ ğ›º, ğ‘¡ âˆˆ [0, ğ‘‡]

(1)

where ğ‘¢(ğ‘¥, ğ‘¦, ğ‘¡) denotes the acoustic pressure , ğœ† =
1; where ğ‘ is the wave velocity to be determined, ğ‘ (ğ’“, ğ‘¡)
ğ‘:
is the source term as a function of space and time, ğ›º is a
subset of ğ‘…> .
we define ğ‘“(ğ‘¡, ğ‘¥, ğ‘¦) to be given by the equation (1)
ğ‘“: = ğ‘¢"" + ğ‘¢$$ âˆ’ ğœ†ğ‘¢'' âˆ’ ğ‘†(ğ’“, ğ‘¡)

(2)

the parameters ğœ† of the wave equation as well as the
parameters of neural networks ğ‘“(ğ‘¡, ğ‘¥, ğ‘¦) can be trained by
minimizing the mean squared error loss
c

d

ğ‘€ğ‘†ğ¸: = e
d

c

d

e
d

ilc

ilc

:

fgğ‘¢hğ‘¡ i , ğ‘¥ i , ğ‘¦ i j âˆ’ ğ‘¢i g k +
:

fgğ‘“hğ‘¡ i , ğ‘¥ i , ğ‘¦ i jg k

(3)

to generate a high-resolution data set for this problem, we
have employed the time-domain explicit finite-difference
method based on minimizing the mixed ğ‘˜ (wavenumber)space domain function approximation (wang et al., 2017).

10.1190/segam2019-3216823.1
page 2584

double click here to type your header

downloaded 06/16/20 to 130.238.7.40. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/

examples
2d ibvp with analytical solution
our first example involves an initial boundary value
problems (ibvp) with two-dimensional wave equation.
the ibvp was solved over the circular domain given by
ğ¼ = {(ğ‘¥, ğ‘¦)|ğ‘¥ : + ğ‘¦ : â‰¤ 1} , and over the time interval t âˆˆ
[0,1]. the wave equation is given by
tu v
tu '

tu v

tu v

= ğ‘ : (tu" + tu$),

(4)

where c is the wave speed, taken to be 2, the above pde is
subject to the dirichlet boundary condition
u(ğ‘¥, y, t) = 0 , (ğ‘¥, ğ‘¦) âˆˆ ğœ•ğ¼

(5)

and initial conditions
u(ğ‘¥, y, t) = ğ½\ fğœ†] ^ğ‘¥ : + ğ‘¦ : k (x, y) âˆˆ ğ¼
`a
`b

(ğ‘¥, ğ‘¦, 0) = 0

(x, y) âˆˆ ğ¼

(6)
(7)

ğ½\ (âˆ™) represent a bessel function of the first kind,
(fc)g "

:h
ğ½\ (r) = âˆ‘j
,
hl\ (h!)u (:)

(8)

and Î»] represents the 4th zero of ğ½\ (âˆ™). the ibvp has the
analytical solution
ğ‘¢(ğ‘¥, ğ‘¦, ğ‘¡) = ğ½\ fğœ†] ^ğ‘¥ : + ğ‘¦ : kcos (ğ‘ğœ†] ğ‘¡).
the analytical solution to (4) are shown in figure 1.

(9)

figure 1. the analytical solution of ibvp.
2d acoustic wave equation with source term
this example aims to highlight the ability of our method to
handle acoustic wave equation. the acoustic wave equation
is a classical field equation that is used to study the p-wave
propagation in the medium. as shown in (1), we employ the
ricker wavelet as source term, the center frequency is set
16hz. in order to assess the accuracy of our method, we have
simulated (1) using explicit finite difference method with
spatial arbitrary even-order accuracy. the model size is
3ğ‘˜ğ‘š Ã— 6ğ‘˜ğ‘š with 10m gridding space.
given numerical scattered wavefield ğ‘¢(ğ‘¡, ğ‘¥, ğ‘¦), our goal is
to identify the unknown parameters ğ‘£(ğ‘¥, ğ‘¦), for the complex
model, the velocity varies with spatial position. to
demonstrate the ability of our method to learn from scattered
training data, we have chosen ğ‘ = 500000, corresponding
to a mere 0.3% of the total available simulation data resulted
by the forward modeling. the neural network architecture
(as shown in figure 2) used here consists of 8 layers with 20
neurons in each layer.
the velocity inverse process with pinns is depicted in
figure 3, it performs the inverse of the velocity model from
wavefield, in the realistic practice, itâ€™s hard to get the
ground-truth model, and the pinns is designed to minimize
the difference between the inversed velocity model and
ground-truth one by optimizing the mse loss function (3).
as shown in figure 4, we test several layered models to
validate the ability of pinns to inverse the velocity from
seismic data. after comparing with fwi results, we can see
the accuracy and flexibility of pinns to build the velocity
model.
table 1. correct pde along with the identified one
obtained by learning

1d ibvp
2d ibvp

true parameter

identified parameter

ğ‘¢"" âˆ’ 0.25ğ‘¢'' = 0

ğ‘¢"" âˆ’ 0.24992ğ‘¢'' = 0

ğ‘¢"" + ğ‘¢$$ âˆ’ 0.25ğ‘¢'' = 0

ğ‘¢"" + ğ‘¢$$ âˆ’ 0.24986ğ‘¢'' = 0

Â© 2019 seg
seg international exposition and 89th annual meeting

relative error
0.32%
0.56%

10.1190/segam2019-3216823.1
page 2585

downloaded 06/16/20 to 130.238.7.40. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/

double click here to type your header

figure 2. acoustic wave informed neural networks: a plain vanilla densely connected neural network, with 8 hidden layers and
20 neurons per hidden layer per output variable, take the input variable ğ‘¡, ğ‘¥, ğ‘¦ and output ğ‘¢, as for the activation function, we use
ğœ(ğ‘¥) = tan(ğ‘¥). for illustration purpose only, the network depicted in this figure comprises three hidden layers and six neurons
per hidden layer. we employ automatic differentiation to obtain the required derivatives to compute the residual (physicsinformed) networks ğ‘“. the total loss function is composed of the regression loss of the wave pressure ğ‘¢ on the training data and
the loss imposed by the differential equations ğ‘“. here, the differential operators ğœ•' , ğœ•" , and ğœ•$ are computed using automatic
differentiation and can be thought of as â€˜activation operatorsâ€™. moreover, the gradients of the loss function are back-propagated
through the entire network to train the neural network parameters using the adam optimizer.
model building by encoding acoustic wave equation and
discover the parameters. in this work, the data-driven
algorithm for the velocity inversion demonstrates the
capability of pinns which encodes the underlying physical
laws. the comparison of results to fwi shows the accuracy
and reliability of our proposed method.

figure 3. the inversion work flow, the new models are
predicted after optimized the loss function of pinns.
conclusions
physics-informed neural networks (pinns) consist of an
uniformed feedforward neural network and another network
induced by the physical law in the form of pde. here for the
first time we implement a pinns to geophysical velocity

Â© 2019 seg
seg international exposition and 89th annual meeting

the future work forward to two main directions: switch the
input data to seismic record rather than the wavefield
snapshots, this deficiency also limits our application of this
method to real data. the second line of work must extend the
method to three-dimensional data, as for a machine learning
approach, which is mainly a scalability problem rather than
a fundamental one.
acknowledgments
i give my gratitude to the financial support of china
scholarship council (csc). this research is based on the
opensource from mr. maziar raissi on github
(https://github.com/maziarraissi

10.1190/segam2019-3216823.1
page 2586

ground truth (real velocity models)

fwi

pinns

updated velocity model

5000

5000

4900

4900

5000

4900

10

100

10
4800

4800

4800

20
4700

4600

30

4500

4400

4700

4700

200

30

4600

4600

40

4500

depth (m)

20

300

4500

4400

50

40

4400

400

4300

4300

4300

60
4200

50

4100

4200

4200

500

70
4100

4100

60

4000

20

40

60

80

100

120

80

4000

20

40

60

80

100

120

140

160

600

4000

200

400

600

800

1000

1200

distance (m)
updated velocity model
4500

4500

4500

10
10

4000

100

4000

4000

20
20

3500

30

3000

200

3500

30

40

3000

depth (m)

downloaded 06/16/20 to 130.238.7.40. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/

double click here to type your header

3500

300

3000

50
40

2500

2500

400

2500

60
50

2000

60

1500

20

40

60

80

100

120

2000

70

80

140

1500

20

40

60

80

100

120

140

160

500

2000

600

180

1500

200

400

600

800

1000

1200

1400

distance (m)

4500

4500

4400

4200

100

100

100

4000

200

3800

4000

4000

200

200

3600

300

3400

300

3500

3500

300

3200

400

400

400

3000
3000

3000

2800

500

500

500

2600

2400

600
50

100

150

2500

600

200

50

100

150

200

2500

600
50

100

150

200

figure 4. the results methods showcase a series of promising results for a diverse collocation of velocity models. here, we
compare these results predicted by the pinns (right row) with fwi (middle row). the predicted model closely resembles the
ground truth in structure and velocity.

Â© 2019 seg
seg international exposition and 89th annual meeting

10.1190/segam2019-3216823.1
page 2587

downloaded 06/16/20 to 130.238.7.40. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/

references
alipanahi, b., a. delong, m. t. weirauch, and b. j. frey, 2015, predicting the sequence specificities of dna-and rna-binding proteins by deep
learning: nature biotechnology, 33, 831â€“838, doi: https://doi.org/10.1038/nbt.3300.
krizhevsky, a., i. sutskever, and g. e. hinton, 2012, imagenet classification with deep convolutional neural networks: advances in neural information processing systems.
lake, b. m., w. zaremba, r. fergus, t. gureckis, 2015, deep neural networks predict category typicality ratings for images: cognitive science
society.
raissi, m., p. perdikaris, and g. em karniadakis, 2017, physics informed deep learning (part ii): data-driven discovery of nonlinear partial differential
equations. arxiv preprint arxiv:1711.10566.
rÃ¶th, g., and a. tarantola, 1994, neural networks and inversion of seismic data: journal of geophysical research: solid earth, 99, 6753â€“
6768.https://doi.org/10.1029/93jb01563
wang, y., 2016, seismic inversion: theory and applications: john wiley and sons.
wang, z., j. li, b. wang, y. xua, and x. chena, 2018, a new central compact finite difference scheme with high spectral resolution for acoustic wave
equation: journal of computational physics, 366, 191â€“206, doi: https://doi.org/10.1016/j.jcp.2018.03.030.

Â© 2019 seg
seg international exposition and 89th annual meeting

10.1190/segam2019-3216823.1
page 2588



multiple-input fourier neural operator (mifno) for
source-dependent 3d elastodynamics
fanny lehmanna,b , filippo gattib , didier clouteaub
a

arxiv:2404.10115v1 [cs.lg] 15 apr 2024

b

cea, dam, dif, , arpajon, f-91297, , france
lmps-laboratoire de mÃ©canique paris-saclay, universitÃ© paris-saclay, ens paris-saclay,
centralesupÃ©lec, cnrs, , gif-sur-yvette, 91190, , france

abstract
numerical simulations are essential tools to evaluate the solution of the wave equation in
complex settings, such as three-dimensional (3d) domains with heterogeneous properties.
however, their application is limited by high computational costs and existing surrogate
models lack the flexibility of numerical solvers.
this work introduces the multiple-input fourier neural operator (mifno) to deal with
structured 3d fields representing material properties as well as vectors describing the source
characteristics. the mifno is applied to the problem of elastic wave propagation in the
earthâ€™s crust. it is trained on the hemews -3d database containing 30 000 earthquake simulations in different heterogeneous domains with random source positions and orientations.
outputs are time- and space-dependent surface wavefields.
the mifno predictions are assessed as good to excellent based on goodness-of-fit
(gof) criteria. wave arrival times and wave frontsâ€™ propagation are very accurate since 80 %
of the predictions have an excellent phase gof (larger than 8). the fluctuations amplitudes
are good for 87 % of the predictions (envelope gof larger than 6) and excellent for 28 %.
the envelope score is hindered by the small-scale fluctuations that are challenging to capture
due to the spectral bias inherent to neural networks and the complex physical phenomena
associated with high-frequency features. nevertheless, the mifno can generalize to sources
located outside the training domain and it shows good generalization ability to a real complex
overthrust geology. when focusing on a region of interest, transfer learning is an efficient
approach to improve the accuracy with limited additional costs, since gof scores improved
by more than 1 gof unit with only 500 additional specific samples.
the mifno is the first surrogate model offering the flexibility of an earthquake simulator
with varying sources and material properties. its good accuracy and massive speed-up offer
new perspectives to replace numerical simulations in many-query problems.
1. introduction
predicting the intensity of ground motion is a crucial challenge for earthquake early warning and seismic hazard analyses. for early warning, one should be able to estimate ground
motion before seismic waves reach the area distant from the epicenter (allen and melgar,
april 17, 2024

2019; cremen and galasso, 2020); and seismic hazard analyses require repeated evaluations
of ground motion in varying configurations (baker, 2013). therefore, fast estimations of
ground shaking intensity are key for both tasks. to this end, recorded seismograms as well
as numerical simulations have been used since they provide complimentary information.
on the one hand, seismograms recorded at different stations for the same earthquake provide means to predict the relationship between earthquake parameters and ground motion
intensity. this type of analysis relies on the existence of earthquake catalogs that are scrutinised with machine learning. mousavi and beroza (2023) reviews machine learning methods
such as random forests, support vector machines, artificial neural networks, etc. that have
been used to design non-parametric ground motion models. the development of deep learning is also leading to early warning models able to predict ground motion intensity from
the first seconds of seismograms recorded close to the epicenter, using convolutional neural
networks (cnn, hsu and huang (2021); jozinoviÄ‡ et al. (2020)), graph neural networks
(gnn, bloemheuvel et al. (2023)), or recurrent neural networks (rnn, datta et al. (2022);
wang et al. (2023)). one main limitation in these approaches is their need for a large number
of recordings in the area under study. in regions with poor data coverage or low-to-moderate
seismicity, these purely data-driven methods fall short.
on the other hand, numerical simulations provide ground motion timeseries in any seismological context. they allow a great flexibility since parameters describing geological properties, source characteristics, stations location, etc. can be specified on demand. physics-based
simulations should be conducted in three-dimensional (3d) domains to correctly model the
propagation of waves beyond the too simplistic two-dimensional (2d) setting (moczo et al.,
2018; smerzini et al., 2011; zhu et al., 2020). in the meantime, numerical results should be
accurate up to frequencies of a few hz to allow meaningful engineering applications (bradley
et al., 2017). independently from the numerical method used to solve the wave equation (e.g.
finite differences, finite elements, spectral elements, among others), these constraints lead to
high computational costs that can become a limiting factor when quantifying uncertainties.
indeed, the computed ground motion strongly depends on the simulation parameters, such
as the regional geological properties and the source characteristics. due to the difficulty of
conducting geophysical measurements and accurately inverting the source properties, these
parameters are associated with large uncertainties. therefore, with a monte-carlo approach,
many simulations should be conducted to assess the influence of parameters uncertainties on
ground motion response. the computational cost of high-fidelity simulations prohibits this
approach.
with the joint development of high-performance computing and deep learning, there is
a growing interest in developing surrogate models able to mimic numerical simulations. the
aim is to predict space-dependent ground motion timeseries that give much more detailed
information than ground motion models restricted to some intensity measures. thus, neural
operators (li et al., 2021; lu et al., 2021; brandstetter et al., 2022) are a suitable framework to
provide solutions of the wave propagation equation from a set of input parameters describing
the geological configuration (velocity of p and s waves, soil density, attenuation factors) and
the source (epicenter location, source orientation, magnitude).
neural operators were shown to be efficient surrogate models for the elastic wave equation
2

in 2d (oâ€™brien et al., 2023; yang et al., 2023; zhang et al., 2023) and 3d (kong and rodgers,
2023; lehmann et al., 2023, 2024a; zou et al., 2023). these works use variants of the fourier
neural operator (li et al., 2021) to learn the propagation of seismic waves by the means
of their frequency representation. some studies consider both the source location and the
geological parameters as input variables (kong and rodgers, 2023; oâ€™brien et al., 2023; yang
et al., 2023; zou et al., 2023). yet, these works do not take into account the different nature
of the geological and source parameters: a structured 2d or 3d field for the geology and a
vector of coordinates for the source. the source position is encoded as a binary field where
a single pixel of value 1 describes the source location.
in addition, an earthquake source cannot be described solely by the source location since
its orientation also plays a crucial role, through the definition of the moment tensor. with
the notable exception of oâ€™brien et al. (2023) that restricts their analyses to 2d and mention
"significant challenges" to extend their methodology to 3d, the above-mentioned works consider an isotropic explosive source. in this situation, the moment tensor is fixed and does not
need to be learnt by the surrogate model. but it prevents earthquake modeling that requires
a moment tensor description of the source. the current study proposes an alternative architecture to include both the source location and moment tensor by considering their vector
representation.
some neural operators have been designed specifically to account for multiple inputs,
possibly with different structures. the general neural operator transformer (gnot, hao
et al. (2023)) uses different multi-layer perceptron (mlp) blocks to encode each input before
transmitting it to a transformer network. other models were inspired by the deep operator
network (deeponet, lu et al. (2021)) by implementing one specific branch network for
each input (mionet, jin et al. (2022), fourier-mionet (jiang et al., 2023)). however, the
complexity of these models makes 3d applications challenging.
in this study, we propose a multiple input fourier neural operator (mifno) that takes
as inputs the 3d geological properties, the source position and the source orientation, and
predicts the surface ground motion as the solution of the 3d elastic wave equation. the
inherent complexity of 3d variables is handled with a factorized fourier neural operator
(f-fno, tran et al. (2023)). the mifno involves a convolutional branch transforming
information from the source into structured fields that get through the factorized fourier
layers. the mifno is trained on the hemews -3d database containing 30 000 simulations
of elastic wave propagation in 3d heterogeneous geologies with randomly located and oriented
sources.
in the following, section 2 describes the data and the neural operators used for training.
section 3 analyses the predictions, discusses the evaluation metrics, and the influence of
inputs on the outcomes. section 4 explores generalization to out-of-distribution data and
section 5 extends it with transfer learning to a real earthquake. finally, section 6 draws
conclusions.

3

2. data and methods
2.1. problem setting
we consider a cubic domain Ï‰ = [0; Î»] âŠ‚ r3 of size 9.6 km Ã— 9.6 km Ã— 9.6 km. seismic
waves propagate inside this domain from the source up to the upper surface denoted âˆ‚Ï‰top .
except the traction-free upper surface âˆ‚Ï‰top , all other external surfaces have absorbing conditions to mimic a semi-infinite propagation domain.
the propagation domain is described by geological parameters (velocity of p and s waves,
soil density, attenuation factors). for the sake of simplicity, we assume that the geology is
entirely defined by the s-wave velocity vs and other parameters can be computed from
deterministic relationships involving only vs (in particular, the ratio vp /vs is fixed to 1.7).
in the general setting, the propagation domain is characterized by a function a : Ï‰ â†’ r that
gives the value of vs at each spatial point x âˆˆ Ï‰.
the propagation of waves inside the domain obeys the elastic wave equation
Ï

h
i
âˆ‚ 2u
t
=
âˆ‡Î»
(âˆ‡
Â·
u)
+
âˆ‡Âµ
âˆ‡u
+
(âˆ‡u)
+ (Î» + 2Âµ) âˆ‡ (âˆ‡ Â· u) âˆ’ Âµâˆ‡ Ã— âˆ‡ Ã— u + f
âˆ‚t2

(1)

where Ï : Ï‰ â†’ r is the material unit mass density, Î» : Ï‰ â†’ r, Âµ : Ï‰ â†’ r are the
lamÃ© parameters (all contained in our general function a), characterizing the thermodynamically reversible mechanical behaviour of the material, f is the body force distribution, and
u : Ï‰ Ã— [0, t ] â†’ r3 is the displacement field. the forcing term f (x, t) = div m(x) Â· s(t)
is the divergence of a moment tensor density m, localized at a point-wise location xs . in
this study, the moment tensor is given as a vector Î¸s and the source position is denoted by
the vector xs . then, equation 1 can be rewritten under the general form
l(a, u) = f (xs , Î¸s )

(2)

we aim at a surrogate model gÏ• (a, xs , Î¸s ) predicting the velocity field uÌ‡ (obtained as the
time derivative of the displacement u) from the sole knowledge of the geology a and the source
characteristics xs , Î¸s . reference velocity fields are obtained with physics-based numerical
simulations. to reduce the memory requirements, simulation outputs are synthetized only
at the surface of the propagation domain âˆ‚Ï‰top . for the sake of clarity, u denotes surface
velocity fields uÌ‡|âˆ‚Ï‰top in the following. the surrogate model is trained to minimize the error
between the predicted surface velocity fields uÌ‚ and the simulated ones u.
2.2. data
each data sample is composed of four elements: the description a of s-wave velocity inside
Ï‰, the source position xs , the source orientation Î¸s , and the simulated surface velocity field
u.
s-wave velocity models are designed as random horizontal models augmented with random
heterogeneities. the horizontal models are built as a stack of horizontal layers with random
thickness and random value. they describe the baseline geophysical knowledge of the domain.
then, log-normal random fluctuations are added independently inside each layer to represent
4

figure 1: composition of the hemews -3d database. velocity models were built from
the addition of randomly chosen horizontal layers and heterogeneities drawn from random
fields. combined with the source position and the source orientation, they form the inputs of
the neural operator. outputs of the spectral element code sem3d are velocity wavefields
synthetized at the surface of the domain by a grid of virtual sensors.
geological heterogeneities (fig. 1). these random fields are stationary and characterized
by their coefficient of variation Ïƒ following a folded normal distribution |n (0.1, 0.2)|, their
correlation length â„“c randomly chosen in {1.5, 3, 4.5, 6 km}, and their power spectral density
or correlation function. after the addition of random heterogeneities, s-wave velocities are
clipped between vs,min = 1071 m/s and vs,max = 4500 m/s. table 1 summarizes the velocity
models parameters and a more detailed description of the velocity models can be found
in (lehmann et al., 2024b). more advanced random fields could be defined for practical
applications accounting for vp , vs , Ï variability together with controlled anisotropy (ta
et al., 2010). however, it is believed that the proposed model spreads a large enough set
to demonstrate the applicability of our surrogate model to other probabilistic models of the
geological formation.
the source position xs = (xs , ys , zs ) is chosen from a latin hypercube sampling inside
the propagation domain, not too close from the boundaries to avoid numerical issues due to
absorbing boundary conditions:
xs âˆˆ [1.2; 8.4 km]
ys âˆˆ [1.2; 8.4 km]
zs âˆˆ [âˆ’9.0; âˆ’0.6 km]
the moment tensor is a 3 Ã— 3 symmetric matrix that represents the source of a seismic
event. when the seismic event is an earthquake, the moment tensor can be derived from
three angles (strike, dip, and rake) that characterize the seismic rupture. the angles were
sampled from a latin hypercube with a strike between 0Â° and 360Â°, dip between 0Â° and
5

parameter
number of heterogeneous layers nâ„“
layers thickness h1 , Â· Â· Â· , hnâ„“
mean vs value per layer
layer-wise coefficient of variation
layer-wise correlation length along x
layer-wise correlation length along y
layer-wise correlation length along z

statistical distribution
u({1, 2, 3, 4, 5, 6})
u ({(h1 , Â· Â· Â· , hnâ„“ ) > 0|h1 + Â· Â· Â· + hnâ„“ = 7.8 km})
u([1785, 3214 m/s])
|n (0.2, 0.1)|
u({1.5, 3, 4.5, 6 km})
u({1.5, 3, 4.5, 6 km})
u({1.5, 3, 4.5, 6 km})

table 1: statistical distribution of each parameter describing the velocity models a. mean
vs values, coefficients of variation, and correlation lengths are chosen independently in each
layer. since the bottom layer has a constant thickness of 1.8 km and value vs =4500 m/s, it
is not included in these parameters.
90Â°, and rake between 0Â° and 360Â°. this constitutes the first representation of the source
orientation, Î¸s âˆˆ r3 . alternatively, the equivalent moment tensor was computed for each set
of angles. then, the source orientation is described by the six components of the symmetric
moment tensor Î¸s âˆˆ r6 . the mifno can be trained with any of these representations. the
source amplitude corresponds to a seismic
moment m0 = 2.47 Ã— 1016 n m and the source

t
time evolution is given by t 7â†’ 1 âˆ’ 1 + Ï„t eâˆ’ Ï„ with Ï„ = 0.1 s.
numerical simulations are conducted with the sem3d code1 (touhami et al., 2022) based
on the spectral element method (faccioli et al., 1997; komatitsch and tromp, 1999). the
simulation domain is discretized in elements of size 300 m and 7 gauss-lobatto-legendre
quadrature points per side of mesh element allow the propagation of waves up to a 5 hz
frequency. velocity fields u are synthetized on a regular grid of 32 Ã— 32 virtual sensors
located at the surface of the domain. a time window of [0s; 6.4s] is selected as it contains
the significant ground motion and the time sampling is 0.02 s. it should be noted that the
velocity field at each sensor contains three components, denoted as e-w (east-west), n-s
(north-south), and z (vertical).
it is important to emphasize that velocity fields are seen as 3d variables depending on the
coordinate (x, y) of the surface sensor and time t. therefore, both the geological parameters
given as inputs (function of (x, y, z)) and the output surface wavefields (function of (x, y, t))
are 3d variables, with a depth-to-time conversion happening in the transformation.
predicting the space- and time-dependent solution of the 3d elastic wave equation is
a complex high-dimensional problem. the complexity can be assessed with two notions
of intrinsic dimension. the principal component analysis (pca) dimension describes the
number of linear components required to capture 95 % of the variability in the ground motion
wavefields database. the intrinsic dimension can also be estimated from the maximum
likelihood estimation (mle) of the size of the wavefieldsâ€™ neighbourhood levina and bickel
1

https://github.com/sem3d/sem

6

(2004). the intrinsic dimensions is at least of the order of 100 when estimated with the
non-linear mle and it reaches nearly 5000 with the linear pca (fig. s3). although it is
impossible to conclude on the exact dimensionality of the ground motion wavefields, these
results emphasize the complexity of the problem.
2.3. factorized fourier neural operator (f-fno)
the fourier neural operator (fno) was proposed by li et al. (2021) to extend neural
networks to functional spaces. it is especially convenient to solve problems related to partial
differential equations (pdes) since it can be interpreted as a means to learn the frequency
representation of the pde solution. the fno is based on fourier layers that involve computing the fast fourier transform (fft) of the model internal variables. although only the
first fourier modes are preserved to limit the model complexity, the fft yields a large number of model parameters when dealing with 3d variables. this increases the generalization
gap of the fno (lehmann et al., 2024a) and hinders its accuracy.

figure 2: the f-fno is made of one uplift block p , a succession of l factorized fourier
layers, and three projection blocks qe , qn , qz . the details of a factorized fourier layer are
given to show the decomposition of the fft along each dimension.
therefore, a more efficient approach was introduced with the factorized fourier neural
operator (f-fno, tran et al. (2023)). the general architecture of the f-fno is similar to
the fno and is illustrated in fig. 2. inputs consist of the concatenation of four 3d variables:
the velocity model a and the grid of x, y, and z coordinates. the concatenation is performed
along the fourth dimension representing the channels. then, a shallow linear block p uplifts
the 4-channel input variable into a variable v0 with dv channels (dv =16 in this work). p
acts point-wise on the inputs and allows to locate the geological parameters at their exact
location. then, a succession of l factorized fourier layers is applied to transform v0 into vl .
7

at the end of the f-fno, three shallow linear blocks qe , qn , qz project vl onto each of
the velocity component ue , un , uz .
in the f-fno, the factorized fourier layers transform the internal variable vâ„“ into vâ„“+1
following
vâ„“+1 = vâ„“ + m lp (kâ„“ (vâ„“ ))

m lp (k) = wâ„“2 Ïƒ wâ„“1 k + b1â„“ + b2â„“

(3)

where Ïƒ is the activation function, wâ„“1 and wâ„“2 are bias matrices acting point-wise, b1â„“ and
b2â„“ are scalar biases, and kâ„“ is a neural network playing the role of the integral operator.
the majority of the f-fno weights are contained in the kâ„“ network. the core idea of this
network (li et al., 2021) stems from the convolution theorem that allows one to write an
integral operator k depending on the stationary kernel ÎºÏˆ (where Ïˆ denotes the learnable
weights)
z
ÎºÏˆ (x, y)v(y)dy

(4)

k(v) = f âˆ’1 (rÏˆ Â· f(v))

(5)

k(v) =
as the product

in equation 5, f denotes the fourier transform and rÏˆ = f(ÎºÏˆ ) is the tensor of learnable
complex weights. in comparison with the original fno formulation, the kernel kâ„“ of the
factorized fourier layer replaces the 3d fourier transform f by the sum of three 1d fourier
transforms along each dimension (f(1) , f(2) , f(3) ):



âˆ’1
âˆ’1
âˆ’1
kâ„“ (vâ„“ ) = f(1)
r(1),â„“ f(1) (vâ„“ ) + f(2)
r(2),â„“ f(2) (vâ„“ ) + f(3)
r(3),â„“ f(3) (vâ„“ )
(6)
in equation 6, r(1),â„“ , r(2),â„“ , and r(3),â„“ are the neural operatorâ€™s weights that are optimized
during training. the fourier transform is computed efficiently with the fast fourier transform (fft) and only the first fourier modes are selected in each dimension. in this work,
16 fourier modes are selected in the x and y dimensions (m(1),â„“ = m(2),â„“ = 16, âˆ€â„“) and
32 fourier are selected in third dimension except for the first layer (m(3),â„“ = 32, âˆ€â„“ â‰¥ 2,
m(3),1 = 16).
it should also be noted that inputs and outputs have different dimensions in our work.
indeed, inputs are velocity models depending on the three spatial variables (x, y, z), their
size is sx Ã— sy Ã— sz . while outputs are time-dependent velocity fields recorded only at the
surface of the propagation domain. therefore, they depend on the spatial location of the
sensor (x, y) and time t. the size of outputs is then sx Ã— sy Ã— st . dimensions are chosen as
sx = sy = sz = 32 and st = 320. the third dimension increase is done gradually inside the
fourier layers by padding fourier coefficients with zeros before applying the inverse fourier
transform (rahman et al., 2023). the same procedure is applied before the summation to
match the dimensions (block modifydimensions in fig. 2).
works like (kong and rodgers, 2023; zou et al., 2023) add the source position as an
additional input to the neural operator. this is done by inserting a fifth variable alongside
a and the grids x, y, z. this variable is a cube full of zeros with a single pixel of 1 denoting the
8

source position. in this work, we propose an alternative architecture that takes advantage of
the vector representation of the source to include more source parameters. it is worth noticing
that the elastodynamic problem 1 being linear with respect to the force term f , extended
sources such as an extended seismic fault can be easily addressed using the superposition
principle.
2.4. multiple-input fourier neural operator (mifno)
inspired by the mionet architecture that designs separate branches for each input (jin
et al., 2022), we propose a neural operator architecture that encodes the source parameters
in a specific branch. our multiple-input fourier neural operator (mifno) is illustrated in
fig. 3.

figure 3: the mifno is made of a geology branch that encodes the geology with factorized
fourier (f-fourier) layers, and a source branch that transforms the vector of source parameters (xs , Î¸s ) into a 4d variable vs matching the dimensions of the geology branch output vk .
outputs of each branch are concatenated after elementary mathematical operations and the
remaining factorized fourier layers are applied. uplift p and projection qe , qn , qz blocks
are the same as in the f-fno.
in our mifno, the succession of factorized fourier layers is split in two parts. from layers
1 to k, factorized fourier layers act on the geology as in the original f-fno. this forms the
geology branch. in parallel, the source branch takes as input the vector of source coordinates
xs concatenated with the source characteristics Î¸s . even though resolution invariance requires additional conditions (bartolucci et al., 2023), fnos can always be technically applied
to inputs having a different resolution than the resolution used during training. this property
must be preserved in the source branch, which is not trivial for convolutional layers whose
number of weights depend on the size of the inputs. to circumvent this issue, the layers in
the source branch are defined based on the number of modes used in the k-th factorized
fourier layer, denoted mx , my , mz , and not the actual size of the inputs.
several architectures were investigated to design the source branch (not shown) and we
found that the best one was made of a perceptron and two convolutional layers separated by
reshaping operations. our interpretation of the source branch is the following:

9

1. a 2-layer perceptron encodes the source characteristics in the (x, y) plane. its first layer
contains 128 neurons and its second layer 4mx my neurons. the reshaping operation
leads to a 3d variable of size 2mx Ã— 2my Ã— 1.
2. a 2d cnn (with kernel size of 3) creates the third dimension. a first convolutional
layer creates 8 channels and a second layer creates 2mz channels. after reshaping, one
gets a 4d variable of size 2mx Ã— 2my Ã— 2mz Ã— 1.
3. a 3d cnn (kernel size of 3) adds the channel dimension with two convolutional layers.
at the end of the source branch, the modifydimensions layer transforms variables into the
fourier space where fourier coefficients are padded with 0 if necessary, and then inverse
transformed to obtain the same dimension as the output of the geology branch vk , i.e. sx Ã—
s y Ã— s z Ã— dv .
to combine variables from the geology and source branches, we take inspiration from
haghighat et al. (2024) who propose a 2d extended-deeponet for earthquake localization.
following this idea, vk and vs are summed, substracted, and multiplied before being concatenated along the channel dimension. this leads to a variable of size sx Ã— sy Ã— sz Ã— 3dv
that is then sent to the remaining factorized fourier layers, k + 1 to l. the final projection
layers qe , qn , qz are identical to the f-fno.
it can also be noted that the mifno is exemplified with factorized fourier layers but
the architecture can be extended to other types of layers. the mifno used in this work
contains 16 factorized fourier layers in total (l = 16), including 4 layers in the geology
branch (k = 4). the number of channels is fixed to dv = 16. its total number of parameters
amount to 3.40 million. the initial learning rate is 0.0004 and is halved on plateau, and
the loss function is the relative mean absolute error. the choice of hyperparameters and
training strategies comes from extensive investigations conducted for the f-fno in lehmann
et al. (2024a). 27 000 samples were used for training and 3000 for validation. training was
performed on 4 nvidia a100 gpus for 200 epochs, which took 29.6 h.
some care should be taken to normalize the inputs and outputs since their variability
ranges are significantly different. geological models are normalized to a gaussian distribution, by centering them with respect to the mean geology and normalizing by four times the
geological standard deviation to yield values approximately in [âˆ’0.50.5]. the source coordinates and the source angles are mapped to the [0, 1]3 cube. velocity wavefields predicted
by the mifno are normalized with a scalar (i.e. independent of the sensor coordinates)
that depends only on parameters known beforehand: the s-wave velocity at p
the source location vs (xs ) and the source depth zs . the normalization value is c = vs (xs ) zs2 + (Î»/4)2
where Î» is the length of the domain. the first term relates to the seismic energy released by
an earthquake, which is inversely proportional to the shear modulus (hence, to the s-wave
velocity) at the rupture location. the second term accounts for the amplitude decrease of
seismic waves generated by deep sources since they undergo more geometrical dispersion and
diffraction.

10

3. results
when comparing the surrogate model predictions with the reference numerical simulations, one needs to use appropriate metrics that explain the physical meaning of the misfit.
in particular, the usual mean absolute error (mae) and root mean square error (rmse)
have limited interpretability to analyze time series predictions. in seismology, time series are
often compared in terms of time-frequency envelope and phase misfits kristekovÃ¡ et al. (2009).
misfits can be summarized by two scalar values: the envelope goodness-of-fit (gof) and
the phase gof. the envelope gof expresses the error of the predicted amplitude compared
to the reference time series while the phase gof describes the error of the phase arrivals.
gofs are given on a scale between 0 and 10 where 10 means a perfect agreement and the
following score assessment is well accepted: 0-4 is a poor score, 4-6 is fair, 6-8 is good, 8-10
is excellent kristekovÃ¡ et al. (2009).
3.1. predictions analyses
figure 4 illustrates the mifno predictions for a geology in the test dataset (fig. 4a)
and a source located at (3.9, 2.6, âˆ’6.2 km) with a strike of 298.7Â°, dip of 85.3Â°, and rake of
15.4Â°. the time series show an excellent accuracy on the three components since the wave
arrival times are exactly reproduced: p-waves around 1.6 s and s-waves around 2.7 s. the
peaks amplitude is also very similar between the prediction and the simulated ground truth.
the frequency representation confirms the excellent agreement for all frequency ranges (fig.
4c) and the three components.
at the depicted sensor, the envelope gof is 8.7 and the phase gof is 9.5. since both
scores are excellent, they reflect the visual agreement that can be observed in fig. 4. the
envelope gof is slightly lower than the phase gof due to the presence of small-scale and
small-amplitude fluctuations that are challenging to predict with the mifno (visible around
4 s in fig. 4b).
when looking at the spatial evolution of the predictions, fig. 5 shows that it is very
accurate for the different time steps. the wavefront propagates from the source (epicenter
denoted with the white star) with the correct speed since the arrival times are correct for
all spatial locations. the source orientation is clearly visible in the first snapshot and it is
correctly reproduced by the mifno. it is also remarkable that both p waves and s waves are
well predicted, although their amplitude is very different (maximum east-west amplitude
around 2 cm/s for the p waves and around 20 cm/s for the s waves). one can however notice
that the predictions are smoother after the main wavefront, which reflects that small-scale
fluctuations are under-estimated. this is visible for instance at t=2.00 s in fig. 5 where the
predicted wavefields show less fluctuations than the simulated ground truth.
3.2. metrics analyses
to quantify the predictions accuracy more systematically, the envelope and phase gofs
were computed for samples in the training and validation datasets. figure 6 shows that the
gof distributions are very similar for training and validation data. this indicates that the
mifno is not subject to overfitting. although the loss evolution during training shows a
11

(a)

(b)

(c)
figure 4: (a) geology in the test dataset corresponding to the predictions in panels (b) and
(c). the source is located at (3.9, 2.6, âˆ’6.2 km) with a strike of 298.7Â°, dip of 85.3Â°, and rake
of 15.4Â°. (b) velocity time series simulated (black) and predicted (dashed red line) in the
three components: east-west (e), north-south (n), vertical (z). (c) for the same sensor as
panel (b), amplitude of the fourier coefficients of the velocity time series.

12

figure 5: east-west component of the simulated (upper row) and predicted (center row)
velocity fields for the geology illustrated in fig. 4 at five time instants. the error between
simulation and prediction is given in the lower row. the white star indicates the epicenter.
stable convergence with a small generalization gap (fig. s1), this does not seem to impact
the prediction accuracy.
the gofs distributions confirm that the phase accuracy is better than the envelope
accuracy (fig. 6 and tab. 2). indeed, 80.2 % of predictions have an excellent phase gof
(phase gof larger than 8). envelope gofs are good for 87.3 % of predictions (envelope
gof larger than 6), and 27.8 % are excellent (envelope gof larger than 8).
table 3 indicates that the relative root mean square error (rrmse) is 33 % on the
validation dataset (first quartile). the relative rmse is computed between predicted and
simulated velocity timeseries at each sensor. errors are then averaged over all sensors and
samples. relative errors on the order of 30 % are common in 3d pde predictions, especially
when complexity arises from the presence of heterogeneous parameters.
due to the limited interpretability of the rmse, it is complemented by the frequency
13

figure 6: distribution of the envelope gof (left) and phase gof (right) for each sensor and
1000 samples in the training (grey area) and validation (blue line) datasets.
model
mifno 8 layers
mifno 16 layers

envelope gof
6.32 ; 7.89
6.57 ; 8.08

phase gof
8.07 ; 9.17
8.20 ; 9.21

table 2: 1st and 3rd quartiles of the gofs computed on 1000 validation samples. gofs
range from 0 to 10 where 10 is the best score
biases, computed as the errors between the amplitudes of fourier coefficients in simulated
u(x, y) and predicted uÌ‚(x, y) velocity fields. frequency biases are given for low frequency
(0-1hz), medium frequency (1-2hz), and high frequency (2-5hz) components following
rfftlow =

f (uÌ‚(x, y))low âˆ’ f (u(x, y))low

with
f (u(x, y))low =

f (u(x, y))low
1 x
|f(u(x, y))(f )|
nf 0â‰¤f <1

(7)

(8)

in equation (8), nf denotes the number of frequencies f between 0 and 1hz and f is the
fourier transform. definitions naturally extend to medium and high frequencies. it should
be noted that positive frequency biases indicate an overestimation of the frequency content
compared to the reference (f (uÌ‚(x, y)) â‰¥ f (u(x, y))) while negative values indicate underestimation. noting that the mean of fourier coefficients amplitudes f (u(x, y)) is always
positive, this implies that the frequency biases cannot be lower than âˆ’1. this explains the
steep decrease of the high-frequency distribution around âˆ’1 in fig. s2.
14

model
mifno 8 layers
mifno 16 layers

rrmse
0.34 ; 0.59
0.33 ; 0.59

rfftlow
-0.54 ; -0.09
-0.49 ; -0.05

rfftmid
-0.63 ; -0.15
-0.57 ; -0.09

rffthigh
-0.68 ; -0.18
-0.62 ; -0.11

table 3: 1st and 3rd quartiles of the metrics computed on 1000 validation samples. rrmse:
relative rmse (0 is best), rfftlow : relative frequency bias 0-1hz (0 is best), rfftmid :
relative frequency bias 1-2hz (0 is best), rffthigh : relative frequency bias 2-5hz (0 is best).
for frequency biases, negative values indicate underestimation while positive values indicate
overestimation.
table 3 indicates that the mifno mainly underestimates the frequency content of signals
since frequency biases are mostly negative. this can be observed in fig. 4 where the small
fluctuations at the end of the signal tend to be ignored in the predictions. previous findings on
the f-fno for a dataset with a fixed source corroborate these results (lehmann et al., 2024a).
also, the mifno leads to larger underestimations for high-frequency components (the interquartile range is âˆ’62 % ; âˆ’11 % for high-frequency components while it reduces to âˆ’49 %;
âˆ’5 % for low frequencies, tab. 3). this reflects the well-known spectral bias, which states
that small-scale (i.e. high-frequency) features are more difficult to predict than large-scale
features (basri et al., 2020; rahaman et al., 2019). in addition, the small-scale fluctuations
also reflect complex physical phenomena due to the refraction of seismic waves on scatterers.
indeed, fluctuations are mostly observed in the coda, after the s-wave arrival. hence, they
originate from the late arrivals of diffracted waves. late velocity fields are subject to a
large inter-sample variability, both between different sensors and between similar geologies
with different locations of scatterers. these factors make high-frequency predictions very
challenging.
one can also notice that the deeper mifno with 16 layers gives better predictions than
the mifno with 8 layers (tab. 2 and 3). therefore, increasing the complexity of the model
is beneficial to improve its accuracy and does not lead to overfitting. this improvement is not
reflected by the relative rmse while it is obvious on the other metrics, thereby emphasizing
the need to consider physically-rooted metrics to evaluate the predictions accuracy.
it can also be noted that there is no significant difference between the mifno taking
as inputs the three angles describing the source orientation or the corresponding moment
tensor (tab. s1). therefore, the mifno with angles is used in the following for more direct
interpretations of the results.
3.3. comparison with baseline models
due to the difficulty of finding existing implementations of models for 3d and vector
inputs, the mifno is compared to f-fno models with less flexibility. this is meant to
ensure that the additional complexity induced by the source branch does not deteriorate
the predictions. to do so, we use three different databases that contain the same geological
models but different surface velocity wavefields depending on the source parametrization:

15

1. in the hemew-3d database, both the source position and orientation are fixed. the
source is located in the middle of the bottom layer: xs =4.8 km, ys =4.8 km, zs =âˆ’8.4 km
and its orientation is fixed to strike=48Â°, dip=45Â°, rake=88Â° (lehmann et al., 2024a).
this database contains ntrain =27 000 training, nval =3000 validation and ntest =1000
test samples.
2. in an intermediate database, the source is located randomly inside the bottom layer,
i.e. xs âˆ¼ u([1200; 8100 m]), ys âˆ¼ u([1200; 8100 m]), zs âˆ¼ u([âˆ’9000; âˆ’8100 m]) and
its orientation is fixed to the same value as the hemew-3d database. this database
contains ntrain =20 000 training, nval =2000 validation and ntest =1000 test samples.
3. the hemews -3d database described in section 2.2 contains sources located randomly
anywhere in the domain and with any source orientation. it contains ntrain =27 000
training, nval =3000 validation and ntest =1000 test samples.
first, let us consider the prediction accuracy with respect to the geological properties.
the baseline model is the 16-layer f-fno described in section 2.3. to match the number of
parameters of the mifno, the f-fno was designed with dv =16 channels in layers 1 to 4, and
dv =48 channels in layers 5 to 16. the f-fno was trained on the first database hemew-3d
with fixed source location and orientation. the mifno was trained as described above on the
hemews -3d database with random source location and orientation. therefore, it should
be noted that the training task of the mifno is much more complex than the f-fnoâ€™s task.

figure 7: for 1000 samples in the hemew-3d test dataset (fixed position and fixed orientation) and each sensor, the frequency bias of the mifno (vertical axis) is shown against the
f-fno frequency bias (horizontal axis). each subplot corresponds to a different frequency
bias, 0-1hz (left), 1-2hz (middle), 2-5hz (right). optimal predictions correspond to a frequency bias of 0.
at inference stage, both the f-fno and the mifno were used to predict surface velocity
wavefields for 1000 test geologies with a fixed source corresponding to the reference source
in the hemew-3d database. figure 7 compares the f-fno predictions with the mifno
predictions for each sensor and each sample. one can firstly notice that points are widespread
on both sides of the diagonal, meaning that the mifno or the f-fno can give better
predictions depending on the situation. however, points are more densely distributed in the
16

lower right triangle of the graph, close to the diagonal (fig. 7). this indicates that the
f-fno is in average more accurate than the mifno when predicting only solutions with a
fixed source.
however, the difference between the f-fno and the mifno is around 0.8 gof units,
which remains acceptable knowing the extended complexity of the mifnoâ€™s task (tab. s2).
figure s4 illustrates velocity time series predicted by the f-fno and the mifno. it shows
that the mifno predictions are already reasonable while the f-fno locally improves the
accuracy.
for the next comparison, we focus on the influence of the source location. to do so,
we designed a f-fno taking as input the geological parameters and a binary encoding of
the source position, i.e. a cube full of zeros with a single 1 indicating the position of the
source. the mifno was trained with only the source coordinates given as inputs to the
source branch. in this experiment, both the f-fno and the mifno were trained on the
second database (random source location and fixed orientation).
model
f-fno
mifno

dataset with a random source position and fixed source orientation
rrmse
rfftlow
rfftmid
rffthigh
eg
pg
0.11 ; 0.20 -0.22 ; 0.06 -0.29 ; 0.05 -0.34 ; 0.05 7.49 ; 8.63 8.74 ; 9.42
0.12 ; 0.21 -0.23 ; 0.05 -0.30 ; 0.04 -0.36 ; 0.04 7.36 ; 8.53 8.64 ; 9.36

table 4: 1st and 3rd quartiles of the metrics computed on 1000 validation samples. rrmse:
relative rmse (0 is best), rfftlow : relative frequency bias 0-1hz (0 is best), rfftmid :
relative frequency bias 1-2hz (0 is best), rffthigh : relative frequency bias 2-5hz (0 is best),
eg: envelope gof (10 is best), pg: phase gof (10 is best). for frequency biases, negative
values indicate underestimation. both models contain 16 layers and were trained with 20 000
samples for 300 epochs.
when both models have 8 layers, the mifno predictions are slightly better than the
f-fno (tab. s3). with 16 layers, both models give a similar accuracy (second digit fluctuations in tab. 4 are within the variability of the model due to a different random initialization).
phase gofs are excellent for both models and the envelope gofs are close to excellent (tab.
4). overall, these results show that the source branch architecture in the mifno preserves
the expressivity of the f-fno while providing the flexibility to add more source parameters.
3.4. influence of the source parameters
in this section, we highlight the influence of the source parameters on the predictions and
on the error. for a given geology, figure 8a compares the predictions and the ground truth
for three different positions of the source (with the same orientation). one can firstly note
that the predicted wavefields are very similar to the ground truth for all source positions.
especially, their spatial location closely follows the source position. this confirms that the
source position encoding in the source branch is accurate.
figure 8a also illustrates the variability of surface wavefields that can be obtained by
moving the source in a heterogeneous geology. indeed, the same geology was used for all
17

(a)

(b)

figure 8: comparison of the mifno predictions (lower rows) with the simulated ground
truth (upper rows) for a given geology and time. (a) the source position changes (indicated
by the white star) but the orientation remains constant. (b) the source position is fixed but
the orientation changes and is indicated with the (strike, dip, rake) angles.
three snapshots and the source orientation is fixed. since seismic waves originating from
different source locations encounter different scatterers, they give rise to different surface
wavefields. this large variability on the outputs - only due to the source position - reiterates
the complexity of the task we aim at solving.
next, we focus on the source orientation while fixing the source position. a geology with
low heterogeneities is chosen to emphasize the source orientation. three sets of (strike, dip,
rake) angles are examined in fig. 8b. surface wavefields are depicted shortly after the first p
waves arrived at the surface of the domain. the wavefields phases are accurately predicted for
all orientations, meaning that the mifno can predict the influence of the source orientation
on ground motion. as already mentioned, a slight amplitude underestimation is visible in
some regions but this is not due to the source orientation.
for more systematic analyses, the 1-2 hz frequency bias is evaluated on 1000 samples with
various geologies, source locations, and source orientations. figure 9 shows the frequency
bias against the source coordinates. similar errors are obtained for all source longitudes and
latitudes, meaning that the prediction accuracy is independent from the epicenter location.
there is a slightly positive correlation between the source depth and the frequency bias but
the inter-sample variability is of the same order. figure 9 suggests that the underestimation of frequency content is greater when the source is deeper while errors are smaller and
overestimation is more frequent for shallow sources.
since both the phase and envelope gofs indicate better results for shallow sources (fig.
s6), this effect is likely due to the increased complexity of the wavefields generated by long
propagation paths. indeed, seismic waves originating from deep sources have encountered
18

figure 9: for 1000 samples, the 1-2 hz frequency bias is shown against the source position
(x, y, z). negative (resp. positive) values indicate underestimation (resp. overestimation) of
the frequency content.
more heterogeneities on their path, which leads to diffraction and dispersion and hence,
perturbs the surface wavefields. therefore, it is generally more complex to predict accurate
wavefields generated by deep sources.
additionally, it can be observed that the source orientation parameters (strike, dip, and
rake angles) have no effect on the prediction accuracy (fig. s5). this confirms that the
mifno produces equally accurate predictions for all types of sources.
4. generalization ability
neural networks and neural operators are not meant to extrapolate data beyond the training distribution. however, when evaluating predictions on real data, one cannot guarantee
that test data are perfectly in-distribution. although predictions are not expected to be
excellent in this situation, it is crucial that they remain reasonable. for the elastic wave
equation, this especially means that predicted amplitudes should be of the right order of
magnitude and wave arrival times should be close to the reference.
4.1. generalization to out-of-distribution sources
in the hemews -3d database, the propagation domain extends from 0 km to 9.6 km and
the sources are located between 1.2 km and 8.4 km. however, since the source location is
provided as a vector of coordinates, the mifno is not constrained to sources located inside
the domain. thus, in this section, we explore its ability to predict surface ground motion
originating from a source outside the training domain.
to simulate the reference ground motion, we create 1000 new geologies that are twice
larger than the original propagation domains and have the same depth. they are all het19

erogeneous geologies with parameters following the same distributions as the hemews -3d
database (tab. 1). in these large geologies, sources are located randomly within [âˆ’3.6km; 13.2km]
and their orientation is also randomly chosen. the reference ground motion is acquired only
at the surface of the original propagation domains, i.e. [0km; 9.6km].

figure 10: 25-th percentile of envelope gof (left) and phase gof (right) for 1000 samples
where the source is located outside the training domain (dashed square) or even outside
the physical domain (dotted square). for reference, 1000 samples with a source inside the
training domain are shown.
figure 10 shows that the mifno maintains very good predictions when the source is
located outside the training domain, both for the envelope gof and the phase gof. when
the source remains inside the propagation domain (dotted square in fig. 10), the interquartile range is 6.24-7.70 for the envelope gof and 7.41-8.77 for the phase gof. these
good to excellent results are satisfying for out-of-distribution predictions since they are not
too far from the reference results on in-distribution data (tab. 2). when the source locations
are farther from the training domain, predictions worsen. this can be seen in fig. 10 for
sources out of the training domain and it is perfectly expected since neural operators are not
designed for severe extrapolation tasks.
4.2. generalization to out-of-distribution geologies
to test the generalization ability of the mifno to out-of-distribution geologies, a 3d
overthrust model was adapted to the size of our geological domain (aminzadeh et al., 1997).
this model is a 3d extension of the well-used marmousi model. it shows a succession of thin
layers with contrasting velocities and several folds. it is downscaled to a spatial resolution
of 300 m to match the resolution of the hemews -3d database (fig. 11a). the source is
located at (1.2 km, 4.8 km, âˆ’9.0 km) with strike = 48Â°, dip = 45Â°, rake = 88Â°.
20

(a)

(b)

(c)
figure 11: (a) overthrust geology downscaled to 300 m. (b) distribution of the envelope
and phase gof for the hemews -3d database (grey, darker colors indicate higher points
density) and for each surface sensor predicted with the overthrust geology (green). (c) eastwest component of the simulated (upper row) and predicted (middle row) velocity fields at
five time instants. the white star denotes the position of the source (depth=âˆ’9 km). the
error between simulation and prediction is given in the lower row.
21

figure 11 shows that the predicted wavefronts are close to the simulations, in all parts of
the domain. in particular, the wave arrival times are accurately predicted and the influence
of the source orientation is well preserved since the phases correspond to the reference.
the amplitude of the main fluctuations is also accurate. the p wave peaks tend to be
overestimated (see t=2.50 s in fig. 11c) but they have a small amplitude on the east-west
component. the focus is instead on the s waves, for which the amplitudes agree well with
the reference simulation (fig. s8).
the envelope gofs are good for a large majority of sensors and phase gofs are very
good to excellent (fig. 11b). the gof distributions are concentrated around the mean
of the gofs obtained on the hemews -3d database, thereby showing the generalization
ability of the mifno to real complex geologies. though fig. 11b shows a significant gof
variability between sensors for this out-of-distribution geology (around 2 gof units), fig.
s7 illustrates that gofs below 6 are concentrated on small areas of the domain and gofs
remain good or excellent on most of the surface. a similar variability was observed on the
hemew-3d database when geological fluctuations create complex wavefields patterns in
localized regions.
finally, one can notice the absence of late fluctuations in the predictions (mainly visible
for low amplitudes around t=3 s). this was expected from our previous analyses since these
ground motion fluctuations are created by the multiple wave reflections and refractions on
the thin geological layers and they are difficult to predict.
4.3. generalization to higher resolution
although zero-shot super resolution can always be technically performed with fnos,
fnos are not invariant with respect to the resolution in the general framework (bartolucci
et al., 2023). this means that high-resolution predictions may be less accurate than predictions at the training resolution. since there is no theoretical result for fnos including
source and pde parameter, we investigate zero-shot super resolution experimentally with
the mifno. while the mifno was trained with inputs of spatial resolution of 32 Ã— 32 Ã— 32,
geologies are interpolated to 64 Ã— 64 Ã— 32 to obtain high-resolution predictions. the vertical
dimension is preserved to maintain the same depth-to-time conversion. since the source is
given with absolute coordinates, there is no need to modify the source branch when modifying
the resolution.
figure 12 shows that it is possible to obtain accurate velocity wavefields with a resolution
of 64 that has not been seen during training. the wavefronts are better defined with the
increased resolution, which allows to capture peaks more accurately in this example (see
snapshots at t=1.40 s in fig. 12 for instance). however, some inaccuracies are visible at
higher resolution, especially on the boundaries. these artefacts can be explained by the fact
that high-resolution predictions are in advance at the edge of the domain, which induces a
phase difference between the prediction and the reference (fig. s9).
5. transfer learning on sources and geologies
when focusing on a region of interest, it is common to have prior knowledge of the local
geology and/or the type of sources that can generate earthquakes. then, transfer learning is
22

figure 12: east-west component of the velocity wavefield at five time instants. first row:
simulation with 32 Ã— 32 sensors. second row: prediction with 32 Ã— 32 sensors. third row:
simulation with 64 Ã— 64 sensors. fourth row: prediction with 64 Ã— 64 sensors. the white star
denotes the position of the source (depth=âˆ’1.3 km).

23

a well-used approach to improve the mifno accuracy for this specific setting. with transfer
learning, one designs a small database tailored to the region under study and the mifno
is fine-tuned on these data. since one benefits from the knowledge acquired on the general
database, transfer learning involves limited simulation and training costs. our case study
is chosen as the le teil earthquake (mw4.9, france, 2019), the most severe earthquake in
metropolitan france in the last decade.
5.1. a database for the le teil earthquake
based on a local 1d geological model, we designed a database of 4000 geologies tailored to
the le teil region. the 1d model contains six homogeneous horizontal layers (smerzini et al.
(2023), tab. s4). to create realistic variations of this geology, heterogeneities were added
as random fields inside each layer. random fields follow the same parameter distributions as
the hemews -3d database (tab. 1).
several studies estimated the source parameters of the le teil earthquake, both in the
form of a fault plane and an equivalent point-source. in our le teil database, we choose the
source locations close to the fault plane obtained by vallage et al. (2021). the fault is 7 kmwide, 4 km-high and extends between âˆ’780 m and âˆ’4210 m. following a latin hypercube
sampling, 4000 point sources were randomly placed along the fault plane, with a normal
distance to the fault plane up to 100 m to account for uncertainties in the fault localization
(fig. 13).

figure 13: position of the 4000 sources in the le teil database, colored by their depth. the
red star denotes the source that generated the ground motion in fig. 14
each point source is assigned a random moment tensor, whose angles are chosen from the
moment tensor inversion conducted by delouis et al. (2021): strike between 30Â° and 70Â°, dip
between 20Â° and 70Â°, rake between 70Â° and 120Â°. similarly to the hemews -3d database,
6.4 s of ground motion were generated at the 32 Ã— 32 surface sensors.
24

within this framework, we obtain 4000 samples (geology, source, ground motion wavefields) tailored to the le teil earthquake. data are considered in-distribution since parameters
are within the range of the hemews -3d database but vary in smaller intervals. among the
4000 samples in the le teil database, up to 3000 were used for training, 300 for validation,
and 700 for testing.
5.2. transfer learning predictions
figure 14 illustrates the velocity time series obtained for a test sample in the le teil
database. predictions are very close to the simulated ground truth for all wave arrivals
and amplitudes. one can especially notice the numerous high fluctuations that are very well
captured at all sensors. the mifno is also able to reproduce the late small-scale fluctuations
(between 3 s and 5 s, fig. 14) thanks to the transfer learning. phase gofs are excellent or
close to excellent for all sensors, while envelope gofs are excellent for the sensors with the
largest fluctuations (two bottom rows in fig. 14) and good for the other sensors. these
metrics confirm the excellent visual agreement.

figure 14: east-west velocity time series at 8 sensors aligned at x=2.25 km for a test sample.
predictions (red dashed lines) were obtained with the mifno fine-tuned on 2000 samples.
the reference ground motion is given by the numerical simulation (black lines). for each
sensor, its position along the y axis is indicated, and the envelope gof (eg) and phase gof
(pg) are also reported.
quantitatively, 700 predictions in the test dataset were assessed with the relative rmse,
the frequency biases and the gofs. when predicting ground motion directly with the generic
mifno (i.e. without transfer learning, line ntl =0 in tab. 5), metrics are in line with the
general results on the hemews -3d database (tab. 2 and 3), although slightly better due to
the absence of deep sources in the le teil database. with only 100 transfer learning samples,
25

the gofs already improve by 1 to 2 units, thereby illustrating the considerable benefits of
transfer learning, even with a limited number of data. after even a light transfer learning
(ntl =100), 68 % predictions reach a good envelope gof and 61 % an excellent phase gof.
# samples
ntrain =3000
ntl =0
ntl =100
ntl =250
ntl =500
ntl =1000
ntl =2000
ntl =3000

le teil database with a random source along the fault plane
rrmse
rfftlow
rfftmid
rffthigh
eg
0.40 ; 0.73 -0.49 ; -0.02 -0.60 ; -0.06 -0.67 ; -0.09 6.60 ; 8.43
0.64 ; 0.98 -0.44 ; 0.33 -0.64 ; 0.06 -0.68 ; -0.05 5.98 ; 7.69
0.41 ; 0.78 -0.44 ; 0.08 -0.56 ; 0.00 -0.66 ; -0.05 6.51 ; 8.32
0.38 ; 0.75 -0.41 ; 0.09 -0.52 ; 0.03 -0.61 ; -0.01 6.70 ; 8.47
0.37 ; 0.74 -0.38 ; 0.11 -0.49 ; 0.06 -0.58 ; 0.02 6.87 ; 8.57
0.35 ; 0.69 -0.42 ; 0.03 -0.52 ; -0.02 -0.59 ; -0.04 6.92 ; 8.62
0.33 ; 0.68 -0.38 ; 0.05 -0.47 ; 0.01 -0.54 ; -0.01 7.10 ; 8.72
0.33 ; 0.68 -0.34 ; 0.08 -0.43 ; 0.04 -0.51 ; 0.02 7.20 ; 8.78

pg
8.25 ; 9.31
6.30 ; 8.44
8.17 ; 9.25
8.38 ; 9.35
8.51 ; 9.41
8.62 ; 9.46
8.72 ; 9.51
8.76 ; 9.53

table 5: 1st and 3rd quartiles of the metrics computed on 700 test samples specific to the
le teil region. (upper row): training with only 3000 specific data. in other experiments,
transfer learning was used with 100 to 3000 samples (ntl = number of transfer learning
samples). rrmse: relative rmse (0 is best), rfftlow : relative frequency bias 0-1hz (0 is
best), rfftmid : relative frequency bias 1-2hz (0 is best), rffthigh : relative frequency bias
2-5hz (0 is best), eg: envelope goodness-of-fit (10 is best), pg: phase goodness-of-fit (10
is best). for frequency biases, negative values indicate underestimation.
for comparison purposes, the mifno was trained solely on 3000 samples from the le teil
database (line ntl =3000 in tab. 5). however, the predictions accuracy was worse than the
predictions obtained with only 250 transfer learning samples. this shows the major benefits
of initializing the weights with the pre-trained mifno.
table 5 also shows that the metrics continuously improve when the number of transfer
learning samples increases. the first quartile of frequency biases improves by 23 % between
ntl =100 and ntl =3000. the gofs also benefit from larger transfer learning databases
and the improvement is mainly visible on the first quartile, meaning that good predictions
with ntl =100 tend towards excellent with ntl =3000 this is illustrated by comparing the
mifno predictions obtained with 2000 transfer learning samples in fig. 14 and 500 samples
in fig. s11. while the main fluctuations are already accurate with 500 transfer learning
samples, increasing the number of samples allows to better capture the late fluctuations with
a small amplitude.
6. conclusion
we introduced a multiple-input fourier neural operator (mifno) that takes as input
a 3d geological domain and a vector describing the source position and orientation. the
structured geology is processed with factorized fourier layers while the source parameters
are transformed via convolutional layers while maintaining resolution invariance properties.
26

the mifno was trained on 30 000 earthquake simulations from the hemews -3d database
covering a large variety of heterogeneous geological models, source positions and source orientations. it predicts 3-component surface ground motion between 0 s and 6.4 s for frequencies
up to 5 hz.
the mifno performs on-par with the reference f-fno model as was verified by restraining the mifno training task to a dataset with i) a fixed source and ii) a randomly
located source with a fixed orientation. this shows that the flexibility offered by our proposed
architecture preserves the accuracy of the f-fno.
mifno predictions are considered as good to excellent for a vast majority of sensors,
following the common understanding of goodness-of-fit (gof) criteria. gofs are better
for the phase than for the envelope, which indicates that the mifno is well suited to predict
wave arrival times and the spatial propagation of wavefronts. with 87 % of predictions having
a good envelope gof, the main peaks are correctly predicted, which is of crucial importance
for seismological applications. complex wave propagation phenomena due to heterogeneities
yield small-scale fluctuations with small amplitude that are difficult to predict accurately,
thereby hindering the envelope gof.
these difficulties are exacerbated by the design choices of the hemews -3d database
that purposely contains geologies with strong heterogeneities (mean coefficient of variation
of 20 %). however, such large coefficients of variations are rarely found in reality for large
correlation lengths. they are included in the hemews -3d database to enhance its variability in a scientific machine learning perspective. therefore, predictions on common 1d
velocity models augmented by moderate heterogeneities will fall on the most accurate range
of the mifno predictions.
we found that the mifno tends to underestimate the frequency content of the ground
motion fluctuations. these fluctuations are created by the seismic wave reflections and
refractions on geological heterogeneities. they are stronger in the high-frequency components
of the signals and they are also more present when ground motion originates from deep
sources. this explains why we found that the frequency underestimation is more pronounced
in high-frequency components and for deep sources.
using a more complex mifno (i.e. with more layers and more channels) is a privileged
approach to increase the accuracy, as illustrated by the difference between the 8-layer and 16layer mifno. previous investigations on the f-fno also showed that this architecture scales
well when the number of parameters increases (lehmann et al., 2024a). we hypothesize that
similar results hold true for the mifno. as a consequence, provided that one can obtain
enough training data and bear the training costs of deeper models, the mifno will become
more accurate.
the mifno is also able to generalize well to different out-of-distribution data. the source
was located out of the training domain with only a moderate degradation of the accuracy.
although it could also be placed outside the propagation domain, this large distribution
shift worsened the predictions. in addition, the mifno can be applied to geologies that
have a different resolution than the training resolution. although there is no guarantee that
the accuracy will systematically be maintained at higher resolutions, the illustrated example
showed that finer predictions improve the predictions of peaks amplitudes. these results
27

show the robustness of the mifno when applied on test data, which is crucial to move
towards real-case applications.
generalization to geologies that are far from the training database is challenging but
the mifno predicts accurate ground motion for an overthrust geology with thin folded
layers. there is a good visual agreement between the main predicted wavefronts and the
simulations, and the gof distributions with the overthrust geology follow the distributions
on the hemews -3d database. this illustrates the good generalization ability of the mifno
for real complex geologies. however, the thin layers in the overthrust geology create many
wave reflections, which are challenging to predict with a generic surrogate model.
when aiming for more accurate predictions in complex settings, transfer learning should
be used to specialize the mifno on a smaller database tailored to the region under study.
this approach was adopted for the le teil earthquake and it showed that only 250 transfer
learning samples lead to good accuracy. the fine-tuned mifno surpasses predictions of the
generic mifno and a mifno trained solely on the le teil database. when increasing the
number of transfer learning samples, predictions reach excellent accuracy and reproduce even
the small-amplitude fluctuations.
in conclusion, the mifno is the first surrogate model providing the flexibility of an
earthquake emulator while reducing significantly the computational costs. once duly trained,
mifno predictions provide 6.4 s of ground motion in around 10 ms (with one gpu) while
numerical simulations take around 1 day (1 cpu equivalent). this massive speed-up paves
the way for 3d uncertainty quantification analysis, design optimization, and inverse problems
that are beyond reach with traditional methods.
we believe that the methodology presented in this work can be applied to other challenges related to propagation phenomena. indeed, most wave partial differential equations
(pdes) involve a space-dependent parameter describing the material properties and source
parameters. since both types of parameters are handled by the mifno, it can certainly
benefit several scientific communities.

28

appendix a. supplementary material
appendix a.1. supplementary material for prediction results

figure s1: evolution of the training loss (solid line) and validation loss (dashed line) as a
function of the number of epochs.
table s1 compares the 8-layer mifno taking as input the 6 components of the moment
tensor (mxx , myy , mzz , mxy , mxz , myz ) or the equivalent representation with 3 angles (strike,
dip, rake).
model
angle
moment

rrmse
0.341 ; 0.589
0.338 ; 0.589

rfftlow
-0.545 ; -0.095
-0.536 ; -0.091

rfftmid
-0.63 ; -0.15
-0.64 ; -0.16

rffthigh
-0.68 ; -0.18
-0.70 ; -0.20

eg
6.32 ; 7.89
6.29 ; 7.86

pg
8.07 ; 9.17
8.06 ; 9.16

table s1: 1st and 3rd quartiles of the metrics computed on 1000 validation samples. rrmse:
relative rmse (0 is best), rfftlow : relative frequency bias 0-1hz (0 is best), rfftmid :
relative frequency bias 1-2hz (0 is best), rffthigh : relative frequency bias 2-5hz (0 is best),
eg: envelope goodness-of-fit (10 is best), pg: phase goodness-of-fit (10 is best). for
frequency biases, negative values indicate underestimation.

29

figure s2: distribution of the frequency bias for each sensor and each sample in the training
(grey area) and validation (blue line) dataset.

figure s3: intrinsic dimension estimated with a linear principal component analysis (pca,
left) and a non-linear maximum likelihood estimator (mle, right). for each method, the
intrinsic dimension is estimated on the hemew-3d database where the source is fixed and
the hemews -3d database with a random source used to train the mifno

30

appendix a.2. supplementary material for comparison with baseline models
table s2 compares a 16-layer f-fno with a 16-layer mifno predicting surface velocity
wavefields when the source has a fixed position and orientation. it should be noted that the
mifno was not specifically trained on this database.
model
f-fno
mifno

dataset with a fixed source position and fixed source orientation
rrmse
rfftlow
rfftmid
rffthigh
eg
pg
0.14 ; 0.25 -0.30 ; 0.03 -0.39 ; 0.01 -0.44 ; 0.01 7.09 ; 8.37 8.49 ; 9.32
0.20 ; 0.31 -0.40 ; -0.03 -0.51 ; -0.06 -0.56 ; -0.08 6.36 ; 7.65 7.71 ; 8.83

table s2: 1st and 3rd quartiles of the metrics computed on 1000 validation samples. rrmse:
relative rmse (0 is best), rfftlow : relative frequency bias 0-1hz (0 is best), rfftmid :
relative frequency bias 1-2hz (0 is best), rffthigh : relative frequency bias 2-5hz (0 is best),
eg: envelope goodness-of-fit (10 is best), pg: phase goodness-of-fit (10 is best). for
frequency biases, negative values indicate underestimation.
figure s4 illustrates the f-fno and mifno predictions when the source has a fixed
position and orientation. envelope gofs of the f-fno are between 8.3 and 8.7, mifno
ones are between 7.5 and 8.7. phase gofs of the f-fno are between 9.2 and 9.6, mifno
ones between 8.2 and 9.4.
dataset with a random source position and fixed source orientation
model
rrmse
rfftlow
rfftmid
rffthigh
eg
f-fno 8 layers 0.14 ; 0.24 -0.31 ; 0.01 -0.41 ; -0.03 -0.48 ; -0.04 6.71 ; 8.05
mifno 8 layers 0.13 ; 0.23 -0.27 ; 0.05 -0.37 ; -0.00 -0.43 ; -0.00 6.93 ; 8.24

pg
8.26 ; 9.19
8.40 ; 9.26

table s3: 1st and 3rd quartiles of the metrics computed on 1000 validation samples. rrmse:
relative rmse (0 is best), rfftlow : relative frequency bias 0-1hz (0 is best), rfftmid :
relative frequency bias 1-2hz (0 is best), rffthigh : relative frequency bias 2-5hz (0 is best),
eg: envelope goodness-of-fit (10 is best), pg: phase goodness-of-fit (10 is best). for
frequency biases, negative values indicate underestimation. both models were trained with
20 000 samples for 300 epochs.

31

figure s4: east-west component of ground motion from simulations (black solid line), ffno predictions (blue dashed line), and mifno predictions (red dashed line)

32

appendix a.3. supplementary material for influence of the source parameters

figure s5: for 1000 samples, the 1-2 hz frequency bias is shown against the source orientation
(strike, dip, rake).

figure s6: for 1000 samples, the envelope and phase gof of mifno predictions is shown
against the source depth.

33

appendix a.4. supplementary material for out-of-distribution data

figure s7: envelope and phase gof for the geology depicted in fig. 11. the white star
denotes the epicenter.

34

figure s8: east-west component of ground motion from simulations (black solid line) and
mifno predictions (red dashed line) for the overthrust geology depicted in fig. 11

35

figure s9: east-west component of ground motion from simulations (black solid line) and
mifno predictions with resolution 64 (orange dashed line)

36

appendix b. supplementary material for transfer learning

figure s10: vs distribution in
the le teil database. for each of
the 32 vertical levels, the mean
and percentiles are computed
over all horizontal points and
samples (shaded areas). blue
lines show some individual vertical profiles.

thickness (m)
600
600
300
600
5700
1800

vs (m/s)
2100
3500
1200
2300
3500
4500

vp (m/s)
3570
5950
2040
3910
5950
7650

Ï (kg/m3 )
2329
2706
1923
2380
2706
3170

table s4: reference 1d geological model for the leteil
region. each layer from top to bottom is described by
its thickness, s-wave velocity (vs ), p-wave velocity (vp ),
and density Ï (adapted from smerzini et al. (2023)).

37

figure s11: same as figure 14 but only 500 transfer learning samples

38

data availability statement
code of the mifno and the hemews -3d database will be released publicly upon acceptation of the manuscript.
disclosure of interest
the authors have no conflict of interest to declare.

39

references
allen, r.m., melgar, d., 2019. earthquake early warning: advances, scientific challenges,
and societal needs. annual review of earth and planetary sciences 47, 361â€“388. doi:10.
1146/annurev-earth-053018-060457.
aminzadeh, f., brac, j., kunz, t., 1997. 3-d salt and overthrust models. seg/eage 3-d
modeling series no.1.
baker, j.w., 2013. probabilistic seismic hazard analysis. technical report.
bartolucci, f., de bÃ©zenac, e., raoniÄ‡, b., molinaro, r., mishra, s., alaifari, r., 2023.
are neural operators really neural operators? frame theory meets operator learning.
url: http://arxiv.org/abs/2305.19913, arxiv:2305.19913.
basri, r., galun, m., geifman, a., jacobs, d., kasten, y., kritchman, s., 2020. frequency
bias in neural networks for input of non-uniform density, in: proceedings of the 37th
international conference on machine learning, jmlr.org.
bloemheuvel, s., van den hoogen, j., jozinoviÄ‡, d., michelini, a., atzmueller, m., 2023.
graph neural networks for multivariate time series regression with application to seismic
data. international journal of data science and analytics 16, 317â€“332. doi:10.1007/
s41060-022-00349-6.
bradley, b.a., pettinga, d., baker, j.w., fraser, j., 2017. guidance on the utilization
of earthquake-induced ground motion simulations in engineering practice. earthquake
spectra 33, 809â€“835. doi:10.1193/120216eqs219ep.
brandstetter, j., worrall, d., welling, m., 2022. message passing neural pde solvers, in:
international conference on learning representations. url: https://openreview.net/
forum?id=vsix3hpyksu.
cremen, g., galasso, c., 2020. earthquake early warning: recent advances and perspectives.
earth-science reviews 205, 103184. doi:10.1016/j.earscirev.2020.103184.
datta, a., wu, d.j., zhu, w., cai, m., ellsworth, w.l., 2022. deepshake: shaking intensity
prediction using deep spatiotemporal rnns for earthquake early warning. seismological
research letters doi:10.1785/0220210141.
delouis, b., oral, e., menager, m., ampuero, j.p., guilhem trilla, a., rÃ©gnier, m., deschamps, a., 2021. constraining the point source parameters of the 11 november 2019
mw 4.9 le teil earthquake using multiple relocation approaches, first motion and full
waveform inversions. comptes rendus. gÃ©oscience 353, 1â€“24. doi:10.5802/crgeos.78.
faccioli, e., maggio, f., paolucci, r., quarteroni, a., 1997. 2d and 3d elastic wave propagation by a pseudo-spectral domain decomposition method. journal of seismology 1,
237â€“251. doi:10.1023/a:1009758820546.
40

haghighat, e., waheed, u.b., karniadakis, g., 2024. en-deeponet: an enrichment approach for enhancing the expressivity of neural operators with applications to seismology.
computer methods in applied mechanics and engineering 420, 116681. doi:10.1016/j.
cma.2023.116681.
hao, z., wang, z., su, h., ying, c., dong, y., liu, s., cheng, z., song, j., zhu, j.,
2023. gnot: a general neural operator transformer for operator learning, in: krause,
a., brunskill, e., cho, k., engelhardt, b., sabato, s., scarlett, j. (eds.), proceedings of
the 40th international conference on machine learning, pmlr. pp. 12556â€“12569. url:
https://proceedings.mlr.press/v202/hao23c.html.
hsu, t.y., huang, c.w., 2021. onsite early prediction of pga using cnn with multiscale and multi-domain p-waves as input. frontiers in earth science 9, 626908. doi:10.
3389/feart.2021.626908.
jiang, z., zhu, m., li, d., li, q., yuan, y.o., lu, l., 2023. fourier-mionet: fourierenhanced multiple-input neural operators for multiphase modeling of geological carbon
sequestration. url: http://arxiv.org/abs/2303.04778, arxiv:2303.04778.
jin, p., meng, s., lu, l., 2022. mionet: learning multiple-input operators via tensor product. siam journal on scientific computing 44, a3490â€“a3514. doi:10.1137/22m1477751.
jozinoviÄ‡, d., lomax, a., Å¡tajduhar, i., michelini, a., 2020. rapid prediction of earthquake
ground shaking intensity using raw waveform data and a convolutional neural network.
geophysical journal international 222, 1379â€“1389. doi:10.1093/gji/ggaa233.
komatitsch, d., tromp, j., 1999. introduction to the spectral element method for threedimensional seismic wave propagation. geophysical journal international 139, 806â€“822.
doi:10.1046/j.1365-246x.1999.00967.x.
kong, q., rodgers, a., 2023. feasibility of using fourier neural operators for 3d elastic
seismic simulations. technical report llnl-tr-854521. lawrence livermore national
laboratory. url: https://www.osti.gov/servlets/purl/2001189.
kristekovÃ¡, m., kristek, j., moczo, p., 2009. time-frequency misfit and goodness-of-fit
criteria for quantitative comparison of time signals. geophysical journal international
178, 813â€“825. doi:10.1111/j.1365-246x.2009.04177.x.
lehmann, f., gatti, f., bertin, m., clouteau, d., 2023. fourier neural operator surrogate
model to predict 3d seismic waves propagation, in: 5th eccomas thematic conference
on uncertainty quantification in computational sciences and engineering, athens, greece.
doi:10.7712/120223.10339.20362.
lehmann, f., gatti, f., bertin, m., clouteau, d., 2024a. 3d elastic wave propagation with
a factorized fourier neural operator (f-fno). computer methods in applied mechanics
and engineering 420, 116718. doi:10.1016/j.cma.2023.116718.
41

lehmann, f., gatti, f., bertin, m., clouteau, d., 2024b. synthetic ground motions in
heterogeneous geologies: the hemew-3d dataset for scientific machine learning. earth
system science data discussions 2024, 1â€“26. doi:10.5194/essd-2023-470.
levina, e., bickel, p., 2004. maximum likelihood estimation of intrinsic dimension, in: saul,
l., weiss, y., bottou, l. (eds.), advances in neural information processing systems,
mit press. url: https://proceedings.neurips.cc/paper_files/paper/2004/file/
74934548253bcab8490ebd74afed7031-paper.pdf.
li, z., kovachki, n.b., azizzadenesheli, k., liu, b., bhattacharya, k., stuart, a., anandkumar, a., 2021. fourier neural operator for parametric partial differential equations, in:
international conference on learning representations. doi:10.48550/arxiv.2010.08895.
lu, l., jin, p., pang, g., zhang, z., karniadakis, g.e., 2021. learning nonlinear operators
via deeponet based on the universal approximation theorem of operators. nature machine
intelligence 3, 218â€“229. doi:10.1038/s42256-021-00302-5.
moczo, p., kristek, j., bard, p.y., stripajovÃ¡, s., hollender, f., chovanovÃ¡, z., kristekovÃ¡,
m., sicilia, d., 2018. key structural parameters affecting earthquake ground motion in
2d and 3d sedimentary structures. bulletin of earthquake engineering 16, 2421â€“2450.
doi:10.1007/s10518-018-0345-5.
mousavi, s.m., beroza, g.c., 2023.
machine learning in earthquake seismology. annual review of earth and planetary sciences 51, 105â€“129. doi:10.1146/
annurev-earth-071822-100323.
oâ€™brien, g.s., bean, c.j., meiland, h., witte, p., 2023. imaging and seismic modelling inside volcanoes using machine learning. scientific reports 13, 630. doi:10.1038/
s41598-023-27738-6.
rahaman, n., baratin, a., arpit, d., draxler, f., lin, m., hamprecht, f., bengio, y.,
courville, a., 2019. on the spectral bias of neural networks, in: chaudhuri, k., salakhutdinov, r. (eds.), proceedings of the 36th international conference on machine learning,
pmlr. pp. 5301â€“5310. url: https://proceedings.mlr.press/v97/rahaman19a.html.
rahman, m.a., ross, z.e., azizzadenesheli, k., 2023. u-no: u-shaped neural operators.
transactions on machine learning research url: https://openreview.net/forum?id=
j3oqf9cojd.
smerzini, c., paolucci, r., stupazzini, m., 2011. comparison of 3d, 2d and 1d numerical approaches to predict long period earthquake ground motion in the gubbio
plain, central italy. bulletin of earthquake engineering 9, 2007â€“2029. doi:10.1007/
s10518-011-9289-8.
smerzini, c., vanini, m., paolucci, r., renault, p., traversa, p., 2023. regional physicsbased simulation of ground motion within the rhÃ´ne valley, france, during the mw 4.9
42

2019 le teil earthquake. bulletin of earthquake engineering 21, 1747â€“1774. doi:10.1007/
s10518-022-01591-w.
ta, q.a., clouteau, d., cottereau, r., 2010. modeling of random anisotropic elastic media
and impact on wave propagation. european journal of computational mechanics 19,
241â€“253. doi:10.3166/ejcm.19.241-253.
touhami, s., gatti, f., lopez-caballero, f., cottereau, r., de abreu corrÃªa, l., aubry,
l., clouteau, d., 2022. sem3d: a 3d high-fidelity numerical earthquake simulator for
broadband (0â€“10 hz) seismic response prediction at a regional scale. geosciences 12,
112. doi:10.3390/geosciences12030112.
tran, a., mathews, a., xie, l., ong, c.s., 2023. factorized fourier neural operators,
in: the eleventh international conference on learning representations. url: https:
//openreview.net/forum?id=tmiimpl4ipa.
vallage, a., bollinger, l., champenois, j., duverger, c., trilla, a.g., hernandez, b., pichon,
a.l., listowski, c., mazet-roux, g., menager, m., pinel-puyssÃ©gur, b., vergoz, j., 2021.
multitechnology characterization of an unusual surface rupturing intraplate earthquake:
the m l 5.4 2019 le teil event in france. geophysical journal international 226, 803â€“
813. doi:10.1093/gji/ggab136.
wang, a., li, s., lu, j., zhang, h., wang, b., xie, z., 2023. prediction of pga in earthquake early warning using a long short-term memory neural network. geophysical journal
international , ggad067doi:10.1093/gji/ggad067.
yang, y., gao, a.f., azizzadenesheli, k., clayton, r.w., ross, z.e., 2023. rapid seismic
waveform modeling and inversion with neural operators. ieee transactions on geoscience and remote sensing , 1â€“1doi:10.1109/tgrs.2023.3264210.
zhang, t., trad, d., innanen, k., 2023. learning to solve the elastic wave equation with
fourier neural operators. geophysics , 1â€“63doi:10.1190/geo2022-0268.1.
zhu, c., riga, e., pitilakis, k., zhang, j., thambiratnam, d., 2020. seismic aggravation in
shallow basins in addition to one-dimensional site amplification. journal of earthquake
engineering 24, 1477â€“1499. doi:10.1080/13632469.2018.1472679.
zou, c., azizzadenesheli, k., ross, z.e., clayton, r.w., 2023. deep neural helmholtz
operators for 3d elastic wave propagation and inversion. url: http://arxiv.org/
abs/2311.09608, arxiv:2311.09608.

43



ieee transactions on geoscience and remote sensing, vol. 62, 2024

4507611

seismic wavefields modeling with variable
horizontally layered velocity models via
velocity-encoded pinn
jingbo zou , cai liu, pengfei zhao, and chao song

abstractâ€” seismic modeling is crucial for tackling waveformbased inverse problems in geophysics. physics-informed neural
networks (pinns) have become a popular tool for simulating
seismic waves. their ability to incorporate partial differential
equations (pdes), initial conditions (ics), and boundary conditions directly into the loss function allows for physically accurate
modeling. the prevalent approach in the current literature treats
the wave equation as a parametric pde. however, the majority
of the existing studies simulate wavefields for a specific velocity
model, necessitating network retraining for different models,
thereby diminishing modeling efficiency. in response, we present
a velocity-encoded (ve) pinn (ve-pinn) that introduces feature
parameters to represent various layered velocity models, integrating them into the network. drawing inspiration from supervised
learning, our approach employs a ve method to compute
initial wavefields for variable layered models. remarkably, our
proposed ve-pinn demonstrates the ability to generalize across
different ics within the dataset. this eliminates the need to
retrain the network for each new solution, offering significant
efficiency gains. numerical results show that the ve-pinn
significantly enhances efficiency in solving the acoustic wave
equation for various layered velocity models compared with
finite-difference methods (fdms). subsequently, we extend the
application of our method to time-domain simulation for variable
source locations, demonstrating that the ve-pinn yields the
results that are consistent with numerical wavefields.
index termsâ€” physics-informed neural network (pinn), seismic modeling, variable velocity models, velocity-encoded (ve)
strategy.

i. i ntroduction
eismic wave simulation relies on solving the wave
equation for a well-defined geological model, constituting
a fundamental component of full waveform inversion and
imaging. numerical solutions to the wave equation predominantly employ methods, such as the finite-difference method
(fdm) [1], the finite-element method [2], and the spectral
element method [3]. nevertheless, these conventional numerical methods require a mesh, which may cause numerical
dispersion when the mesh spacing is coarse, and also pose

s

a challenge when the domain has a complex shape, such as
topography.
the advancements in scientific computing have paved the
way for utilizing deep learning in developing numerical
solutions for pdes. neural networks, proven to be effective, are employed to approximate the intricate nonlinear
inputâ€“output relationships within complex systems [4]. in particular, raissi et al. [5] proposed a physics-informed neural
network (pinn), which has received a lot of attention in many
scientific fields. pinn integrates pdes into the loss function,
encompassing both initial and boundary conditions. this integration converts the task of solving pdes into an optimization
problem. operating as a mesh-free methodology, pinn simplifies meshing complexities and mitigates numerical dispersion.
furthermore, it diminishes the necessity for extensive training
data, enhancing interpretability in comparison with traditional
data-driven deep learning methods. pinn finds applications
in diverse problem domains, such as chemistry [6], fluid
mechanics [7], material [8], and earth systems [9]. in spite of
the accuracy and flexibility features of pinn solutions, solving
pdes using pinns is often expensive, especially for complex
solutions, such as wavefields [10]. various strategies have been
proposed to enhance the performance of pinn. these include
fractional pinn (fpinn) [11] for fractional equations, parareal
pinn (ppinn) [12] designed to further enhance training
efficiency for long-time physical problems, variational pinn
(vpinn) [13] to reduce the order of differential operators,
and pinn with hard constraints (hpinn) [14] incorporating
hard constraints through the penalty method and augmented
lagrangian method.
recently, pinn is also applied to tackle the forward
and inverse problems in geophysics [40]. the foundational work by waheed et al. [15] established a framework for solving the eikonal equation in isotropic media.
subsequently, waheed et al. [16] introduced transfer learning to accelerate convergence. addressing the helmholtz
equation, alkhalifah et al. [10] proposed the simulation of
frequency-domain scattered wavefields as a means to avoid
the challenges associated with point-source singularities. this
approach was further extended to anisotropic media by song
and alkhalifah [17] and song et al. [18]. to optimize the
training process, song et al. [19] incorporated an adaptive
activation function, and huang and alkhalifah [20] enhanced
the network convergence and accuracy by using frequency
upscaling and neuron splitting. for multifrequency wavefields,

manuscript received 19 december 2023; revised 4 march 2024 and 21 april
2024; accepted 17 may 2024. date of publication 7 june 2024; date of current
version 20 june 2024. this work was supported in part by the national key
research and development program of china under grant 2023yfc3707901
and in part by the national natural science foundation of china under grant
41874125. (corresponding author: chao song.)
the authors are with the department of geophysics, college of
geo-exploration science and technology, jilin university, changchun,
jilin 130026, china (e-mail: zoujb21@mails.jlu.edu.cn; liucai@jlu.edu.cn;
zhaopf@jlu.edu.cn; chaosong@jlu.edu.cn).
digital object identifier 10.1109/tgrs.2024.3411472
1558-0644 Â© 2024 ieee. personal use is permitted, but republication/redistribution requires ieee permission.
see https://www.ieee.org/publications/rights/index.html for more information.

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

4507611

ieee transactions on geoscience and remote sensing, vol. 62, 2024

huang and alkhalifah [21] introduced a novel loss function
utilizing a reference frequency, while song and wang [22]
developed a pinn framework with embedded fourier features.
seeking to capture nonsmooth features more effectively in
wavefields, wu et al. [23] designed new neurons using a
quadratic function within the activation function and integrated
perfectly matched layer (pml) conditions into the loss function. in addition, alkhalifah and huang [24] utilized gabor
functional solutions of the helmholtz equation within pinns,
which considerably helped convergence.
in the application of time-domain seismic wave simulation,
moseley et al. [25], [26] proposed a â€œcurriculum learningâ€
strategy to enhance training efficiency and scaled pinn to a
large computational domain known as the finite basis pinn
(fbpinn). in addition, rasht-behesht et al. [27] demonstrated
that pinn could simulate wave propagation and execute full
waveform inversion by leveraging numerical wave equation
solvers to provide wavefields as initial conditions (ics).
models incorporating continuous time-dependent point-source
functions, as exemplified by alkhadhr and almekkawy [28],
monitored the effects of the soft or hard constraints on
accuracy. addressing the challenge of estimating velocity
and density without necessitating a good initial model,
zhang et al. [29] devised a seismic inversion strategy encoding first-order acoustic wave equations, partially observed
seismograms, and well-logging data into the loss function.
furthermore, wang et al. [30] explored pinn for predicting
transcranial ultrasound wave propagation. by introducing an
absorbing boundary condition (abc), ren et al. [9] established a pinn model for solving elastic wave equations in
semi-infinite domains. to overcome training difficulties associated with pinn, cheng wong et al. [31] proposed sf-pinns
with sinusoidal feature mapping appended to a typical pinn
with tanh activation in subsequent hidden layers. recognizing
the hindrance posed by the multiterm objective function of
pinn, nosrati and emami niri [32] suggested two main
modifications to the loss calculation: logarithmic and sigmoidal self-adaptive regularization multipliers. zou et al. [33]
addressed the issue of balancing different terms in the loss
function by introducing a task-decomposed pinn (td-pinn),
involving three training steps: pretraining, full learning, and
physics-enhanced training.
pinn has demonstrated success in solving wave equations. the current research predominantly employs wavefields
derived from conventional numerical methods (e.g., fdm
and finite-element method) as ics. nevertheless, a significant
challenge arises in efficiently transferring knowledge across
varied configurations. when treating the wave equation as
a parametric pde with velocity as a variable parameter,
it becomes apparent that the majority of the aforementioned
methods are tailored and optimized for a specific velocity
model, rendering them incapable of predicting solutions for
another velocity model with training that may cost as much
as the original training. for the eikonal equation, taufik
and alkhalifah [34] suggested pretraining an autoencoder to
represent a velocity distribution in the latent space and then
used the latent representation of the velocity models as input
to train the pinns for a distribution of velocity models.

however, the cost of such extended training is not cheap even
for solving the eikonal equation. in pursuit of constructing
a more direct solver that integrates variable velocity models
and source locations, we utilize continuous feature parameters,
such as velocity, layer interface, and point-source position as
inputs for the network.
using pinns as a foundation, we developed a framework
to perform seismic modeling with varying source locations
and velocity models. the notable contributions of this study
encompass the following.
1) to enhance the networkâ€™s generalization across diverse
velocity models, it is essential to include the velocity
variable v p as an input parameter within the system. in this study, drawing inspiration from supervised
learning, the training process contains two sequential steps: velocity-encoded (ve) wavefield predictor
and physics-guided seismic modeling. in the first part,
we incorporate wavefields corresponding to different
velocity models, computed using the fdm, into our
training dataset. subsequently, a neural network, with
velocity models as inputs, is employed to generate initial
wavefields for a specific velocity model, which serve as
the ics in the subsequent step. our primary objective
is to establish a mapping between distinct velocity
models and their corresponding wavefields. different
from the conventional data-driven methods, we only
use limited training data. in the physics-guided stage,
pinn is deployed to predict wavefield solutions for a
predetermined velocity model identical to the one in the
first step. adhering to the constraints imposed by the
wave equation and ics, pinn computes wavefields at
various time steps. the ve wavefield predictor furnishes
a more dependable foundation for initializing the pinn
parameters in the second step, enhancing the generalizability of the improved pinn.
2) in response to the direct incorporation of v p as an
input to the network, this study identifies and delves
into the challenges inherent in its practical implementation. notably, simulations conducted in inhomogeneous
media are vulnerable to interfacial discontinuities,
thereby exerting a substantial influence on the accuracy
of network predictions. building upon the framework
of pinn, we introduce a novel approach termed vepinn. this research distinguishes between various
velocity models by incorporating feature parameters,
including interface depth and velocity in every layer.
the introduction of these feature parameters as inputs
in the initial step, labeled â€œve wavefield predictor,â€
enhances the networkâ€™s ability to capture velocity
variations and the corresponding characterization of
wavefields.
3) this method enables the direct simulation of timedomain wavefields for various source locations, simply
by using source location as an input. neural networks, known for their aptitude in capturing nonlinear
relationships between continuous parameters and their
corresponding solutions, make this approach particularly effective. the underlying efficacy of this strategy

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

zou et al.: seismic wavefields modeling with variable horizontally layered

stems from its fundamental principle, where continuous
parameters are utilized to represent information that
inherently possesses discontinuities. this study streamlines the workflow for modeling wave equations across
various velocity models and source locations. once the
ve part of the network is trained, there is no necessity
to recalculate initial solutions using traditional numerical methods for different models. multisource seismic
wave simulation using pinn has achieved remarkable
success in the frequency domain. however, its efficacy
in time-domain modeling remains challenging. in the
application of time-domain simulation, we propose to
employ a network to predict the initial solutions for
various source locations within a narrower temporal
scope, which serve as the ics for physics-guided training
across the entire domain. leveraging the output from
the initial part as ics, ve-pinn can accurately simulate
seismic wave propagation.
the remainder of this article is structured as follows.
section ii provides a comprehensive overview of ve-pinn,
detailing its design and implementation. in section iii,
we illustrate the efficacy and accuracy of ve-pinn in seismic
modeling for variable velocity models, encompassing both
two-layered and three-layered configurations. examples are
presented to showcase the frameworkâ€™s performance. following this, ve-pinn is applied to simulate time-domain
wave propagation for diverse source locations, demonstrating
its versatility. this article concludes with a brief summary
in section iv.
ii. m ethodology
a. acoustic wave equations
the homogeneous wave equation for a 2-d acoustic
medium, under the conditions of constant density, takes the
form
 2

âˆ‚ 2u
âˆ‚ 2u
2 âˆ‚ u
=
c
+
(1)
âˆ‚t 2
âˆ‚x2
âˆ‚z 2
where c is the velocity model, a function of space (x, z).
u represents the wavefields to be solved, and (x, z, t) are
spatial and temporal coordinates.
b. physics-informed neural network
pinn incorporates pdes into the loss function as regularization terms. this inclusion serves to enhance interpretability,
distinguishing it from conventional data-driven deep learning
methodologies. for the acoustic wave equation given in (1),
pinn uses a neural network to represent the solution of the
equation u(t, x, z), so the pde residual is given by
 2

âˆ‚ 2u
âˆ‚ 2u
2 âˆ‚ u
+ 2 .
(2)
f := 2 âˆ’ c
âˆ‚t
âˆ‚x2
âˆ‚z
in order to train the neural network, we define the loss
function as follows:
mse = mseic + Îµmsepde

(3)

4507611

fig. 1. conventional pinn workflow for the 2-d acoustic wave equation
in a homogeneous model with a velocity of 1.0 km/s (ic corresponds to the
initial conditions).

where
n s1
1 x
2
mseic =
u(t = t1 , xi , z i ) âˆ’ u s1
ns1 i=1

+ Â·Â·Â· +

n sm
1 x
2
u(t = tm , xi , zi ) âˆ’ u sm
nsm i=1

(4)

nf

msepde =

1 x  (i) (i) (i)  2
f tf , x f , z f
n f i=1

(5)

where {xi , z i } represent the spatial coordinate samples for the
initial wavefields (u si ) at the time steps between t1 and tm .
(i) n f
{x f(i) , z (i)
f , t f }i=1 specify the collocation points sampled across
the entire domain. ns1 , nsm , and n f are the numbers of
sampling points. msepde denotes the physics loss controlled
by the wave equation, and mseic denotes the loss associated
with the ic for the initial wavefield constraint. the parameter Îµ
serves as a weight factor added to the loss function to balance
the contributions of the two terms. we employ wavefields at m
time steps as ics. abcs are not set here, under the assumption
that seismic waves propagate in an infinite space.
the loss function is minimized to update the network
parameters to approximate the solution of the partial differential equation (pde). it incorporates both an initial loss and
a physics loss, guaranteeing the uniqueness of the solution and
constraining the output within the bounds of a specified physical law. in contrast to conventional data-driven approaches,
the inclusion of the physics loss in pinn serves the dual
purpose of mitigating the need for numerical wavefield solutions and ensuring reasonable accuracy beyond the confines of
the training dataset. automatic differentiation facilitates more
straightforward gradient calculations, allowing for efficient
updates to the network parameters. to explain this workflow,
we present the application of the proposed approach to the
2-d acoustic wave equation, as depicted in fig. 1.
c. velocity-encoded pinn
by incorporating pdes into the loss function, along with the
inclusion of initial and boundary conditions, pinn reframes
the problem of solving pdes as an optimization task.

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

4507611

ieee transactions on geoscience and remote sensing, vol. 62, 2024

fig. 2. ve-pinn workflow for the 2-d acoustic wave equation for variable two-layered models, where d represents the layer interface depth and (vup , vlow )
specify the velocity in the top and bottom layers, respectively. there are two networks: neural network 1 to predict limited steps of the wavefield evolution
(top) and network 2 trained using physics-guided seismic modeling (bottom). numerical wavefields ufdm for some given velocity models are datasets to train
nn 1. its outputs are predicted solutions uÎ¸1 for a certain model. furthermore, uÎ¸1 is used to provide the ics to train nn 2. the outputs are wavefields on
arbitrary coordinates in the entire domain of a fixed model.

nonetheless, challenges persist in addressing the time-domain
wave equation, attributed to the intricate nature of seismic
wavefields and the singularity arising from point sources.
notably, a limitation of the conventional pinn is its exclusive
applicability to solve the wave equation for a specific velocity
model, posing an evident constraint in practical scenarios.
to overcome this limitation, we propose leveraging neural
networks to establish mappings that connect diverse velocity models and source locations to the corresponding wave
propagation.
to perform seismic wave modeling for multiple velocity
models, we propose to take the velocity model v p as an
input of the network. by incorporating diverse velocity models
along with their corresponding wavefields as ics, the goal
is for the neural network to adeptly learn the wavefields
associated with new velocity models. nevertheless, we observe
that the network struggles to effectively capture abrupt variations, particularly those associated with discontinuities at
surfaces. furthermore, the training of the network proves
challenging due to the discrete nature of velocity, as illustrated
in section iii-b. building upon the preceding discussion,
we introduce parameters, such as interface depth and velocity,
for each layer to represent distinct velocity models. these
parameters, denoted as feature parameters in this article,
exhibit continuous variation within specific ranges, devoid
of noticeable interruptions. the challenges stemming from

interface discontinuities are mitigated by directly incorporating
interface locations as inputs for the network.
on the other hand, the computing complexity and cost
increase significantly with the feature parameters as inputs to
pinn. in such instances, the neural network is required to
comprehend the wavefields across diverse feature parameters.
thus, it is essential to forecast the wave propagation for a specific model. consequently, we introduce a novel decomposed
training approach aimed at diminishing the input dimension
of the pinn and enforcing its training stability. the workflow
is illustrated in fig. 2, and the specific details are explained
below.
1) ve wavefield predictor: as previously discussed, pinn
exclusively solves the wave equation for a specific
velocity model. it means that repeated numerical simulations are required for different velocity models and
source locations. inspired by supervised learning, we use
the ve method with limited training data. initially,
numerical wavefields ufdm are computed for distinct
velocity models using the fdm. we use feature parameters (d, vup , vlow ) to symbolize random two-layered
models, where d represents the layer interface depth
and (vup , vlow ) specify the velocity in the top and bottom layers, respectively. a subset of these wavefields
is then randomly sampled across the entire domain
(x, z, t, d, vup , vlow , ufdm ) to form a training dataset d.

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

zou et al.: seismic wavefields modeling with variable horizontally layered

4507611

with this dataset d as constraints, neural network 1
undergoes training to predict wavefields corresponding
as a function of the two-layered velocity models. subsequently, the wavefields at three distinct time steps are
chosen as ics for the second stage of the ve-pinn.
once trained, neural network 1 can extrapolate wavefields for variable velocity models without requiring
retraining of the network. the loss function is defined
as follows:
n

msenn1 =

v

1 x
2
u xi , z i , ti , di , vupi , vlowi âˆ’ d
nv i=1

fig. 3. neural network xs workflow for the 2-d acoustic wave equation for
variable source locations. xs denotes the horizontal source location.

(6)
where (xi , z i , ti , di , vupi , vlowi ) specify the collocation
points sampled across the entire domain, nv is the
number of sampling points, and d is the limited training
data from fdm.
2) physics-guided seismic modeling: our investigation
shows that the pivotal aspect in training pinn lies
in fortifying the constraints related to the ics, as this
directly influences prediction accuracy. in this stage,
we introduce wavefields at three temporal intervals,
generated using nn1, as the ics for conducting further seismic simulations for a specific velocity model.
by incorporating constraints related to the ics and
pdes, pinn can compute solutions with reasonable
accuracy. notably, for various velocity models, there is
no need to recalculate the corresponding initial wavefields through traditional numerical methods. in an effort
to mitigate the computational complexity associated with
high-dimensional problems, we opted not to consider
feature parameters as inputs. consequently, the loss
function is defined as follows:
msenn2 = mseic + Îµmsepde

(7)

where
np

msepde =

2
1 x
f t pi , x ip , z ip
n p i=1

(8)

mseic = mseic1 + mseic2 + mseic3
=

ns1
1 x
|u(xi , z i , ti = ts1 ) âˆ’ u s1 |2
ns1 i=1

+

ns2
1 x
|u(xi , z i , ti = ts2 ) âˆ’ u s2 |2
ns2 i=1

+

ns3
1 x
|u(xi , z i , ti = ts3 ) âˆ’ u s3 |2
ns3 i=1

(9)

where mseic1 , mseic2 , and mseic3 denote the residuals
between the given numerical wavefields and the network
prediction results for three time steps. ns1 , ns2 , ns3 , and
n p are the numbers of sampling points for the ics and
pde loss.
fig. 2 shows the workflow of ve-pinn for variable
two-layered velocity models. it can be extended to seismic
modeling for different source locations, which only requires

the replacement of neural network 1 with neural network xs
in fig. 3.
d. network training
to reduce the computational cost, we use two separate
neural networks to tackle different tasks, which are trained
independently. neural network 1 focuses on predicting wavefields for various velocity models without requiring retraining.
for a specific velocity model to be solved, neural network 2
performs wavefields simulation across the entire domain.
in neural network 1 or network xs , the inputs comprise
a series of sampling points (x, z, t) and feature parameters
(d, vup , vlow ) or (xs ), yielding the output wavefield uÎ¸1 corresponding to specific velocity models or source locations. in the
feature parameters, d represents the layer interface depth, and
(vup , vlow ) specify the velocity in the top and bottom layers,
respectively. xs denotes the horizontal source location. during
the training of network 1, a subset of the numerical wavefields
ufdm from fdm is employed as the dataset to constrain
the training process, utilizing a learning rate of 5 Ã— 10âˆ’4 .
in neural network 2, the inputs consist of sampling points
(x, z, t), producing an output wavefield for a specific velocity
model or source location. the training of network 2 involves
utilizing uÎ¸1 at specific time steps as ics, with the learning rate
exponentially decaying from an initial value of 1 Ã— 10âˆ’3 every
5000 epochs. Îµ is set to 1 Ã— 10âˆ’4 for all cases. the network
architecture employs a fully connected neural network with a
hyperbolic tangent activation function [35]. there are 80 000
training points randomly sampled in the entire domain. the
training employs an adam optimizer [36]. all the numerical
examples are performed on an rtx 3090 gpu. the number
of neurons per hidden layer is {64, 64, 64, 64, 64, 32, 32, 32}
for all cases. algorithm 1 details the training process. the
training pipeline for variable source locations mirrors this
procedure, requiring only the replacement of velocity models
(d, vup , vlow ) with source locations (xs ).
iii. r esults
in this section, we test the performance of ve-pinn
for variable layered velocity models, including two-layered
and three-layered models. utilizing the feature parameters
as inputs, ve-pinn demonstrates enhanced generalization
capabilities across diverse velocity models. in the last case,
we conduct seismic modeling for varying source locations.

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

4507611

ieee transactions on geoscience and remote sensing, vol. 62, 2024

algorithm 1 ve-pinn for seismic modeling for variable
two-layered velocity models
notes: d represents the layer interface depth and (vup , vlow )
specify the velocity in the top and bottom layers, respectively.
part i: ve wavefield predictor
goal: establish a mapping from the velocity models to their
corresponding wavefields.
initiate: neural network 1 parameters Î¸1 .
for each epoch in neural network 1 do
input: coordinates (x, z, t) and feature parameters
(d, vup , vlow ).
output: uÎ¸1 (x, z, t, d, vup , vlow ).
calculate the loss function of equation (6).
update: parameters of neural network 1 Î¸1 .
stop training of neural network 1.
part ii: physics-guided seismic modeling
goal: simulate the wave propagation as time progresses for
a specific velocity model.
initiate: neural network 2 parameters Î¸2 .
for each epoch in neural network 2 do
input: coordinates (x, z, t).
output: uÎ¸2 (x, z, t).
calculate the loss function of equation (7). initial wavefields used in m s e ic are obtained from uÎ¸1 .
update: parameters of neural network 2 Î¸2 .

fig. 4. three velocity models and their corresponding numerical solutions
(model (a): two-layered model with a velocity of 0.8 and 1.0 km/s in the
top and bottom layers, respectively, and the interface depth is 0.5 km; model
(b): two-layered model with a velocity of 1.0 and 1.4 km/s in the top and
bottom layers, respectively, and the interface depth is 0.3 km; and model
(c): two-layered model with a velocity of 1.0 and 1.2 km/s in the top and
bottom layers, respectively, and the interface depth is 0.6 km).

through conditioning the network on the source locations,
we observe its capacity to generalize over the given ics,
obviating the necessity for retraining the network for each
solution.
a. ve wavefield predictor
as mentioned earlier, we are focused on the 2-d acoustic
wave equation. first, we use fdm to solve numerical wavefields ufdm for various two-layer velocity models (d, vup , vlow )
or source locations (xs ). we choose a ricker wavelet as
the source function to simulate seismic waves. the model
size is 101 Ã— 101 with a spatial spacing of 0.01 km. for
an example velocity model, we plot snapshots (from ufdm )
at four timesteps in fig. 4. the coordinates and feature
parameters (x, z, t, d, vup , vlow ) are inputs to neural network 1
(fig. 2), in which we use random points in the domain
(x, z, t, d, vup , vlow , ufdm ) for training with ufdm as target. the
predicted solutions subsequently serve as the ics for neural
network 2.
b. solving the wave equation for various layered models
1) seismic modeling for various two-layered models:
first, we demonstrate the performance for two-layered velocity
models with a source depth of 0.2 km. the feature parameters consist of the interface depth d âˆˆ [0.2, 1 km] and the
velocities in the top and bottom layers, denoted as vup , vlow âˆˆ
[0.6, 2 km/s]. we randomly select 12 samples within the
range (d, vup , vlow ), representing 12 velocity models. in the

fig. 5. comparison between the wavefields predicted by ve-pinn and fdm.
the first row displays ve-pinn solutions, the second one exhibits numerical
wavefields from fdm, and the difference is shown in the third row (model
(a): two-layered model with a velocity of 1.4 and 0.8 km/s in the top and
bottom layers, respectively, and the interface depth is 0.4 km and model (b):
two-layered model with a velocity of 1.0 and 1.2 km/s in the top and bottom
layers, respectively, and the interface depth is 0.5 km).

domain x, z âˆˆ [0, 1 km], t âˆˆ [0, 1 s], we use fdm to
calculate numerical wavefields ufdm for the 12 velocity
models and randomly sample 15 000 points in the domain
(x, z, t, d, vup , vlow , ufdm ) as training data. after 30 000 epochs
of neural network 1, the outputs are wavefields uÎ¸1 at
t = 0.1, 0.2, and 0.4 s for the velocity models in fig. 5
(which are not included in the training set). the training
of network 1 is then halted. in the stage â€œphysics-guided
seismic modeling,â€ we take uÎ¸1 as the ics to train neural
network 2 for 60 000 epochs. pinn can predict wavefields
within t = 0 âˆ’ 1 s, and the results are shown in fig. 5.
the first row displays ve-pinn solutions, while the second
row exhibits numerical wavefields from fdm. both rows
depict the wave propagation over time, demonstrating accurate
agreement between ve-pinn solutions and the reference ones.
by conditioning network 1 on velocity models, ve-pinn can
effectively simulate wavefields on models not included in the
original dataset.

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

zou et al.: seismic wavefields modeling with variable horizontally layered

4507611

table i
c omparison b etween c omputational c osts for c ases 1â€“3

fig. 6. comparison between the wavefields predicted in cases 1â€“3 (using the
conventional pinn, we add the feature parameters as inputs (case 1) and the
entire velocity models v p as inputs (case 2); in case 3, we use the proposed
ve-pinn).

to illustrate the effectiveness of ve-pinn, we compare its performance with the conventional pinn. take
model (b) in fig. 5 as an example, with a velocity of 1.0 and
1.2 km/s in the top and bottom layers, respectively, and an
interface depth of 0.5 km. using pinn, we add the feature
parameters as inputs (case 1) and the entire velocity models
v p as inputs (case 2). the ics for network training in
cases 1 and 2 are the same as ones in ve-pinn, including
12 velocity models and their corresponding wavefields. fig. 6
illustrates a comparison among case 1, case 2, and ve-pinn
(case 3). the computational time and error records are presented in table i. it is evident from the results that ve-pinn,
as discussed in this study, exhibits a notable advantage by
requiring less time for achieving higher accuracy. moreover,
it demonstrates an enhanced capability in capturing the variations of wavefields at the interface.
we present the training loss curve for the current case,
depicted in fig. 7. for case 2, the loss curve converges slowly.
for case 3, neural network 2 conducts seismic wavefield simulation for a specific velocity model, utilizing predictions from
network 1 as the ic. it shows a better and faster convergence
compared with case 1, due to the decreased computational
complexity. although the loss curve for case 1 diminishes
to a lower value, the high-dimensional modeling in pinns
increases the computational complexity, leading to diminished
efficiency and accuracy, as demonstrated in table i.
2) seismic modeling for various three-layered models:
the proposed method is readily applicable to models featuring
three or more layers, which only needs to modify the feature
parameters. in consideration of geological circumstances, it is
generally observed that, under most conditions, the velocity
within the deeper layer surpasses that in shallower one.

fig. 7. training loss for the two-layered velocity model as a function of
epochs for cases 1â€“3 (using the conventional pinn, we add the feature
parameters as inputs (case 1) and the entire velocity models v p as inputs
(case 2); in case 3, we use the proposed ve-pinn and plot the training loss
of the network for physics-guided seismic modeling).

fig. 8.

three-layered model used in this article.

assuming that the velocity in the sedimentary rock is within
the range 1.5â€“6.0 km/s, we set a three-layered horizontal
model. the thickness and velocity specifics for each layer are
illustrated in fig. 8.
we set a velocity model shown in fig. 8, and the
domain interest spans the range x, z âˆˆ [0, 2 km] and
t âˆˆ [0, 1.2 s]. the feature parameters include the interface
depth (d1 , d2 ) âˆˆ [0, 2 km], in which d1 < d2 , and the
velocity vup , vlow , vmid âˆˆ [0.3, 3 km/s] in the top, middle, and
bottom layers, respectively. we randomly select 14 sampling
points in the domain (d1 , d2 , vup , vlow , vmid ), corresponding
to 14 velocity models. we use fdm to calculate numerical
wavefields ufdm and randomly sample 30 000 points in the
domain (x, z, t, d1 , d2 , vup , vlow , vmid , ufdm ) as training data.
after 30 000 epochs of neural network 1, wavefields uÎ¸1 are
generated at t = 0.1, 0.2, and 0.5 s for the velocity model
in fig. 8 (not used in the training set), and subsequently, the
training of network 1 is halted. in the stage â€œphysics-guided
seismic modeling,â€ we take uÎ¸1 as the ics to train neural network 2 for 60 000 epochs. pinn can predict wavefields within
t = 0 âˆ’ 1.2 s, and the results are shown in fig. 9. wavefields
predicted by ve-pinn and numerical modeling are shown in
the first and second rows, respectively. we observe that the

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

4507611

ieee transactions on geoscience and remote sensing, vol. 62, 2024

table ii
c omparison of p rediction t ime c ost b etween ve-pinn and fdm

fig. 10. three-layered model used in this section for multisource seismic
modeling.

fig. 9. comparison between wavefields predicted by ve-pinn and numerical
solutions.

error between the predicted solutions and reference ones is
relatively small. encoding the interface depth directly into the
neural network input enables faster capture of the velocity
variation within each layer. ve-pinn has the capability to
simulate wave phenomena, including reflected and transmitted
waves generated at various interfaces.
ve-pinn can generalize over given velocity models in the
dataset and simulate their corresponding wavefields. in contrast to prevailing methods that necessitate the recalculation
of numerical solutions using fdm for each unique ic,
ve-pinn obviates the need for repetitive computations
once the network has undergone training. a comparison
of the computational cost associated with predicting initial wavefields through ve-pinn and fdm is presented
in table ii.
c. seismic modeling for various source locations
the proposed methods described above readily apply themselves to the extension of time-domain seismic simulation for
various source locations. multisource seismic modeling in the
frequency domain has found extensive application. however,
challenges persist in the time domain, primarily concerning
computational cost and complex wave dynamics. in this
section, we position the point source at a depth of 0.2 km.
we use a three-layered model with a velocity of 0.8, 1.0, and
1.2 km/s in the top, middle, and bottom layers, respectively,
with interfaces in the depth of 0.4 and 0.7 km (see fig. 10).
the domain interest spans the range x, z âˆˆ [0, 1 km], t âˆˆ
[0, 1.1 s]. the horizontal coordinates of the source location,
denoted as xs and falling within the range [0, 1 km], serve
as inputs to the network. the network, denoted as neural

fig. 11. comparison between solutions predicted by ve-pinn and numerical
wavefields (xs = 0.2 km).

network xs , is trained using numerical wavefields obtained
at source locations xs = 0.3, 0.4, 0.6, and 0.7 km. the resultant dataset is utilized to train neural network xs , which,
in turn, produces wavefields uÎ¸s corresponding to three new
source locations xs = 0.2, 0.5, and 0.8 km at three distinct
timesteps t = 0.1, 0.3, and 0.6 s after 30 000 training epochs.
employing uÎ¸s as ics, and guided by the wave equation
as physical constraints, neural network 2 predicts solutions,
as illustrated in figs. 11â€“13, after 60 000 training epochs. the
wave phenomena can be captured by the network, including
reflected waves generated by the interfaces. during the training
of network xs , the temporal domain of interest spans the
range t âˆˆ [0, 0.6 s]. in network 2, it is trained within the
range t âˆˆ [0, 1.1 s]. different from multisource simulation
in the frequency domain, we use two networks to perform

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

zou et al.: seismic wavefields modeling with variable horizontally layered

4507611

table iii
c omparison of n etwork t raining t ime c ost b etween ve-pinn and pinn

fig. 12. comparison between solutions predicted by ve-pinn and numerical
wavefields (xs = 0.5 km).

fig. 13. comparison between solutions predicted by ve-pinn and numerical
wavefields (xs = 0.8 km).

time-domain modeling. the first network serves as a supervised training for various sources, while network 2 employs
physics-informed fine-tuning across the entire domain. in this
scenario, ve-pinn effectively simulates wave propagation
and demonstrates generalization across varying source locations. a comparison of the computational cost associated with
network training through ve-pinn and the conventional one
is presented in table iii.
iv. d iscussion and c onclusion
we developed a seismic modeling approach referred to
as ve pinn. this method involves the encoding of feature
parameters, representing velocity models and source locations,

into the network inputs. recognizing the neural networkâ€™s
inclination to adeptly capture nonlinear connections between
continuous parameters and their associated solutions, our
study introduces continuous feature parameters to represent the discontinuous velocity models. this approach not
only aligns with the neural networkâ€™s capability, but also
addresses the inherent discontinuity of the velocity model.
consequently, for the simulation of diverse velocity models or source locations, the necessity to recompute initial
wavefields using conventional numerical methods is obviated, resulting in a notable reduction in computational
expenses.
inspired by principles of supervised learning, the proposed
method uses limited data from fdm to train a first neural
network to forecast solutions across diverse velocity models. the findings demonstrate the networkâ€™s capability to
generalize over the provided ics within the dataset. these
solutions, in turn, furnish a more dependable foundation
for initializing pinn in the subsequent stage. adhering
to the constraints imposed by both the ics and pdes,
pinn exhibits a commendable ability to predict solutions
with reasonable accuracy. the numerical outcomes indicate
that the ve-pinn successfully predicts wave propagation
for timesteps beyond its training data and accommodates
changes in velocity models or source locations. this adaptability renders ve-pinn applicable in seismic migration and
inversion.
from the accuracy aspect, ve-pinn is immune to numerical dispersion, which is a problem for numerical wave equation
solvers, such as fdm. from the efficiency aspect, ve-pinn
has the potential to implement seismic wave simulation for
various models and sources more efficiently than fdm. as a
mesh-free method, it also mitigates the challenge we often face
when the domain has a complex shape, such as topography.
for the performance of ve-pinn on the velocities out of
the given range, it is an important aspect we will test in
the further research. the method can also be applicable to
problems defined in a larger domain or models featuring
three or more layers, which only requires a modification to
the input coordinates and feature parameters. for nonhorizontally layered models, we can also use feature parameters
to represent various reflectors, such as faults or anomalies.
the limitation lies in the challenge of generalizing across
velocity models with varying numbers of layers. in addition,
new methods need to be proposed to perform seismic wave
simulation for complex nonlayered models, such as marmousi
models. alternatively, we can use more advanced deep learning techniques. song et al. [37] used physics-informed fourier
neural operator to simulate multisource seismic travel times in
variable velocity models. moseley et al. [38] used a wavenet

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

4507611

ieee transactions on geoscience and remote sensing, vol. 62, 2024

architecture to simulate seismic waves in horizontally layered
media and proposed a conditional autoencoder network to deal
with faulted media with arbitrary layers and fault properties.
zhou et al. [39] used u-net to predict various complex
subsurface velocity fields, including anticlines, synclines, and
anomalous velocity models. these methods will be further
investigated in the future work.
r eferences
[1] z. alterman and f. karal jr., â€œpropagation of elastic waves in layered
media by finite difference methods,â€ bull. seismol. soc. amer., vol. 58,
no. 1, pp. 367â€“398, 1968.
[2] j. lysmer and l. a. drake, â€œa finite element method for seismology,â€ methods comput. phys., adv. res., vol. 11, pp. 181â€“216,
jan. 1972.
[3] d. komatitsch and j.-p. vilotte, â€œthe spectral element method:
an efficient tool to simulate the seismic response of 2d and 3d
geological structures,â€ bull. seismo. soc. amer., vol. 88, pp. 368â€“392,
apr. 1998.
[4] s. cuomo, v. s. di cola, f. giampaolo, g. rozza, m. raissi, and
f. piccialli, â€œscientific machine learning through physicsâ€“informed
neural networks: where we are and whatâ€™s next,â€ j. sci. comput., vol. 92,
no. 3, p. 88, sep. 2022.
[5] m. raissi, p. perdikaris, and g. e. karniadakis, â€œphysics-informed
neural networks: a deep learning framework for solving forward and
inverse problems involving nonlinear partial differential equations,â€
j. comput. phys., vol. 378, pp. 686â€“707, feb. 2019.
[6] w. ji, w. qiu, z. shi, s. pan, and s. deng, â€œstiff-pinn: physicsinformed neural network for stiff chemical kinetics,â€ j. phys. chem. a,
vol. 125, no. 36, pp. 8098â€“8106, sep. 2021.
[7] s. cai, z. mao, z. wang, m. yin, and g. e. karniadakis,
â€œphysics-informed neural networks (pinns) for fluid mechanics:
a review,â€ acta mechanica sinica, vol. 37, no. 12, pp. 1727â€“1738,
dec. 2021.
[8] w. wu, m. daneker, m. a. jolley, k. t. turner, and l. lu, â€œeffective data sampling strategies and boundary condition constraints of
physics-informed neural networks for identifying material properties in
solid mechanics,â€ appl. math. mech., vol. 44, no. 7, pp. 1039â€“1068,
jul. 2023.
[9] p. ren, c. rao, s. chen, j.-x. wang, h. sun, and y. liu, â€œseismicnet: physics-informed neural networks for seismic wave modeling in
semi-infinite domain,â€ comput. phys. commun., vol. 295, feb. 2024,
art. no. 109010.
[10] t. alkhalifah, c. song, u. b. waheed, and q. hao, â€œwavefield solutions
from machine learned functions constrained by the helmholtz equation,â€
artif. intell. geosci., vol. 2, pp. 11â€“19, dec. 2021.
[11] g. pang, l. lu, and g. e. karniadakis, â€œfpinns: fractional physicsinformed neural networks,â€ siam j. sci. comput., vol. 41, no. 4,
pp. a2603â€“a2626, jan. 2019.
[12] x. meng, z. li, d. zhang, and g. e. karniadakis, â€œppinn:
parareal physics-informed neural network for time-dependent
pdes,â€ comput. methods appl. mech. eng., vol. 370, oct. 2020,
art. no. 113250.
[13] e. kharazmi, z. zhang, and g. e. karniadakis, â€œvariational physicsinformed neural networks for solving partial differential equations,â€
2019, arxiv:1912.00873.
[14] l. lu, r. pestourie, w. yao, z. wang, f. verdugo, and s. g. johnson,
â€œphysics-informed neural networks with hard constraints for inverse
design,â€ siam j. sci. comput., vol. 43, no. 6, pp. b1105â€“b1132,
jan. 2021.
[15] u. b. waheed, e. haghighat, t. alkhalifah, c. song, and q. hao,
â€œeikonal solution using physics-informed neural networks,â€ in
proc. eage annu. conf. exhib. online, 2020, vol. 2020, no. 1,
pp. 1â€“5.
[16] u. b. waheed, e. haghighat, t. alkhalifah, c. song, and q. hao,
â€œpinneik: eikonal solution using physics-informed neural networks,â€
comput. geosci., vol. 155, oct. 2021, art. no. 104833.
[17] c. song and t. a. alkhalifah, â€œwavefield reconstruction inversion via
physics-informed neural networks,â€ ieee trans. geosci. remote sens.,
vol. 60, 2022, art. no. 5908012.

[18] c. song, t. alkhalifah, and u. b. waheed, â€œsolving the frequencydomain acoustic vti wave equation using physics-informed neural networks,â€ geophys. j. int., vol. 225, no. 1, pp. 846â€“859,
dec. 2020.
[19] c. song, t. alkhalifah, and u. b. waheed, â€œa versatile framework to solve the helmholtz equation using physics-informed neural networks,â€ geophys. j. int., vol. 228, no. 3, pp. 1750â€“1762,
sep. 2021.
[20] x. huang and t. alkhalifah, â€œpinnup: robust neural network wavefield
solutions using frequency upscaling and neuron splitting,â€ j. geophys.
res., solid earth, vol. 127, no. 6, jun. 2022, art. no. e2021jb023703.
[21] x. huang and t. alkhalifah, â€œsingle reference frequency loss for
multifrequency wavefield representation using physics-informed neural networks,â€ ieee geosci. remote sens. lett., vol. 19, pp. 1â€“5,
2022.
[22] c. song and y. wang, â€œsimulating seismic multifrequency wavefields
with the fourier feature physics-informed neural network,â€ geophys.
j. int., vol. 232, no. 3, pp. 1503â€“1514, nov. 2022.
[23] y. wu, h. s. aghamiry, s. operto, and j. ma, â€œhelmholtz-equation
solution in nonsmooth media by a physics-informed neural network
incorporating quadratic terms and a perfectly matching layer condition,â€
geophysics, vol. 88, no. 4, pp. t185â€“t202, jul. 2023.
[24] t. alkhalifah and x. huang, â€œphysics-informed neural wavefields with
gabor basis functions,â€ 2023, arxiv:2310.10602.
[25] b. moseley, a. markham, and t. nissen-meyer, â€œsolving the wave
equation with physics-informed deep learning,â€ 2020, arxiv:2006.11894.
[26] b. moseley, a. markham, and t. nissen-meyer, â€œfinite basis physicsinformed neural networks (fbpinns): a scalable domain decomposition
approach for solving differential equations,â€ adv. comput. math.,
vol. 49, no. 4, p. 62, aug. 2023.
[27] m. rasht-behesht, c. huber, k. shukla, and g. e. karniadakis,
â€œphysics-informed neural networks (pinns) for wave propagation and
full waveform inversions,â€ j. geophys. res., solid earth, vol. 127, no. 5,
may 2022, art. no. e2021jb023120.
[28] s. alkhadhr and m. almekkawy, â€œwave equation modeling via physicsinformed neural networks: models of soft and hard constraints for
initial and boundary conditions,â€ sensors, vol. 23, no. 5, p. 2792,
mar. 2023.
[29] y. zhang, x. zhu, and j. gao, â€œseismic inversion based on acoustic
wave equations using physics-informed neural network,â€ ieee trans.
geosci. remote sens., vol. 61, 2023, art. no. 4500511.
[30] l. wang, h. wang, l. liang, j. li, z. zeng, and y. liu, â€œphysicsinformed neural networks for transcranial ultrasound wave propagation,â€
ultrasonics, vol. 132, jul. 2023, art. no. 107026.
[31] j. cheng wong, c. ooi, a. gupta, and y.-s. ong, â€œlearning in
sinusoidal spaces with physics-informed neural networks,â€ ieee trans.
artif. intell., vol. 5, no. 3, pp. 985â€“1000, jul. 2022.
[32] h. nosrati and m. emami niri, â€œmanipulating the loss calculation
to enhance the training process of physics-informed neural networks
to solve the 1d wave equation,â€ eng. comput., vol. 2023, pp. 1â€“29,
sep. 2023.
[33] j. zou, c. liu, c. song, and p. zhao, â€œnumerical solver-independent
seismic wave simulation using task-decomposed physics-informed neural networks,â€ ieee geosci. remote sens. lett., vol. 20, pp. 1â€“5, 2023.
[34] m. h. taufik and t. alkhalifah, â€œlatentpinns: generative physicsinformed neural networks via a latent representation learning,â€ 2023,
arxiv:2305.07671.
[35] s. sharma, s. sharma, and a. athaiya, â€œactivation functions in neural
networks,â€ towards data sci., vol. 4, no. 12, pp. 310â€“316, 2017.
[36] d. p. kingma and j. ba, â€œadam: a method for stochastic optimization,â€
2014, arxiv:1412.6980.
[37] c. song, t. zhao, u. b. waheed, c. liu, and t. you, â€œseismic traveltime
simulation for variable velocity models using physics-informed fourier
neural operator,â€ 2023, arxiv:2311.03751.
[38] b. moseley, t. nissen-meyer, and a. markham, â€œdeep learning for fast
simulation of seismic waves in complex media,â€ solid earth, vol. 11,
no. 4, pp. 1527â€“1549, aug. 2020.
[39] y. zhou, l. han, p. zhang, j. zeng, x. shang, and w. huang, â€œmicroseismic data-direct velocity modeling method based on a modified
attention u-net architecture,â€ appl. sci., vol. 13, no. 20, p. 11166,
oct. 2023.
[40] s. m. mousavi, g. c. beroza, t. mukerji, and m. rasht-behesht,
â€œapplications of deep neural networks in exploration seismology:
a technical survey,â€ geophysics, vol. 89, no. 1, pp. wa95â€“wa115,
jan. 2024.

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.

zou et al.: seismic wavefields modeling with variable horizontally layered

4507611

jingbo zou received the b.s. degree from the college of geo-exploration science and technology,
jilin university, changchun, china, in 2021, where
she is currently pursuing the ph.d. degree.
she is currently a visiting student with the
resource geophysics academy, imperial college
london, london, u.k. her research interests include
seismic modeling, full waveform inversion, and
machine learning applications in geophysics.

pengfei zhao received the b.s. degree in information and computing sciences, the m.s. degree in
operations research and cybernetics, and the ph.d.
degree in engineering mathematics from jilin university, changchun, china, in 2005, 2007, and 2010,
respectively.
from 2012 to 2013, he was a visiting scholar
at the department of scientific computing, florida
state university, tallahassee, fl, usa. he is currently an associate professor with jilin university.
his research interests include geostatistical inversion
and machine learning applications in geophysics.

cai liu received the b.s. degree in applied
geophysics and the m.s. and ph.d. degrees in
geo-exploration and information technology from
jilin university, changchun, china, in 1986, 1993,
and 1999, respectively.
from 1996 to 2000, he was an associate professor
at jilin university, where he became a professor,
in 2000, and a distinguished professor of jilin
province, china, in 2008. he was the director of
the college of geo-exploration science and technology, jilin university, from 2004 to 2017. he has
been with the ten thousand talent program since 2018. his research interests
include integrated research of geophysics and geology.

chao song received the b.s. degree in geophysics
from the college of geo-exploration science and
technology, jilin university, changchun, china,
in 2013, and the ph.d. degree from the seismic wave analysis group (swag), king abdullah
university of science and technology (kaust),
thuwal, saudi arabia, in 2020.
he worked as a research associate at the centre
for reservoir geophysics, imperial college london,
london, u.k., from 2021 to 2022. he is currently a
professor of geophysics with jilin university. his
research interests include full waveform inversion and machine learning
applications in geophysics.

authorized licensed use limited to: universidad eafit. downloaded on july 31,2024 at 19:49:51 utc from ieee xplore. restrictions apply.



deep learning seismic substructure detection using the frozen
gaussian approximation
james c. hateley, jay roberts, kyle mylonakis, xu yang

arxiv:1810.06610v4 [physics.geo-ph] 5 nov 2019

department of mathematics, university of california, santa barbara, ca 93106, usa

abstract
we propose a deep learning algorithm for seismic interface and pocket detection with neural
networks trained by synthetic high-frequency displacement data efficiently generated by the
frozen gaussian approximation (fga). in seismic imaging high-frequency data is advantageous since it can provide high resolution of substructures. however, generation of sufficient
synthetic high-frequency data sets for training neural networks is computationally challenging. this bottleneck is overcome by a highly scalable computational platform built upon the
fga, which comes from the semiclassical theory and approximates the wavefields by a sum
of fixed-width (frozen) gaussian wave packets.
training data for deep neural networks is generated from a forward simulation of the elastic wave equation using the fga. this data contains accurate traveltime information (from
the ray path) but not exact amplitude information (with asymptotic errors not shrinking to
zero even at extremely fine numerical resolution). using this data we build convolutional
neural network models using an open source api, geoseg, developed using keras and tensorflow. on a simple model, networks, despite only being trained on data generated by the
fga, can detect an interface with a high success rate from displacement data generated by
the spectral element method. benchmark tests are done for p-waves (acoustic) and p- and swaves (elastic) generated using the fga and a spectral element method. further, results with
a high accuracy are shown for more complicated geometries including a three-layered model,
a sine interface, and a 2d-pocket model where the neural networks are trained by both clean
and noisy data.
keywords: seismic tomography, convolutional neural network, elastic wave equation,
high-frequency wavefield, frozen gaussian approximation, image segmentation
1. introduction
various geophysical aspects, e.g., tectonics and geodynamics [1, 20, 19, 29], can be better understood by images of substructures (e.g. locations of seismic interfaces) of the earth
generated by seismic tomography. neural networks excel at recognizing shapes, patterns, and
âˆ—

corresponding author
email addresses: hateleyjc@gmail.com (james c. hateley), jayroberts@math.ucsb.edu (jay
roberts), kmylonakis@math.ucsb.edu ( kyle mylonakis), xuyang@math.ucsb.edu (xu yang)

preprint submitted to elsevier

november 6, 2019

sorting relevant from irrelevant data; this makes them good for image recognition and classification. in particular, convolutional neural networks allowed for rapid advances in image
classification and object detection [14], and in fact networks have been created for specific
tasks, such as, fault detection [2], earthquake detection, convnetquake [18], deepdetect [26]
and seismic phase arrival times, phasenet [30]. one obstacle in building a neural network to
detect seismic structures is having an ample data set for training. there is constant waveform
data being collected by seismic stations across the globe, and generating data by resampling
of this seismic data to train a network can be done, but is limited by the nyquist frequency.
seismic data can not be resampled with a nyquist frequency lower than the highest usable
frequency in the data, thus high frequency data is usually preferred as it tends to lead to improved resolution of the substructures. other difficulties of gathering an ample data lie within
the differences in geological locations, natural phenomenon (e.g. earthquakes) and unnatural phenomenon (e.g. fracking). using these data sets to train a general neural network is a
daunting task, and thus it is natural to use synthetic data for the training of neural networks.
the dominant frequency of a typical earthquake is around 5 hz [17] leading to demanding,
and at times, unaffordable computational cost. this makes generation of sufficient synthetic
high-frequency data sets for training neural networks computationally challenging to wellknown methods. we overcome this difficulty by building a highly scalable computational
platform upon the frozen gaussian approximation (fga) method for the elastic wave equation
[9], which comes from the semiclassical theory. the fga approximates the wavefields by a
sum of fixed-width (frozen) gaussian wave packets. the dynamics of each gaussian wave
packet follow ray paths with the prefactor amplitude equation derived from an asymptotic
expansion on the phase plane. the whole set of governing equations are decoupled for each
gaussian wave packet, and thus, in theory, each corresponding ode system can be solved on
its own process, making the algorithm embarrassingly parallel.
using synthetic data, araya-polo et al. perform inverse tomography via fully connected
neural networks with great success in [3] . their networks use low dimensional features
extracted from seismic data as input. using deeper convolutional neural networks trained
on seismogram data may allow the network to pick up on previously unknown signals. the
increase in input dimensionality necessitates more sophisticated deep learning techniques than
those presented in [3].
in this paper, we propose a deep learning algorithm for seismic interface detection, with
the neural networks trained by synthetic high-frequency seismograms. we first generate the
time series of synthetic seismogram data by the fga, which we use to train neural networks
made with an open source api, geoseg, developed using keras and tensorflow. despite only
being trained on fga generated data we observe the networks are able to detect a 1d interface with a high success rate on data generated by spectral element method. this method
more acucurately represents true seismic signals when fine time step and mesh sizes are used
in the computation. we conjecture that this robustness is due to the fact that although fga
does not carry exact amplitude information (with asymptotic errors proportional to the ratio
of wavelength over domain size), it contains accurate traveltime information. for this simple problem it is straight-forward in geophysics to identify the traveltime as a key factor in
interface location; however, this is not built into the network and so its use must be learned.
with the success of the 1d interface detection, we further apply the deep learning algorithms
for geometries with more complicated structures, including a three layered model and a 2d
2

pocket model, both of which show a high accuracy. we also investigate the effect of noise by
studying the performance of deep learning algorithms on noisy validation data, with the neural
networks trained using clean and noisy data, respectively.
the paper is outlined as follows: in section 2, we review briefly the mathematical background of fga and describe how the synthetic data is generated. in section 3, we describe
the details of the network design including network and block architectures. in section 4 we
show the performance of various networks on a series of geometries with different substructures, using both clean and noisy data. concluding remarks are made in section 5.
2. frozen gaussian approximation
we summarize the mathematical theory of fga in this section; for full exposition and
details for the elastic wave equation, see [9]; and for the acoustic wave equation, see [5].
the core idea of the fga is to approximate seismic wavefields by fixed-width gaussian
wave packets whose dynamics follow ray paths with the prefactor amplitude equation derived
from an asymptotic expansion on the phase plane. the ode system governing the dynamics
for each wave packet are decoupled. in theory, each ode system can be solved on its own
process, hence it is embarrassingly parallel. the implementation, as in previous works [9], is
with fortran using message passage interface (mpi). the implementation has a speed up factor
of approximately 1.94; hence, doubling the number of cores nearly halves the computational
time. the equation for the forward modeling to generate the training data set we use is the
elastic wave equation [7],
Ïâˆ‚t2 u = (Î» + Âµ)âˆ‡(âˆ‡ Â· u) + Âµâˆ†u + f,

(1)

where Ï, Î», Âµ, : r3 â†’ r is the material density, the first and second lameÌ parameters respectively and u : r Ã— r3 â†’ r3 is displacement. the differential operators are taken in terms of
the spacial variables. eq. (1) has a natural separation into divergence and curl free components
and can also be written as
âˆ‚t2 u = c2p âˆ‡(âˆ‡ Â· u) âˆ’ c2s âˆ‡ Ã— âˆ‡ Ã— u + fÏ .

(2)

this decomposition represents p-wave, and s-wave respectively with velocities
c2p (x) =

Î»(x) + 2Âµ(x)
,
Ï(x)

c2s (x) =

Âµ(x)
,
Ï(x)

(3)

with cp (x) representing the p-wave speed and cs (x) representing the s-wave speed.
2.1. the fga formulation
presented below is an outline for the fga. for derivation and benchmarking tests we
refer to [9, 5]. we introduce the fga formula for the elastic wave equation (2), with initial
conditions
(
u(0, x) = f k (x),
(4)
âˆ‚t u(0, x) = gk (x),

3

where the superscript k represents the wavenumber. for a sake of simplicity and clarity, we
shall also use the following notations:
âˆš
â€¢ i = âˆ’1 : the imaginary unit;
â€¢ subscripts/superscripts â€œpâ€ and â€œsâ€ indicate p- and s-waves, respectively;
â€¢ Â± indicates the two-way wave propagation directions correspondingly;
â€¢ nÌ‚ p,s (t): unit vectors indicating the polarized directions of p- and s-waves;
â€¢ nÌ‚p,s : the initial directions of p- and s-waves.
the fga approximates the wavefield uk (t, x) in eq. (1) by a summation of dynamic frozen
gaussian wave packets,
x

ukf (t, x) â‰ˆ

ap nÌ‚ p Ïˆpk ikp p Â·(xâˆ’q )âˆ’ k |xâˆ’q |2
p
p Î´qÎ´p
2
e
9/2
p (2Ï€/k)

(q,p)âˆˆgÂ±

x

+

as nÌ‚ s Ïˆsk ikp s Â·(xâˆ’qs )âˆ’ k |xâˆ’qs |2
2
e
Î´qÎ´p,
(2Ï€/k)9/2
s

(5)

(q,p)âˆˆgÂ±

with the weight functions
k
Ïˆp,s
(q, p) =

z

k
Î±p,s
(y, q, p) =

k

2

k
Î±p,s
(y, q, p)eâˆ’ikpÂ·(yâˆ’q)âˆ’ 2 |yâˆ’q| dy,

1


k
k
kf
(y)c
|p|
Â±
ig
(y)
Â· nÌ‚p,s .
p,s
2kcp,s |p|3

(6)
(7)

in eq. (5), gp,s
Â± refers to the initial sets of gaussian center q and propagation vector p for pand s-waves, respectively. in eq. (7), the â€œÂ±â€ on the right-hand-side of the equation indicate
k
correspond to (q, p) âˆˆ gp,s
that the Î±p,s
Â± . we refer [9] for the derivation, accuracy and explanation of fga, and only summarize the formulation as follows. the ray path is given by the
hamiltonian system with hamiltonian h(q, p ) = Â±cp,s (q)|p |. the â€œÂ±â€ give the two-way
wave propagation directions; e.g. for the â€œ+â€ wave propagation, (q, p) âˆˆ gp,s
+ , the gaussian
center qp,s (t, q, p) and propagation vector p p,s (t, q, p) follow the ray dynamics
ï£±
dqp,s
p p,s
ï£´
ï£²
= cp,s (qp,s )
,
dt
|p p,s |
ï£´
ï£³ dp p,s = âˆ’âˆ‚ c (q )|p |,
q p,s
p,s
p,s
dt

(8)

with initial conditions
qp,s (0, q, p) = q

and p p,s (0, q, p) = p.

4

(9)

wave reconstruction

offset

time propagation

transmitted

reflected
incident

initial decomposition

offset

figure 1: a cartoon illustration of fga algorithms: step 1, decompose the initial wavefield into a sum of
gaussian wave packets with corresponding weights given by (6); step 2, propagate gaussian wave packets
following (8), (10), (11) and (12), with the reflection-transmission conditions described in section 2.2; step 3,
reconstruct the wavefield by summing all gaussian wave packets using (5).

the prefactor amplitudes ap,s (t, q, p) satisfy the following equations, where s-waves have
been decomposed into sh- and sv-waves,



âˆ‚qp cp Â· p p 1
dap
âˆ’1 dzp
= ap Â±
+ tr zp
,
(10)
dt
|p p |
2
dt



âˆ‚qs cs Â· p s 1
dnÌ‚ sh
dasv
âˆ’1 dzs
= asv Â±
+ tr zs
âˆ’ ash
Â· nÌ‚ sv ,
(11)
dt
|p s |
2
dt
dt



âˆ‚qs cs Â· p s 1
dash
dnÌ‚ sh
âˆ’1 dzs
= ash Â±
+ tr zs
Â· nÌ‚ sv ,
(12)
+ asv
dt
|p s |
2
dt
dt
with the initial conditions ap,sv,sh = 23/2 , and nÌ‚ sv and nÌ‚ sh are the two unit directions perpendicular to p s , referring to the polarized directions of sv- and sh-waves, respectively. with
the short-hand notations,
âˆ‚z = âˆ‚q âˆ’ iâˆ‚p ,

zp,s = âˆ‚z (qp,s + ip p,s ).

(13)

we illustrate the algorithm by figure 1, and refer to the figures 5 and 6 in [9] for the
performance of efficiency of fga.
2.2. interface conditions
interface conditions are important as the direct and reflected waves from an interface are
picked up by the receiver, which records the time series of wavefield at certain location. this
gives travel time information; which in turn enables the depth of an interface to be computed.
for this exposition we only consider a flat interface, in general, we can use tangential-normal
5

z
gtrs
Ï†t

gtrp

Î¸t

layer 2 (câˆ¨
p,s )
z = z0

x

layer 1 (câˆ§
p,s )
Î¸i

Î¸r
Ï†r

gin
p

gre
p
gre
s

figure 2: cartoon illustration of an incident gaussian wave packet for p-wave hitting the interface at z = z0 ,
stands for
and then reflected and transmitted as gaussian wave packets for p- and sv-waves. here the gin,re,tr
p,s
the gaussian wave packet for the incident, reflected and transmitted p- and sv-waves, respectively. we denote
Î¸i , Î¸r , Î¸t to be the incident, reflection and transmission angles of p-waves, and Ï†r , Ï†t to be the reflection and
transmission angles of sv-waves, respectively.

coordinates. the derivation was detailed in appendix b in [9], with the idea of using the
continuity of level set functions corresponding to the hamiltonian dynamics (8). a cartoon
illustration on the behavior of gaussian wave packet is given in figure 2. for a flat interface
z = z0 , the wave speeds of the two layers near the interface are assumed to be,
(
(
câˆ¨s (x) z > z0
câˆ¨p (x) z > z0
.
(14)
,
c
(x)
=
cp (x) =
s
câˆ§s (x) z < z0
câˆ§p (x) z < z0
as a gaussian wave packet hits an interface, several of its quantities need to be defined. first,
ap,s and p p,s , are determined by snellâ€™s law and the zoeppritz equations [27]. if one denotes
Î¸i , Î¸r , Î¸t to be the p-wave incident, reflection and transmission angles, and Ï†r , Ï†t to be the
sv-wave reflection and transmission angles, respectively, then the zoeppritz equations read as
ï£« re ï£¶ ï£«
ï£¶
ap
cos(Î¸r )
ï£¬ are
ï£· ï£¬ sin(Î¸r ) ï£· in
s ï£·
ï£¬
ï£·
(15)
mï£¬
tr
ï£­ ap ï£¸ = ï£­ cos(2Ï†r ) ï£¸ ap ,
cos(2Î¸r )
atr
s
with the matrix m as
ï£«
câˆ§
p
cos(Î¸r )
sin(Ï†r )
câˆ§
s
ï£¬
âˆ§
cp
ï£¬ âˆ’ sin(Î¸ )
cos(Ï†r )
ï£¬
r
câˆ§
s
m =ï£¬
ï£¬ âˆ’ cos(2Ï†r )
âˆ’ sin(2Ï†r )
ï£­
câˆ§
sin(2Î¸r ) âˆ’( cpâˆ§ )2 cos(2Ï†r )
s

câˆ§
p
cos(Î¸t )
câˆ¨
p
âˆ§
cp
sin(Î¸t )
câˆ¨
p
Ï2
cos(2Ï†t )
Ï1
âˆ¨ 2
Ï2 (câˆ§
p cs )
âˆ§ 2 sin(2Î¸t )
Ï1 (câˆ¨
p cs )

6

câˆ§

âˆ’ cpâˆ¨ sin(Ï†t )
s

ï£¶

câˆ§
p
cos(Ï†t ) ï£·
ï£·
câˆ¨
s
ï£·,
Ï2
âˆ’ Ï1 sin(2Ï†t ) ï£·
ï£¸
2
Ï2 (câˆ§
p)
cos(2Ï†
)
t
Ï1 (câˆ§ )2
s

ï£·

(16)

where Ï1,2 are the densities for the layers 1 and 2, respectively. let n denote the normal to
the interface at the point of incidence then qin,re,tr is the gaussian center at the point of incidence, and pin,re,tr corresponds to the propagation vector of incident, reflected and transmitted
gaussian wave packet for either p- or s-waves. qin = qre = qtr and pre,tr is updated as
follows

q
in
tr,re
in |ntr,re âˆ’ |pin | âˆ’ (pin Â· n)2 âˆ’ (p Â· n) n,
|p
(17)
ptr,re
=
p
+
sgn(p
)
p,s
p,s
p,s

tr
âˆ¨ âˆ§
where ntr,re
p,s denotes the index of refraction for the new respective direction, e.g. np = cp /cp .
also zp,s needs to be updated, requiring use of conservation of level set functions defined in
the eulerian frozen gaussian approximation formula [15, 25].

âˆ‚z qre,tr = âˆ‚z qin f,

|pre,tr |
re,tr
re,tr
in
in
âˆ‚
q
Â·
âˆ‡c(q
)
âˆ’
âˆ‚
q
Â·
âˆ‡c(q
)
n,
z
z
c(qre,tr )pre,tr Â· n
(18)
t
âˆ’1
f and w are two 3 Ã— 3 matrices, f = w , and
ï£¹
ï£®

2
1
0
0
c(qre,tr )
ï£º
ï£¯
0
1
0
.
f =ï£°
ï£» , with Îº =
re,tr
c(qin )
(Îº âˆ’ 1) ppinx (Îº âˆ’ 1) ppiny Îº ppz in
âˆ‚z pre,tr = âˆ‚z pin w âˆ’

z

z

z

2.3. advantage of fga for generating training data
the data points used for our experiments are generated from the forward simulation of the
elastic wave equation using the fga. we record the displacement data from the wavefield at
various points near the surface; these points represent the receiver locations. given an initial
condition, as in eq. (4), the initial wave packet decomposition can be saved for generating
a data set for training. that is, the same data can be loaded as the parameters which vary
from data point to data point; e.g. interface height, pocket location, pocket size, etc. furthermore, if the initial condition is independent of the wave velocities, the same wave packet
decomposition can be used to generate data from simulation with varying velocities.
for a single forward simulation; after the initial wave packet decomposition generated and
saved, loading the initial wave packet decomposition, running an ode solver, and recording
the displacement are the only tasks required to generate a data point. for generation of a data
set, the simulation can be restarted at t = 0 with another set of parameters. as the initial wave
packet decomposition is already loaded in memory, all that is required to generate the rest of
the data set is running an ode solver, and recording the displacement. the ode system for
the fga is uncoupled for each wave packet, the speed of a single simulation greatly benefits
from a parallel implementation.
3. network design
the goal of full waveform inversion (fwi) is to extract wave speed data from seismic
data. in its purest form, this is a regression type problem and was addressed with fully con7

nected networks in [3]. our work approaches the problem from a segmentation perspective.
we address a simplified version of fwi and attempt to detect subsurface structures by classifying them as regions of low or high wavespeed, thus transforming the regression problem
into a segmentation problem. these sorts of segmentation problems have been addressed with
great success by cnns [22]. semantic segmentation of images is the process of labeling each
pixel in an image with a class label for which it belongs. in semantic segmentation problems
the correct pixel label map is referred to as the ground truth. in our work the â€œimage is the
n-dimensional slice in the depth direction which is partitioned into n bins. the i, j th â€œpixelâ€
is the signal value from receiver i at depth bin j.
each bin is then labeled depending on whether it came from a region of high or low velocity. these velocity regions are our classes. our work diverges substantially from traditional
semantic segmentation of images, as our input is time series data which must be transformed
by the network. this is opposed to the traditional case where the input itself is labeled. the
goal of our network is to infer the presence of high and low wavespeed regions and the interfaces between them from seismogram data. the input to the network is x âˆˆ rm Ã—dÃ—r , where
m is the number of timesteps, d is the spatial dimension of media, and r is the the number of
receiver. the output of the network is
n (x) = (pki1 ...in ) âˆˆ rm1 Ã—Â·Â·Â·Ã—mn Ã—n ,

ij âˆˆ {1, ..., mj }
,
k âˆˆ {1, . . . , n }

(19)

where pki1 Â·Â·Â·in is the probability that bin i1 Â· Â· Â· in belongs to the k th class. in this paper d = 3,
n = 1, 2, and n = 1, 2, 3.
for example, possible output and groundtruth could be
ï£®
ï£¹
ï£® ï£¹
0.1 0.9
1
ï£° 0.2 0.8 ï£» , ï£°1ï£» .
0.55 0.45
1
here, at depth indexed by 1, the network believes with 10% probability that this bin is a low
speed region and with 90% probability that it is a high speed region, and similarly for the other
rows. the accuracy of a given inference is found by taking the argmax along the last axis of
the output tensor and comparing against the groundtruth. taking a max along the last axis
recovers the probability, interpreted as a confidence of the prediction. the above example has
66.67% accuracy, and the confidence is [0.9, 0.8, 0.55]t .
in [3], araya-polo et al. perform inverse tomography via deep learning and achieve
impressive results. our model is fundamentally different than geodnn in that: geodnn
is a fully connected network whereas geosegâ€™s is fully convolutional, and geodnn uses
semblance panels from cmp data as features for the network and geoseg uses the raw seismograph data. moreover, araya-polo et al. address the fwi problem and provide the wave
speeds in a two dimensional region and we tackle high and low velocity detection, shifting the
problem from regression to segmentation.
1
the networks were built using an open source api, geoseg , developed using keras and
1

https://github.com/kylemylonakis/geoseg

8

tensorflow. geoseg supports unet, fully convolutional segmentation network, or feed forward cnns as a base meta-architecture, using any of residual, dense, or convolutional blocks,
with or without batch normalization [21, 22, 11, 12, 13]. geoseg also allows for easy hyperparameter selection for network and block architectures, and for training optimizers and parameters. the optimizers used were nadam with default parameters [6], sometimes followed
by minibatch stochastic gradient descent (sgd), or sgd alone. the network structures are
described by their meta-architecture and their blocks. the meta-architecture describes the
global topology of the network and how the blocks interact with each-other. each block either
begins or ends with a decoding or encoding transition layer respectively. encoding transition
layers downsample their inputs with a strided convolution. decoding transition layers upsample thier inputs with a strided deconvolution. tranistion layers will not have dropout.
meta-architectures. while geoseg supports many kinds of feed-forward cnnâ€™s and
encoder-decoder networks with different choices of blocks, unet architectures with dense
blocks performed the best and will be the only type of network reported.
geodude-l refers to a unet architecture from [21]. these architectures have proven
highly efficient at image segmentation for road detection [28] and in biomedical applications [21]. these networks feed their input into a transfer branch, then an encoder branch
of length l, bridge block, and then a decoder branch of length l. the last layer is a convolutional layer followed by a softmax which outputs predictions as described above. the defining
feature of these networks are the â€œrungsâ€ connecting the encoder and decoder branches (see
figure 3). in this way, the network can incorporate both low and high resolution data [21, 28].
for the one dimensional problems the transfer branch is not necessary and can be omitted.
convolutional layers. the layer is broken first into a bottleneck convolution followed by
the main convolution. the bottleneck is a convolution which uses a 1x1 kernel to expand the
number of feature channels before performing the full convolution. it is suggested in [10, 23]
that such a bottleneck can reduce the number of necessary feature maps and so improve computational efficiency. we use rectified linear units (relus) [8] for our activation and size
3x3 (3x1 for 1d interface problems) filter kernels for our convolutions. as in [12], we use
batch-normalization [13] to help smooth training. the setup is shown in figure 4
dense blocks. though geoseg supports multiple block types, all the networks reported
in this paper use dense blocks. these are stacks of convolutional layers as shown in figure 4.
the defining features of these blocks, introduced in [12] is that every layer receives input
from all previous layers in the block via concatenation. such architectures have been shown
to greatly improve results in image classification while reducing computational burden [12].
transfer branch. all of our meta-architectures preserve resolution of their input and so
our detection resolution is limited by input resolution. this is not a problem in the temporal
axis, which translates to the z axis in output, since we have a large number of time samples;
however, the x-axis resolution is limited by the number of receivers we have for our input. to
increase the resolution in this direction, we place a small l-layer cnn before the main network
which upsamples the receiver axis, via strided deconvolutions, by a factor of 2l .

9

figure 3: meta-architecture of a two-layered unet, geodude-2, with transfer branch used in deep learning
algorithms. for 2d problems the input is upsampled along the receiver axis by deconvolutions in the transfer
branch. unetâ€™s have â€œrungsâ€ that connects the encoder and decoder branches. in this way, the network can
incorporate both low and high resolution data.

(a) convolutional layer

(b) dense block

figure 4: the type of blocks used in geoseg for this paper: (a) block compositions of a basic convolutional
layer using a bottleneck convolution to expand the filter channels before the full convolution; (b) a corresponding
dense block. each layer of the block recieves input from all previous layers allowing information to flow through
the whole block.

10

4. numerical experiments
here we present the performance of deep learning algorithms for the three detection experiments: 1d interface problme, three-layered media model, and a 2d single cylindrical pocket
model. the architecture used for all experiments is a unet with dense blocks (geodude).
each dense block will be made of four constituent bottle-necked convolutional layers with a
bottle neck factor of 4. for all 1d networks the dense blocksâ€™ convolutions use a kernel size
of 3 Ã— 1 in the base of the block and 2 Ã— 1 at each transition layer, while for the 2d networks a
3 Ã— 3 kernel size is used in the base block with a 2 Ã— 2 kernel size in the transition layer. the
meta-architectures had 16 filter channels except for the 1d interface model with p-wave data
which only used 4.
our primary evaluation metric is accuracy which is the number of correctly predicted
pixels over total pixels, i.e.,
accuracy =

number of correct pixels
,
total number of pixels

where we set the ground truth as follows for measuring accuracy; if any part of a pixel contains
a low velocity region, that pixel is counted as part of the low velocity region.
for the 2d pocket model, we will also consider the intersection over union metric which
better captures segmentation performance.
in the 2d pocket model, a two-layer transfer branch was used. each layer was a convolution, two-strided in the receiver direction with a kernel size of 3 Ã— 3 with 4 filter channels.
during training, these layers had a drop out probability of 0.2.
the wavespeeds cp and cs are given by (3), which will be specified as piecewise linear
functions (or constants) detailed in each numerical example. the initial p-wave data is generated with source function

fjk (x) = cos(k(xj âˆ’ x0,j )) exp âˆ’ 2k|x âˆ’ x0 |2 ,
(20)
and the p,s-wave initial data is generated from the greenâ€™s function
fjk (x) =

3
x
(xi âˆ’ xi,0 )(xj âˆ’ xj,0 )
i=1

4Ï€Ïc2p r3

fj (t0 âˆ’ r/cp )+

r2 Î´ij âˆ’ (xi âˆ’ xi,0 )(xj âˆ’ xj,0 )
fj (t0 âˆ’ r/cs )+
4Ï€Ïc2s r3
z
3(xi âˆ’ xi,0 )(xj âˆ’ xj,0 ) âˆ’ r2 Î´ij r/cs
sfj (t0 âˆ’ s) ds,
4Ï€Ïr5
r/cp

(21)

p
where fj (t) = cos (kt) exp(âˆ’2kt2 ), Î´ij is the kronecker delta, and t0 = 2 1/k,
x = (x1 , x2 , x3 ), x0 = (x0,1 , x0,2 , x0,3 ) is the location of the source, r = kx âˆ’ x0 k and Ï is
the density.
the data is generated on the cluster, pod, at the center for scientific computing at uc santa

11

2

barbara using 64 processes with a 4th order runge-kutta solver for the ode system. as the
initial condition is independent of the wavespeed, only one wave packet decomposition needs
to be computed and saved for all data points to be generated. this saves a tremendous amount
of time as only the ode system needs to be solved for various wavespeeds and interface
heights. for example to generate the p-wave data, when 804672 total beams are used, each
data point is generated in approximately 2.5 minutes. this is compared to specfem3d
which takes is approximately 45 minutes to generate a data point.
all of the networks were trained on the google cloud platform, or on the cluster pod at the
center for scientific computing at uc santa barbara with keras 2.2.2 and tensorflow 1.10.0
as a backend using a single nvidia tesla v100 gpu.
4.1. 1d interface
to provide a proof of concept we first experimented with a two-layered flat interface
model. we also use this case to investigate whether our network is simply inverting the fga
by comparing performance of a network trained on fga but evaluated on data generated by
sem.
4.1.1. p-wave data
dataset. the p-wave data set is generated with a computation domain of [0, 2] kmÃ—[0, 2]
kmÃ—[0, 2.5] km with a source centered at x0 = (0.5, 0.5, 0.5) km and k = 128 in (20),
which corresponds to approximately 20.37 hz. the stations are located on the surface at
s1 : (1.5, 1.5, 0) km, s2 : (1.8, 1.5, 0) km, s3 : (1.6, 1.9, 0) km. the interface is a plane,
z = z0 that varies from depth 1 km to 2.5 km. above the interface the wavespeed cp varies
linearly from .78 km/s to 1.22 km/s, below the interface the wavespeed cp varies linearly from
1.29 km/s to 1.56 km/s. see figure 5.
each data point is a (6000, 3, 3) tensor. prior to training, we further down sample the temporal dimension by a factor of 25 and normalize the amplitude of the seismogram data. there
were a total of 7790 examples. the mini-batch size during training was 256 examples.
network details. as described above our architecture was a 1d geodude-3 where each
convolutional layer in the dense block had 4 feature channels. the during training the dropout
probability was set to 0.5 and a nadam optimizer was used with default parameters.
results. network evaluations were performed with data generated by the fga and specfem.
notably, the networks are never trained on any specfem data. this was to investigate
whether the network was sensitive to the asymptotic error produced by the fga.
after 3500 epochs of training geodude-3 achieved a 96.97% evaluation accuracy on data
generated by the fga. when evaluated on data generated by specfem dataset geodude-3
achieved a 94.29% evaluation accuracy, only a 2.68% decrease. we remark in [16, 24], it
was shown even small perturbations in input can affect network classification results. this
suggests that the asymptotic errors present in the fga do not greatly affect the segmentation
problem. visualizations of the output for geodude-3 are shown in figure 6. figure 7 shows
the heatmap. recall this displays the confidence the network places on the pixels prediction.
2

http://csc.cnsi.ucsb.edu/clusters/pod

12

(a) computational domain

(b) seismogram

figure 5: the locations of source and receivers, and the generated synthetic p-wave seismograms for the 1d
interface problem. we take k = 128 for generating the synthetic data. (a) the source is located at (.5,.5,.5) km
as a star and the 3 receivers are located on the surface. the interface presented is at a depth of 2 km. (b) a
visualization of typical data point, which is a collection of 3 seismograms from the forward simulation using the
fga.

(a) fga: actual

(b) fga: predicted

(c) fga: difference

(d) sem: actual

(e) sem: predicted

(f) sem: difference

figure 6: 1d interface predicted by geodude-3 using p-wave data. each column of pixels represents a data
point. the value of each pixel describes whether the material at the depth corresponding to that pixelâ€™s column
belongs to either the high or low velocity region. the blue pixels represent the low velocity region, while the
yellow represent the high velocity region. subfigures (c), (f) show the difference between the predicted and
actual velocity profile, where the accuracy is measured by the wrong-labeled pixels (blue) over the total number
of pixels in the figures (c), (f). in fact, after 3500 epochs of training geodude-3 achieved a 96.97% evaluation
accuracy on data generated by the fga.

13

(a) fga

(b) sem

figure 7: p-wave confidence distribution comparison produced by geodude-3 for 1d interface problem. regions of low confidence correspond to areas where an interface is likely. the color bar is a probability spectrum
from 0.5 to 1. in general, the closer the network gets to the interface the less confident its prediction becomes.

4.1.2. p,s-wave data set
dataset. the p,s-wave dataset is generated with a computation domain of [0, 2] kmÃ—[0, 2]
kmÃ—[0, 3] km with a source centered at x0 = (0.5, 0.5, 0.5) km, and wavenumber k = 32, or
approximately 5.09 hz. the stations lie in a plane and are located just below the surface at
s1 : (1.1, 0.5, 0.1) km, s2 : (1.4, 0.5, .1) km, s3 : (1.8, 0.5, 0.1) km. the interface is a plane,
z = z0 that varies from depth 1 km to 2 km. above the interface cp varies linearly from 0.75
km/s to 1.10 km/s, below the interface cp varies linearly from 1.12 km/s to 1.48 km/s and we
fix cs = cp /1.7 (corresponding the case Î» â‰ˆ Âµ). see figure 8. there are a total of 6,400
data points in the p,s-wave dataset. each data point is a (2048,3,3) tensor. prior to training
each example is down-sampled along the temporal axis by a factor of 8. each network used a
mini-batch training size of 256. similarly to the p-wave dataset, 100 additional samples were
generated using specfem3d for evaluation after training.
network details. geodude-2 and geodude-3 with default parameters were used.
both networks were trained using a nadam optimizer with a dropout probability of 0.5.
results. both networks were trained for 3500 epochs. the most successful network
was geodude-2, with 98.26 % evaluation accuracy on fga data, and 97.55 % evaluation
accuracy on the specfem data . we find that the evaluation accuracy goes down for deeper
networks. in particular, geodude-3 performed worse with only a 92.34 % evaluation accuracy, especially compared to the same network architecture on the p-wave dataset. this is
likely due to overfitting of the data causing an increase in generalization error. similarly to the
p-wave dataset, evaluation accuracies on specfem3d data are only marginally worse than
their fga counterparts, with a max difference of 1.17% between the datasets. see table 1 for
the summary of the results and figures 9, 10 and 11.

14

(a) computational domain

(b) seismogram

figure 8: the locations of source and receivers, and the generated synthetic p- and s-wave seismograms for the
1d interface problem. we take k = 32 for generating the synthetic data. (a) the source is located at (.5,.5,.5)
as a star and the 3 receivers are located on the surface. the interface presented is at a depth of 2 km. (b) a
visualization of typical data point, which is a collection of 3 seismograms from the forward simulation using the
fga.

table 1: p,s-data network comparisons for 1d interface problem. here eval. acc. = evaluation accuracy,
train. acc. = training accuracy, and sem acc. = evaluation accuracy tested by sem synthetic data.

network eval. acc. train. acc. sem acc.
geodude-2
98.26 %
99.97 %
97.55 %
geodude-3
97.64 %
99.90 %
96.47 %

(a) evaluation accuracy

(b) training accuracy

figure 9: ps-wave training results for 1d interface problem, with synthetic data generated for k = 32 in (20):
the evaluation data set for this figure only contains data generated by the fga.

15

(a) fga: actual

(b) fga: predicted

(c) fga: difference

(d) sem: actual

(e) sem: predicted

(f) sem: difference

figure 10: 1d interface predicted by geodude-2 using p,s-wave data. each column of pixels represents a
sample. the value of each pixel describes whether the material at the depth corresponding to that pixelâ€™s column
belongs to either the high or low velocity region. the blue pixels represent the low velocity region, while the
yellow represent the high velocity region. subfigures (c), (f) show the difference between the predicted and
actual velocity profile, where the accuracy is measured by the wrong-labeled pixels (blue) over the total number
of pixels in the figures (c), (f). we give the statistical accuracy in table 1, which shows an accuracy of over 96%.

(a) geodude-2:fga

(b) geodude-2:sem

figure 11: p,s-wave heat-map distribution comparison produced by geodude-2 for 1d interface problem.
regions of low confidence correspond to areas where an interface is likely. the color bar is a probability spectrum
from 0.5 to 1. in general, the closer the network gets to the interface the less confident its prediction becomes.

16

(a) computational domain

(b) seismograph

figure 12: the locations of source and receivers, and the generated synthetic p- and s-wave seismograms for
the three-layered media model. we take k = 32 for generating the synthetic data. (a) the source is located at
(.5, 1, .5) km as a star, the 32 receivers are located on the surface on the plane y = 1 km, and the interfaces
presented are at a depth of 1.5 km and 2 km. (b) a visualization of typical data point, which is a collection of 32
seismograms from the forward simulation using the fga.

4.2. three-layered media
dataset. a natural extension of the model is to include one or more low velocity regions
in the computational domain. for this experiment we consider a three-layered media with a
low velocity region in the middle, the velocities in each region will be fixed.the p-wave speed
is cp = 1.3, 0.9, 1.7 km/s for the top, middle, and bottom layers, receptively. the s-wave
speed is set to cs = cp /1.7 for each layer. the lower interface will be in a rage of 1.8 km and
2.8 km by an increment of 1 m. similarly the upper interface will vary from .2 km to 1.2 km
by an increment of 1m. see figure 12. there were 10201 samples with a batch size of 64.
network details. geodude-3 was used. during training the dropout probability was
0.12. training was performed with stochastic gradient descent with a learning rate of 0.001.
results. the network achieved a training accuracy of 99.51% and an evaluation accuracy
of 95.51% after 3000 epochs. see figures 13 and 14.
4.3. sine interface model
dataset. for this experiment we consider a more complicated interface, which is given
by the level set function; f (x, z) = z âˆ’ 0.1 sin(Ï€f x). the level set values f (x, z) = d vary
between 0.3 km and 2.3 km by an increment of .001 km. and the phase factor f will ranges
from 1 to 2 by an increment of .001. the nondimensionalized is set to wavenumber k = 64
and cp = 1.1, 1.5 for the top and bottom layers will all be fixed. as before, cs will be a fixed
multiple of cp by 1.7. we record the displacement for 10 s; see figure 15 for source, receiver
details.
network details. geodude-4 was used with a dropout probability of 0.2. training
was performed with 1500 epochs using the nadam optimizer followed by 1000 epochs of
17

(a) actual

(b) predicted

(c) difference

figure 13: predictions for three-layered media by geodude-3: each column of pixels represents a sample. the
value of each pixel describes whether the material at the depth corresponding to that pixelâ€™s column belongs to
either the high or low velocity region. the color bar is a probability spectrum from 0.5 to 1. in general, the closer
the network gets to the interface the less confident its prediction becomes. there is a slight loss of confidence for
the network detecting the lower interface.

figure 14: confidence map for three-layered media model produced by geodude-3. regions of low confidence
correspond to areas where an interface is likely. the color bar is a probability spectrum from 0.5 to 1. in general,
the closer the network gets to the interface the less confident its prediction becomes.

18

(a) computational domain

(b) displacement color channel

figure 15: the locations of source and receivers, and the generated synthetic p- and s-wave seismograms for the
sine interface model. we take k = 64 for generating the synthetic data. (a) the source is located at (0.4, 1, 2.7)
km as a star, the 32 receivers are located on the surface on the plane y = 1 km, and the interfaces presented are
at a depth of 1.5 km. (b) visualization of network input as image for sine interface model. each color channel
(inverse rgb) represents a coordinate of the displacement.

stochastic gradient descent with a learning rate of 0.001. there were 10050 date points generated. 9150 data points are used for training with a mini-batch size of 25; 900 data points were
used for evaluation.
results. the network achieved a training accuracy of 99.92% and an evaluation accuracy
of 99.28% after 2500 epochs. see figure 16 for evaluation of a typical data point.
4.4. 2d low velocity pocket
dataset: we now investigate whether the network can learn more complex 2d geometries.
the considered models each will be a three-layered problem with a low velocity cylindrical
region in the middle layer. the source will be located at (.5, 1, 1.5) km. the interfaces located
at 1 km and 2.5 km will be fixed. a cylinder with center (x, z) and radius r will be randomly

(a) actual

(b) predicted

(c) confidence map

figure 16: typical results from training phase factor f = 1.83, level set value d = .8898. (a), (b) ground
truth and prediction for sine interface geodude-4. (c) confidence map for sine interface model. regions of low
confidence correspond to areas where an interface is likely.

19

(a) computational domain

(b) displacement color channel

figure 17: the locations of source and receivers, and generated synthetic p- and s-wave seismograms for the
2d pocket model. we take k = 32 for generating the synthetic data. (a) the source is located at (.5, 1, 1.5) km
as a star and the receivers are located on the surface on the plane y = 1 km. the interfaces are fixed at a depth
of 1 km and 2.5 km. visualization of network input as image for 2d pocket model. each color channel (inverse
rgb) represents a coordinate of the displacement.

generated x âˆˆ [0.85, 1.65] km, z âˆˆ [1.35, 2.15] km, and r âˆˆ [.05, .3] km with samples taken
from a uniform distribution. see figure 17. 11350 data points are generated with 1000 being
saved for evaluation. the p-wave speeds will be fixed and are cp = 1.1, 1.3, 1.7 km/s, for
the top, middle and bottom layers respectively. the s-wave speed, cs will be a fixed multiple
of cp by 1.7 for each layer. inside the pocket the p-wave speed is set to cp = 0.5 km/s and
the s-wave speed is set to zero, cs = 0. only p -waves will propagate through the cylinder;
however, s-wave can transmit to p-wave going in the pocket and p-wave can transmit to p,swaves coming out of the pocket. unlike previous models the goal is to identify a low velocity
region in a three layered media in a 2d slice of the computational domain. a batch size of 20
examples was used.
network design. a geodude-4 network was used with a two layer transfer branch
before its input. the dropout probability was 0.2.
results. the network achieved a training accuracy of 99.95% and an evaluation accuracy
of 99.73% after 1428 epochs. in figure 19 we see the networks are indeed learning geometry.
this is particularly interesting given that the network only â€seesâ€ images like figure 18(b).
these results suggest the network is transforming the data in some way which we hope to
explore in future work.
4.5. effect of noisy data
we now consider the 2d pocket example with additive white noise. normally, noise is
added to the training data set to increase the size of the set and lead to a more robust network. we take an evaluation set of 1000 data points and add i.i.d. (independent identically
distributed) gaussian noise to each time step of the displacement field data. for an individual

20

(a) x-coordinate

(b) y-coordinate

(c) z-coordinate

figure 18: visualization of network input using normalized displacement data for 2d pocket model.

(a) actual

(b) predicted

(c) confidence map

figure 19: 2d pocket results predicted by geodude-4, with a typical data point chosen for visualization. the
pocket is recovered with the networks confidence wavering on the boundary of the pocket.

21

(a) no noise

(b) noise strength of 1% of maximum recorded data

figure 20: comparison of seismograms with noise and no noise for the 2d pocket model. (a) seismogram with
no noise. (b) additive gaussian white noise at 1% of max |u|. this shows that 1% of the maximum recorded
displacement is enough to mask the reflected data from the pocket.

data point, the noise strength can be calculated by
wi =

Ïƒ
,
r max |ur |

(22)

where r is the reflection coefficient and max |ur | is the maximum displacement from the reflected wave. the noise strength will be given by w , which is the approximate average value
of wi across the data set. the standard deviation Ïƒ is chosen so that w can be interrupted as
a percentage of the reflected wave displacement, e.g., w = 20 gives of the a noise strength of
20% of the average max displacement of the reflected wave. we notice that with noise generated with a strength of 1% of the maximum of direct recorded displacement, the reflected
data from the pocket is the same order of magnitude of the noise, effectively masking it. see
figure 20.
network design: to compare results, we use the same model as in the previous section 4.4 and train a network with the same parameters, with a noise strength of w = 20.
results. a geodude-4 was trained for 2000 epochs with additional noise for a final
evaluation accuracy of 99.731% evaluation accuracy. however, evaluation accuracy can be
a misleading metric for network performance in pocket detection since assigning the high
velocity class to every pixel could get an accuracy up to 80% on some samples. instead
intersection over union (iou) is used (see [4] for a more detailed explanation). figures 21
and 22 show the histograms the iou scores of networks trained with and without white noise
evaluated on the evaluation data with no additional noise, additional noise strength w =
10, and additional noise strength w = 50 respectively. while both networks display good
iou scores on the unperturbed data and when the data is only perturbed with noise strength
w = 10, the benefits of additional noise in training become clear when the noise strength
is increased to w = 50: the iou scores of the network trained without noise on noisy data
22

unperturbed perturbed by w = 10 perturbed by w = 50
trained without noise
0.8163
0.7335
0.1308
trained with noise
0.8706
0.7576
0.5249
table 2: iou scores for geodude-4 trained with and without noise for the 2d pocket model.

(a) ground truth

(b) iou=0.1403

(c) iou=0.2052

figure 21: visualization of ious by geodude-4 for the 2d pocket model. results taken from network trained
with noise. data is augmented with noise with a noise strength of 50%. (a) ground truth for comparision. (b)
iou=0.1403. (c) iou= 0.2052. for each displayed results, the networks are able to detect the location of the
pocket. with additional noise the network is unable to resolve the geometry.

plummets, effectively misclassifying almost every pocket, while the iou score of the network
trained with noise decreases, but maintains many correct classifications. the average iou
scores are summarized in table 2. evaluating on higher noise strength collapses the networkâ€™s
output to no pocket detected.
4.6. structured noise
we now consider the 2d pocket example with additive structured noise. from our noiseless
evaluation data set, we add structured low frequency noise to each receiver. numbering each

(a) no noise

(b) w=10

(c) w=50

figure 22: network with trained without noise, 1000 data points are plotted in each histogram. subfigures (a),
(b), (c) show the iou metric with no noise, 10% noise strength, and 50% noise strength respectively.

23

(a) no noise

(b) w=10

(c) w=50

figure 23: performance of the network geodude-4 trained with noise strength at 20% of the average max
displacement of the reflected wave for the 2d pocket model. 1000 data points are plotted in each histogram.
subfigures (a), (b), (c) show the iou metric, with no noise, 20% noise strength, and 50% noise strength, respectively.

(a) actual

(b) predicted, iou=0.7585

(c) confidence map

figure 24: a visualization of a result from the structured noise (23) added to the noiseless evaluation dataset.
the network used is geodude-4 trained with noise from section 4.5. (a) ground truth. (b) prediction, with an
iou value of 0.7585. (c) confidence map.

receiver, sj = (j2/31, 1, 0), for j = 0, . . . , 31 we add noise to receiver sj as
a
4
cos(2t)
,
2
3(32 âˆ’ j)

(23)

where the amplitude a is modulus of the maximum displacement of the wavefield reflected
from the interface. we add this noise to each component of the wavefield. we remark that at
receiver at x = 2, the noise is the strongest at with an amplitude of 2a/3. that is, the noise
strength is two thirds of the height of the modulus of the maximum displacement wavefield
reflected from the interface. this is a stronger noise than the w = 50 case of additive white
noise. the structured noise decreases to an amplitude of a/48 at the receiver located at x = 0
the network is able to detect the pocket with good success. the network has full confidence, as can be seen from fig. 24c; however, the boundary for the prediction is perturbed.
this gives a slight false result which is reflected in the iou metric, see fig. 25. with the
structured noise, the prediction has a lower average iou value of .7443 compared to 0.8706
with no noise.
24

figure 25: the iou metric for structured noise as the input into geodude-4 for the 2d pocket model trained
with noise from section 4.5. the average iou is .7443.

5. conclusions and future work
the use of the fga to generate large amounts of seismic data provides a quick way to generate labeled synthetic data for statistical learning of the inverse tomography problem. casting
the inverse problem as a segmentation problem resulted in high evaluation accuracy networks
for piecewise constant two-layer models on both fga and sem datasets. the unet architectures with dense blocks displayed superior accuracy compared to simpler network architectures, however, deeper networks did not necessarily outperform their shorter counterparts. on
the two layer benchmark problem the networks exhibited good invariance of prediction in regard to which numerical method was used to generate the dataset, likely because the fga and
sem exhibit the same traveltime information. having a network independent of numerical
method is important, and the fga can help to train such a network as it generates synthetic
seismic data that carries the correct traveltime information of the real-world data. further,
analogous meta-architectures also exhibit high evaluation and iou accuracy for pocket detection in noisy data.
the success of the networks on the substructure geometries in the paper act as a stepping
stone to tackle more complicated and realistic geological models. by developing the api
geoseg, available at https://github.com/kylemylonakis/geoseg, it is easy to implement neural networks designed for the reported example models and more general segmentation problems of seismogram data than those discussed in this paper. together with the fga, the task
of training a deep neural network on sufficiently large amounts of seismogram data becomes a
computationally affordable task. immediate future directions to be explored are multi-pocket
models, multi-nonlinear interface models with and without pockets present. long term goal
is to develop a neural network model to tackle fully 3d substructure geometries and develop
a neural network trained on synthetic seismic data capable of making inferences from real
seismic data.

25

acknowledgement
we acknowledge support from the center for scientific computing from the cnsi, mrl:
an nsf mrsec (dmr-1720256) and nsf cns-1725797. the work was partially supported
by the nsf grant dms-1818592. xy also thanks professors haizhao yang and kui ren for
useful discussions.
references
[1] k. aki and w. lee. determination of the three-dimensional velocity anomalies under a
seismic array using first p arrival times from local earthquakes 1. a homogeneous intial
model. j. geophys. res., 81:4381â€“4399, 1976.
[2] m. araya-polo, t. dahlke, c. frogner, c. zhang, t. poggio, and d. hohl. automated
fault detection without seismic processing. the leading edge, 36(3):208â€“214, 2017.
[3] m. araya-polo, j. jennings, a. adler, and t. dahlke. deep-learning tomography. the
leading edge, 37(1):58â€“66, 2018.
[4] m. atiqur rahman and y. wang. optimizing intersection-over-union in deep neural
networks for image segmentation. volume 10072, pages 234â€“244, 12 2016.
[5] l. chai, p. tong, and x. yang. frozen gaussian approximation for 3-d seismic wave
propagation. geophysical journal international, 208(1):59â€“74, 2017.
[6] t. dozat. incorporating nesterov momentum into adam. 2016.
[7] a. m. dziewonski and d. l. anderson. preliminary reference earth model. physics of
the earth and planetary interiors, 25(4):297â€“356, 1981.
[8] x. glorot, a. bordes, and y. bengio. deep sparse rectifier neural networks. in aistats,
2011.
[9] j. c. hateley, x. yang, l. chai, and p. tong. frozen gaussian approximation for 3d elastic wave equation and seismic tomography. geophysical journal international,
216(2):1394â€“1412, 11 2019.
[10] k. he, x. zhang, s. ren, and j. sun. deep residual learning for image recognition.
corr, abs/1512.03385, 2015.
[11] k. he, x. zhang, s. ren, and j. sun. deep residual learning for image recognition. in
proceedings of the ieee conference on computer vision and pattern recognition, pages
770â€“778, 2016.
[12] g. huang, z. liu, and k. q. weinberger. densely connected convolutional networks.
corr, abs/1608.06993, 2016.
[13] s. ioffe and c. szegedy. batch normalization: accelerating deep network training by
reducing internal covariate shift. corr, abs/1502.03167, 2015.
26

[14] y. lecun, y. bengio, and g. e. hinton. deep learning. nature, 521(7553):436â€“444,
2015.
[15] j. lu and x. yang. frozen gaussian approximation for general linear strictly hyperbolic
systems: formulation and eulerian methods. multiscale model. simul., 10:451â€“472,
2012.
[16] s. moosavi-dezfooli, a. fawzi, o. fawzi, and p. frossard. universal adversarial perturbations. corr, abs/1610.08401, 2016.
[17] h. nakamichi, h. hamaguchi, s. tanaka, s. ueki, t. nishimura, and a. hasegawa.
source mechanisms of deep and intermediate-depth low-frequency earthquakes beneath
iwate volcano, northeastern japan. geophysical journal international, 154(3):811â€“828,
2003.
[18] t. perol, m. gharbi, and m. denolle. convolutional neural network for earthquake
detection and location. science advances, 4(2), 2018.
[19] n. rawlinson, s. pozgay, and s. fishwick. seismic tomography: a window into deep
earth. phys. earth planet. inter., 178(3-4):101â€“135, 2010.
[20] b. romanowicz. seismic tomography of the earthâ€™s mantle. annu. rev. earth planet.
sci., 19:77â€“99, 1991.
[21] o. ronneberger, p. fischer, and t. brox. u-net: convolutional networks for biomedical
image segmentation. corr, abs/1505.04597, 2015.
[22] e. shelhamer, j. long, and t. darrell. fully convolutional networks for semantic segmentation. 2015 ieee conference on computer vision and pattern recognition (cvpr),
pages 3431â€“3440, 2015.
[23] j. t. springenberg, a. dosovitskiy, t. brox, and m. a. riedmiller. striving for simplicity: the all convolutional net. corr, abs/1412.6806, 2014.
[24] c. szegedy, w. zaremba, i. sutskever, j. bruna, d. erhan, i. j. goodfellow, and r. fergus. intriguing properties of neural networks. corr, abs/1312.6199, 2013.
[25] d. wei and x. yang. eulerian gaussian beam method for high frequency wave propagation in heterogeneous media with discontinuities in one direction. commun. math. sci.,
10:1287â€“1299, 2012.
[26] y. wu, y. lin, z. zhou, d. c. bolton, j. liu, and p. johnson. deepdetect: a cascaded
region-based densely connected network for seismic event detection. ieee transactions
on geoscience and remote sensing, pages 1â€“14, 2018.
[27] o. yilmaz. seismic data analysis: processing, inversion, and interpretation of seismic
data. society of exploration geophysicists, 2001.
[28] z. zhang, q. liu, and y. wang. road extraction by deep residual u-net. corr,
abs/1711.10684, 2017.
27

[29] d. zhao. tomography and dynamics of western-pacific subduction zones. monogr.
environ. earth planets, 1:1â€“70, 2012.
[30] w. zhu and g. c. beroza. phasenet: a deep-neural-network-based seismic arrival
time picking method. arxiv e-prints, mar. 2018.

28



downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

geophysics, vol. 88, no. 1 (january-february 2023); p. wa105â€“wa114, 10 figs., 3 tables.
10.1190/geo2022-0159.1

enhancing one-way wave equation-based migration with deep learning

yuwei yu1, yaxing li1, xinming wu1, and xiaofeng jia1

abstract
one advantage of one-way wave equation-based migration is
its low computational cost. however, due to the limited wavefield
propagation angle, it is difficult to use one-way wave equationbased migration for high-precision imaging of structures with
large inclinations due to issues such as inaccurate amplitudes
and migration image artifacts. in addition, when the model has
large horizontal velocity differences, it is difficult for the oneway wave propagator to calculate an accurate wavefield phase.
reverse time migration (rtm) based on the two-way wave
propagator has a high resolution and avoids the issues associated
with one-way wave propagators; however, it has a high computational cost in practical applications. we develop a convolutional
neural network (cnn) application mode that improves one

introduction
seismic migration is a detection technique that returns the energy
of recorded data to its original position. the returned energy creates
an image that outlines underground structures; these migration images can be used for resource exploration and disaster prevention.
after years of research, various migration methods (shragge and
shan, 2008; etgen et al., 2009; leveille et al., 2011) have been proposed in response to different detection requirements. among them,
reverse time migration (rtm) based on the two-way wave propagation operator (baysal et al., 1983; mcmechan, 1983) is preferred
by researchers because of its outstanding imaging performance.
however, in prestack rtm, the zero-lag crosscorrelation between
the two-way source wavefield and the two-way receiver wavefield
produces artifacts, many of which have low-frequency features.
many methods for addressing this problem have been proposed
(leveille et al., 2011). the laplacian operator (youn and zhou,

migration method by learning from another one and design a
cnn with a structure similar to u-net that combines the advantages of both migration methods. the cnn label is the rtm result, and the corresponding input is the result of one-way wave
migration with a generalized screen propagator (gsp). the
trained cnn model improves the amplitude in the one-way wave
migration image and removes the errors caused by large lateral
velocity perturbations. moreover, by maintaining the high migration calculation efficiency, our cnn model allows for a high resolution, few artifacts, and accurate images of steep structures in
the one-way wave migration result. with our method, the accuracy of the one-way wave migration result is close to that of the
rtm result. the use of gsp-based migration in our cnn model
rather than conventional rtm to generate prospecting images can
considerably reduce the calculation costs.

2001; zhang and sun, 2009) can be used to process rtm images,
effectively attenuating low-frequency artifacts. mulder and plessix
(2003, 2004) use a high-pass filter to address low-frequency artifacts. fletcher et al. (2005, 2006) introduce sponge boundary conditions in the nonreflection wave equation to attenuate reflected
waves, thereby reducing the artifacts in rtm images. yoon and
marfurt (2006) propose a new imaging condition with propagation
angle constraints using the poynting vector, which eliminated false
imaging at nonreflecting points. guitton et al. (2007) propose a
least-squares filtering method that removes low-frequency artifacts
while maintaining accurate imaging results. liu et al. (2007, 2011)
apply various techniques, such as the hilbert transform, to replace
the crosscorrelation of the two-way wavefields with the directed
crosscorrelation of the one-way wavefields, which prevents the generation of artifacts in the rtm process. based on the previous research, fei et al. (2015) propose a de-primary imaging condition
with fewer wavefield components to address these artifacts. rocha

manuscript received by the editor 17 march 2022; revised manuscript received 20 july 2022; published ahead of production 12 september 2022; published
online 28 october 2022.
1
university of science and technology of china, school of earth and space sciences, laboratory of seismology and physics of earthâ€™s interior, hefei, china
and university of science and technology of china, national geophysical observatory at mengcheng, hefei, china. e-mail: yuyuwei@mail.ustc.edu.cn;
yxli2017@mail.ustc.edu.cn; xinmwu@ustc.edu.cn; xjia@ustc.edu.cn (corresponding author).
Â© 2023 society of exploration geophysicists. all rights reserved.
wa105

downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

wa106

yu et al.

et al. (2016) propose a new inner product technique for rtm and
full-waveform inversion (fwi), which can attenuate reflection in
any ideal direction to remove rtm artifacts.
at present, the aforementioned methods for removing the rtm
artifacts satisfy different detection requirements. the main impediment to the widespread adoption of regular rtm is the high computational cost (etgen et al., 2009; zhang and sun, 2009; leveille et al.,
2011). it takes a considerable amount of time to calculate the twoway wavefields, and the extensive writing and reading operations require a large amount of storage space. some methods for removing
the artifacts (fletcher et al., 2005, 2006; yoon and marfurt, 2006;
guitton et al., 2007; liu et al., 2007, 2011; fei et al., 2015; rocha
et al., 2016) also require substantial computational costs. griewank
(1992) proposes a checkpointing algorithm for reducing the storage
required by writing and reading operations, and the few stored checkpoint wavefields could be used to reconstruct any wavefield.
griewank and walther (2000) develop an optimized checkpointing
method that greatly reduces the memory requirements while slightly
increasing the computational load. symes (2007) introduces this optimal checkpointing technology (griewank and walther, 2000) into
rtm, which reduces the reliance of rtm on disk input/output. anderson et al. (2012) present a modified version of the optimal checkpointing technology (griewank and walther, 2000) for isotropic
elastic rtm and anisotropic viscoelastic fwi. for wavefield
reconstruction in attenuated media, yang et al. (2016) propose a
checkpointing-assisted technique that requires less wavefield information than the optimal checkpointing technique. although these
methods can address the issue of limited computer memory, the large
computational load of rtm remains a challenge. to reduce the computational cost, morton and ober (1998), jing et al. (2000), and zhao
et al. (2016) propose methods for loading multiple pairs of seismic
sources and recording data simultaneously in one migration. due to
the mismatch between the shot and the shot gather, the migration
image often includes noise and crosstalk artifacts. perrone and sava
(2012) propose a hybrid encoding method that combined the advantages of random shot-encoding migration and plane-wave migration.
their method maintains the resolution of traditional rtm while
reducing noise in the images. jia et al. (2020) develop a supervirtual
shot-encoding scheme to attenuate artifacts in the simultaneous migration of multishot-gather data. this method has higher computational efficiency than plane-wave encoding methods.
solving the wavefield in only one direction (claerbout, 1970;
stoffa et al., 1990; ristow and rÃ¼hl, 1994) can significantly reduce
the calculation time. however, the limited wavefield propagation
angle of the one-way wave propagator introduces large migration errors. ristow and rÃ¼hl (1994) propose a hybrid migration method that
combines phase-shift migration and finite-difference schemes; this
method can be applied with relatively large velocity variations and
steeply inclined structures. to expand the propagation angle of
one-way elastic waves in arbitrarily heterogeneous media, wu (1994)
develops a thin-slab formulation of one-way elastic wave equations. grimbergen et al. (1998) propose a modal expansion method
to construct an intrinsically stable one-way wave propagator, which
can improve the lateral resolution of migration. xie and wu (1998)
use the padÃ© approximation to deal with the one-way wave equation
and propose a new screen method. the proposed generalized screen
propagator (gsp) simulates the wavefield relatively accurately with
high velocity contrast and has a larger propagation angle than the
previous screen propagator. huang et al. (1999) propose an extended

born fourier migration to address the instability of the application
caused by the singularity problem and to accurately handle the steep
interface problem. zhang and yao (2011) propose a compensation
scheme with high computational efficiency to reduce the numerical
dispersion and two-way splitting error of the finite-difference
method. based on the chebyshev polynomials, zhang et al. (2010)
and song et al. (2022) develop new fourier propagators with propagation angles greater than 60Â° within 1% phase error. sandberg and
beylkin (2009) use spectral projectors to remove the evanescent
waves in full-wave equation extrapolation, and their method preserves wave propagations at all angles. you et al. (2020) use the
derivative of the one-way wavefield to depth to realize the full-wave
equation depth migration. jia and wu (2009) develop a super-wide
angle one-way wave propagator to simulate turning waves. the migration based on their propagator could image overhanging structures. however, although one-way wave migration has been fully
developed, it still faces various problems (etgen et al., 2009; zhang
and sun, 2009; leveille et al., 2011): amplitude and phase inaccuracy, low resolution, and difficulty imaging large angle interfaces.
deep learning is an artificial intelligence technology in computer
science that can identify data features and potentially replace humans
to complete tasks efficiently (ronneberger et al., 2015; sun et al.,
2017; shi et al., 2019; wu et al., 2020). in addition, deep learning
can be effective for problems that people cannot easily solve (richardson and feller, 2019; yi and bui, 2020; avila et al., 2021; zhang et al.,
2021). as deep learning has developed, it has become widely used in
various fields. ronneberger et al. (2015) use convolutional neural networks (cnns) to segment images of biological cells, and the network
accurately delineates the cell boundaries. the u-shaped network
(u-net) structure designed has been widely used in other studies.
sun et al. (2017) build a highly accurate deep learning model for plant
classification. yi and bui (2020) apply deep learning to predict traffic
congestion. in the field of geophysics, richardson and feller (2019)
use a u-net for seismic data denoising. wu et al. (2020) use a cnn for
geologic interpretation and accurately identify fault locations. zhang
et al. (2021) use a cnn for least-squares rtm to reduce computational costs. avila et al. (2021) propose a migration deconvolution
method that fits the inverse operator with a u-net rather than calculating the inverse hessian matrix. this method has a higher resolution
than the hessian filter least-squares migration method. in addition,
deep learning has been successfully applied to inversion (yang and
ma, 2019; li et al., 2020; wang et al., 2021), sparse data imaging
(huang and nowack, 2020), and first-break picking (yuan et al.,
2018). due to its wide applicability and perfect performance, deep
learning has broad application prospects in the field of geophysics.
in this paper, we propose a cnn-based method for enhancing oneway wave equation-based migration images. our network is a u-net
with residual neural networks (resnets). we use the rtm result based
on the two-way wave propagator as the label of our network. the input
is the one-way wave equation-based migration result. the one-way
wave propagator is realized by a gsp. the gsp-based migration is
faster than rtm. we intend to use the trained network to compensate
for the image amplitude and phase defects of gsp-based migration.

theory
two-way wave equation-based migration
seismic vibration is an intuitive manifestation of seismic wave
propagation, and the propagation of underground seismic waves

enhancing one-way wave migration

downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

can be expressed by wave equations. the 2d acoustic wave equation can be written as

8


>
< âˆ‚uÃ°x;z;tÃ¾ Ã¾ Ïv2 Ã°x; zÃ¾ âˆ‚sx Ã°x;z;tÃ¾ Ã¾ âˆ‚sz Ã°x;z;tÃ¾ Â¼ 0
âˆ‚t

âˆ‚x

âˆ‚z

>
: Ï âˆ‚sÃ°x;z;tÃ¾ Ã¾ â–¿uÃ°x; z; tÃ¾ Â¼ 0

;

(1)

âˆ‚t

where uÃ°x; z; tÃ¾ and t are the acoustic wavefield and time, respectively; x and z are the spatial coordinates in the horizontal and vertical directions, respectively; Ï and vÃ°x; zÃ¾ represent the density and
acoustic velocity, respectively; the particle vibration velocity field
has the form sÃ°x; z; tÃ¾ Â¼ Ã°sx Ã°x; z; tÃ¾; sz Ã°x; z; tÃ¾Ã¾; and â–¿ is the gradient operator. in this work, the derivatives with respect to time
and space in equation 1 are approximated by the taylor series. the
time derivative has the second-order precision, whereas the spatial
derivative has the fourth-order precision. the final image is produced by applying the imaging condition to the finite-difference
simulated wavefields.
we use rtm to generate the label image of the cnn; the rtm
imaging condition is

iÃ°x; zÃ¾ Â¼

z t
0

us Ã°x; z; tÃ¾ug Ã°x; z; t âˆ’ tÃ¾dt;

(2)

where iÃ°x; zÃ¾ denotes the imaging result; dt refers to the time interval; us Ã°x; z; tÃ¾ and ug Ã°x; z; t âˆ’ tÃ¾ represent the forward-propagation
wavefield of the source and the backward-propagation wavefield of
the recorded data, respectively; and t is the total time of the recorded
data. in this paper, we use the laplacian filter (youn and zhou, 2001)
to suppress the low-frequency artifacts of rtm.

one-way wave equation-based migration
in the frequency domain, the 2d acoustic wave equation can be
expressed in the following form:




âˆ‚2
âˆ‚2 uÃ°x; zÃ¾
2 2
Ã¾
k
r
Â¼ 0;
uÃ°x; zÃ¾ Ã¾
r
2
âˆ‚z2
âˆ‚x

wa107

z
uÃ°x; zÃ¾ Â¼

dkx pÃ°z; kx Ã¾eikx x ;

(6)

where kx represents the x-direction wavenumber and pÃ°z; kx Ã¾eikx x
refers to a part of the wavenumber-domain wavefield. according to
equations 5 and 6, we have





Î´v
k2x
 âˆ’dzkr a 2 pÃ°z0 ;kx Ã¾;
pÃ°z1 ;kx Ã¾Â¼exp i dzkz âˆ’dzkr
v
kr
(7)
where dz is the spatial distance between z1 and z0 ; a Â¼ 0.5Â½Ã°1=rÃ¾
âˆ’1; r is obtained from the fourier transform of r, where  is the
wavenumber-domain convolution; Î´v Â¼ v âˆ’ vr is the velocity perturbation; and kz is the z-direction wavenumber. the right side of equation 7 includes three parts: the first two parts represent the phase
screen solution and the third part is a modifier that allows the propagator to adapt to large velocity perturbations and wide propagation
angles. to increase the computational efficiency, equation 7 can be
converted to a dual-domain format (xie and wu, 1998):






a âˆ‚2
âˆ’idzkr Î´v
âˆ’1
idzk
z
v
pÃ°x;z1 Ã¾ Â¼ 1Ã¾idz
f e
pÃ°x;z0 Ã¾ ;
Ã—f e
kr âˆ‚x2
(8)
where fÂ½: represents the fourier transform and fâˆ’1 Â½: represents the
inverse fourier transform. the gsp proposed in equation 8 can be
used to simulate wavefields in media with strong local velocity perturbations. the gsp models wide-angle wavefields with relatively
high accuracy and therefore can be used for migration. however,
the gsp is a one-way wave propagator, and thus its accuracy for wavefield simulations is lower than that of the two-way wave propagator.
we use the zero-lag crosscorrelation imaging condition in the
frequency domain:

(3)

where kr Â¼ Ï‰=vr is the reference wavenumber; Ï‰ and vr are the
angular frequency and the reference velocity, respectively; and r Â¼
vr =v is the refraction index. allowing for only the one-way wavefield, we decompose equation 3 and obtain the one-way wave equation of the forward propagation as

âˆ‚uÃ°x; zÃ¾
Â¼i
âˆ‚z

sï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
âˆ‚2
Ã¾ k2r r2 uÃ°x; zÃ¾;
âˆ‚x2

(4)

where i is the imaginary unit. we use the first-order padÃ© approximation to approximate the square root term of equation 4, yielding
(xie and wu, 1998)

2
3
1 1 âˆ‚2
2
2
2
âˆ‚uÃ°x; zÃ¾
2 kr r âˆ‚x
5uÃ°x; zÃ¾:
Â¼ irkr 41 Ã¾
2
âˆ‚z
1 Ã¾ 14 21 2 âˆ‚ 2
kr r âˆ‚x

according to fourier transform,

(5)

figure 1. the topology of u-net1. in each rectangular frame, the
horizontal annotation represents the number of channels and the
vertical annotation represents the ratio of the size of the current image to the size of the original image. conv, convolution; bn, batch
normalization; relu, rectified linear unit activation function; maxpool, max pooling; and cat, concatenation.

yu et al.

wa108

i 0 Ã°x; zÃ¾ Â¼

x

ps Ã°x; z; Ï‰Ã¾pâ‹„g Ã°x; z; Ï‰Ã¾;

p1 Â¼ i 1 Ã¾ Î±;

(9)

(10)

downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

Ï‰

to obtain the input image of the cnn, where ps Ã°x; z; Ï‰Ã¾ and
pg Ã°x; z; Ï‰Ã¾ are the forward-propagated source wavefield and the
backward-propagated receiver wavefield, respectively, and â‹„ represents the complex conjugate.

u-net for enhancing one-way wave equation-based
migration image
the image produced by one-way wave equation-based migration
can be regarded as

where p1 denotes the actual image obtained by one-way wave
migration, i 1 is the corresponding true image, and Î± represents
the artifacts and errors. similarly, the image produced by twoway wave equation-based migration can be expressed as

p2 Â¼ i 2 Ã¾ Î²;

where p2 is the actual rtm result and i 2 and Î² represent the true
image and the rtm artifacts, respectively. to reduce the influence
of Î² on p2 as much as possible, we use the laplacian filter (youn
and zhou, 2001) to attenuate the artifact Î².
we use a u-net to establish the mapping relationship from p1 to p2 . the label of the u-net is
the image produced by two-way wave migration,
and the input is the result produced by one-way
wave migration. here, Î² is greatly attenuated by
the laplacian filter. in addition, because i 1 and Î±
have almost no similarities to Î², it is difficult to
capture the similarities by the u-net and use them
to create i 1 âˆ’ Î² and Î± âˆ’ Î² connections; therefore,
the mapping relationship from p1 to p2 is essentially the mapping relationship from p1 to i 2 .
thus, training the u-net is similar to fitting a
function f, and we have

i 2 Â¼ fÃ°p1 Ã¾ Â¼ fÃ°i 1 Ã¾ Î±Ã¾:

(12)

because Î± and i 2 share almost no structural similarities, the ideal function f removes Î± and corrects i 1 to ensure that it is as similar to i 2 as
possible.
we train and validate two networks (u-net1 and
u-net2) with different depths for comparison. the
u-net topologies are shown in figures 1 and 2.
the u-net structure can be divided into two main components: downsampling and upsampling. in figure 2, the structure of the encoding
block is displayed in the azure rectangle. the different encoding
blocks are resnets with different convolution kernels or weights.
the resnets are introduced to solve the degradation problem caused
by the increased network depth, allowing the cnn to increase the
depth to extract more target features and become easier to optimize.
the mapping that results due to the addition of the resnets is sensitive to changes in the output. the network inside the brown rectangle in figure 2 represents the decoding block, with the interpolation
function and different resnets forming different decoding blocks. unet structures can identify target characteristics at different scales.
the skip concatenations introduce original features with different
scales to restore the target scale, and these original features can
modify the output of the network.
three loss functions are used to train u-net1 and u-net2: the
mean absolute error (mae), the mean-squared error (mse), and
smoothl1. these loss functions are expressed in equations 13, 14,
and 15, respectively. here, m and n denote the outputs and labels of
the cnn, respectively; q is the total number of elements in a batch;
and Î¸ is a variable that can be selected by the user. due to the limitation of gspâ€™s one-way attribute, gsp-based migration images
contain many artifacts and outliers. at the beginning of training,
these outliers are likely to be passed to the network output, causing

figure 2. the topology of u-net2. the details of the encoding and decoding blocks are
shown in the azure and brown rectangles, respectively. in the encoding blocks, a resnet
acts as an encoding block; these resnets have the same structure and different convolution
kernels or weights. the image size determines whether the operations in the parentheses
are performed. an encoding operation consists of a certain number of encoding blocks.
the decoding block consists of an interpolation (ipn) function and a resnet.

figure 3. (a) true velocity model and (b) migration velocity model.

(11)

downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

enhancing one-way wave migration

wa109

the mse network to be unstable. however, the
mse network has a relatively stable gradient
when the loss is close to zero as well as a relatively fast convergence speed. the l1 norm of the
difference between the output and the label is insensitive to outliers in the output. nonetheless, it
is difficult for the mae network to calculate an
effective gradient when the loss is close to zero.
if we choose a reasonable Î¸, the smoothl1 network has the advantages of the first two networks
(girshick, 2015)

1x
jm âˆ’ nj j;
q jÂ¼1 j

(13)

1x
Ã°m âˆ’ nj Ã¾2 ;
q jÂ¼1 j

(14)

q

maeÃ°m; nÃ¾ Â¼

q

mseÃ°m; nÃ¾ Â¼

smoothl1Ã°m;nÃ¾
8 p
< 1 qjÂ¼1 Â½0.5Ã°mj âˆ’nj Ã¾2 =Î¸; if jmj âˆ’nj j<Î¸
q
Â¼ 1 pq
:
:
otherwise
jÂ¼1 Â½jmj âˆ’nj jâˆ’0.5Î¸;
q

figure 4. the velocity models and the results of one-way wave migration and rtm.
(a) the rectangular area from figure 3a, (b) the gsp-based migration image for the
model shown in (a), (c) the rtm image for the model shown in (a), (d) a model including formations with large dips, (e) the gsp-based migration image for the model
shown in (d), and (f) the rtm image for the model shown in (d).

(15)
data set establishment
migration image
because our neural network models are designed to act on migration images, we need to
compute the migration images to build the data
set. a true velocity model is shown in figure 3a.
figure 3b depicts a smooth version of the model,
which is used for migration. we use the true
model to generate synthetic data. the source is
defined by a ricker wavelet with a dominant
frequency of 25 hz. the grid spacing is 5 m.
the source is distributed across the surface, with
a source spacing of 80 m. geophones are distributed on both sides of the source with a spatial interval of 5 m and a maximum offset of 2.5 km. we
crop the adequately illuminated part of the rtm
image to use as a high-quality label. similarly, the
input is a cropped gsp-based migration image. it
should be noted that all of the images and velocity
models shown in the following have been freely
cropped. figure 4a shows the velocity model from
the rectangular box in figure 3a. the gsp-based
migration image corresponding to figure 4a is
shown in figure 4b, whereas figure 4c displays
the corresponding rtm image. the one-way
wave equation-based migration image has some
artifacts, and the structure at this depth is indistinct. compared with the true model, the spatial
position of the structure (such as the peak of
the uppermost anticline) in the migration image

figure 5. the seg/eage overthrust model as well as its variant and slices. (a) the seg/
eage overthrust model, (b) a variant of the seg/eage overthrust model, (c) the velocities extracted from the models shown in (a and b) along the black line indicated in (a),
(d) slice model 1, (e) slice model 2, (f) slice model 3, (g) slice model 4, and (h) slice
model 5.

yu et al.

downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

wa110

is higher than the true position. these issues are not present in the
two-way wave equation-based migration image.
when the model includes large inclination structures and severe
lateral velocity variations, the wavefield and image calculated by
gsp are biased. figure 4e and 4f shows the gsp-based migration
image and rtm image, respectively, for the model shown in figure 4d. there are numerous artifacts in the gsp-based migration
image. most of these errors are caused by strong lateral velocity
differences. in contrast, the rtm image is clear and has few artifacts. low-frequency artifacts in the rtm image can be attenuated
with a simple laplacian filter. due to the preceding features of the
rtm image, we use rtm images as the labels in our work.
to ensure the richness of the u-net input samples, the migration
parameters were defined with large ranges. the dominant frequency
of the ricker wavelet ranged from 17 to 40 hz. the grid spacing
(geophone spacing) ranged from 2.5 to 6 m. the maximum offset

ranged from 1.5 to 3.5 km. the source spacing varied between 20
and 100 m.

expansion of velocity models
rich data sets are required for neural network training, verification, and testing. to expand the data sets, several velocity models
have been developed. some methods for expanding the velocity

table 1. the proportion of different types of models.

type

proportion (%)

t01

t02

t03

t04

t05

58.8

14.9

12.4

7.8

6.1

t01: simple models, such as those shown in figure 4a and 4d; t02: models derived
from the marmousi model; t03: models derived from the overthrust model; t04:
models with steep faults; and t05: models with steep salt domes.

figure 6. loss convergence curves for the combination of u-net2
and smoothl1.

figure 7. the cnn model outputs for the input shown in figure 4.
(a) the output for the input shown in figure 4b; (b) the amplitudes
of the images shown in figure 4b, 4c, and (a) (distance = 0.7 km and
depth = 0â€“1.28 km); (c) the output for the input shown in figure 4e;
(d) the amplitudes of the images shown in figure 4e, 4f, and (c) (distance = 0.1 km and depth = 0â€“1.28 km); and (e) the amplitudes
extracted from the images shown in figure 4e, 4f, and (c) along
the dashed black line indicated in figure 4d (note that the image
amplitudes are normalized to facilitate comparison of the relative
magnitude of artifacts).

table 2. the average losses and ssims of different combinations for the validation and test sets.

u-net1
outputs
average loss
average ssim

u-net2

mae

mse

smoothl1

mae

mse

smoothl1

0.0555  0.0001
0.91  0.01

0.0633  0.0001
0.90  0.01

0.0634  0.0001
0.89  0.01

0.0533  0.0001
0.92  0.01

0.0581  0.0001
0.91  0.01

0.0527  0.0001
0.92  0.01

to facilitate comparison, the output losses are uniformly calculated based on the mae; the corresponding average ssim of the inputs is 0.23.

downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

enhancing one-way wave migration
models include cropping, rotation, vertical (or horizontal) interpolation, vertical (or horizontal) flips, and outward expansion (by horizontally copying the value of the model boundary). we apply one or
more of these operations to enrich our model.
figure 5a shows the seg/eage overthrust model, whereas
figure 5b depicts its variant. figure 5c exhibits the velocities extracted from the overthrust model and its variant along the black line
in figure 5a. compared with the overthrust model, its variant has a
wider velocity range and a larger average velocity gradient, allowing
rtm to image steep structures. based on the overthrust model and its
variant, we have designed the models shown in figure 5d and 5e.
moreover, we also have introduced some special structures into
the generated models, as shown in figure 5fâ€“5h.
our model library also includes models based on the benchmark
marmousi model. the models discussed previously are only a subset of our model library, with
each type of model occupying a reasonable proportion of the library. the detailed proportions
are shown in table 1.

wa111

results
to generate the results for the validation and test sets, the u-net2
model with smoothl1 is the ideal cnn model. figure 7a shows the
output of the network for the migration image in figure 4b. compared with the input, the deep structures are visible in the output.
figure 7b shows the amplitudes extracted along the line (distance = 0.7 km and depth = 0â€“1.28 km) in the migration images
in figures 4b, 4c, and 7a. compared with the input, the output
has an improved amplitude that essentially corresponds to the
amplitude of the label. the same situation can be observed in
figures 4e, 4f, 7c, and 7d. in addition, the trained network effectively removes the artifacts in the input shown in figure 4e caused
by the large horizontal velocity perturbations, and thus the output

training and validation
u-net1 and three loss functions are combined
individually for training and validation, with the
same operations applied to u-net2. we have
2889 pairs of input and label, of which the validation set accounts for approximately 10% and the
rest is the training set. we use pytorch version
1.10.0 on a geforce rtx 3090 graphics processing unit (gpu) for training and validation. the
average losses of different combinations for the
validation and test sets after 200 epochs of training
are provided in table 2. to facilitate comparison,
the average losses are uniformly calculated based
on the mae. to quantitatively evaluate the effectiveness of different combinations, the structural
similarity index measure (ssim) is introduced
(avila et al., 2021) by

ssim Â¼

Ã°2Î¼o Î¼l Ã¾ c1 Ã¾Ã°2Ïƒ ol Ã¾ c2 Ã¾
;
2
Ã°Î¼o Ã¾ Î¼2l Ã¾ c1 Ã¾Ã°Ïƒ 2o Ã¾ Ïƒ 2l Ã¾ c2 Ã¾
(16)

where Î¼o and Î¼l are the mean values of o and
l, respectively; Ïƒ ol represents their covariance;
and Ïƒ 2 represents the variance. in addition, c1 Â¼
Ã°0.01lÃ¾2 and c2 Â¼ Ã°0.03lÃ¾2 , where l is the
value range of the image amplitude. here, l represents the cnn label and o refers to either the output or the input. for the validation and test sets, the
average ssim of the inputs is 0.23. the corresponding average ssims for the outputs of different combinations are shown in table 2. the model
with u-net2 and smoothl1 has the lowest average
loss and the highest average ssim. the model
with u-net2 and smoothl1 greatly improves the
average ssim of the inputs. the convergence
curve of this model during training is shown in
figure 6.

figure 8. the inputs and outputs for the models displayed in figure 5. (a) the gspbased migration result for the model in figure 5d, (b) the output for the input shown in
(a), (c) the gsp-based migration result for the model in figure 5e, (d) the output for the
input shown in (c), (e) the gsp-based migration result for the model in figure 5f, (f) the
output for the input shown in (e), (g) the gsp-based migration result for the model in
figure 5g, (h) the output for the input shown in (g), (i) the gsp-based migration result
for the model in figure 5h, and (j) the output for the input shown in (i).

downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

wa112

yu et al.

shown in figure 7c contains very few artifacts. there is no velocity
difference at the position of the dashed black line in figure 4d, in
which the image amplitude should be zero. figure 7e shows the
amplitudes of the images displayed in figures 4e, 4f, and 7c at this
location, and it demonstrates that our method has the function of
artifact attenuation.
the cnn model also performs well for complex models.
figure 5d depicts a fault model, and its gsp-based migration image is shown in figure 8a. the image contrast at the bottom of
figure 8a is very low, and the fault in the black rectangular frame
is unrecognizable. as shown in figure 8b, our network model restores the fault structure and enhances the contrast in the deep
area. when the formation has a very large inclination (figure 5e),
gsp-based migration has difficulty recovering the structure (figure 8c). in the input image, the inclined strata are visually separated, and it is difficult to determine whether the structures
between the separated strata are artifacts. figure 8d shows how
our cnn model recovered the fault structure displaying a clear
image. high-velocity anomalies such as salt domes often have
steep boundaries and strong velocity contrast, as shown in figure 5f,
and imaging these structures with one-way wave equation-based
migration is challenging. the gsp-based migration image shown
in figure 8e has had difficulty restoring the steep boundaries. however, in the network output result (figure 8f), the boundaries
are clearly recovered. similarly, the cnn model can reconstruct images of steep fault planes. figure 8g and 8h and figure 8i and 8j

figure 9. (a) the marmousi model, (b) the gsp-based migration
result, (c) the cnn output, (d) the rtm result, and (eâ€“h) the magnifications of (aâ€“d) in the red focus area shown in (a), respectively.

depict two pairs of inputs and outputs with steep faults with the corresponding models shown in figure 5g and 5h, respectively. compared with the inputs, the output images of the steep interfaces are
significantly improved. the structure indicated by the black arrow in
figure 8g is improved in figure 8h. furthermore, the image shown in
the black rectangular frame in figure 8i is clearly illustrated in
figure 8j.
moreover, we use the cnn model to process a larger image.
figure 9a shows the marmousi model, with figure 9b depicting
the corresponding gsp-based image. the images in the training
set are all 256 Ã— 288 (points). however, the size of the image in
figure 9b is 388 Ã— 848. our network structure has an implicit
nature that can naturally handle images of any size. with the input
in figure 9b, we can obtain higher resolution output, as shown in
figure 9c. the images of the interfaces, especially near the red
arrow in figure 9a, have been enhanced. in the area below the
arrow in the model, the output has fewer artifacts and clearer structures than the input. figure 9d shows the corresponding rtm result, and figure 9eâ€“9h shows the magnifications of figure 9aâ€“9d
in the red focus area shown in figure 9a, respectively. figure 9eâ€“
9h clearly shows that our method has a higher resolution than
gsp-based migration and rtm in these cases. to verify the generalizability of our cnn model, we take the gsp-based migration
image for a slice of the 3d seg/eage salt model (figure 10a) as
the cnn input (figure 10b). note that our velocity model library
does not cover the 3d seg/eage salt model. the cnn output
shown in figure 10c has a higher resolution than the input, especially for the salt dome boundary indicated by the red arrows

figure 10. (a) a profile of the 3d seg/eage salt model, (b) the
gsp-based migration result, (c) the cnn output, (d) the rtm image, and (eâ€“h) the magnifications of (aâ€“d) in the area indicated by
the upper red arrow in (a), respectively. note the phase improvement of (g) compared with (f).

enhancing one-way wave migration

wa113

downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

table 3. the ssims for the inputs and outputs of the velocity models.

figure (model) label

input
output

4a

4d

5d

5e

5f

5g

5h

9a

0.23  0.01
0.96  0.01

0.08  0.01
0.99  0.01

0.22  0.01
0.91  0.01

0.23  0.01
0.94  0.01

0.27  0.01
0.94  0.01

0.20  0.01
0.93  0.01

0.22  0.01
0.94  0.01

0.03  0.01
0.68  0.01

shown in figure 10a. figure 10d exhibits the corresponding rtm
image. figure 10eâ€“10h shows the magnifications of figure 10aâ€“
10d in the area indicated by the upper red arrow in figure 10a,
respectively. the comparisons of figure 10eâ€“10h indicate that
our method can improve the image of one-way wave equationbased migration. however, it is still challenging for the network
to deal with such a steep and subtle salt boundary, allowing for
absolutely nothing here in the input image.
the ssims of the preceding numerical examples are listed in
table 3. it is clear from tables 2 and 3 that our network greatly
improves the ssims between the gsp-based migration images and
the rtm images. thus, with our cnn model, the migration results
based on the one-way wave propagator are as close as possible to the
rtm results based on the two-way wave propagator.
although the rtm can be used to accurately image complex
structures, it has a considerable computational cost. the use of
a one-way wave propagator rather than a two-way wave propagator for migration can save substantial calculation costs. with the
same cpu usage, the calculation time of the rtm image is approximately 4.2 times that of the gsp-based migration image.
our cnn model generates an output from a gsp-based migration
image in seconds. as discussed previously, the proposed method
compensates for the shortcomings of one-way wave equationbased migration while saving considerable calculation costs in
subsurface investigations.

conclusion
we have proposed a method for improving one-way wave
equation-based migration images by using cnn. the cnn
mimics u-net and uses residual neural networks (resnets) to
improve its performance. the input of the network is the result
of a one-way wave equation migration based on a gsp. the label
is an rtm result, which has high imaging accuracy. we have
tuned the network with different depths and different loss functions and tested the corresponding network models to obtain an
ideal network model.
although the gsp-based migration has lower calculation costs
than rtm with a two-way wave propagator, it has difficulties handling steep structures and considerable velocity contrast. the ideal
cnn model improves the resolution of the gsp-based migration
image while removing the errors and artifacts caused by large horizontal velocity perturbations. furthermore, because the proposed
cnn model significantly mitigates amplitude deviations in the
gsp-based migration image, the output image can precisely depict
formations with large dips. with our cnn model, the gsp-based
migration image is as close as possible to the rtm image, and the
ssim between the gsp-based migration result and the rtm result
is improved. in addition to generating accurate imaging results, the

computational burden of gsp-based migration with our cnn
model is considerably lower than that of conventional rtm. the
method described in this paper has potential applications in realworld subsurface investigations. in addition, our method provides
a new cnn application mode that improves one migration method
by learning from another one.

acknowledgments
this study is supported by the national natural science foundation of china (nos. 42074125 and 41774121). we deeply thank
the editors and three anonymous reviewers for helping us improve
this paper.

data and materials availability
data associated with this research are available and can be
obtained by contacting the corresponding author.

references
anderson, j. e., l. tan, and d. wang, 2012, time-reversal checkpointing
methods for rtm and fwi: geophysics, 77, no. 4, s93â€“s103, doi: 10
.1190/geo2011-0114.1.
avila, m. r. v., l. n. osorio, j. de castro vargas fernandes, a. bulcÃ£o, b.
pereira-dias, b. de souza silva, p. m. barros, l. landau, and a. g. evsukoff, 2021, migration deconvolution via deep learning: pure and applied geophysics, 178, 1677â€“1695, doi: 10.1007/s00024-021-02707-0.
baysal, e., d. d. kosloff, and j. w. c. sherwood, 1983, reverse time
migration: geophysics, 48, 1514â€“1524, doi: 10.1190/1.1441434.
claerbout, j. f., 1970, coarse grid calculations of waves in inhomogeneous
media with application to delineation of complicated seismic structure:
geophysics, 35, 407â€“418, doi: 10.1190/1.1440103.
etgen, j., s. h. gray, and y. zhang, 2009, an overview of depth imaging in
exploration geophysics: geophysics, 74, no. 6, wca5â€“wca17, doi: 10
.1190/1.3223188.
fei, t. w., y. luo, j. yang, h. liu, and f. qin, 2015, removing false images
in reverse time migration: the concept of de-primary: geophysics, 80,
no. 6, s237â€“s244, doi: 10.1190/geo2015-0289.1.
fletcher, r. f., p. fowler, p. kitchenside, and u. albertin, 2005, suppressing
artifacts in prestack reverse time migration: 75th annual international
meeting, seg, expanded abstracts, 2049â€“2051, doi: 10.1190/1.2148113.
fletcher, r. p., p. j. fowler, p. kitchenside, and u. albertin, 2006, suppressing unwanted internal reflections in prestack reverse-time migration:
geophysics, 71, no. 6, e79â€“e82, doi: 10.1190/1.2356319.
girshick, r., 2015, fast r-cnn: proceedings of the ieee international
conference on computer vision, 1440â€“1448.
griewank, a., 1992, achieving logarithmic growth of temporal and spatial
complexity in reverse automatic differentiation: optimization methods
and software, 1, 35â€“54, doi: 10.1080/10556789208805505.
griewank, a., and a. walther, 2000, algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation: acm transactions on mathematical software, 26,
19â€“45, doi: 10.1145/347837.347846.
grimbergen, j. l. t., f. j. dessing, and k. wapenaar, 1998, modal expansion of one-way operators in laterally varying media: geophysics, 63,
995â€“1005, doi: 10.1190/1.1444410.

downloaded 01/17/23 to 202.38.84.171. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/geo2022-0159.1

wa114

yu et al.

guitton, a., b. kaelin, and b. biondi, 2007, least-squares attenuation of
reverse-time-migration artifacts: geophysics, 72, no. 1, s19â€“s23, doi:
10.1190/1.2399367.
huang, j., and r. l. nowack, 2020, machine learning using u-net convolutional neural networks for the imaging of sparse seismic data: pure and
applied geophysics, 177, 2685â€“2700, doi: 10.1007/s00024-019-02412-z.
huang, l.-j., m. c. fehler, and r.-s. wu, 1999, extended local born fourier
migration method: geophysics, 64, 1524â€“1534, doi: 10.1190/1.1444656.
jia, x., w. chen, and b. chen, 2020, an efficient super-virtual shot encoding scheme for multisource reverse time migration: geophysics, 85, no. 6,
s405â€“s416, doi: 10.1190/geo2019-0479.1.
jia, x., and r.-s. wu, 2009, superwide-angle one-way wave propagator and
its application in imaging steep salt flanks: geophysics, 74, no. 4, s75â€“
s83, doi: 10.1190/1.3124686.
jing, x., c. j. finn, t. a. dickens, and d. e. willen, 2000, encoding multiple shot gathers in prestack migration: 70th annual international meeting,
seg, expanded abstracts, 786â€“789, doi: 10.1190/1.1816188.
leveille, j. p., i. f. jones, z.-z. zhou, b. wang, and f. liu, 2011, subsalt
imaging for exploration, production, and development: a review: geophysics, 76, no. 5, wb3â€“wb20, doi: 10.1190/geo2011-0156.1.
li, s., b. liu, y. ren, y. chen, s. yang, y. wang, and p. jiang, 2020, deep
learning inversion of seismic data: ieee transactions on geoscience and
remote sensing, 58, 2135â€“2149, doi: 10.1109/tgrs.2019.2953473.
liu, f., s. a. morton, j. p. leveille, and g. zhang, 2007, reverse-time migration using one-way wavefield imaging condition: 77th annual international meeting, seg, expanded abstracts, 2170â€“2174, doi: 10.1190/1
.2792917.
liu, f., g. zhang, s. a. morton, and j. p. leveille, 2011, an effective imaging condition for reverse-time migration using wavefield decomposition: geophysics, 76, no. 1, s29â€“s39, doi: 10.1190/1.3533914.
mcmechan, g. a., 1983, migration by extrapolation of time-dependent
boundary values: geophysical prospecting, 31, 413â€“420, doi: 10.1111/
j.1365-2478.1983.tb01060.x.
morton, s. a., and c. c. ober, 1998, faster shot-record depth migrations using phase encoding: 68th annual international meeting, seg, expanded
abstracts, 1131â€“1134, doi: 10.1190/1.1820088.
mulder, w. a., and r.-e. plessix, 2003, one-way and two-way wave-equation migration: 73rd annual international meeting, seg, expanded
abstracts, 881â€“884, doi: 10.1190/1.1818081.
mulder, w. a., and r.-e. plessix, 2004, a comparison between one-way and
two-way wave-equation migration: geophysics, 69, 1491â€“1504, doi: 10
.1190/1.1836822.
perrone, f., and p. sava, 2012, wave-equation migration with dithered plane
waves: geophysical prospecting, 60, 444â€“465, doi: 10.1111/j.1365-2478
.2011.01011.x.
richardson, a., and c. feller, 2019, seismic data denoising and deblending using deep learning, https://arxiv.org/abs/1907.01497, accessed 23 january 2022.
ristow, d., and t. rÃ¼hl, 1994, fourier finite-difference migration: geophysics, 59, 1882â€“1893, doi: 10.1190/1.1443575.
rocha, d., n. tanushev, and p. sava, 2016, acoustic wavefield imaging
using the energy norm: geophysics, 81, no. 4, s151â€“s163, doi: 10
.1190/geo2015-0486.1.
ronneberger, o., p. fischer, and t. brox, 2015, u-net: convolutional networks
for biomedical image segmentation, in n. navab, j. hornegger, w. wells,
and a. frangi, eds., medical image computing and computer-assisted intervention, lecture notes in computer science 9351: springer, 234â€“241, doi:
10.1007/978-3-319-24574-4_28.
sandberg, k., and g. beylkin, 2009, full-wave-equation depth extrapolation
for migration: geophysics, 74, no. 6, wca121â€“wca128, doi: 10.1190/1
.3202535.
shi, y., x. wu, and s. fomel, 2019, saltseg: automatic 3d salt segmentation using a deep convolutional neural network: interpretation, 7, no. 3,
se113â€“se122, doi: 10.1190/int-2018-0235.1.
shragge, j., and g. shan, 2008, prestack wave-equation depth migration in
elliptical coordinates: geophysics, 73, no. 5, s169â€“s175, doi: 10.1190/1
.2956349.

song, h., j. zhang, and y. zou, 2022, direct expansion of fourier extrapolator for one-way wave equation using chebyshev polynomials of the second kind: geophysics, 87, no. 2, s63â€“s73, doi: 10.1190/geo2021-0114.1.
stoffa, p. l., j. t. fokkema, r. m. de luna freire, and w. p. kessinger, 1990,
split-step fourier migration: geophysics, 55, 410â€“421, doi: 10.1190/1
.1442850.
sun, y., y. liu, g. wang, and h. zhang, 2017, deep learning for plant identification in natural environment: computational intelligence and neuroscience, 2017, 7361042, doi: 10.1155/2017/7361042.
symes, w. w., 2007, reverse time migration with optimal checkpointing:
geophysics, 72, no. 5, sm213â€“sm221, doi: 10.1190/1.2742686.
wang, w., g. a. mcmechan, and j. ma, 2021, elastic isotropic and anisotropic full-waveform inversions using automatic differentiation for gradient calculations in a framework of recurrent neural networks:
geophysics, 86, no. 6, r795â€“r810, doi: 10.1190/geo2020-0542.1.
wu, r.-s., 1994, wide-angle elastic wave one-way propagation in heterogeneous media and an elastic wave complex-screen method: journal of
geophysical research: solid earth, 99, 751â€“766, doi: 10.1029/93jb02518.
wu, x., z. geng, y. shi, n. pham, s. fomel, and g. caumon, 2020, building realistic structure models to train convolutional neural networks for
seismic structural interpretation: geophysics, 85, no. 4, wa27â€“wa39,
doi: 10.1190/geo2019-0375.1.
xie, x.-b., and r.-s. wu, 1998, improve the wide angle accuracy of screen
method under large contrast: 68th annual international meeting, seg,
expanded abstracts, 1811â€“1814, doi: 10.1190/1.1820283.
yang, f., and j. ma, 2019, deep-learning inversion: a next-generation seismic velocity model building method: geophysics, 84, no. 4, r583â€“r599,
doi: 10.1190/geo2018-0249.1.
yang, p., r. brossier, l. mÃ©tivier, and j. virieux, 2016, wavefield
reconstruction in attenuating media: a checkpointing-assisted reverse-forward simulation method: geophysics, 81, no. 6, r349â€“r362, doi: 10
.1190/geo2016-0082.1.
yi, h., and k.-h. n. bui, 2020, an automated hyperparameter search-based
deep learning model for highway traffic prediction: ieee transactions on
intelligent transportation systems, 22, 5486â€“5495, doi: 10.1109/tits
.2020.2987614.
yoon, k., and k. j. marfurt, 2006, reverse-time migration using the pointing
vector: exploration geophysics, 37, 102â€“107, doi: 10.1071/eg06102.
you, j., j. cao, and j. wang, 2020, two-way wave equation prestack depth
migration using the matrix decomposition theory: chinese journal of
geophysics, 63, 3838â€“3848, doi: 10.6038/cjg2020o0027.
youn, o. k., and h.-w. zhou, 2001, depth imaging with multiples: geophysics, 66, 246â€“255, doi: 10.1190/1.1444901.
yuan, s., j. liu, s. wang, t. wang, and p. shi, 2018, seismic waveform
classification and first-break picking using convolution neural networks:
ieee geoscience and remote sensing letters, 15, 272â€“276, doi: 10
.1109/lgrs.2017.2785834.
zhang, j.-h., w.-m. wang, s.-q. wang, and z.-x. yao, 2010, optimized
chebyshev fourier migration: a wide-angle dual-domain method for media with strong velocity contrasts: geophysics, 75, no. 2, s23â€“s34, doi:
10.1190/1.3350861.
zhang, j.-h., and z.-x. yao, 2011, reducing two-way splitting error of ffd
method in dual domains: geophysics, 76, no. 4, s165â€“s175, doi: 10.1190/1
.3590214.
zhang, w., j. gao, t. yang, x. jiang, and w. sun, 2021, least-squares reverse time migration using convolutional neural networks: geophysics,
86, no. 6, r959â€“r971, doi: 10.1190/geo2021-0006.1.
zhang, y., and j. sun, 2009, practical issues in reverse time migration: true
amplitude gathers, noise removal and harmonic source encoding: first
break, 27, 53â€“59, doi: 10.3997/1365-2397.2009002.
zhao, z., m. k. sen, and p. l. stoffa, 2016, double-plane-wave reverse time
migration in the frequency domain: geophysics, 81, no. 5, s367â€“s382,
doi: 10.1190/geo2015-0687.1.

biographies and photographs of the authors are not available.



international journal of hydrogen energy 61 (2024) 137â€“161

contents lists available at sciencedirect

international journal of hydrogen energy
journal homepage: www.elsevier.com/locate/he

underground hydrogen storage leakage detection and characterization based
on machine learning of sparse seismic data
kai gao âˆ—, neala m. creasy, lianjie huang, michael r. gross
earth and environmental sciences division, los alamos national laboratory, los alamos, nm 87545, usa

article

info

keywords:
underground hydrogen storage
hydrogen leakage detection
sparse seismic data
machine learning

abstract
underground hydrogen storage (uhs) is considered as a scalable approach for massive storage and seasonal
extraction of hydrogen (h2 ). although conventional leakage detection and characterization methods based on
time-lapse seismic imaging and inversion generally apply to h2 leakage detection problem, a high-fidelity yet
cost effective geophysics approach is still missing to reliably inform leakage location and properties based
on very sparse data. in response, we develop a novel supervised machine learning method to detect and
characterize h2 leakage from uhs. the input to our neural network are sparse time-lapse seismic waveforms,
while the output from the neural network includes the spatial location and physical properties of a h2 leakage.
we generate high-quality time-lapse waveforms using the elastic-wave equations to train the neural network.
we train and validate our machine learning model and find that it attains high accuracy in using extremely
sparse time-lapse seismic data to detect and characterize h2 leakage. our investigation is the first systematic
study that focuses on applying machine learning to subsurface h2 leakage detection and characterization and
could potentially serve as a cost-effective geophysical tool for underground hydrogen leakage detection and
characterization with high fidelity.

1. introduction
renewable energies in various forms, including wind, solar, water
flow, waves, geothermal, and so on, are considered to be the key to
achieving net-zero emission and slowing global warming. however,
a major issue associated with renewable energies is the imbalance
between the energy demand and the spatialâ€“temporal availability of
renewable energies [1]. some of these renewable energies are also
difficult to be transported with an economic viable approach, leading
to geospatial limitations in using these energies.
hydrogen is considered to be one of the most important clean
alternatives to fossil energy. h2 can be naturally generated from the
reaction of ultramafic and mafic rocks that are rich in iron (fe) and
magnesium (mg) with anoxic water at temperatures >200 â—¦ c as a
spontaneous reaction [2], potentially offering an almost limitless renewable energy resource. however, natural h2 generation is far from
commercial scale exploration. a more commercially feasible way for
producing h2 is through water electrolysis, which can be driven by
redundant energy (mainly in the form of electricity) generated by
other forms of renewable energy, a paradigm known as power-to-gas
technology [3]. generated h2 can be stored both above ground or
within the subsurface, which is later extracted to meet seasonal or
cyclic energy demand peaks with high portability.

underground hydrogen storage (uhs) is considered to be a scalable approach for massive storage and cyclic extraction [4â€“6]. uhs
can leverage depleted oil and gas fields similar to geological carbon
sequestration (gcs) [7,8], as well as saline aquifers and salt or hard
rock caverns [9] because of their superb sealing capability. however,
massive subsurface storage of h2 , as in the case of gcs, may cause
significant hydrodynamical, geomechanical, and/or biological consequences, and face a number of fundamental scientific and engineering
challenges for mitigating these issues [10]. from a geophysical point of
view, seasonal and cyclic h2 injection and extraction could potentially
induce seismicity by changing subsurface stress [11,12] and may cause
leakage of h2 through hidden leakage channels (e.g., breakage of
sealing caprock or reactivation of nearby faults) [13], both of which
may cause significant societal concerns and environmental risks.
long-term safe operation of any uhs therefore requires high-fidelity
monitoring capable of evaluating operational risks and ensuring the
integrity of overlying caprocks or aquifers. when h2 leaks into surrounding geological layers, it may alter the petrophysical properties
of the rock, including the fluid saturation, density, elastic moduli, and
thus apparent seismic wave propagation velocities. a geophysical way
to achieving high-fidelity monitoring of subsurface reservoir leakage is
to perform active-source time-lapse seismic imaging, where one uses

âˆ— corresponding author.

e-mail address: kaigao@lanl.gov (k. gao).
https://doi.org/10.1016/j.ijhydene.2024.02.296
received 4 december 2023; received in revised form 16 february 2024; accepted 22 february 2024
available online 1 march 2024
0360-3199/published by elsevier ltd on behalf of hydrogen energy publications llc.

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

to address these challenges, we develop a novel supervised neural
network (nn) that consists of a 2d cnn module and a dense neural
network (dnn) module (e.g., [39]). the cnn modules extract spatial
features from the input time-lapse seismic waveforms that are represented by a single-channel 2d image and transfers the learned features
to the dnn module for regression. the output from the nn contains
the location (measured using the spatial range of the leakage: ğ±min , ğ±max
where ğ± = (ğ‘¥, ğ‘¦, ğ‘§)) and the properties (including the leakage mass ğ‘€
and leakage volume ğ‘‰ ). the motivation of our nn can be represented
by the following simple diagram:

the differential waveforms between the leakage-stage and the baseline
stage to image or invert for the spatial location and induced medium
parameter changes (e.g., [14â€“17]). there are several issues associated
with active-source time-lapse monitoring of h2 leakage. firstly, in
some cases, the non-repeatable factors in a time-lapse seismic survey,
such as source time function, observation system inconsistency, and
coherent noise, need to be processed first or be mitigated, which is
a challenging task even with specialized processing methods [18,19].
secondly, although time-lapse seismic imaging and inversion are plausible and usually the most reliable geophysical method because of their
appropriate sensitivity to reservoir changes at the exploration scale,
active-source time-lapse seismic monitoring can be expensive because
dense data acquisition system needs to be deployed, some in a marine
environment. thirdly, it is usually very challenging, if not impossible,
to obtain directly time-lapse changes of medium parameters other
than seismic velocities (ğ‘‰ğ‘ and ğ‘‰ğ‘  ), for instance, porosity, saturation,
and so on, from seismic waveforms, because of the highly nonlinear
relationship between seismic waveforms and petrophysics parameters
(e.g., [20]). the efficacy of conventional time-lapse seismic imaging
and inversion methods can be further stymied in addressing our unique
problem: h2 leakage characterization. because h2 is a small-molecule
gas and may only cause limited rock property changes, the resulting
time-lapse signals can be weak, adding another layer of uncertainty to
high-fidelity time-lapse monitoring of leakage. the unique challenges
associated with uhs leakage can be schematically depicted in fig. 1.
machine learning (ml) provides a prospect for solving challenging
problems in science based on a coalescence of numerical optimization
and statistical inference. first derived in computer vision and natural
language processing (e.g., [21â€“24]), ml has gained wide applications
to geophysical problems, including earthquake data phase picking (e.g.,
[25,26]), geological interpretation (e.g., [27,28]), signal and image
processing (e.g., [29,30]), subsurface characterization (e.g., [31â€“33]),
petrophysics of uhs (e.g., [34]), to name only a few. exploiting ml
for subsurface reservoir leakage detection and characterization is not
new. in the context of gcs monitoring, an application that is similar
to uhs, wang et al. [35] used a supervised ml method to infer
co2 leakage saturation based on time-lapse seismic data. however,
the study considers only acoustic-wave propagation as the underlying
physics, which, from a geophysical point of view, is far from sufficiency
to describe the complex time-lapse changes caused by co2 leakage as
the earth is essentially elastic, if not more complex like viscoelastic and
poroelastic with or without anisotropies. zhou et al. [36] developed a
supervised ml method based on convolutional neural network (cnn)
and long short-term memory (lstm) modules to infer co2 leakage
mass from time-lapse seismic data. in addition to a higher computational cost inherent to the lstm element, their method can only infer
leakage mass. there is also a class of methods that was developed to
infer co2 leakage and associated properties from post-stack seismic
image [17,37,38] but not from seismic data directly. leong et al. [20]
developed an ml method for estimating co2 saturation from timelapse waveforms, and validated the method using frio-ii field data.
this method, however, was developed in the context of cross-well monitoring system, which usually enjoys simpler and less noisy waveforms
compared with surface seismic surveys where reflection waves are the
main carrier of leakage signals. moreover, none of these methods is
developed to specifically handle sparse seismic data, and therefore does
not indicate a cost-effective prospect when applied to field applications.
these methods do not provide the inference capability to estimate the
spatial location of co2 , and it is unclear to what efficacy and accuracy
these methods can simultaneously locate the leakage and estimate its
leakage properties. for these reasons, it is unclear if these existing ml
models can be reliably applied to our unique problem (h2 leakage)
with a specific goal (simultaneous inference of leakage location and
leakage properties) using specific data (extremely sparse and/or noisy
time-lapse seismic data).

sparse time-lapse seismic data
cnn + dnn regression

â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†’
â† multiple leakage properties: ğ±min , ğ±max , ğ‘€, ğ‘‰ .
in this sense, our nn is a multi-task supervised nn that can infer
multiple critical quantities associated with a h2 leakage from uhs.
we generate the training data for this nn using a novel rock physics
model for h2 -saturated porous media and using a finite-differencebased elastic-wave equation solver to generate time-lapse seismic data.
to train a high-fidelity nn that is robust to random noise, one of most
significant factors that impacts time-lapse imaging and inversion, we
also add gaussian random noise to the training data and investigate
how random noise can affect the accuracy of our nn. for convenience,
we call our ml method hydrogennet thereinafter.
the rest of the paper will be organized as follows. in the methodology section, we will describe the architecture of hydrogennet in details,
including the architecture of both cnn and dnn modules; we will also
describe how we build the novel rock physics model to describe h2
leakage and how we generate high-quality dataâ€“label pairs for training
and validating hydrogennet. in the results section, we use a synthetic
uhs model to validate the efficacy and accuracy of our hydrogennet.
in particular, we will investigate how the number of permanent seismic
stations and the level of random noise may affect the accuracy of
hydrogennet. we summarize our findings in the conclusions.
2. methodology
2.1. the fundamental mechanism of hydrogennet
our hydrogennet is a supervised ml method with the input being
the time-lapse seismic waveforms, while the output includes the parameters associated with the potential h2 leakage. the entire workflow of
h2 consists of three essential steps:
1. training data generation: build the baseline and leakagestage elastic parameter models for the target area, and perform elastic-wave modeling using a finite-difference or spectralelement solver using the field survey sparse geometry;
2. training hydrogennet: train hydrogennet using the data,
where input is noisy or noise-free time-lapse waveform while
the output includes the parameters of leakage;
3. application to field data: prepare the field time-lapse waveform and feed it to the trained hydrogennet to obtain h2 leakage
information.
the process must be re-performed for each different uhs region
(different baseline geological models) and different observation geometry (including the spatial distribution of the permanent stations and
location of sources). in the following subsections, we will provide the
details regarding the above three steps.
2.2. h2 leakage rock physics model and data generation
our hydrogennet is a supervised nn and therefore relies on highquality training data to obtain high-accuracy generalization for practical applications. therefore, building realistic models, and in particularly, models that comply with the rock physics of h2 leakage, is
138

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 1. schematic of the h2 leakage characterization problem investigated in this work. we use red stars to represent sparse, repeatable seismic sources and use white dots to
represent sparse, permanent, multi-component seismic stations. time-lapse waveforms (i.e., the difference between the monitoring and baseline stage reflection waveforms) are
observed when h2 leaks from the h2 storage reservoir and causes changes of rock properties.

particularly important. we aim to build such a h2 leakage rock physics
model that we can relate the mass and volume of h2 leakage with the
change of seismic velocities (ğ‘‰ğ‘ and ğ‘‰ğ‘  ) of the host rock; with this
relationship, we can obtain differential seismic waveforms associated
with the h2 leakage based on the linear elastic wave equation (e.g.,
[40]).
we adopt a similar rock physics model for describing co2 leakage
in porous rocks [41] to quantify the elastic medium property changes
caused by h2 leakage. note that in the work we simulate h2 leakage
from a saline aquifer uhs reservoir sealed by some caprock layer on the
top. the leaked h2 escapes from the reservoir through activated faults
or stress-created fractures through the caprock, and enters into the
porous rocks above the caprock, altering the elastic medium properties
of these rocks. our rock physics model does not consider the h2 -brine
transport physics, the complex interactions among h2 , brine, and rock,
the injection/withdrawal rate and schemes, nor the wettability of a
uhs as discussed by, for instance, pan et al. [42], all of which can
significantly impact the h2 storage properties and therefore leakage
mechanism. therefore, the following model is only a simplified representation of h2 leakage and focuses more on the elastic medium
parameter changes rather than h2 diffusion process. our ml model
detailed in the next section, correspondingly, focuses on characterizing
these changes through seismic signals, rather than the state of a h2
reservoir itself.
firstly, we use the national institute of standards and technology
(nist) equation of state for h2 to calculate the corresponding sound
speed [43]. the bulk modulus and density of effective pore fluid is
estimated using inverse bulk modulus averaging [44] and arithmetic
averaging of densities of independent fluid phases involved in the
porous system, respectively [45]. specifically, we compute the effective
pore fluid bulk modulus ğ¾ğ‘“ and the effective pore fluid density ğœŒğ‘“ using
1 âˆ’ ğ‘†ğ‘”
ğ‘†ğ‘”
1
=
+
,
ğ¾ğ‘“
ğ¾ğ‘¤
ğ¾ğ‘”

(2)

ğœŒğ‘“ = (1 âˆ’ ğ‘†ğ‘” )ğœŒğ‘¤ + ğ‘†ğ‘” ğœŒğ‘” ,

where ğ‘†ğ‘” is the volume fraction of gas (h2 ) saturation, ğ¾ğ‘” is the bulk
modulus of h2 , ğ¾ğ‘¤ is the bulk modulus of brine, ğœŒğ‘” is the density of
h2 , and ğœŒğ‘¤ is the density of brine.
secondly, we estimate the bulk and shear moduli (ğ¾ğ‘š and ğœ‡ğ‘š )
of mineral grains by taking voigtâ€“reussâ€“hill averaging [46] of the
mineral constituents with published experimental moduli [47,48]. we
also assume 70% quartz and 30% clay and assume ğ¾ğ‘š and ğœ‡ğ‘š have negligible dependence on stress within near-surface reservoirs (e.g., uhs)
investigated in this work.
lastly, we determine the bulk and shear moduli of the rock framework with the hertzâ€“mindlin contact theory [49] and hashinâ€“strikman
(h-s) lower bound [50] to account for pressure dependence effects of a
cyclic stress environment on the h2 -saturated porous rocks. in specific,
we compute the h-s rock frame moduli (ğ¾ğ»ğ‘† and ğœ‡ğ»ğ‘† ) based on
]1âˆ•3
[
2ğ‘ƒ
ğ¶ 2 (1 âˆ’ ğœ™ğ‘ )2 ğœ‡ğ‘š
ğ‘‘
,
(3)
ğ¾ğ‘ =
18ğœ‹ 2 (1 âˆ’ ğœˆ)2
[
]1âˆ•3
2
2 2
5 âˆ’ 4ğœˆ 3ğ¶ (1 âˆ’ ğœ™ğ‘ ) ğœ‡ğ‘š ğ‘ƒğ‘‘
ğœ‡ğ‘ =
,
(4)
10 âˆ’ 5ğœˆ
2ğœ‹ 2 (1 âˆ’ ğœˆ)2
(
)
ğœ‡
9ğ¾ğ‘ + 8ğœ‡ğ‘
ğœ‰= ğ‘
,
(5)
6
ğ¾ğ‘ + 2ğœ‡ğ‘
âˆ’1

â¡ ğœ™âˆ•ğœ™
1 âˆ’ ğœ™âˆ•ğœ™ğ‘ â¤â¥
4
ğ‘
ğ¾ğ»ğ‘† = â¢
+
âˆ’ ğœ‡ğ‘ ,
4
â¢ğ¾ + 4ğœ‡
â¥
3
ğ¾ğ‘š + 3 ğœ‡ğ‘ â¦
â£ ğ‘ 3 ğ‘
[
]âˆ’1
ğœ™âˆ•ğœ™ğ‘
1 âˆ’ ğœ™âˆ•ğœ™ğ‘
ğœ‡ğ»ğ‘† =
+
âˆ’ ğœ‰,
ğœ‡ğ‘ + ğœ‰
ğœ‡ğ‘š + ğœ‰

(6)

(7)

where ğœ™ğ‘ is the critical porosity, ğ¶ = 2.8âˆ•ğœ™ğ‘ is the average number of
contacts per spherical grain, ğ‘ƒğ‘‘ is the differential pressure between the
confining pressure and the pore pressure, ğœˆ is the poissonâ€™s ratio of the
rock, and ğœ™ is the porosity.

(1)
139

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 2. a schematic of the architecture of hydrogennet used for inferring uhs leakage properties from sparse seismic waveforms. â€˜â€˜conv (ğ‘, ğ¶ = ğ‘›1 â†’ ğ‘›2 )â€™â€™ is a 2d convolutional
layer with a kernel size of 3, a spatial size of ğ‘, and a total of ğ¶ = ğ‘›1 input channels and ğ¶ = ğ‘›2 output channels, following the notation of pytorch [51]. â€˜â€˜reluâ€™â€™ is a rectified
linear unit layer, â€˜â€˜bnâ€™â€™ is a batch normalization layer. definitions of these layers can be found in, for example, murphy [39]. the adaptive average pooling layer [52] transforms
any input features to a desired size (16 Ã— ğ‘ğ‘ ğ‘ğ‘  ğ‘âˆ•16 for hydrogennet).

with these rock physics and fluid physics models, the saturated bulk
and shear moduli (ğ¾ğ‘  and ğœ‡ğ‘  ) of a porous rock after h2 saturation read
)âˆ’1
(
) (
ğ¾ğ»ğ‘† 2
1 âˆ’ ğœ™ ğ¾ğ»ğ‘†
ğœ™
ğ¾ğ‘  = ğ¾ğ»ğ‘† + 1 âˆ’
+
âˆ’
,
(8)
ğ¾ğ‘š
ğ¾ğ‘“
ğ¾ğ‘š
ğ¾ğ‘š2

elastic-wave equations in the first-order velocity-stress form to generate
synthetic seismic data. in this work, we only consider the 2d case,
where the elastic-wave equations read [40]:

(9)

(14)

ğœ‡ğ‘  = ğœ‡ğ»ğ‘† .

ğœ•ğœğ‘¥ğ‘¥
ğœ•ğ‘£
ğœ•ğ‘£
= ğ¶11 ğ‘¥ + ğ¶13 ğ‘§ ,
ğœ•ğ‘¡
ğœ•ğ‘¥
ğœ•ğ‘§
ğœ•ğœğ‘§ğ‘§
ğœ•ğ‘£ğ‘¥
ğœ•ğ‘£ğ‘§
= ğ¶13
+ ğ¶33
,
ğœ•ğ‘¡
ğœ•ğ‘¥
ğœ•ğ‘§)
(
ğœ•ğ‘£ğ‘¥ ğœ•ğ‘£ğ‘§
ğœ•ğœğ‘¥ğ‘§
= ğ¶55
+
,
ğœ•ğ‘¡
ğœ•ğ‘¥
ğœ•ğ‘§
(
)
ğœ•ğ‘£ğ‘¥
ğœ•ğœğ‘¥ğ‘¥ ğœ•ğœğ‘¥ğ‘§
= ğœŒâˆ’1
+
+ ğ‘“ğ‘¥ (ğ‘¡)ğ›¿(ğ± âˆ’ ğ±ğ‘  ),
ğœ•ğ‘¡
ğœ•ğ‘¥
ğœ•ğ‘§
(
)
ğœ•ğ‘£ğ‘§
ğœ•ğœğ‘¥ğ‘§ ğœ•ğœğ‘§ğ‘§
= ğœŒâˆ’1
+
+ ğ‘“ğ‘§ (ğ‘¡)ğ›¿(ğ± âˆ’ ğ±ğ‘  ),
ğœ•ğ‘¡
ğœ•ğ‘¥
ğœ•ğ‘§

in addition, the effective density can be computed as
ğœŒğ‘  = (1 âˆ’ ğœ™)ğœŒğ‘Ÿ + ğœ™ğœŒğ‘“ ,

(10)

where ğœŒğ‘Ÿ is the density of the rock frame. the apparent seismic velocities in the h2 -saturated porous rock can then be obtained straightforwardly (e.g., [44]):
â› ğ¾ğ‘  + 4 ğœ‡ğ‘  â
3
âŸ
ğ‘‰ğ‘ = âœ
âœ
âŸ
ğœŒğ‘ 
â
â 
( )1âˆ•2
ğœ‡ğ‘ 
ğ‘‰ğ‘  =
.
ğœŒğ‘ 

(13)

(15)
(16)
(17)

1âˆ•2

,

where ğ¯ = (ğ‘£ğ‘¥ , ğ‘£ğ‘§ ) is the partial velocity vector, ğˆ = (ğœğ‘¥ğ‘¥ , ğœğ‘§ğ‘§ , ğœğ‘¥ğ‘§ )t
is the stress tensor, ğ¶ğ‘–ğ‘— â€™s are the spatially varying elasticity constants,
ğœŒ is the spatially varying mass density, ğŸ(ğ‘¡)ğ›¿(ğ± âˆ’ ğ±ğ‘  ) = (ğ‘“ğ‘¥ , ğ‘“ğ‘§ ) is the
external force vector located at ğ± = ğ±ğ‘  . we consider isotropic elastic
medium, where ğ¶11 = ğ¶33 = ğœŒğ‘‰ğ‘2 , ğ¶55 = ğœŒğ‘‰ğ‘ 2 , and ğ¶13 = ğ¶11 âˆ’ 2ğ¶55 =
(
)
ğœŒ ğ‘‰ğ‘2 âˆ’ 2ğ‘‰ğ‘ 2 .
we use a high-order version of the standard staggered-grid finitedifference scheme [53] to solve the above elastic-wave equations. we
use complex frequency shifted perfected matched layers in an auxiliary

(11)

(12)

given a baseline or leakage-stage elastic medium model (including
ğ‘‰ğ‘ , ğ‘‰ğ‘  , and ğœŒ) built using the above rock physics theory, the key is to
relate the velocity and density changes induced by h2 leakage with
the observed seismic waveforms. to achieve this goal, we use the
140

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 3. (a) ğ‘‰ğ‘ , (b) ğ‘‰ğ‘  , and (c) density of the baseline uhs model. the dark-blue region at 1.3 km in depth is a h2 storage reservoir. (for interpretation of the references to color
in this figure legend, the reader is referred to the web version of this article.)

fig. 4. four examples of leakage-stage ğ‘‰ğ‘ models for generating the validation dataset.

differential equation form [54] to absorb outgoing wavefields at the
domain boundaries, including the top boundary. therefore, the generated data in our tests do not contain surface waves. in practice however,
depending on the complexity of near-surface conditions, surface waves
can possess strong energy that may leak into time-lapse waveforms.
in such a case, some careful data processing might be needed to
minimize the influence of surface waves on training and application
of hydrogennet.

the time-lapse seismic data ğ›¥ğ can then be generated using
ğ›¥ğ = ğleakage âˆ’ ğbaseline + ğœ†ğ°,

(18)

where ğleakage is the leakage stage data, ğbaseline is the baseline stage
data, ğœ† is a weighting factor indicating the amplitude of noise, and ğ° is
gaussian random noise that is generated using the following procedure:
ğ° â† îˆº (0, 1),
141

(19)

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 5. the ğ‘‰ğ‘ changes caused by h2 leakage between the leakage-stage models displayed in fig. 4aâ€“d and the baseline model displayed in fig. 3a. the time-lapse changes of ğ‘‰ğ‘ 
and ğœŒ models have the same spatial pattern although different values and/or units.

ğ° â† îˆ³ğœ=(1,0) [ğ°],

(20)

ğ° â† ğ° âˆ’ ğœ‡(ğ°),
ğ°
ğ°â†
,
max{|ğ°|}

(21)

ğ° â† max{|ğleakage âˆ’ ğbaseline |}ğ°,

(23)

(22)

where â† represents an elementwise value assignment, îˆº (0, 1) represents a gaussian distribution with a mean of 0 and a standard deviation
of 1, îˆ³ğœ=(1,0) [â‹…] represents a 2d gaussian smoothing with ğœ = 1 (with a
unit of sample) in the temporal axis while ğœ = 0 (i.e., no smoothing) in
the horizontal direction. in addition, ğœ‡(ğ°) represents the mean value of
ğ°, max{[â‹…]} represents the maximum value of the elementwise absolute
value correspondence to a field. consequently, the level of noise added
to noise-free training data can be solely determined by the scalar value
ğœ†. the case where ğœ† = 0 indicates the training data are noise-free. in
this work, all the fields in eq. (18), including ğleakage , ğbaseline , and ğ°,
have the same dimension of ğ‘1 Ã— ğ‘2 = ğ‘ğ‘¡ Ã— (ğ‘ğ‘ ğ‘ğ‘  ğ‘ğ‘Ÿ ) with ğ‘ğ‘¡ = 256.
2.3. network architecture of hydrogennet and loss function
our hydrogennet is a lightweight, supervised nn that consists of
two parts: a convolutional neural network (cnn) module and a dense
neural network (dnn) module connected by an adaptive average layer
in between. the architecture of hydrogennet is displayed in fig. 2.
the input to hydrogennet is a single-channel 2d image with a size
of ğ‘ğ‘¡ Ã— ğ‘ğ‘ ğ‘ğ‘  ğ‘ğ‘Ÿ , where ğ‘ğ‘¡ = 256 represents the number of samples
along the temporal axis (the vertical dimension), ğ‘ğ‘ is the number of
channels (2 for 2d case and 3 for 3d case), ğ‘ğ‘  is the number of sources,
and ğ‘ğ‘Ÿ is the number of monitoring stations. a relatively small value
of ğ‘ğ‘¡ , 256, is adopted in this work to resample time-lapse waveforms
along the temporal axis. our trial-and-error tests show that ğ‘ğ‘¡ does
not play a significant role in impacting the inference accuracy, and
other values could be used as long as the resampled data can properly
represent the original time-lapse waveforms. however, larger ğ‘ğ‘¡ can
lead to higher computational cost in both the training and validation
phases.

fig. 6. seismic data generated in the baseline uhs model displayed in fig. 3. the
data contain two components (ğ‘£ğ‘¥ on the left and ğ‘£ğ‘§ on the right).

the input image then goes through a series of 2d convolutional
layers, where the numbers of features increase while the sizes of the
features decrease. each of the convolutional layers is followed by a
rectified linear unit (relu) activation layer and a batch normalization
layer (e.g., [39]). the intermediate conv units also include max pooling
layers with a kernel size of 2 to reduce the size of the features. the
feature maps are then transferred to an adaptive average pooling layer,
resulting in a feature map with a size of ğ‘ = 16 Ã— ğ‘ğ‘ ğ‘ğ‘  ğ‘ğ‘Ÿ âˆ•16 and
ğ¶ = 256 channels.
the feature maps are then flattened and regressed by three dense
layers to six scalar values that represent the properties of a h2 leakage,
142

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 7. (aâ€“d) leakage-stage seismic data generated for the four models displayed in fig. 4, overlain by the time-lapse amplitude changes (displayed in absolute value and jet
colormap with a transparent lower end). the csg is the third of the eight csgs. (for interpretation of the references to color in this figure legend, the reader is referred to the
web version of this article.)

3. results

including the spatial range of the leakage (ğ‘¥min , ğ‘¥max , ğ‘§min and ğ‘§max ),
the leakage mass ğ‘€, and the leakage volume ğ‘‰ . for 3d applications,
there are two additional properties, ğ‘¦min and ğ‘¦max , which describe the
spatial range of the leakage along the ğ‘¦-axis.
we use a ğ¿2 -norm loss function to train hydrogennet:
îˆ¸ = (ğ‘¦Ì‚ âˆ’ ğ‘¦)2 ,

we use a synthetic uhs model to validate the efficacy and accuracy
of our hydrogennet. the 2d model, as displayed in fig. 3, contains a
sandstone aquifer with 30% porosity at 1000 m deep and is approximately 100 m thick. above the aquifer, there is a caprock that is 50 m
thick with a low porosity (5%). a uhs â€“ a 50 m thick layer with an
80% saturation within the porous sandstone â€“ is created in this model
by surface h2 injection. for the overlaying burden, we create a series
of layers with random thickness and porosity. in addition, we assume
an average geothermal temperature gradient in the crust of 20 â—¦ c per
1000 m. for pressure, we assume an overlying atmospheric pressure
of 1 atm (i.e., 101,325 pa) and an average density of 2450 kg/m3 . for
pore pressure, we use the standard hydrostatic pressure for fresh water,
i.e., 9.8 Ã— 103 pa/m.
based on the quantitative relationship between h2 leakage and
elastic property changes in overburden rocks, we build 1000 leakage-

(24)

Ì‚ ğ‘‰Ì‚ ) represents the ground-truth leakwhere ğ‘¦Ì‚ = (ğ‘¥Ì‚ min , ğ‘¥Ì‚ max , ğ‘§Ì‚ min , ğ‘§Ì‚ max , ğ‘€,
age properties, and ğ‘¦ represents hydrogennet-predicted predictions.
because we use a sigmoid activation layer at the end of the regression
as displayed in fig. 2, we normalize the ground-truth values, ğ‘¦, using
the model dimensions and the upper limits of ğ‘€ and ğ‘‰ , respectively,
when preparing the training data. the strategy ensures the labels and
predictions are within in the range of [0, 1]. after prediction, we scale
the predicted values back to the physical domain to obtain interpretable
results.
143

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 8. (aâ€“d) time-lapse pp images associated with the four leakage models displayed in fig. 5 generated using ğ‘ğ‘  = 8, ğ‘ğ‘Ÿ = 4, and ğ‘ğ‘ = 2. black curves are ground-truth profiles
of the random h2 leakages. the images are generated using all csgs. (for interpretation of the references to color in this figure legend, the reader is referred to the web version
of this article.)

stage models that contain simulated h2 leakage. the leakage in each
model has a random shape that is constructed using bÃ©zier curves [55],
has a random location, and has a random size varying from 30 to
300 m. fig. 4aâ€“d show four of the 1000 random leakage models that
contain shallow to deep and small to large amount of h2 leakage
cases. note that here the shape of the h2 leakage (or equivalently,
the induced medium parameter changes) is not computed through
the actual leakage-diffusion physics governing equation. however, the
randomness of these shapes is a proxy for us to mimic a wide range
of possibilities corresponding to actual leakage in a computationally
efficient way.
using the elastic-wave equation solver described in the methodology section, we generate the baseline seismic waveforms displayed
in fig. 6, and leakage-stage seismic waveforms displayed in fig. 7.
the wave energy, displayed as colorized absolute values of the timelapse waveform amplitude, evidently show that different leakages cause
different spatialâ€“temporal distribution of time-lapse waveform energy
in the data. for instance, comparing fig. 7b and d, we can know that
shallower leakage may cause the time-lapse waveform energy concentrating in the early arrivals. comparing fig. 7a and b, we can know
that a larger-area leakage causes more time-lapse waveform energy
difference w.r.t. the baseline data. these differences are essential for
hydrogennet to learn the spatialâ€“temporal characteristics of time-lapse
waveforms for leakage detection and characterization.
we generate time-lapse waveforms by subtracting the baseline
waveforms from the leakage-stage waveforms. the waveforms are then
resampled in the temporal domain and selected from the total ğ‘ğ‘ Ã—
ğ‘ğ‘  Ã— ğ‘ğ‘Ÿ = 2 Ã— 8 Ã— 400 traces to compose the training data. in the
first test, we select four traces from each common-shot gather (csg)
of each component of the time-lapse waveforms displayed in fig. 7 to
form a subset of the time-lapse data. the subset therefore contains eight
csgs, where each csg contains fours traces, and each trace contains
two components (ğ‘£ğ‘¥ and ğ‘£ğ‘§ ).
because of the linearity of small perturbation imaging in the context
of seismic migration, we perform time-lapse imaging directly using
the four time-lapse waveforms based on vector elastic reverse-time

migration (ertm) [56]. fig. 8a to d show the time-lapse images
associated with the four leakage models displayed in fig. 5. each of
these four images is a summation from eight csg images. in addition,
we use black curves to indicate the ground-truth spatial locations of
leakages. it is evident that because there are only four traces in each
csg, which is far below the data density requirement of conventional
imaging methods, the time-lapse images are full of strong, coherent
migration artifacts. although in some cases (e.g., fig. 8c), it is possible
to somehow interpret the location of leakage based on the amplitude of
the time-lapse image, in other cases, however, it is very difficult, if not
impossible, to perform reliable interpretation of the leakage location.
a natural question may arise by analyzing the results displayed in
fig. 8: if we increase the number of seismic stations for time-lapse
seismic imaging, will we obtain a more interpretable image? fig. 9
displays the time-lapse pp images generated using the data acquired
at all the 400 seismic receivers. that is, the data is 100 times denser
that those for generating the time-lapse images in fig. 8. we display
the time-lapse images in a blueâ€“red colormap while the ground-truth
locations of leakage using black curves. although in this case imaging
artifacts are attenuated because of improved image stacking (more
receivers are equivalent to more sources according to the reciprocity
principle (e.g., [40])), determining the locations of leakage from these
images is still challenging. for instance, it is still impossible to tell
where the leakage is in fig. 9b â€“ there are at least two strong reflectors
in this image, one at approximately 0.7 km, the other at approximately
1.4 km. in addition, it is impossible to tell the horizontal range of
the leakage from this image. the other images have similar level of
interpretability. the results indicate that even with 100 times denser
data acquisition and by using state-of-the-art imaging method, it can
be challenging to determine the location of h2 leakage on time-lapse
seismic images generated based on sparse time-lapse data with high
fidelity.
by contrast, for our hydrogennet method, we split the 1000 timelapse data into two groups: a training set that contains 900 data, and a
validation set that contains 100 data. there is no overlapping between
these two groups during training to ensure an unbiased training. we
144

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 9. (aâ€“d) time-lapse pp images associated with the four leakage models displayed in fig. 5 generated using ğ‘ğ‘  = 8, ğ‘ğ‘Ÿ = 400, and ğ‘ğ‘ = 2. black curves are ground-truth
profiles of the random h2 leakages. the images are generated using all csgs.
table 1
a comparison between the ground-truth and predicted leakage mass and volume (in
their respective scaled forms) for the four models displayed in fig. 4 with ğ‘ğ‘Ÿ = 4
permanent stations.
model

ground-truth ğ‘€

inferred ğ‘€

ground-truth ğ‘‰

inferred ğ‘‰

1
2
3
4

2.779514eâˆ’1
6.001993eâˆ’1
7.925413eâˆ’1
4.745404eâˆ’1

3.255999eâˆ’1
5.998499eâˆ’1
7.754422eâˆ’1
4.879152eâˆ’1

3.878758eâˆ’1
7.382875eâˆ’1
8.164659eâˆ’1
5.008962eâˆ’1

4.600800eâˆ’1
7.457242eâˆ’1
8.048993eâˆ’1
5.301056eâˆ’1

result in fig. 11a is too narrow and misses the ground truth, the
other predictions are in good agreement with the ground truth leakage
regions, even for the fourth leakage model where the leakage region is
very small. regarding the leakage properties, the scaled ground-truth
leakage mass and volume for the first model (displayed in fig. 5a) are
Ì‚ ğ‘‰Ì‚ ) = (2.779514 Ã— 10âˆ’1 , 3.878758 Ã— 10âˆ’1 ), while the hydrogennet
(ğ‘€,
predictions are (ğ‘€, ğ‘‰ ) = (3.255999 Ã— 10âˆ’1 , 4.600800 Ã— 10âˆ’1 ). we compare
the scaled ground-truth and prediction values for these four models
in table 1. the results indicate that our hydrogennet can produce
reasonably accurate estimation on the leakage mass and volume in
addition to the spatial location of leakage simultaneously.
additionally, we display the hydrogennet prediction results based
on 16 stations (i.e., ğ‘ğ‘Ÿ = 16) for these four leakage models in fig. 12.
with more stations and therefore more data for the training, the prediction accuracy improves over that associated with ğ‘ğ‘Ÿ = 4, particularly
for the first and the fourth models. in fact, hydrogennet produces
a very accurate estimation of leakage location for the first model as
displayed in fig. 12a in both the vertical and horizontal directions. for
the fourth model, the estimated leakage range is visually identical to
the ground truth. the results evidently demonstrate that hydrogennet
can estimate h2 leakage location using extremely sparse time-lapse
seismic data with a high accuracy, although it cannot delineate the
actual shape of a leakage.
to further investigate the efficacy and accuracy of hydrogennet,
particularly the robustness to noise in the data, we perform the following series of tests for all the random leakage models and the associated
training and validation data.

fig. 10. training (solid) and validation (dashed) loss convergence curves for the case
of ğ‘ğ‘Ÿ = 4. (for interpretation of the references to color in this figure legend, the
reader is referred to the web version of this article.)

train hydrogennet using these data using an adam optimizer [57] with
a learning rate of 1Ã—10âˆ’5 . we display the loss convergence curves associated with the training and validation using blue solid and blue dashed
curves in fig. 10. each epoch of the training takes approximately 5 s on
a nvidia rtx 3090 graphics processing unit (gpu) card. the inference
phase for each input image takes approximately 0.0083 s (0.01 s for
the ğ‘ğ‘Ÿ = 8 case and 0.015 s for the ğ‘ğ‘Ÿ = 16 case), therefore can be
considered as real-time.
we display the hydrogennet prediction results in fig. 11 based on
a set of hydrogennet weights trained using ğ‘ğ‘  = 8, ğ‘ğ‘Ÿ = 4, and
ğ‘ğ‘ = 2. we use blue regions to indicate the ground-truth leakages,
while red rectangles to indicate hydrogennet predictions. note that
hydrogennet outputs four values (ğ‘¥min , ğ‘¥max , ğ‘§min , ğ‘§max ) associated with
the leakage location based on our design, and these four values define a
rectangle (or a cuboid in 3d with six values). except the first prediction
145

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 11. (aâ€“d) ground-truth leakages (represented as blue regions) and hydrogennet predictions (represented as red rectangles) associated with the four leakage models displayed
in fig. 5 generated using ğ‘ğ‘  = 8, ğ‘ğ‘Ÿ = 4, and ğ‘ğ‘ = 2. all the four models are associated with time-lapse data in the validation dataset therefore cannot be seen by the training.

fig. 12. (aâ€“d) ground-truth leakages (represented as blue regions) and hydrogennet predictions (represented as red rectangles) associated with the four leakage models displayed
in fig. 5 generated using ğ‘ğ‘  = 8, ğ‘ğ‘Ÿ = 16, and ğ‘ğ‘ = 2. all the four models are associated with time-lapse data in the validation dataset therefore cannot be seen by the training.

fig. 13 shows three examples of training data with ğ‘ğ‘Ÿ = 4 associated
with three different random leakage models. the waveform traces are
sorted by component (ğ‘£ğ‘¥ and ğ‘£ğ‘§ ), source index (from 1 to 8), and
receiver index (from 1 to 4). we resampled the data along the temporal
axis to 256 samples. fig. 13d and e display the hydrogennet inference
results for the training and validation datasets, respectively, for the six

target quantities, including ğ‘¥min , ğ‘¥max , ğ‘§min , ğ‘§max , ğ‘€, and ğ‘‰ . to quantify
the accuracy of the inference results, we also display the ğ‘…2 scores
associated with the inference of each quantity. the ğ‘…2 score, or the
coefficient of determination, is
âˆ‘ğ‘
(ğ‘¦Ì‚ğ‘– âˆ’ ğ‘¦ğ‘– )2
,
(25)
ğ‘…2 = 1 âˆ’ âˆ‘ğ‘ğ‘–=1
Ì‚ 2
ğ‘–=1 (ğ‘¦Ì‚ğ‘– âˆ’ ğœ‡(ğ‘¦))
146

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 13. (aâ€“c) three examples of noise-free training data (i.e., time-lapse seismic-waveform changes) with ğ‘ğ‘Ÿ = 4, and (dâ€“e) training and validation results measured by an ğ‘…2
score (higher is better). the red dashed line in panels (d) and (e) represents the ğ‘…2 = 1 line where a prediction is exactly equal to the ground-truth value.

where ğ‘¦ğ‘– are the estimated values, ğ‘¦Ì‚ğ‘– are the ground-truth values,
and ğœ‡(ğ‘¦)
Ì‚ is the mean of the ground-truth values. ğ‘…2 = 0 means no
correlation between predicted and ground-truth values, while ğ‘…2 = 1
indicates a perfect correlation.
we find that for the training dataset, all the six ğ‘…2 scores are
high (close to 1), indicating a successful training. the more important
implication of the successful training is that there exists a definitive
mapping (although perhaps highly nonlinear) between the time-lapse
waveforms and the six quantities that define a h2 leakage scenario. for
the validation data that are not in the training dataset, we observe good
consistency between the ground-truth values and hydrogennet inference results. the accuracy of the predictions for different parameters
are ğ‘¥min , ğ‘¥max > ğ‘§min , ğ‘§max > ğ‘€, ğ‘‰ . we can interpret the results from an
intuitive point of view: because we use surface seismic data as training
data, the estimation of leakage range in the horizontal direction can
generally be better than that in the vertical direction. given another
type of observation system (e.g., vertical seismic profiling system),
the result could be reversed. meanwhile, the estimation of spatial
range of h2 leakage is better than that of leakage mass or volume,
because leakage mass and volume change medium properties more
â€˜â€˜implicitlyâ€™â€™ than the leakage range â€“ a different leakage range usually
results in very different time-lapse waveforms in either amplitude
or spatial characteristics of energy distribution. a different leakage
mass/volume, however, may not necessarily correspond to strikingly

different medium property changes that result in sufficient time-lapse
waveform changes.
we now train and validate hydrogennet using noisy time-lapse
data. fig. 14aâ€“c display three examples of noisy data in the training
dataset, to which we add a mild level of gaussian random noise to the
noise-free data displayed in fig. 13aâ€“c by drawing ğœ† from a uniform
random distribution î‰ (0, 13 ) for different training data, where 0 and 13
are the lower and upper bound of this uniform distribution. fig. 14d
and e display the training and validation results associated with these
weakly noisy data. judging from the ğ‘…2 scores associated with the
inferences for the training and validation data, we again find that the
accuracy of inferences follows a similar pattern to the noise-free case:
ğ‘¥min , ğ‘¥max > ğ‘§min , ğ‘§max > ğ‘€, ğ‘‰ , but in this case the differences among
the three tiers are larger. by adding a higher level of random noise to
the data where we draw ğœ† from a uniform random distribution î‰ ( 13 , 23 )
for different training data, we obtain the training and validation results
displayed in fig. 15. note that the noise level in this case is very
high, making it difficult or even impossible to identify the effective
time-lapse signals using visual inspection, as are shown in fig. 15aâ€“
c. not surprisingly, the ğ‘…2 scores in this case are worse than those
in fig. 14, especially for the leakage mass and volume estimations.
nevertheless, we still observe an accurate estimation of the horizontal
range of leakage, and a decent estimation of the vertical range of
147

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 14. (aâ€“c) three examples of mildly noisy training data with ğ‘ğ‘Ÿ = 4, and (dâ€“e) training and validation results.

results with ğ‘ğ‘Ÿ = 8 stations. compared with the case of ğ‘ğ‘Ÿ = 4
displayed in fig. 13, we find the inference accuracy of all the six
parameters of the validation dataset is improved. this is consistent with
our intuition: more seismic data and therefore more data provide better
constraints in learning and inference, leading to better generalization.
higher data density also results in improved accuracy for noisy data
as shown in figs. 18 and 19. the inference accuracy can be further
improved by using ğ‘ğ‘Ÿ = 16 traces (therefore a total of 256 traces of
waveforms), as displayed in figs. 21â€“23. with ğ‘ğ‘Ÿ = 16, our hydrogennet can generate high-accuracy estimation for all the six parameters
even with strong noise in the training data.
again, we display the bar charts of misfits associated with the six
parameters of the validation dataset in the noise-free, mildly noisy,
and strongly noisy cases in fig. 20a, b, and c, respectively, which
are consistent with the correlation plots in figs. 17d, 18d, and 19d,
respectively. the influence of random noise on the inference results
resembles that in the ğ‘ğ‘Ÿ = 4 case displayed in fig. 16.
we further display the distribution of misfits associated with the six
parameters of the validation dataset in the noise-free, mildly noisy, and
strongly noisy cases in fig. 24a, b, and c, respectively, consistent with
the correlation analyses in figs. 21c, 22c, and 23c, respectively. again,
an increased level of random noise deteriorates the inference accuracy.
however, compared with the error distributions displayed in figs. 16

leakage, thanks to the powerful learning capability of hydrogennet
built upon the hybrid 2d cnn and dnn architecture.
we display the distribution of misfits associated with the six parameters of the validation dataset in the noise-free, mildly noisy and
strongly noisy cases in fig. 16a, b, and c, respectively. these distributions are consistent with the correlation plots in figs. 13e, 14e, and
15e, respectively. in all the three cases, the predictions of horizontal
range are the best, while the predictions of vertical range are less
accurate as manifested by a wider range of error distribution. the
predictions of leakage mass and volume are the worst and have the
widest error distribution. the maximum error appears in the volume
prediction, which is 0.254. we consider this deviation being acceptable
in this multi-value regression problem due to the complexity of timelapse waveforms. in these plots, we also âˆš
display the fitted gaussian in
the form of ğ‘“ (ğ‘¥) = exp[âˆ’(ğ‘¥ âˆ’ ğœ‡)2 âˆ•2ğœ 2 ]âˆ•ğœ 2ğœ‹ of the prediction errors,
where a wider gaussian indicates a larger standard deviation ğœ (thus
less accurate predictions) and vice versa. we observe that for each
parameter, stronger noise in the data will result in wider gaussian
(poorer prediction), while the prediction of horizontal leakage range
has narrower gaussian (better prediction) than that associated with the
vertical range, leakage mass, and leakage volume.
we now investigate how the number of permanent seismic stations
affect the training and inference accuracy of hydrogennet. fig. 17 displays two examples of noise-free training data, training and validation
148

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 15. (aâ€“c) three examples of strongly noisy training data with ğ‘ğ‘Ÿ = 4, and (dâ€“e) training and validation results.
table 2
a comparison of ğ‘…2 scores associated with different validation datasets under different
levels of noise. higher values are better.

and 20, it is straightforward to find that more permanent stations result
in an improved inference accuracy.
to summarize the results, we use table 2 to compare the ğ‘…2 scores
of different validation datasets in the above tests. the values marked
by a bond font indicate the best ğ‘…2 scores under different noise levels.
it is evident that by using more stations, the inference accuracy can
be improved. meanwhile, a higher noise level in the training data can
result in reduced accuracy in the generalization phase.

parameter

noise-free

weak noise

strong noise

ğ‘ğ‘Ÿ = 4

ğ‘¥min
ğ‘¥max
ğ‘§min
ğ‘§max
ğ‘€
ğ‘‰

0.995
0.996
0.969
0.974
0.876
0.846

0.992
0.993
0.933
0.941
0.794
0.739

0.987
0.987
0.837
0.874
0.673
0.544

ğ‘ğ‘Ÿ = 8

ğ‘¥min
ğ‘¥max
ğ‘§min
ğ‘§max
ğ‘€
ğ‘‰

0.997
0.997
0.985
0.983
0.926
0.905

0.995
0.995
0.958
0.964
0.870
0.830

0.993
0.992
0.907
0.917
0.721
0.621

ğ‘ğ‘Ÿ = 16

ğ‘¥min
ğ‘¥max
ğ‘§min
ğ‘§max
ğ‘€
ğ‘‰

0.998
0.998
0.989
0.983
0.947
0.935

0.995
0.995
0.972
0.968
0.895
0.861

0.991
0.990
0.943
0.940
0.809
0.747

4. discussions
the proceeding section has demonstrated the efficacy and accuracy of our ml method for detecting and characterizing h2 leakage
from uhs based on very sparse seismic data. note that by conventional seismic imaging/inversion methods, it is possible to delineate
the actual shape the time-lapse changes of earth medium induced by
h2 leakage, but it usually requires dense data coverage to achieve
sufficient accuracy and interpretability. by hydrogennet, however, it
is only possible to know the location (measured as minâ€“max range
values) and leakage properties of a leakage, yet we have demonstrated
that it requires significantly sparser data coverage than conventional
seismic imaging/inversion methods to achieve reliable leakage location
and characterization. we summarize the advantages and disadvantages
of hydrogennet compared with conventional imaging and inversion
methods in table 3.

it is curious to know if there are alternative or even better ml
architectures to hydrogennet developed in this work for the h2 leakage detection and characterization problem. while there are many
149

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 16. (aâ€“c) statistics of parameter prediction errors associated with the validation dataset with ğ‘ğ‘Ÿ = 4 in the noise-free, mildly noisy, and strongly noisy cases, respectively.
the error range shows the minimum and maximum values of the prediction errors associated with the parameters. red dashed curves in these plots represent fitted gaussian with
a mean of ğœ‡ and a standard deviation of ğœ displayed in the title of each panel.

table 3
a comparison between conventional seismic imaging/inversion methods and our hydrogennet for h2 leakage detection and characterization.
metric

conventional imaging and inversion

hydrogennet

required data coverage

dense; usually hundreds or thousands of sources and stations
are required

very sparse; only a few sources and stations are
required

leakage location
estimation

actual shape (and therefore location) of time-lapse medium
changes

spatial range represented as minâ€“max values

leakage mass/volume
estimation

indirect and usually difficult based solely on seismic
waveform data

straightforward

computational cost

high, particularly for 3d

low to moderate; effort is mainly spent on training
data generation

adaptability

any model (with the same governing equation for wave
propagation) and any observation system; developed software
is mostly applicable to a different survey

fixed model and observation system; requires
re-generation of training data and re-training for a
different baseline model or observation system

ml models that can perform multi-value regression, a straightforward
alternative may consider a time-lapse seismic waveform as a 1d time
series, rather than a 2d image as in our hydrogennet. in other words,
one could design an architecture as displayed in fig. 25, where the
input is a 2d tensor with a size of 256 and a total of ğ‘ğ‘ ğ‘ğ‘  ğ‘ğ‘Ÿ channels,
instead of a 3d tensor with a size of 256 Ã— (ğ‘ğ‘ ğ‘ğ‘  ğ‘ğ‘Ÿ ) but with one
channel input to hydrogennet as displayed in fig. 2. in such a case,
the cnn part in this 1d variant of hydrogennet is a 1d cnn instead
of 2d cnn. for convenience, we name this nn the hydrogennet-1d.
we now train and validate hydrogennet-1d using the same training
and validation datasets as for hydrogennet. we only choose the case
noise-free data with ğ‘ğ‘Ÿ = 4 for a comparison. fig. 26a and b display the
correlation between the ground-truth and predicted parameter values
and the statistics of parameter prediction errors associated with the
validation dataset, respectively. while the predictions of the vertical
range (ğ‘§min and ğ‘§max ) have the same level of accuracy compared with
hydrogennet as displayed in fig. 13e, the accuracy associated with
the predictions of horizontal range, mass, and volume is notably lower

compared with that associated with hydrogennet. for instance, using hydrogennet, the ğ‘…2 scores associated with ğ‘¥min and ğ‘¥max are
higher than 0.99 and close to 1 (perfect prediction), while the scores
are merely 0.916 and 0.896 using hydrogennet-1d. meanwhile, the
distributions of misfits generated by hydrogennet-1d as displayed in
fig. 26b are notably wider than those generated by hydrogennet
as displayed in fig. 16a, especially for ğ‘¥min , ğ‘¥max , ğ‘€ and ğ‘‰ . the
results indicate that with the same dataset, hydrogennet is more
accurate in predicting the spatial location and leakage properties compared with hydrogennet-1d. the reason behind this difference could
be qualitatively analyzed: hydrogennet leverages 2d cnn to learn
the spatial features embedded in the time-lapse seismic waveforms,
while hydrogennet-1d focuses more on the temporal features of the
waveforms. as indicated by the data plots in fig. 7 using colors,
hydrogennet-1d may miss important information embedded in the
spatial pattern of time-lapse waveforms, which by contrast can be
exploited by our hydrogennet to capture the characteristics of waveform changes induced by h2 leakage and improve the accuracy of
150

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 17. (aâ€“b) two examples of noise-free training data with ğ‘ğ‘Ÿ = 8, and (câ€“d) training and validation results.

horizontal range and mass/volume predictions. it is possible that other
nn architectures can also perform the learning and prediction from
time-lapse waveform data. however, a thorough comparison among all
these possible architectures is not the focus of this work.

the first systematic study that focuses on applying machine learning
to subsurface h2 leakage detection and characterization and could
potentially serve as a cost-effective geophysical tool for underground
hydrogen leakage detection and characterization with high fidelity.

5. conclusions
declaration of competing interest
we have developed a novel supervised machine learning method
to detect and characterize h2 leakage from underground hydrogen
storage. we use a 2d convolutional neural network module along with
a dense neural network module to construct the supervised machine
learning model. the input to this neural network is a set of spatially
sparse time-lapse seismic waveforms, while the output from the neural
network includes six scalar values that describe the spatial and physical
properties of a h2 leakage. to generate high-quality training data, we
develop a novel rock physics model to accurately quantify the relations
among the elastic properties (including the elastic wave velocities and
mass density), leakage mass, and leakage volume. we generate timelapse waveforms based on the first-order stress-velocity elastic-wave
equations and train the neural network using the resample data with or
without random noise. we compare the training and validation results
for different datasets and find that higher data density generally results
in improved inference accuracy, while a higher level of noise in the
training data can reduce the inference accuracy. our investigation is

the authors declare that they have no known competing financial interests or personal relationships that could have appeared to
influence the work reported in this paper.

acknowledgments
the work was supported by laboratory directed research and development (ldrd) program of los alamos national laboratory (lanl),
usa under project numbers 20230625er and 20230022dr. lanl
is managed and operated by triad national security, llc for the
u.s. department of energy (doe) national nuclear security administration (nnsa) under contract no. 89233218cna000001. the research used high-performance computing resources provided by lanlâ€™s
institutional computing (ic) program.
151

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 18. (aâ€“b) two examples of mildly noisy training data with ğ‘ğ‘Ÿ = 8, and (câ€“d) training and validation results.

152

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 19. (aâ€“b) two examples of strongly noisy training data with ğ‘ğ‘Ÿ = 8, and (câ€“d) training and validation results.

153

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 20. (a-c) distribution of parameter prediction errors associated with the validation dataset with ğ‘ğ‘Ÿ = 8 in the noise-free, mildly noisy, and strongly noisy cases, respectively.
red dashed curves in these plots represent fitted gaussian.

154

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 21. (a) an example of noise-free training data with ğ‘ğ‘Ÿ = 16, and (bâ€“c) training and validation results.

155

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 22. (a) an example of mildly noisy training data with ğ‘ğ‘Ÿ = 16, and (bâ€“c) training and validation results.

156

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 23. (a) an example of strongly noisy training data with ğ‘ğ‘Ÿ = 16, and (bâ€“c) training and validation results.

157

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 24. (aâ€“c) statistics of parameter prediction errors associated with the validation dataset with ğ‘ğ‘Ÿ = 16 in the noise-free, mildly noisy, and strongly noisy cases, respectively.
red dashed curves in these plots represent fitted gaussian.

158

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

fig. 25. a schematic of the 1d variant of hydrogennet where the 2d cnn in fig. 2 is replaced with a 1d cnn.

fig. 26. (a) the correlation between the ground-truth and predicted values and (b) the statistics of parameter prediction errors associated with the validation dataset with ğ‘ğ‘Ÿ = 4
in the noise-free case. the nn used for training and validation is the 1d variant of hydrogennet, with an architecture displayed in fig. 25.

159

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

references

[24] vaswani a, shazeer n, parmar n, uszkoreit j, jones l, gomez an, kaiser Å‚,
polosukhin i. attention is all you need. in: guyon i, luxburg uv, bengio s,
wallach h, fergus r, vishwanathan s, garnett r, editors. in: advances in neural
information processing systems, vol. 30, curran associates, inc.; 2017.
[25] zhu w, beroza gc. phasenet: a deep-neural-network-based seismic arrival-time
picking method. geophys j int 2018;216(1):261â€“73. http://dx.doi.org/10.1093/
gji/ggy423.
[26] mousavi sm, ellsworth wl, zhu w, chuang ly, beroza gc. earthquake
transformer â€“ an attentive deep-learning model for simultaneous earthquake
detection and phase picking. nature commun 2020;11(1):3952. http://dx.doi.
org/10.1038/s41467-020-17591-w.
[27] wu x, liang l, shi y, fomel s. faultseg3d: using synthetic data sets to train
an end-to-end convolutional neural network for 3d seismic fault segmentation.
geophysics 2019;84(3):im35â€“45. http://dx.doi.org/10.1190/geo2018-0646.1.
[28] gao k, huang l, zheng y. fault detection on seismic structural images using
a nested residual u-net. ieee trans geosci remote sens 2022;60:1â€“15. http:
//dx.doi.org/10.1109/tgrs.2021.3073840.
[29] pham n, li w. physics-constrained deep learning for ground roll attenuation.
geophysics 2021;87(1):v15â€“27. http://dx.doi.org/10.1190/geo2020-0691.1.
[30] kaur h, pham n, fomel s. seismic data interpolation using deep learning
with generative adversarial networks. geophys prospect 2021;69(2):307â€“26. http:
//dx.doi.org/10.1111/1365-2478.13055.
[31] wu y, lin y. inversionnet: an efficient and accurate data-driven full waveform
inversion. ieee trans comput imaging 2020;6:419â€“33. http://dx.doi.org/10.
1109/tci.2019.2956866.
[32] leong zx, zhu t. direct velocity inversion of ground penetrating radar data
using gprnet. j geophys res: solid earth 2021;126(6):e2020jb021047. http:
//dx.doi.org/10.1029/2020jb021047.
[33] gao k, donahue c, henderson bg, modrak rt. deep-learning-guided highresolution subsurface reflectivity imaging with application to ground-penetrating
radar data. geophys j int 2022;233(1):448â€“71. http://dx.doi.org/10.1093/gji/
ggac468.
[34] pan b, song t, yue m, chen s, zhang l, edlmann k, neil cw, zhu w,
iglauer s. machine learning-based shale wettability prediction: implications for
h2 , ch4 and co2 geo-storage. int j hydrogen energy 2024;56:1384â€“90. http:
//dx.doi.org/10.1016/j.ijhydene.2023.12.298.
[35] wang z, dilmore rm, harbert w. inferring co2 saturation from synthetic surface
seismic and downhole monitoring data using machine learning for leakage
detection at co2 sequestration sites. int j greenh gas control 2020;100:103115.
http://dx.doi.org/10.1016/j.ijggc.2020.103115.
[36] zhou z, lin y, zhang z, wu y, wang z, dilmore r, guthrie g. a data-driven
co2 leakage detection using seismic data and spatialâ€“temporal densely connected
convolutional neural networks. int j greenh gas control 2019;90:102790. http:
//dx.doi.org/10.1016/j.ijggc.2019.102790.
[37] liu m, grana d. petrophysical characterization of deep saline aquifers for
co2 storage using ensemble smoother and deep convolutional autoencoder.
adv water resour 2020;142:103634. http://dx.doi.org/10.1016/j.advwatres.
2020.103634.
[38] li d, peng s, guo y, lu y, cui x. co2 storage monitoring based on time-lapse
seismic data via deep learning. int j greenh gas control 2021;108:103336.
http://dx.doi.org/10.1016/j.ijggc.2021.103336.
[39] murphy kp. probabilistic machine learning: an introduction. mit press; 2022.
[40] carcione jm. wave fields in real media: wave propagation in anisotropic,
anelastic, porous and electromagnetic media. 3rd ed.. amsterdam, netherlands:
elsevier; 2015.
[41] creasy n, huang l, gasperikova e, harbert w, bratton t, zhou q. enabling
reliable accounting for co2 during geologic carbon storage using compliant
porosity and rock frame weakening. nat commun earth environ 2023. under
review.
[42] pan b, liu k, ren b, zhang m, ju y, gu j, zhang x, clarkson cr, edlmann k,
zhu w, iglauer s. impacts of relative permeability hysteresis, wettability,
and injection/withdrawal schemes on underground hydrogen storage in saline
aquifers. fuel 2023;333:126516. http://dx.doi.org/10.1016/j.fuel.2022.126516.
[43] wagner w, pruÃŸ a. the iapws formulation 1995 for the thermodynamic
properties of ordinary water substance for general and scientific use. j phys
chem ref data 2002;31(2):387â€“535. http://dx.doi.org/10.1063/1.1461829.
[44] mavko g, mukerji t, dvorkin j. the rock physics handbook: tools for seismic
analysis of porous media. 2nd ed.. cambridge university press; 2009, http:
//dx.doi.org/10.1017/cbo9780511626753.
[45] kumar mk, sateesh b, prabhakar s, sastry gn, vairamani m. generation
of regiospecific carbanions under electrospray ionisation conditions and their
selectivity in ion-molecule reactions with co2 . rapid commun mass spectrom
2006;20(6):987â€“93. http://dx.doi.org/10.1002/rcm.2400.
[46] hill r. the elastic behaviour of a crystalline aggregate. proc phys soc a
1952;65(5):349. http://dx.doi.org/10.1088/0370-1298/65/5/307.
[47] kimizuka h, ogata s, shibutani y. atomistic characterization of structural
and elastic properties of auxetic crystalline sio2 . phys. status solidi (b)
2007;244(3):900â€“9. http://dx.doi.org/10.1002/pssb.200572709.

[1] muhammed ns, haq b, al shehri d, al-ahmed a, rahman mm, zaman e. a
review on underground hydrogen storage: insight into geological sites, influencing factors and future outlook. energy rep 2022;8:461â€“99. http://dx.doi.org/10.
1016/j.egyr.2021.12.002.
[2] osselin f, soulaine c, fauguerolles c, gaucher ec, scaillet b, pichavant m.
orange hydrogen is the new green. nat geosci 2022;15(10):765â€“9. http://dx.
doi.org/10.1038/s41561-022-01043-9.
[3] gahleitner g. hydrogen from renewable electricity: an international review of
power-to-gas pilot plants for stationary applications. int j hydrogen energy
2013;38:2039â€“61. http://dx.doi.org/10.1016/j.ijhydene.2012.12.010.
[4] zivar d, kumar s, foroozesh j. underground hydrogen storage: a comprehensive
review. int j hydrogen energy 2021;46(45):23436â€“62. http://dx.doi.org/10.
1016/j.ijhydene.2020.08.138.
[5] miocic j, heinemann n, edlmann k, scafidi j, molaei f, alcalde j. underground
hydrogen storage: a review. geol soc lond spec publ 2023;528(1):73â€“86. http:
//dx.doi.org/10.1144/sp528-2022-88.
[6] krevor s, de coninck h, gasda se, ghaleigh ns, de gooyert v, hajibeygi h,
juanes r, neufeld j, roberts jj, swennenhuis f. subsurface carbon dioxide
and hydrogen storage for a sustainable energy future. nat rev earth environ
2023;4(2):102â€“18. http://dx.doi.org/10.1038/s43017-022-00376-8.
[7] lysyy m, fernÃ¸ ma, ersland g. seasonal hydrogen storage in a depleted oil
and gas field. int j hydrogen energy 2021. http://dx.doi.org/10.1016/j.ijhydene.
2021.05.030.
[8] feldmann f, hagemann b, ganzer l, panfilov m. numerical simulation of hydrodynamic and gas mixing processes in underground hydrogen storages. environ
earth sci 2016;75(16):1165. http://dx.doi.org/10.1007/s12665-016-5948-z.
[9] chen f, ma z, nasrabadi h, chen b, saad mehana mz, van wijk j. capacity
assessment and cost analysis of geologic storage of hydrogen: a case study in
intermountain-west region usa. int j hydrogen energy 2023;48(24):9008â€“22.
http://dx.doi.org/10.1016/j.ijhydene.2022.11.292.
[10] heinemann n, alcalde j, miocic jm, hangx sjt, kallmeyer j, ostertaghenning c, hassanpouryouzband a, thaysen em, strobel gj, schmidthattenberger c, edlmann k, wilkinson m, bentham m, stuart haszeldine r,
carbonell r, rudloff a. enabling large-scale hydrogen storage in porous media â€“
the scientific challenges. energy environ sci 2021;14:853â€“64. http://dx.doi.org/
10.1039/d0ee03536j.
[11] ellsworth wl. injection-induced earthquakes. science 2013;341(6142):1225942.
http://dx.doi.org/10.1126/science.1225942.
[12] glubokovskikh s, saygin e, shapiro s, gurevich b, isaenkov r, lumley d,
nakata r, drew j, pevzner r. a small co2 leakage may induce seismicity on
a sub-seismic fault in a good-porosity clastic saline aquifer. geophys res lett
2022;49(12):e2022gl098062. http://dx.doi.org/10.1029/2022gl098062.
[13] miocic jm, gilfillan smv, frank n, schroeder-ritzrau a, burnside nm, haszeldine rs. 420,000 year assessment of fault leakage rates shows geological carbon
storage is secure. sci rep 2019;9(1):769. http://dx.doi.org/10.1038/s41598-01836974-0.
[14] zhang z, huang l. double-difference elastic-waveform inversion with prior
information for time-lapse monitoring. geophysics 2013;78(6):r259â€“73. http:
//dx.doi.org/10.1190/geo2012-0527.1.
[15] yang d, malcolm a, fehler m. using image warping for time-lapse image domain
wavefield tomography. geophysics 2014;79(3):wa141â€“51. http://dx.doi.org/10.
1190/geo2013-0424.1.
[16] raknes eb, rge arntsen b. time-lapse full-waveform inversion of limitedoffset seismic data using a local migration regularization. geophysics
2014;79(3):wa117â€“28. http://dx.doi.org/10.1190/geo2013-0369.1.
[17] liu m, vashisth d, grana d, mukerji t. joint inversion of geophysical data
for geologic carbon sequestration monitoring: a differentiable physics-informed
neural network model. j geophys res: solid earth 2023;128(3):e2022jb025372.
http://dx.doi.org/10.1029/2022jb025372.
[18] liu y, tsvankin i. source-independent time-lapse full-waveform inversion for
anisotropic media. geophysics 2021;87(1):r111â€“22. http://dx.doi.org/10.1190/
geo2021-0306.1.
[19] fu x, innanen ka. time-lapse seismic imaging using shot gathers with nonrepeatable source wavelets. geophysics 2023;88(1):m17â€“30. http://dx.doi.org/10.
1190/geo2022-0072.1.
[20] leong zx, zhu t, sun ay. time-lapse seismic inversion for co2 saturation
with seisco2net: an application to frio-ii site. int j greenh gas control
2024;132:104058. http://dx.doi.org/10.1016/j.ijggc.2024.104058.
[21] lecun y, bottou l, bengio y, haffner p. gradient-based learning applied to
document recognition. proc ieee 1998;86(11):2278â€“324. http://dx.doi.org/10.
1109/5.726791.
[22] he k, zhang x, ren s, sun j. deep residual learning for image recognition. in:
ieee conference on computer vision and pattern recognition (cvpr). 2016, p.
770â€“8. http://dx.doi.org/10.1109/cvpr.2016.90.
[23] goodfellow i, pouget-abadie j, mirza m, xu b, warde-farley d, ozair s,
courville a, bengio y. generative adversarial nets. adv neural inf process syst
2014;27. http://dx.doi.org/10.48550/arxiv.1406.2661.
160

international journal of hydrogen energy 61 (2024) 137â€“161

k. gao et al.

[52] liu s, qi l, qin h, shi j, jia j. path aggregation network for instance
segmentation. in: 2018 ieee/cvf conference on computer vision and pattern
recognition. cvpr, los alamitos, ca, usa: ieee computer society; 2018, p.
8759â€“68. http://dx.doi.org/10.1109/cvpr.2018.00913.
[53] virieux j. p-sv wave propagation in heterogeneous media: velocity-stress finitedifference method. geophysics 1986;51(4):889â€“901. http://dx.doi.org/10.1190/
1.1442147.
[54] zhang w, shen y. unsplit complex frequency-shifted pml implementation
using auxiliary differential equations for seismic wave modeling. geophysics
2010;75(4):t141â€“54. http://dx.doi.org/10.1190/1.3463431.
[55] prautzsch h, boehm w, paluszny m. bÃ©zier and b-spline techniques. springer;
2002, http://dx.doi.org/10.1007/978-3-662-04919-8.
[56] gao k, huang l. an efficient vector elastic reverse time migration method
in the hybrid time and frequency domain for anisotropic media. geophysics
2019;84(6):s511â€“22. http://dx.doi.org/10.1190/geo2018-0644.1.
[57] kingma dp, ba j. adam: a method for stochastic optimization. in: international
conference on learning representations (iclr). 2017, doi:dare.uva.nl/search?
identifier=a20791d3-1aff-464a-8544-268383c33a75.

[48] roach lan, white dj, roberts b. assessment of 4d seismic repeatability and
co2 detection limits using a sparse permanent land array at the aquistore
co2 storage site. geophysics 2015;80(2):wa1â€“wa13. http://dx.doi.org/10.1190/
geo2014-0201.1.
[49] mindlin rd. compliance of elastic bodies in contact. j appl mech
1949;16(3):259â€“68. http://dx.doi.org/10.1115/1.4009973.
[50] hashin z, shtrikman s. a variational approach to the theory of the elastic
behaviour of multiphase materials. j mech phys solids 1963;11:127â€“40. http:
//dx.doi.org/10.1016/0022-5096(63)90060-7.
[51] paszke a, gross s, massa f, lerer a, bradbury j, chanan g, killeen t, lin z,
gimelshein n, antiga l, desmaison a, kopf a, yang e, devito z, raison m,
tejani a, chilamkurthy s, steiner b, fang l, bai j, chintala s. pytorch:
an imperative style, high-performance deep learning library. in: wallach h,
larochelle h, beygelzimer a, dâ€™alchÃ© buc f, fox e, garnett r, editors. in:
advances in neural information processing systems, vol. 32, curran associates,
inc.; 2019, p. 8024â€“35.

161



synthetic seismic data generation with deep learning
g. roncoroni a,â, c. fortini b, l. bortolussi a, n. bienati b, m. pipan a
a
b

university of trieste, department of mathematics and geosciences, trieste, italy
eni â€“ upstream & technical services, san donato milanese, italy

a r t i c l e

i n f o

article history:
accepted 26 april 2021

keywords:
reï¬‚ection seismic
seismic modelling
deep learning
machine learning
synthetic seismogram

a b s t r a c t
we study the applicability of deep learning (dl) methods to generate acoustic synthetic data from 1d models of
the subsurface.
we designed and implemented a neural network (nn) and we trained it to generate synthetic seismograms
(common shot gathers) from 1-d velocity models on two different datasets: one obtained from published results
and the other generated by finite differences (fd) numerical simulation. we furthermore compared the results
from the proposed model with the published one.
moreover, we tried to to add more ï¬‚exibility to this methodology by allowing change of wavelet and the acquisition geometry. we obtained good results in terms of both computation efï¬ciency and quality of prediction.
the main potentialities of the work are the low computational cost, a high prediction speed and the possibility to
solve complex non-linear problems without knowing the physical law behind the phenomenon, which could led
great advantages in the solution also of the inverse problem.
dl to generate 1-d acoustic synthetic seismograms without solving wave equation solution to the 1-d problem
through custom recurrent neural network retraining strategy to improve ï¬‚exibility and applicability computational complexity analysis.

1. introduction
the main objective of this work is the implementation of deep
learning (dl) solutions to generate synthetic seismograms from 1d
acoustic models without solving the wave equation. this is done by
training a nn model which after training is able to predict common
shot gathers from 1-d velocity models.
the wave equation, is non linear with respect to velocity. numerical
solutions schemes, such as ï¬nite difference or pseudo-spectral schemes,
are computationally demanding and the results may be affected by dispersion error and boundary reï¬‚ections.
the nn approach is computational demanding only during nn training and is affected by the problems in the training dataset. the proposed
methodology can solve the forward problem of seismic wave propagation faster than classical methods, especially when high-frequency
source wavelets are considered.
although direct application and computational advantages, being
able to simulate data with a simple 2d geometry but taking into account
the multiples reï¬‚ection, leads the path to possible future works for a
better and faster solution of the inverse problem.
the generation from text data of an audio track that simulates the
human voice, is a similar problem that has been already tackled by dl
â corresponding author.
e-mail address: groncoroni@units.it (g. roncoroni).

approaches (see e.g. oord, dieleman, zen, simonyan, vinyals, graves,
kalchbrenner, senior and kavukcuoglu (2016)). the application of dl
to sciences, and in particular to geophysics, is a new branch that has
been rapidly developing in the last years beneï¬ting to the big improvements in the cost-to-performance ratio of computational resources. the
power of deep learning is due to the capacity of ï¬nding recurrent patterns or causality without the need of user judgment or explicit coding
by capturing statistical relations to provide generative or discriminative
models.
oye and dahl Ã¸ye and dahl (2019) proposed a method based on
convolutive neural network (cnn) to estimate velocity models from
raw shot gathers, with possible applications to full waveform inversion. this is a new approach that can overtake the computational limit
of the old methods (such as genetic algorithms) used to estimate a velocity model. also yang and ma yang and ma (2019) developed a technique to build seismic velocity models using u-net. guo et al. guo et al.
(2019) used bi-directional long short-term memory (lstm) for
seismic impedance inversion, an ill-posed and non-linear problem.
the bi-directional lstm recurrent neural network applied to the inverse
problem of p-impedance estimation can be attractive because it does
not need a prior estimate of the wavelet, one of the main problems in
this type of studies.
moseley and markham moseley et al. (2019a) proposed a wavenetlike net oord et al. (2016), a stack of causal cnn layers, for a fast approximate simulation of acoustic waves.

the result is a model able to learn how to solve the problem in 1-dlayered media and capable of an approach to increase ï¬‚exibility and to
reduce the computational cost in the training process.
the ï¬nal goal is to speed up forward modelling for the wave
equation with respect to the classical methodology and to provide
a starting point for future applications to more complicated scenarios, e.g. elastic approximation, or to a new approach to the inverse
problem.

table 1
parameters used to produce the second dataset.
parameter

set values

sampling rate [s]
listening time [s]
grid point [x, y]
grid spacing [min, max]
ricker peak f [hz]
velocity [min, max]
depth [min, max]
number of layers

0.001 s
1s
[600, 600]
[5, 5] m
20 hz
[1500 m/s, 5000 m/s]
[100 m, 3000 m]
[3â€“8]

2. methods
2.1. datasets

the methodology proposed in this work considers the seismogram
as a time-series (as in speech generation) rather than a matrix of pixels
(as in image processing). we have used a custom made recurrent neural network sherstinsky (2020) model to implement such strategy.

we used two different datasets to train nn and to evaluate the importance of the dataset on different training and prediction features:
the ï¬rst taken by moseley et al. moseley et al. (2019a), hereinafter

fig. 1. the nn is made of two long short term memory (lstm) layers, a convolutive layer, and a ï¬nal lstm layer.

fig. 2. performance on overlapping signals during the training. prediction without convolutive layer (a, b), at epochs 320 and 322 respectively, and with convolutive layer (c), at epochs
180.
2

fig. 3. the dataset is split in four tensors: two for the input and two for the output data (training and validation dataset, respectively). input tensors are made of a number of velocity
functions and output tensors of common shot gather panels (csg).

referred to as mos, which was used to tackle the same task, the other
generated by numerical simulation [with the codes provided in the
devito project by louboutin et al., 2019 louboutin, lange, luporini,
kukreja, witte, herrmann, velesko and gorman (2019)], hereinafter referred to as dev.
both datasets consists of two tensors (training/validation): two for
the input and two for the output data. the input tensor is made of a
number of velocity functions (10,000) and the output tensor of common shot gather panels (csg; 10,000 panels of 11 seismic traces for
mos data and 257 seismic traces for dev data).
we split both test datasets, made of 10,000 input velocity functions
and 10,000 output csg, in training and validation datasets (80% the former and 20% the latter).
we will hereafter use the notation tensor for multi-dimensional arrays: in particular, the output of the nn is a multi-dimensional array
with size [10,000, 257, 1000].
mos is available on github moseley et al. (2019b). it is made of a
tensor with velocity functions and a tensor with the associated csg
panels. the 50% reduction in size of the dataset, compared to the original
20,000 sample moseley's dataset, is due to the fact that our model is
based on a reduced number of weights, compared to moseley's one,
and does not require large datasets. we have applied a random data selection and splitting procedure.
the csg panel was computed by solving the wave equation by using
ï¬nite differences over a 3km x 3km grid in a horizontally layered model.
there are 11 evenly spaced receivers, with horizontal spacing of 200 m,
and the source is at the 6th receiver position.
mos and dev mainly differ in number of receivers and sourcereceiver offset, i.e., dev is the result of numerical simulation with the
parameters shown in table 1. we randomly generated different

fig. 4. training of the proposed net on the mos dataset: loss vs epochs plot. orange line
represents the trend of validation dataset prediction while blue line shows the training
one.

subsurface models by using the velocity and depth constraints shown
in table 1.
in order to generate the input velocity function for fd (in depth) and
for nn (in time) we used the following workï¬‚ow:
for each velocity function:
n= generate a casual number of horizons.
3

kernel sizes in the convolutional layer. after an extensive tests
phase, we were able to deï¬ne the best performing architecture,
shown in fig. 1. this nn consists of two lstm layers hochreiter and
schmidhuber (1997), a convolutional layer simard et al. (2003), and a
ï¬nal lstm layer. we choose lstm because they are recurrent nn,
which means they can deal with causal tasks, and they have a long
short-term memory, so they should better represent the physics behind
the task.
all of the lstm layers have a number of neurons equal to the number
of sensors in the recording array to be simulated. the lstm used in this
model are implemented by the keras layer cudnnlstm, a fast lstm
implementation with nvidia cuda deep neural network library
(cudnn), a gpu-accelerated library of primitives for deep neural networks (dnn) chetlur et al. (2014). in the convolutional layer the kernel
size is [64, 4] and it has 4 ï¬lters with a relu activation function glorot
et al. (2011). this gives the nn the ï¬‚exibility to get values in the window [t - 4: t] from the 64 closer neurons, whit t the time steps, and to
store it in 4 different ï¬lters.
the proposed dl architecture (fig. 1) provides an output h at
time t from three different input, namely: xt, h(t-1) and the cell
state line. the latter is from the method used to obtain the long
term memory. the output is calculated from a series of weights, activation functions and simple mathematical operations. in case of
overlapping signals, such as e.g. multiple/primary reï¬‚ections, such
architecture is unable to perform correct predictions at times larger
than the overlap point. this is the reason for the introduction of the
cnn layer: it increases the number of input to the same neuron
thus allowing a correct prediction even in the post-overlap part of
the record. fig. 2 a,b,c illustrate the performance of such solution
during the training: in fig. 2 a,b (without convolutive layer) the
loss function is stable from epoch 320 and the problem is not
solved. fig. 2 c (with convolutive layer) shows a successful prediction from epoch 180.
in order to train the nn, we need to minimize the norm of the difference between the prediction and the reference output, this is called loss
function: the most used loss functions in regression tasks are the mean
square error and the mean absolute error.
however, in our scenario, geometrical spreading and transmission
losses cause an amplitude decay with time, such that the loss function
is sensitive to the direct wave only due to its dominant amplitude and
nn outputs zeros. accordingly, in the ï¬rst implementation, the nn

fig. 5. loss vs epochs of the training on dev dataset. orange line represents the trend of
validation dataset prediction while blue line shows the training one.

generate n random depth values.
sort depth values and append 0 at the beginning.
generate n + 1 velocity values.
build velocity function in depth.
convert depth to time.
build velocity function in time.
normalize data between 0 (1000 m/s) and 1 (6000 m/s) â€œ.
data in dev dataset has 257 evenly spaced receivers, with horizontal
spacing of 5 m, and the source at the 129th receiver position.
2.2. nn architecture
we tested several architectures, varying the number of hidden
layers (hl), neurons and with different number of ï¬lters and different

fig. 6. prediction and residuals of a predicted csg panel. 0-offset prediction and residuals (a), max-offset prediction and residual (b). in red we highlighted the phase shift in the residuals.

4

fig. 7. residual plot of both models: lstm model in blue, wavenet-like oord et al. (2016) model in orange. vertical axis is logarithmic. horizontal axis, called residual, is made of 50 linearly
divided bins in a range of 0 to 0.001 (mae).

fig. 8. reference dataset (in blue), predicted data (in orange) and residual of 1000 csg panels in frequency domain.

was not able to predict reï¬‚ections because they did not affect the loss
function enough.
in order to sort this problem out we redeï¬ned the loss function as
loss Â¼ â€–ytrue âˆ’ypred â€–1 â‹… g

eventually chose a special case of the adam algorithm, adamax
kingma and ba (2014), which is the most used for regression tasks
and exhibits better performance for this problem. we tested also different batch sizes and we got best stability in loss function with batch
size 512.

Ã°1Ã¾
2

with g deï¬ned as a quadratic function g Â¼ x where x is the two-way
ex
traveltime e
x the median on the whole domain of the function y = x2.
after computing the loss function, the error is backpropagated and
the weights on each neuron are updated by using the backpropagation
algorithm. for this task we tested several algorithms, namely adam
kingma and ba (2014), adamax kingma and ba (2014), adagrad
kingma and ba (2014) and sgd nemirovski et al. (2009) and we

2.3. nn training
we implemented the nn by leveraging on the keras library chollet
et al. (2015). we trained the model on two nvidia tesla p100 gpu
accelerators.
5

fig. 9. two predictions with all 257 offset on two independent proï¬les. both are with the input proï¬le, on the left, the desired output, in the middle, and the predicted output, on the right.

all the images shown in this paper are predicted from blind data,
which means that the net was not trained on them.
hidden layer geometry is the same, but we used different numbers
of neurons in the output layer to reproduce the number of geophones.
fig. 3 shows the structure of the training process with the nn seen as
a black box. we split the generated dataset, made of 10,000 samples, in
training and validation datasets (80% the former and 20% the latter).
with samples we are referring to a the vector of velocity values and its
associated csg panel. the proposed model is based on few weights respect to mosley's model, so we decided to use half dataset. data selection
and splitting is done randomly and no speciï¬c samples were selected.
a major drawback of such approach is the limited ï¬‚exibility, i.e. the
trained nn is able to predict a number of different situations but sourcereceiver offset spacing and input wavelet are limited to the ones it was
trained with. this can overly extend the training time and reduce the
proï¬tability of the method.

therefore, we developed an alternative strategy by retraining the
model, ï¬rst trained with one of the two test datasets, with smaller
datasets that included new features to avoid the need of a complete
training for each modiï¬ed feature.
due to the fact that the nn has to internally encode information like
input wavelet and offset spacing into its weights, a change of simulation
parameters should not vary deeply the main distribution of weights.
by doing so the function to be minimized by the nn should be more
linear kavzoglu (2009) and it should need less epochs to converge to a
good result.
the training process on mos dataset is shown in fig. 4, in which the
loss is plotted against epochs of training, we applied early stop criteria
as keras callback. the training process on dev dataset is shown in
fig. 5: validation and training loss do not diverge too much and they
are close enough to each other, which means prediction is good and it
is not overï¬tting.

6

fig. 10. reference output and prediction of a csg panel with numerical dispersion problems due to finite differences grid dispersion (a and b).

fig. 11. input, reference output, prediction of the retrained model and prediction of the original model.

7

fig. 12. loss vs epochs for the retraining of a model with a different waveform (a) and different offset spacing (b).

fig. 13. prediction of an independent velocity function of the retraining of a model with a different waveform (a) and different offset spacing (b). old prediction (on the right) and the new
prediction (in the middle).

3. results

capability of the net to generalize the problem, i.e. to make correct predictions from datasets different from the training ones.
in this case the training lasted 40 h on two nvidia tesla p100 gpu
accelerators and the loss decreased quite smoothly.
after the training, we tested the nn on a case not utilized in the
training phase: an example of this prediction and of the corresponding

3.1. mos dataset
after the training phase, we tested the net on the dataset generated
by moseley et al. moseley et al. moseley et al. (2019a) to evaluate the
8

in fig. 9-a, prediction is very close to the target, except for the central
area at 950 ms, in which we ï¬nd a small positive amplitude error.
in fig. 9-b, we have a csg with multiple reï¬‚ections: we have plotted
the 2-way traveltime of the primary reï¬‚ections over the predicted data,
so that we can easily identify non-primary reï¬‚ections. we can see a
multiple reï¬‚ection predicted from the model at 600 ms.
in the panel we can also see a signal, similar to the one generated by
reï¬‚ection from edges, reproduced by the net. prediction time for 257
offset csg (dev dataset) is 0.064 s.
3.3. the role of the training dataset
because the nn does not model data from equations, it has to learn
from the output of a forward model how an input of the forward
model is related to its response. this leads to a crucial role of the dataset
used: a bad dataset will lead to a bad model as the net will learn all the
features that are in training data.
in fig. 10, we have a critical level of numerical dispersion, marked
with a and b in fig. 10. despite this, the net is able to accurately reproduce such phenomena.
it is interesting that such predictions come from the same trained
net that made the prediction shown in fig. 9: this means that the net
is also able to discern when and how dispersion has to be predicted.
the nn can discern different scenarios and predicts numerical dispersion quite accurately. the model had problems in predicting multiple reï¬‚ections, as presented in the previous section, and we found a
problem due to inadequate statistics on such events in the training
dataset.
at the beginning the model was trained on a subsurface model with
7 layers: this lead to a very low probability in ï¬nding multiple reï¬‚ections and the net seemed not to learn what these signals were due to.
in order to make an event important in a dataset we had to ï¬nd a lot
of examples of this, and it has to be of signiï¬cant amplitude compared
to others: in this dataset multiple reï¬‚ections were too sparse and of
low amplitude to affect it signiï¬cantly.
to try to solve this problem, we retrained the already trained model
on a dataset generated on purpose to contain several multiple reï¬‚ections. we imposed a 3-layers subsurface model with the ï¬rst two layers
closer to each other than the third one. this led to a dataset with lots of
statistics on multiple reï¬‚ections.
in fig. 11 we can see: the input, the reference output, the prediction
of the retrained model and the prediction on the original model.
as we can see in the retrained model prediction, a multiple reï¬‚ection
at 500 ms is close to the reference signal, while the old net did not predict this signal.

fig. 14. loss vs time for different offsets. we can notice a plateau at (5 âˆ’ 8) Ã— 10âˆ’4. with
letter a, b, c and d we referred to the plateau of, respectively, 3,5, 9, 17 offsets. with letter
e we show the plateau of the remainder data.

residuals is shown in fig. 6. the direct wave is well predicted; there are
still small errors in the magnitude of the gained reï¬‚ections, but the peak
positions are correctly predicted.
in fig. 6 we can split the analysis of the results in two parts, focusing
the part from 0 ms to 350 ms and from 350 ms to the end.
the ï¬rst part of the data contains primary reï¬‚ections and we ï¬nd
some small phase shifts, in red boxes in fig. 6, but the prediction is
good. it is able also to accurately reproduce interference between
signals.
the second part is characterized by the presence of multiple reï¬‚ections only and noise and no primary reï¬‚ections: in this case the nn
does not reproduce anything. this problem can be found in all the
dataset and it is present also in the moseley et al. model moseley et al.
(2019a). this lack of prediction is due to an inadequate statistics on
such event in the training dataset.
in order to evaluate prediction performances of nn, we predicted
1000 panels from the dataset and we analyzed the error by plotting
the residuals of both nets on a histogram with a logarithmic vertical
axis (fig. 7) and the horizontal axis divided in 50 bins linearly spaced
from 0 to 0.001: we can see that using lstm we got a lower error. we
further compared the reference data and the results of prediction of
our net in the frequency domain. in fig. 8 we can see the amplitude
spectrum of reference data (in blue) and predicted data (in orange)
and residual of 1000 csg panels. results are satisfactory as the mean absolute error is 0.89%.

3.4. retraining
in order to improve the applicability of the proposed nn, we had to
make it more ï¬‚exible by allowing rapid inclusion of information like
waveform and offset spacing.
a short scheme of this process follows:

3.2. dev dataset

1. generate a new small dataset ~10% of the dimensions of the ï¬rst
dataset with the new features.
2. train the previously trained model on the new dataset.
as we can see from loss vs epochs plot, in fig. 12, the new training
converges quickly to a minimum and the space in which the gradient
is computed seems to be smooth.
it is apparently not necessary to create new set of weights, because
the old prediction is already in the range of the new right solution: a
limited amount of additional work is enough to reach the best solution
as it reaches a minimum in less than an hour.

we trained the net on another dataset obtained from numerical
simulation to have better control of the training process. that allow
us to study in depth the importance of the dataset and test different critical situations. in order to do this, we kept the offset spacing
and wavelet ï¬xed, but we varied the number of layers in the 1-d
model.
in fig. 9 are presented two predictions with 256 offsets on two independent velocity proï¬les: we show the input proï¬le on the left, the desired output in the middle, and the predicted output on the right.

9

10

exited the plateau. we can also see that 500 epochs are not enough to
solve the problem if the number of offset is 5.
in order to evaluate prediction time we used each trained model to
predict 100 panels and we took the average time for each model. results
are shown in fig. 16. prediction time for a 257 offset csg panel, under
constraints shown in table 1 and with the same hardware, is of 0.08 s
compared to 2.17 s of the fd algorithms used in this paper. according
to this benchmark, this would lead to a method that is 27 times faster
than the classical one.
even though prediction time is smaller compared to that obtained by
classical methods, it is affected by different factors.
prediction time for this net depends only on the number of offsets
used, on temporal discretization and on record length in time.
the fd generation time, instead, does not depend on number of offsets but on model extension and discretization.

table 2
total training time, the moment where the problem starts to be solved and prediction time
for each number of offsets considered.
offsets

total time (h)

solving time (h)

prediction time (s)

3
5
9
17
33
65
129
257

4.80 h
4.74 h
4.80 h
5.05 h
5.83 h
7.24 h
9.46 h
16.21 h

not solved
not solved
4.00 h
2.16 h
0.80 h
0.60 h
0.80 h
0.80 h

0.017 s
0.015 s
0.015 s
0.019 s
0.021 s
0.024 s
0.033 s
0.064 s

fig. 13-a compares the old prediction (on the right) and the new prediction (in the middle), after the retraining, with a different waveform.
while in the old model the source wavelet was a ricker with peak frequency 10 hz, in the retrained model we used a higher 20 hz frequency.
as we have done for the waveform, we have trained a model also to
predict a different offset spacing: the results are shown in fig. 13-b.

4. conclusions
the potentialities of neural network (nn) in the solution of non linear problems are well known and the successful application to the present case, i.e. synthetic seismic data generation in the 1-d acoustic case,
demonstrates such potential.
the proposed methodology is robust and can solve the forward
problem, i.e. numerical modelling of seismic wave propagation starting
from the velocity model faster than classical methods, especially when
high-frequency source wavelets are considered.
for the 1-d acoustic case nn are a good alternative to the classical finite difference (fd) method. they are able to rapidly and accurately
solve the problem.
the waveform is well reproduced and the net manages to predict
multiple reï¬‚ections.
while prediction time is much lower than classical methods
(e.g. fd), the training process needs large times and datasets.
such large training effort may imply that the applicability of the
method is limited.
in order to tackle such issue and increase ï¬‚exibility and applicability
we propose a strategy based on retraining.
the proposed method gives good results and is able to make adjustments of the nn parameters in a short time (less than one hour on two
nvidia tesla p100 gpu accelerators) and with a small dataset.

3.5. computational complexity
we measured the time required to train 500 epochs as a function
of the dimension of the desired output. to do this we trained the same
net increasing the number of columns (offsets) of the output matrix
as 2n + 1.
in order to evaluate the net performance, we checked also the trends
of the loss as a function of time: this is plotted in fig. 14.
as we can see we have different trends but all of them show a plateau. if we analyze the behavior of the function in the surroundings of
such plateau we ï¬nd out that it is the point at which the net really learns
to solve the problem, as shown in fig. 15.
up to a 9-offset threshold the net is not able to solve the problem in
500 epochs; then it seems to learn faster when the number of offsets
increases.
total training time and time required to solve the problem are reported in table 2.
in fig. 15 we can see the exact timing when the problem was understood with 5, 129 and 257 offsets. we got this timing by looking at the
predictions made during the training of the nn at the time the loss

fig. 16. prediction time vs offsets. data are reported in table 2.

fig. 15. comparison between synthetic (fd simulation, column left) and predicted data for 5 (a), 129 (b) and 257 (c) offsets.
11

the authors also thank the landmark's university grants program
(ugp) and the national research program in antarctica of the italian
ministry of university and research for the additional support

in this way, even though we are still linked to fd data generation, we
are able to modify parameters in the nn by using just a small amount of
data generated by fd. our results show that the proposed method allows fast and easy parameter change.
the speciï¬c architecture and the use of lstm allowed to obtain
seismograms with lots of offsets without substantially increasing the
training time.
one possible future work is the application of this methodology for
the inverse problem solution and may offer great beneï¬ts in the solution of challenging cases, such as e.g. velocity function inversion in presence of multiple reï¬‚ections.
this technique may open new perspectives of development, in particular the extension to the 2-d and 3-d elastic case. due to the performance demonstrated in the present work, such extension may be
feasible and they can lead to efï¬cient alternative to classical methods.
nonetheless, the complexity of such extension will require a complete revision of the design of the net.

references
chetlur, s., woolley, c., vandermersch, p., cohen, j., tran, j., catanzaro, b., shelhamer, e.,
2014. cudnn: efï¬cient primitives for deep learning. corr abs/1410.0759. url.
http://arxiv.org/abs/1410.0759.
chollet, f., et al., 2015. keras. https://keras.io.
glorot, x., bordes, a., bengio, y., 2011. deep sparse rectiï¬er neural networks. in: gordon,
g., dunson, d. (eds.), proceedings of the fourteenth international conference on artiï¬cial intelligence and statistics. pmlr, pp. 315â€“323.
guo, r., zhang, j., liu, d., zhang, y., zhang, d., 2019. application of bi-directional long
short-term memory recurrent neural network for seismic impedance inversion.
pp. 1â€“5 url.
hochreiter, s., schmidhuber, j., 1997. long short-term memory. neural comput. 9,
1735â€“1780. https://doi.org/10.1162/neco.1997.9.8.1735.
kavzoglu, t., 2009. increasing the accuracy of neural network classiï¬cation using reï¬ned
training data. environ. model softw. 24, 850â€“858.
kingma, d., ba, j., 2014. adam: a method for stochastic optimization. international conference on learning representations.
louboutin, m., lange, m., luporini, f., kukreja, n., witte, p.a., herrmann, f.j., velesko, p.,
gorman, g.j., 2019. devito (v3.1.0): an embedded domain-speciï¬c language for ï¬nite
differences and geophysical exploration. geoscientiï¬c model development 12,
1165â€“1187 url:. https://www.geosci-model-dev.net/12/1165/2019/ https://doi.
org/10.5194/gmd-12-1165-2019.
moseley, b., markham, a., nissen-meyer, t., 2019a. fast approximate simulation of seismic waves with deep learning.
moseley, b., markham, a., nissen-meyer, t., 2019b. seismic simulation wavenet. url.
https://github.com/benmoseley/seismic-simulation-wavenet.
nemirovski, a., juditsky, a., lan, g., shapiro, a., 2009. robust stochastic approximation approach to stochastic programming. society for industrial and applied mathematics
19, 1574â€“1609.
oord, a.v.d., dieleman, s., zen, h., simonyan, k., vinyals, o., graves, a., kalchbrenner, n.,
senior, a., kavukcuoglu, k., 2016. wavenet: a generative model for raw audio. url.
http://arxiv.org/abs/1609.03499.
Ã¸ye, o., dahl, e., 2019. velocity model building from raw shot gathers using machine
learning. https://doi.org/10.3997/2214-4609.201900039.
sherstinsky, a., 2020. fundamentals of recurrent neural network (rnn) and long shortterm memory (lstm) network. physica d: nonlinear phenomena (0167-2789),
132306 https://doi.org/10.1016/j.physd.2019.132306. https://www.sciencedirect.
com/science/article/pii/s0167278919305974.
simard, p.y., steinkraus, d., platt, j.c., 2003. best practices for convolutional neural networks applied to visual document analysis. seventh international conference on document analysis and recognition. proceedings, pp. 958â€“963 https://doi.org/10.1109/
icdar.2003.1227801.
yang, f., ma, j., 2019. deep-learning inversion: a next-generation seismic velocity model
building method. geophysics 84, r583â€“r599. https://doi.org/10.1190/geo20180249.1.

author statement
g. roncoroni: conceptualization, methodology, software, writing original draft.
n. bienati: conceptualization, supervision, writing - review &
editing
c. fortini: conceptualization, supervision, software, writing - review & editing.
l. bortolussi: conceptualization, supervision, software, writing - review & editing.
m. pipan: conceptualization, supervision, writing - original draft,
writing - review & editing, project administration.
declaration of competing interest
none.
acknowledgments
the authors would like to thank eni, for the opportunity to pursue
this work, also through the use of the computational resources made
available at eni's hpc4,1 and for the approval to bring it to publication.

1
hpc4 is a supercomputer owned by eni spa that has a peak performance of 18.6
petaï¬‚ops

12



geophys. j. int. (2023) 232, 1503â€“1514
advance access publication 2022 october 11
gji marine geosciences and applied geophysics

https://doi.org/10.1093/gji/ggac399

simulating seismic multifrequency wavefields with the fourier feature
physics-informed neural network
chao song

1,2

and yanghua wang

2

1 college of geo-exploration science and technology, jilin university, changchun 130021, china
2 centre for reservoir geophysics, resource geophysics academy, imperial college london, south kensington, london sw7 2bp, uk.
e-mail: yanghua.wang@imperial.ac.uk

summary
to simulate seismic wavefields with a frequency-domain wave equation, conventional numerical methods must solve the equation sequentially to obtain the wavefields for different
frequencies. the monofrequency equation has the form of a helmholtz equation. when solving
the helmholtz equation for seismic wavefields with multiple frequencies, a physics-informed
neural network (pinn) can be used. however, the pinn suffers from the problem of spectral bias when approximating high-frequency components. we propose to simulate seismic
multifrequency wavefields using a pinn with an embedded fourier feature. the input to the
fourier feature pinn for simulating multifrequency wavefields is 4-d, namely the horizontal
and vertical spatial coordinates of the model, the horizontal position of the source, and the
frequency, and the output is multifrequency wavefields at arbitrary source positions. while
an effective fourier feature initialization strategy can lead to optimal convergence in training
this network, the fourier feature pinn simulates multifrequency wavefields with reasonable
efficiency and accuracy.
key words: neural networks, fuzzy logic; numerical modeling; physics-informed neural
network; wave propagation.

1 i n t ro d u c t i o n
calculating the seismic wavefield by solving the wave equation is
an essential step in seismic imaging applications, such as reverse
time migration (rtm), least squares rtm (lsrtm), and full waveform inversion (fwi; tarantola 1984; nemeth et al. 1999; virieux
& operto 2009; wang 2011). lsrtm and fwi are optimization
problems based on data fitting. they require solving the wave equation to obtain the entire wavefields for the computation of the image
and the velocity gradient, and this step usually needs to be repeated
hundreds or even thousands of times to achieve convergence of the
optimization. in the frequency domain implementations, lsrtm
and fwi require many frequency components to cover the entire
wavenumbers of the model (virieux & operto 2009; wang & rao
2009; wang 2016). therefore, the computational costs of lsrtm
and fwi are linearly dependent on the number of frequencies considered and the iterations of model updating. moreover, the computational costs of lsrtm and fwi increase dramatically in complex
media, such as elastic media, anisotropic media and poroelastic media (brossier et al. 2009; lee et al. 2010; yang & malcolm 2021).
the wave equation in the frequency domain is a helmholtz equation for whose numerical solution the finite difference method is
often used (dablain 1986; virieux et al. 2011). it discretizes the
wave equation to form a large sparse impedance matrix which is

multiplied by the solution of the target wavefield. to solve the wavefield in the frequency domain, we need to calculate the inverse of
this large impedance matrix. to achieve this goal, the lu decomposition is usually used. for realistic large models, the calculation
of the wavefields requires an enormous amount of computation. we
can look at the wavefield solutions from a different perspective.
machine learning is making rapid progress in a variety of scientific and technical fields due to the rapid development of computing
resources and the availability of data. in geophysics, a widely used
machine learning method called neural network has been applied
in many important areas, such as first arrival and phase selection
from seismic waveforms (dai & macbeth 1995; gentili & michelini 2006; zhu & beroza 2019), seismogram quality analysis and
evaluation (valentine & trampert 2012), fault and salt body detection from seismic images (shi et al. 2019; wu et al. 2019), seismic
source characterization and localization (van den ende & ampuero
2020; wang & alkhalifah 2021), resolution improvement for migrated images and inverted velocity models (kaur et al. 2020; li
et al. 2021), dispersion curve extraction (dai et al. 2021), etc.
in addition to these applications, neural networks have also been
used to solve partial differential equations (pdes). a framework
called physics-informed neural networks (pinns) uses the underlying physical disciplines as loss functions to solve pdes. pinns
accept spatial and temporal coordinate values as input and set the


c the author(s) 2022. published by oxford university press on behalf of the royal astronomical society. this is an open access
article distributed under the terms of the creative commons attribution license (http://creativecommons.org/licenses/by/4.0/), which
permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

1503

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

accepted 2022 october 1. received 2022 october 1; in original form 2022 february 22

1504

c. song and y. wang

a(x, m, Ï‰)u(xs , Ï‰, xs ) âˆ’ s(x, xs ) = 0

(2)

by optimization. we solve this optimization problem with the help
of a machine learning system, the so-called pinn.
the source function vector s(x, xs ) is very sparse, and therefore
does not provide enough samples for training the network. we use
scattering theory and represent the wave equation in a perturbation
form as
Ï‰2 m(x)Î´u(x, Ï‰, xs ) + âˆ‡ 2 Î´u(x, Ï‰, xs ) = âˆ’Ï‰2 Î´m(x)u 0 (x, Ï‰, xs ),
(3)
where u 0 (x, Ï‰, xs ) is the background wavefield corresponding to a
background modelm 0 (x), Î´m(x) = m(x) âˆ’ m 0 (x) is the model perturbation, and Î´u(x, Ï‰, xs ) is the scattered wavefield. in scattering
theory, the virtual source function âˆ’Ï‰2 Î´m(x)u 0 (x, Ï‰, xs ) in eq. (3),
which is related to Î´m(x) and u 0 (x, Ï‰, xs ), extends over the entire
spatial domain.
the background model m0 (x) is set as an infinite isotropic homogeneous model and the source function s(x, xs ) is a delta function. thus, the background wavefield u 0 (x, Ï‰, xs ) can be obtained
by an analytical solution. for the helmholtz equation (eq. 1), its
âˆš
(1)
solutionu 0 (x, Ï‰, xs ) = i 14 h0 (Ï‰ m 0 |x âˆ’ xs |), is given in terms of
(1)
h0 , the 1st-kind hankel function of order 0 (engquist & zhao
2018).

2.1.2 pinn

2 t h e o ry
2.1 solving wave equation using pinn
2.1.1 wave equation in scattering theory
the propagation of seismic waves can be simulated in the frequency
domain by solving the acoustic wave equation as follows:
Ï‰2 mu(x, Ï‰, xs ) + âˆ‡ 2 u(x, Ï‰, xs ) = s(x, xs ),

where x={x, z} is a vector of spatial coordinates for 2-d media, xs
denotes the coordinate of the source, Ï‰ is the angular frequency, m
is the squared slowness, s(x, xs ) is the source function vector, âˆ‡ 2 =
âˆ‚ 2 /âˆ‚ x 2 + âˆ‚ 2 /âˆ‚z 2 is the laplacian operator and u(x, Ï‰, xs ) is the
wavefield in the frequency domain. each element of the wavefield
vector Î¼ is complex.
eq. (1) is a helmholtz equation and can be written in a compact
form, a(x, m, Ï‰)u(x, Ï‰, xs ) = s(x, xs ), where a(x, m, Ï‰)=Ï‰2 m +
âˆ‡ 2 is the modeling operator. the modeling operator a(x, m, Ï‰)
can be discretized into a sparse matrix, using a finite-difference
method. the inverse of such a sparse matrix a(x, m, Ï‰) can be
calculated using the lu decomposition method. the lu decomposition factorizes the sparse matrix a(x, m, Ï‰) as the product of
a lower triangular matrix (l) and an upper triangular matrix (u)
(schwarzenberg-czerny 1995). the inverse of the stored lower part
(l) and upper part (u) can be computed efficiently and applied
repeatedly to different source functions. the computational cost of
solving the helmholtz equation based on the lu decomposition increases cubically with the increase in model size (trefethen & bau
1997). therefore, the lu decomposition is the largest cost factor
in conventional wavefield simulation. for each frequency, a new
impedance matrix a(x, m, Ï‰) must be created and its inverse calculated. the total costs for solving multifrequency wavefields thus
depend linearly on the number of frequencies.
to improve efficiency, instead of calculating the inverse of matrix
a(x, m, Ï‰), we try to solve the linear system

(1)

neural networks are capable of representing universal functions
(hornik et al. 1989; leshno et al. 1993; van der baan & jutten
2000). assume that a neural network with l + 1 layers is supposed
to approximate the wavefield solution u, and it starts with layer 0
and ends with layer l âˆ’ 1. the number of hidden layers is l âˆ’ 1.
in the lth hidden layer, there are kl neurons. a weighting parameter
wij l is used to connect the ith neuron in layer l âˆ’ 1 and the jth
neuron in layer l. this weighting parameter controls the strength of

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

objective function as output. in geophysics, pinns have been successfully used to solve eikonal equations (smith et al. 2020; waheed
et al. 2021), maxwellâ€™s equations (huang et al. 2021), wave equations in the time and frequency domains for isotropic and anisotropic
media (karimpouli & tahmasebi 2020; moseley et al. 2020; voytan
& sen 2020; alkhadhr et al. 2021; song et al. 2021, 2022; alkhalifah et al. 2021a). based on the traveltime solution of the eikonal
equation, pinn has been used to localize microseismic events and
for seismic tomography (grubas et al. 2021; izzatullah et al. 2022;
smith et al. 2022). pinn has also been applied to fwi based on
the wavefield solution of the wave equation (rasht-behesht et al.
2022; song & alkhalifah 2022).
if solving the helmholtz equation with pinn, a new network
must be trained for each individual source and frequency. alkhalifah et al. (2021b) suggested including the locations of the sources
and the frequency components as additional inputs to the network
so that the network can generate wavefields for multiple sources and
frequencies. however, it is difficult for a single network to generate
equally accurate wavefields for different frequencies because networks prefer low-frequency components when learning, which is
also known as spectral bias (rahaman et al. 2019; xu et al. 2019).
spectral bias describes a weakness of pinn, meaning that pinn
can only represent low-frequency features, while it is unable to approximate high-frequency function (wang et al. 2021). to solve this
problem, the position encoding method has been proposed to transform the input coordinates into fourier basis functions (sinusoids)
with logarithmically spaced, axis-aligned frequencies (mildenhall
et al. 2020; huang et al. 2021). tancik et al. (2020) also proposed
a fourier feature network that transforms the input coordinates into
sinusoids with off-axis frequency distributions. in this way, the target output function can be represented by its fourier decomposition,
which facilitates optimization of the network compared to using the
input coordinates directly.
in this paper, we propose to use the fourier-feature pinn to
solve a helmholtz equation to simulate seismic wavefields with
multiple frequencies and multiple sources. to convey this idea, we
structure the rest of this paper as follows. in the theory section, we
first introduce the helmholtz equation and its scattered form. then
we demonstrate the problem of spectral bias in predicting highfrequency seismic waves using neural networks. next, we explain
how to use fourier feature pinns to solve this problem and propose an effective fourier feature embedding initialization strategy.
in the network building and training section, we explain how to
define the network architecture and train the network. in the results
section, we show the significant improvements of fourier feature
pinns compared to simple pinns in terms of accuracy in generating multifrequency wavefields. finally, we discuss the limitations
and future prospects of the proposed method and summarize our
contribution.

simulating seismic multifrequency wavefields
the connection of two neurons. a bias term bi is also used to shift
the output. the output for the kth neuron in the lth layer (u lk ) can
be determined by a weighted sum of the inputs (outputs in the last
layer ulâ€“1 ) as follows (bishop 2006):
â›
â
kl âˆ’1

lâˆ’1
l
l
l
wk j u j + bk â  ,
uk = Ï† â
(4)
j=1

f =

1 
Ï‰2 m(x)[Î´u r (x, Ï‰, xs ) + Î´u i (x, Ï‰, xs )]
n
+ âˆ‡ 2 [Î´u r (x, Ï‰, xs ) + Î´u i (x, Ï‰, xs )]
2
+ Ï‰2 Î´m(x)[u 0,r (x, Ï‰, xs ) + u 0,i (x, Ï‰, xs )] ,
2

(5)

where n is the number of training samples, and
[Î´u r (x, Ï‰, xs ), Î´u i (x, Ï‰, xs )] is the scattered wavefield in
complex value. the scattered wavefield is a function that depends
on the model coordinates (x), the frequency (f), and the source
coordinates (xs ). we use these parameters as inputs for layer 0
to predict multifrequency scattered wavefields at different source
locations in the last layer. we use only the horizontal source coordinate as input since sources in realistic seismic surveys are usually
placed at the surface with a fixed depth. the inputs to the network
and the resulting scattered wavefields thus contain four dimensions:
the horizontal (x) and vertical (z) spatial coordinates of the model,
the horizontal position of the source (xs ) and the frequency (f). the
second-order partial derivatives of [Î´u r (x, Ï‰, xs ), Î´u i (x, Ï‰, xs )]
with respect to the spatial coordinates of the input are evaluated by
automatic differentiation (baydin et al. 2017). in the loss function,
the background wavefields ([Î´u r (x, Ï‰, xs ), Î´u i (x, Ï‰, xs )]) are
calculated according to the input model coordinate (x), the
angular frequency (Ï‰ = 2Ï€ f ), and the source coordinate (xs ). the
velocity model parameters (m, Î´m) are interpolated at random
model coordinates (x). background wavefields and velocity model
parameters are non-trainable parameters that are included in the
loss function.

2.2 spectral bias and fourier feature pinn

this function u = eikx can be used as a solution of 1-d harmonic
plane wave, where k denotes the wavenumber. according to eulerâ€™s
formula, u = eikx can be expressed as
eikx = cos(kx) + i sin(kx).

(6)

in this case, the seismic waves can be considered as a combination
of sine and cosine waves with different wavenumbers k. we use gr (x)
and gi (x) to denote the real and imaginary parts of eq. (6) which
sum over a group of wavenumbers, expressed as

cos(ki x),
(7)
g r (x) =
i

g i (x) =



sin(ki x),

(8)

i

where i is the index of the wavenumber. if the wavenumbers are
given as k = {5, 10, 15, 20, 25, 30, 35, 40, 45, 50}, we get function
g r (x) and function g i (x) shown in figs 1(a, left-hand side) and (b,
left-hand side), respectively. both g r (x) and g i (x) functions show
complicated waveforms with multiple frequency components. we
calculate their spectra,
g r (k) and g i (k), using the fourier transform

g(k)=f[g(x)]= g(x)eâˆ’ikx dx, where f denotes the operation of
fourier transform. the magnitude of g(k) is denoted as |g(k)| or
|f[g(x)]|, where | Â· | means the modulus taken from a complex
number. figs 1(a, right-hand side) and (b, right-hand side) show
the magnitude of spectra g r (k) and g i (k), and they show consistent wavenumber components in g r (x) and g i (x) with the given
parameters.
we use a neural network to regress g r (x) and g i (x). this network
contains six hidden layers, and each hidden layer has 256 neurons.
we use the adam optimizer with the full-batch gradient descent
method to train the network. the learning rate is 0.0003. a fullbatch training scheme means all training samples are passed to the
optimizer at once. we input 200 samples for the x-axis coordinate
into the network. these samples are sampled uniformly between 0
and 1 m. the 2-d output of this network contains two functions,
denoted by p r (x) and p i (x). they are designed to correspond to
g r (x) and g i (x), respectively. as the number of epochs increases,
the normalized magnitudes of f[ p r (x)] and f[ p i (x)] at different
wavenumbers during training are shown in figs 2(a) and (b), respectively. an epoch in machine learning means a complete run of all
training samples by the optimizer. for all experiments in this paper,
we use a quadro tesla p100 16 gb gpu to train the networks and
obtain the predicted results. in this example, each training epoch
takes 0.003 s, so it takes 3 min to perform 60 000 training epochs.
in fig. 2, the x-axis represents the wavenumber, and the y-axis
the number of training epochs. it is obvious that the low-frequency
components can be recovered quickly with a limited number of
epochs. in comparison, the high-frequency components can only be
partially recovered even with a large number of epochs. the problem
arises from spectral bias, which indicates that the learning priority
of low-frequency features takes precedence over high-frequency
ones in a neural network. this is a major challenge for neural
networks to simulate the propagation of seismic waves that contain
high-frequency information.

2.2.1 spectral bias
neural networks prefer low-frequency components when learning
the features of the target functions of interest, and this phenomenon
is called spectral bias (rahaman et al. 2019). this problem will
cause difficulties in generating high-frequency wavefields. we will
demonstrate this problem in the reconstruction of seismic waves
using neural networks with a simple example.

2.2.2 fourier feature neural networks
to mitigate the problem of spectral bias in predicting the highfrequency wavefield, we propose to use the fourier feature
pinn to simulate multifrequency wavefields (tancik et al. 2020;
wang et al. 2021). in fourier feature pinn, the input data

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

where Ï† is the nonlinear activation function. the activation function
decides which neuron to activate and pass on to the next layer. a
loss function should be defined to provide the connection between
the input and the target output function. training the neural network means an optimization process to find the optimal weighting
parameter wij l that minimizes the defined loss function.
in this paper, we use a fully connected neural network to represent
the real and imaginary parts of the scattered wavefields, denoted as
Î´u r and Î´u i , respectively. unlike conventional data-driven neural
networks that require a large amount of data to create the mapping
between input and output data, we use a pinn based on physical
laws to train the neural network. to solve for scattered wavefields,
we use the scattering form of the wave equation (eq. 3) as the loss
function. as the scattered wavefield is complex-valued, Î´u r and Î´u i
are combined together to form one loss function, in the l2 norm as

1505

1506

c. song and y. wang

figure 2. the spectrum evolution of the neural network output function for each frequency against the training iteration. panel (a) corresponds to g r (k) and
panel (b) corresponds to g i (k). the x-axis represents the wavenumber, and the y-axis represents the training epoch number.

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

figure 1. the (a) g r and (b) g i functions and their corresponding spectrums. in both panels, the left-hand part displays the functions, and the right-hand part
displays the corresponding spectrum using fast fourier transform (f). the given wavenumber k is {5, 10, 15, 20, 25, 30, 35, 40, 45, 50}.

simulating seismic multifrequency wavefields

1507

(v = {x, z, xs , f}) are lifted to a higher dimension which is made up
of a series of sinusoids using the function Î³ , stated as
Î³ (v) =

cos(Ï•v)
,
sin(Ï•v)

(9)

where Ï• represent the wavenumbers in the fourier basis used to approximate the wavefield solution. this process is called fourier feature embedding in the fourier feature pinn, as shown in fig. 2(a).
it is worth noting that the fourier feature Ï• is not a trainable parameter, so the fourier feature pinn does not introduce any additional
trainable parameters compared to the simple pinn. therefore, the
training cost for the fourier feature pinn is similar to that of the
simple pinn. after this step of embedding the fourier feature, Î³ (v)
is passed to the network. in the first layer of the fourier feature
pinn, the process is described as follows:
h1 = Ï† (w1 Î³ (v) + b1 ) ,

(10)

where w1 is the weight matrix; b1 is the bias vector. in Î³ (v), Ï•
is equivalent to the wavenumber k in eq. (6), and we can sample Ï•
from a uniform distribution. the maximum value of this distribution
corresponds to the maximum target wavenumber of the predicted
wavefield.
to understand the mechanism behind the fourier feature pinn,
let us make the connection to the fourier decomposition. if we consider the output function uv (v) as an approximation to the solution
of the wavefield u, the fourier decomposition of u can be expressed
as follows:

(11)
u(x) =
u k eikx ,
where u k represents the fourier coefficients for fourier basis. in
principle, u k is equivalent to the weights w in the fourier feature pinn, and eikx is equivalent to the fourier basis Î³ (v) transferred from the input coordinate values using fourier feature embedding. the weight matrix w1 acts as the fourier coefficients
of the fourier basis which can be optimized when training the
network.

3 network building and
training
the simple pinn and the fourier feature pinn we use in this
work contain eight hidden layers with {128, 128, 64, 64, 32, 32,

32, 32} neurons from the shallow layer to the deep layer, as shown
in figs 3(a) and (b). we use a large number of neurons in the
shallow layer to extract low-level features. then, these features are
passed to subsequent deep layers with fewer neurons. deep, thinner layers can effectively and efficiently learn high-level features
from the extracted low-level features (song & alkhalifah 2022).
the number of neurons and hidden layers is determined by trial
and error. we initialize the weights in the hidden layers using a
xavier initialization with a uniform distribution (glorot & bengio 2010). we use the inverse tangent function as the activation
function to connect layers in the neural network, except for the
last hidden layer which connects the output. in the fourier feature
pinn (fig. 3b), the inputs are transferred into a series of sinusoids
using the fourier feature embedding before being passed on to the
network.
our goal is to simulate seismic wavefields for multiple frequencies at multiple source locations. thus, each of the input samples
for the neural network comprises four dimensions, vj = {xj , zj , xsj ,
fj }, where j is the sample index, xs denotes the source location in the
horizontal direction and f denotes the frequency (in hz), Ï‰ = 2Ï€ f .
we do not input the vertical position of the source into the neural
network because we place the sources at the surface (at the same
depth), which mimics a realistic seismic survey at the surface. the
outputs are real (Î´u r ) and imaginary (Î´u i ) parts of the scattered
wavefields.
we optimize the networks in figs 3(a) and (b) using an adam
optimizer with a stochastic gradient descent method (kingma
& ba 2014) followed by optimization with the limited-memory
broydenâ€“fletcherâ€“goldfarbâ€“shanno algorithm (l-bfgs) (liu &
nocedal 1989; rao & wang 2017; rao et al. 2019). the adam
optimizer is based on the first-order derivative gradient, while lbfgs is a quasi-newton algorithm that depends on the secondorder derivative. l-bfgs can accelerate the convergence rate. the
training of the l-bfgs optimizer stops when the maximum iteration number is reached or the convergence is achieved. the
main problem of the l-bfgs optimizer is that it often does not
converge for randomly initialized networks. therefore, we use the
adam optimizer primarily to provide a good starting point for lbfgs and to speed up training. both optimizers use a gradientbased optimization algorithm based on a full batch. for all experiments conducted in the results section, the number of training
epochs for the adam optimizer is 100 000 and training is automatically terminated after about 50 000 l-bfgs updates; the

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

figure 3. (a) the plain pinn architecture. the input contains 4-d parameters, (x, z, xs , f). the outputs are real (Î´u r ) and imaginary (Î´u i ) parts of the scattered
wavefield. (b) the fourier feature pinn architecture. the only difference between the fourier feature pinn and the plain pinn is that the inputs are transferred
to a series of sinusoids using fourier feature embedding.

1508

c. song and y. wang
the scattered wavefields with the simple pinn. this shows that the
simple pinn cannot predict multifrequency wavefields.

4.1.2 performance of fourier feature pinn with the theoretical Ï•
sampling range

learning rate is 0.001. these hyper-parameters were determined
by trial and error.

4 r e s u lt s
4.1 a velocity section from the sigsbee2a model
we test the proposed method on a layered model, extracted from the
sigsbee2a model (paffenholz et al. 2002), as shown in fig. 4. the
size of the model is 101 Ã— 101 gridpoints with a spatial sampling
of 25 m in both vertical and horizontal directions. we refer to these
gridpoints representing the model as regular gridpoints. we generate
50 000 samples of {x, z, xs , f} as training data. these samples are
randomly generated from a uniform distribution. for x, z and xs ,
the range of the training samples is between 0 and 2.5; while, for
the frequency f, the range is between 5 and 10. the training data
are input into the networks to satisfy the scatter wave equation
in eq. (5).
in fig. 5(a), we show the reference total wavefields calculated
by a numerical method. the numerical solver is an optimal 9-point
frequency-domain finite-difference (fdfd) operator for the wave
equation (jo et al. 1996). it takes 10.93 s to obtain the wavefields
from 5 to 10 hz with an interval of 1 hz for nine sources. fig. 5(b)
shows the background wavefields generated from the analytical solutions for the same source locations and frequencies as in fig. 5(a).
the computational cost of calculating the background wavefields is
almost negligible. we obtain the reference scattered wavefields by
calculating the wavefield difference between figs 5(a) and (b), as
shown in fig. 5(c).

4.1.1 performance of the plain pinn
to compare the performance of plain pinn and fourier feature
pinn, we first train the simple pinn architecture in fig. 3(a) with
the adam optimizer and then with the l-bfgs optimizer. in this
example, each epoch of the adam optimizer takes 0.16 s and each
epoch of the l-bfgs optimizer takes 0.12 s. in total, it takes 6.11
hr to complete the training. after training the network, we input
the regular gridpoints for the numerical solutions into the trained
network to obtain the predicted scattered wavefields, and this takes
only 0.063 s. compared to the fdfd method, the computational
cost of a trained pinn in generating the wavefields is more than 150
times less. the resulting predicted scattered wavefields of the simple
pinn are shown in fig. 6(a). it is clear that we find no evidence of

4.1.3 performance of fourier feature pinn with calibrated Ï•
sampling ranges
to demonstrate the importance of using the effective and simple
sample strategy for Ï•, we first calibrate the maximum Ï• to 0.02.
using the same training setup and training data as the previous
experiments, the predicted wavefields from the fourier feature pinn
with uniformly sampled Ï• from a narrower range of [âˆ’0.02, 0.02]
are shown in fig. 6(c). we find that the scattered wavefields are well
recovered at low frequencies (5âˆ’8 hz), while there are errors at the
edges of the predicted wavefields at high frequencies (9 and 10 hz).
then, we set the maximum Ï• to 0.1 and train the same network
with the same setup. the predicted wavefields from the fourier
feature pinn with uniformly sampled Ï• from this wider range of
[âˆ’0.1, 0.1] are shown in fig. 6(d). due to the effect of spectral bias,
only the low-frequency scattered wavefields are well predicted, and
the errors for the high-frequency ones (9â€“10 hz) are large. in this
experiment, we sample Ï• from a wider range with a uniform distribution. since this wider range goes beyond the target range, we
may get unwanted samples that are not n the target range. consequently, this leads to insufficient sampling for the target Ï•, resulting
in poorer training convergence in network optimization compared
to using the theoretical Ï• range.
in both figs 7(c) and (d), we see that the main errors are distributed at the edges of the high-frequency wavefields. this is because in the training data, the gird points at the edges are not as
well sampled as those in the central region, resulting in inadequate
training. due to the problem of spectral bias, the errors occur mainly
in the predicted high-frequency wavefields.
to further verify the accuracy of the wavefields predicted by
pinns, we calculate the difference between the reference scattered wavefields and pinn-predicted scattered wavefields. fig. 7(a)
shows a large difference between scattered wavefields and indicates the low accuracy of the simple pinn in generating

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

figure 4. a layer velocity model extracted from the sigsbee2a model.

with the fourier feature pinn, it is important to choose sample
Ï• (the wavenumber in the fourier basis) wisely. as we mentioned
earlier, Ï• represents the wavenumber of the seismic waves. so, we
need to sample Ï• within the range of the wavenumber for the target
wavefields. according to the relationship between wavenumber,
frequency and velocity, the maximum wavenumber of the target
wavefield can be calculated by kmax = 2Ï€ f /vmin , where f max is the
maximum frequency of the target wavefield (in this case 10 hz)
and vmin is the minimum velocity value (in this case 1830 m sâ€“1 ).
so the maximum wavenumber of the target wavefield is 0.0343.
to ensure that the maximum wavenumber is sampled, we set the
maximum value of Ï• slightly larger than 0.0343, in this case 0.04.
consequently, all wavenumbers are sampled equally in the range of
[âˆ’0.04, 0.04]. we call this Ï• sampling range the theoretical range.
we train the fourier feature pinn in fig. 3(b) with the same
training setup and training data as the simple pinn experiment.
in fig. 6(b), we observe the wavefields predicted with the fourier
feature pinn in the same configuration as in fig. 5(a). we see that
scattered wavefields from 5 to 10 hz are well recovered for three
different sources.

simulating seismic multifrequency wavefields

1509

figure 6. predicted scattered wavefields (a) from the plain pinn, from fourier feature pinn with Ï• uniformly sampled from the range of (b) [âˆ’0.04, 0.04],
(c) [âˆ’0.02, 0.02] and (d) [âˆ’0.1, 0.1]. the elliptical circles in (c) and (d) indicate the poor prediction results for high-frequency wavefields. the frequencies
and source locations displayed are the same in fig. 5.

multifrequency wavefields. quantitatively, the l2 norm wavefield
difference in fig. 7(a) is 687.37. in fig. 7(b), we find that the
difference is almost zero for all the displayed wavefields, and the
l2 norm wavefield difference for fig. 7(b) is 13.20. this small

wavefield difference confirms the accuracy of the fourier feature
pinn predicted wavefields using the theoretical Ï• sampling range.
by comparison, the accuracy of fourier feature pinn predicted
wavefields will be compromised using a smaller ([âˆ’0.02, 0.02])

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

figure 5. (a) reference wavefields from the finite-difference method, (b) background wavefields from the analytical solution, and (c) reference scattered
wavefields [difference between figures (a) and (b)] at different frequencies and different source locations. in each figure, from the left column to the right
column, the displayed frequencies are from 5 to 10 hz with a 1 hz interval; each row displays the wavefields for three different source locations (s1 , s2 , s3 ) on
the surface.

1510

c. song and y. wang

figure 8. the training loss of different networks (blue dashed line: plain
pinn; orange solid line: fourier feature pinn with Ï• uniformly sampled
from the theoretical range of [âˆ’0.04, 0.04]; red dash-dotted line: fourier
feature pinn with Ï• uniformly sampled from a narrower range of [âˆ’0.02,
0.02]; purple dotted line: fourier feature pinn with Ï• uniformly sampled
from a wider range of [âˆ’0.1, 0.1].)

or a larger ([âˆ’0.1, 0.1]) Ï• sampling range, as shown in figs 7(c)
and (d). they show larger differences in scattered wavefields than
in fig. 7(b), especially at relatively high frequencies (9 and 10 hz).
the l2 norm wavefield difference for figs 7(c) and (d) are 234.62
and 121.82, respectively, which are 17 and 9 times larger than
those of fig. 7(b).
fig. 8 shows the training loss curves for different experiments.
we note that the training loss for the simple pinn decreases very
slowly (blue dashed line). it indicates that simple pinn is not able
to solve for scattered wavefields that satisfy the helmholtz equation

figure 9. another velocity model extracted from the sigsbee2a model.

with multiple frequencies. in comparison, the fourier feature pinn
can achieve training convergence and provide reasonably accurate
results of scattered wavefield prediction. when embedding fourier
features, it is important to choose a good sampling strategy for Ï•.
from our experiments, we know that it is good to choose the
maximum value of Ï• slightly larger than the maximum wavenumber of the target wavefields. in fig. 8, it can be seen that the best
convergence for network training can be achieved when the theoretical range of [âˆ’0.04, 0.04] is sampled for Ï• (according to the
sampling strategy we introduced), as shown by the orange solid
curve. in comparison, sampling Ï• from a narrower or wider range
than the theoretical one causes the training optimization to converge
to a higher loss.

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

figure 7. (a) scattered wavefield difference between the reference wavefields (fig. 5c) and the plain pinn-predicted wavefields. scattered wavefields difference
between the reference wavefields (fig. 5c) and fourier feature pinn-predicted wavefields with Ï• uniformly sampled from the range of (b) [âˆ’0.04, 0.04], (c)
[âˆ’0.02, 0.02] and (d) [âˆ’0.1, 0.1]. the frequencies and source locations displayed are the same in fig. 5.

simulating seismic multifrequency wavefields

1511

table 1. computational time (seconds) comparison between pinn and fdfd for different numbers of frequencies (f)
and sources (xs ). for pinn, the computation time is a sum of the training time of the network (1188.05 s) and the time
for wavefield prediction.
method/numbers
pinn
fdfd

6 (f)Ã—9 (xs )

51 (f)Ã—9 (xs )

501 (f)Ã—9 (xs )

501 (f)Ã—50 (xs )

1188.05 + 0.063
10.93

1188.05 + 0.52
102.01

1188.05 + 5.43
1205.03

1188.05 + 26.46
2044.02

4.2 transferring to another section from the sigsbee2a
model
in the previous experiments, we have shown that the fourier feature
pinn can solve a helmholtz equation with multiple frequencies
for arbitrary source locations. however, the trained network is only
applicable for the specific velocity model used in the training. for
a new velocity model, we need to retrain the network. transfer
learning is a machine learning technique that allows us to use a
previously trained model as a starting point for a new model specified for a new task (pan & yang 2009; weiss et al. 2016). based
on the concept of transfer learning, we can start training with the
pre-trained network obtained from a specific velocity model. since
the pre-trained network has already learned the features of the wavefield solution, a very limited number of l-bfgs iterations is sufficient to adjust the weights that take into account a new velocity
model.
for a new velocity model shown in fig. 9, we start training
with the trained fourier feature pinn using the theoretical sampling strategy for the velocity model in fig. 4. in this case, we
use only 10 000 iterations of the l-bfgs optimization to train
the network. in total, it takes 19.80 min (1188.05 s) to complete the training. by using transfer learning, we can reduce the
training cost of this new velocity model by 18 times. we show
the reference scattered wavefields from the numerical method
in fig. 10(a). the scattered wavefields from the newly trained
fourier feature pinn are shown in fig. 10(b). the difference

between figs 10(a) and (b) is almost not noticeable as shown
in fig. 10(c).
to simulate wavefields at nine sources in the range of 5âˆ’10 hz
with an interval of 1 hz, the fourier feature pinn still takes more
time than the fdfd method, even with the help of transfer learning.
however, if we want to analyze the wavefield signals in the time
domain, we need a large number of wavefields in the frequency
domain to perform the inverse fourier transform and convert them
to the time domain. in addition, field seismic surveys require a large
number of sources to image the subsurface structures. the computational efficiency of the fourier feature pinn becomes apparent
as the number of frequencies and sources for the target wavefields
increases. table 1 shows the computation time of pinn and fdfd
for different number of frequencies (f) and sources (xs ). for pinn,
the training time of the network is a constant (1188.05 s), and the
total computation time is equal to this training time plus the time
for wavefield prediction. we see that pinn can be more efficient
in generating wavefield solutions for a relatively large number of
frequencies and sources.
figs 11 and 12 show the snapshots of the scattered wavefields
in the time domain, which are transformed from the numerical
calculated and the fourier feature pinn-predicted wavefields in the
frequency domain over the frequency range from 5 to 10 hz with
an interval of 0.01 hz. at different time steps, the snapshots of the
wavefield from the fdfd method and the fourier feature pinn
are almost identical, confirming the accuracy of the fourier feature
pinn.

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

figure 10. (a) reference scattered wavefields from the finite-difference method, (b) predicted scattered wavefield from the fourier feature pinn with Ï•
uniformly sampled from the range of [âˆ’0.04, 0.04] and (c) scattered wavefield difference [difference between panels (a) and (b)] at different frequencies and
different source locations for the velocity model in fig. 9. the frequencies and source locations displayed are the same in fig. 5.

1512

c. song and y. wang

figure 12. time-domain scattered wavefield snapshots at (a) 0.6 s, (b) 0.8 , and (c) 1 s transformed from fourier feature pinn-predicted frequency-domain
wavefields ranging from 5 to 10 hz with a 0.01 hz interval.

5 discussion
in this paper, we try to solve the scattered wavefields instead of
the real wavefields directly. the solution of the scattered wavefield is related to the background wavefield that we provide. we
recommend using the analytical wavefield solution corresponding
to an infinite isotropic homogeneous model and a delta source
function. the velocity of the homogeneous background model
should be the same as that at the source gridpoint. this way, the
scattered wavefields around the source will not be dramatically
large. we use a delta function as the source function so that the
trained networks become generators of greenâ€™s function. for the
predicted wavefields at all frequencies, the amplitude and phase
are identical. in realistic seismic applications, we need to recover
the actual frequency component of the source information for real
wavefields.
the main contribution of this work is that we have developed a
machine learning based method to simulate multifrequency wavefields directly from the helmholtz equation. this feature surpasses
the capabilities of conventional numerical methods. in the commonly used fdfd method, we have to solve the helmholtz equation repeatedly for wavefields at different frequencies, which is very
costly for a large number of frequencies. pinn-based helmholtz
equation solvers have been developed to retrieve the wavefield solutions from the machine learning perspective (song et al. 2021,
2022; alkhalifah et al. 2021a), and this work paves the way for
the new method presented in this paper. unlike the previous pinnbased helmholtz equation solvers, the fourier feature pinn-based
solver uses the fourier feature embedding step to establish a physical connection between input coordinates and output wavefields. it
overcomes the problem of spectral bias in pinn and manages to
simulate accurate multi-frequency wavefields.

pinnâ€™s training cost for solving the helmholtz equation is relatively high. however, if we take advantage of the theory of transfer
learning, the training cost of a new velocity model using a pretrained network can be significantly reduced. we can obtain this
trained network with a certain velocity before the actual seismic
surveys. we recommend using a velocity model that is close to the
true model in the study area. if this condition cannot be met, the
corresponding trained network can learn basic features of the target wavefields, even if a certain velocity reflects limited geological
background information. if we need wavefields in the time domain,
hundreds or thousands of frequencies are needed to obtain enough
time samples. the proposed method is faster than the numerical
solver in generating a large number of frequencies for frequency
domain wavefields. in large seismic surveys, thousands of sources
are usually used to obtain sufficient illumination of the subsurface.
in this case, the proposed method can simulate wavefields for thousands of sources in a very short time.
moreover, the proposed method is flexible and versatile for wave
equations in different media, such as anisotropic media, elastic
media, fluid-saturated porous media, etc. for numerical methods,
researchers need to rewrite the codes of wave equation solvers
for different media. in the framework of pinn, we can easily adjust the loss functions corresponding to different wave equations.
moreover, it is easy to convert the pinn-based helmholtz solver
from 2-d models to 3-d models by introducing an additional input for the third-dimensional coordinate. for fdfd wave equation
solvers in complex media or in 3-d models, we need to extend
the impedance matrix to introduce more physical parameters or a
high spatial dimension to simulate seismic waves. as a result, the
computational cost increases significantly. in these applications, the
proposed method will prove its attractiveness in terms of computational efficiency.

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

figure 11. time-domain scattered wavefield snapshots at (a) 0.6 s, (b) 0.8 s and (c) 1 s transformed from numerical frequency-domain wavefields ranging
from 5 to 10 hz with a 0.01 hz interval.

simulating seismic multifrequency wavefields

6 c o n c lu s i o n
we have proposed to simulate seismic multifrequency wavefields
using the fourier feature pinn. since this version of pinn exploits
the fourier feature, it is able to simulate wavefields with different
frequencies and source locations.
the four dimensions are the horizontal and vertical spatial coordinates of the model, the horizontal position of the source, and the
frequency each dimension is represented by a series of sinusoidal
functions that vary with different wavenumbers. these sinusoidal
functions are the fourier basis, and the associated coefficients are
embedded in pinn.
for embedding the fourier feature, we proposed a theoretical
wavenumber sampling strategy to achieve optimal training convergence. numerical experiments have shown the efficiency and accuracy of the method, in generating wavefields of different frequencies
with arbitrary source locations.

ac k n ow l e d g m e n t s
the authors are grateful to the sponsors of the centre for reservoir geophysics, imperial college london, for supporting this research. the research is also supported by the special fund of the
key laboratory of geoghysical exploration equipment, ministry
of education (jilin university).

d ata ava i l a b i l i t y
there are no field data associated with this paper.

conflict of interest
the authors acknowledge that there are no conflicts of interest
recorded.

references
alkhadhr, s., liu, x. & almekkawy, m., 2021. modelling of the forward
wave propagation using physics-informed neural networks, in proceedings of the 2021 ieee international ultrasonics symposium, pp. 1â€“4.
alkhalifah, t., song, c., bin waheed, u. & hao, q., 2021a. wavefield
solutions from machine learned functions constrained by the helmholtz
equation, artif. intell. geosci., 2, 11â€“19.
alkhalifah, t., song, c. & huang, x., 2021b. high-dimensional wavefield
solutions based on neural network functions, in proceedings of the first
international meeting for applied geoscience & energy, society of exploration geophysicists, pp. 2440â€“2444.
baydin, a.g., pearlmutter, b.a., radul, a.a. & siskind, j.m., 2017. automatic differentiation in machine learning: a survey, j. mach. learn. res.,
18, 5595â€“5637.
bishop, c.m., 2006. pattern recognition and machine learning, springer.
brossier, r., operto, s. & virieux, j., 2009. seismic imaging of complex onshore structures by 2d elastic frequency-domain full-waveform inversion,
geophysics, 74(6), wcc105â€“wcc118.
dablain, m.a., 1986. the application of high-order differencing to the scalar
wave equation, geophysics, 51, 54â€“66.
dai, h. & macbeth, c., 1995. automatic picking of seismic arrivals in local
earthquake data using an artificial neural network, geophys. j. int., 120,
758â€“774.
dai, t., xia, j., ning, l., xi, c., liu, y. & xing, h., 2021. deep learning
for extracting dispersion curves, surv. geophys., 42, 69â€“95.
engquist, b. & zhao, h., 2018. approximate separability of the greenâ€™s
function of the helmholtz equation in the high frequency limit, commun.
pure appl. math., 71, 2220â€“2274.
gentili, s. & michelini, a., 2006. automatic picking of p and s phases
using a neural tree, j. seismol., 10, 39â€“63.
glorot, x. & bengio, y., 2010. understanding the difficulty of training
deep feed forward neural networks, in proceedings of the thirteenth
international conference on artificial intelligence and statistics, pp. 249â€“
256.
grubas, s., yaskevich, s. & duchkov, a., 2021. localization of microseismic events using the physics-informed neural-network for traveltime
computation, in proceedings of the 82nd eage annual conference &
exhibition, pp.1â€“5.
hornik, k., stinchcombe, m. & white, h., 1989. multilayer feedforward
networks are universal approximators, neural netw., 2, 359â€“366.
huang, x., alkhalifah, t. & song, c., 2021. a modified physics-informed
neural network with positional encoding, in the first international meeting for applied geoscience & energy, society of exploration geophysicists, pp.2480â€“2484.
huang, x., liu, h., shi, b.,wang, z., yang, k., li, y., weng, b., wang, m.,
chu, h., zhou, j., yu, f., hua, b., chen, l. & dong, b., 2021. solving
partial differential equations with point source based on physics-informed
neural networks, arxiv:2111.01394.
izzatullah, m., yildirim, i.e., waheed, u.b. & alkhalifah, t., 2022. laplace
hypopinn: physics-informed neural network for hypocenter localization
and its predictive uncertainty, mach. learn. sci. technol., 3, 045001.
jo, c.-h., shin, c. & suh, j.h., 1996. an optimal 9-point, finite-difference,
frequency-space, 2-d scalar wave extrapolator, geophysics, 61, 529â€“537.
karimpouli, s. & tahmasebi, p. 2020. physics informed machine learning:
seismic wave equation, geosci. front., 11, 1993â€“2001.
kaur, h., pham, n. & fomel, s., 2020. improving the resolution of migrated images by approximating the inverse hessian using deep learning,
geophysics, 85(4), wa173â€“wa183.
kingma, d.p. & ba, j., 2014. adam: a method for stochastic optimization,
arxiv:1412.6980.
lee, h.y., koo, j.m., min, d. j., kwon, b.d. & yoo, h.s., 2010. frequencydomain elastic full waveform inversion for vti media, geophys. j. int.,
183, 884â€“904.
leshno, m., lin, v.y., pinkus, a. & schocken, s., 1993. multilayer feedforward networks with a nonpolynomial activation function can approximate
any function, neural netw., 6, 861â€“867.

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

in terms of accuracy, the finite difference method requires explicit
meshing to approximate the spatial derivatives, which can lead to
severe numerical dispersion errors if a coarse-mesh scheme is used.
these errors are almost inevitable in acoustic anisotropic media
due to the slow propagating shear waves. in comparison, pinn
computes spatial derivatives using a mesh-free technique, automatic
differentiation, which is immune to dispersion errors (song et al.
2021).
fwi is an important method for inverting the high-resolution
velocity model. pinn has been proposed to perform waveformbased inversion (rasht-behesht et al. 2022; song & alkhalifah
2022). in fwi, multiple frequency components are needed to cover
the entire wavenumber components of the velocity model. our
proposed method is able to provide wavefield solutions for multiple frequencies, which could potentially become an alternative
wave equation solver for fwi, especially for complex media. for
fwi, the velocity model will not change dramatically during update iterations, so a very limited number of l-bfgs trainings
will suffice. fwi requires a large number of wavefields for multiple sources and frequencies. in this case, the proposed method
is very fast in generating wavefields for multiple frequencies and
sources.

1513

1514

c. song and y. wang
let networks learn high frequency functions in low dimensional domains,
adv. neural. inf. process. syst., 33, 7537â€“7547.
tarantola, a., 1984. inversion of seismic reflection data in the acoustic
approximation, geophysics, 49, 1259â€“1266.
trefethen, l.n. & bau, d., 1997. numerical linear algebra, vol. 50, siam.
valentine, a.p. & trampert, j., 2012. data space reduction, quality assessment and searching of seismograms: autoencoder networks for waveform
data, geophys. j. int., 189, 1183â€“1202.
van den ende, m.p. & ampuero, j.p., 2020. automated seismic source
characterization using deep graph neural networks, geophys. res. lett.,
47(17), e2020gl088690.
van der baan, m. & jutten, c., 2000. neural networks in geophysical applications, geophysics, 65, 1032â€“1047.
virieux, j., calandra, h. & plessix, r.eÌ., 2011. a review of the spectral, pseudo-spectral, finite-difference and finite-element modelling techniques for geophysical imaging, geophys. prospect., 59, 794â€“813.
virieux, j. & operto, s., 2009. an overview of full-waveform inversion in
exploration geophysics, geophysics, 74(6), wcc1â€“wcc26.
voytan, d. & sen, m.k., 2020. wave propagation with physics informed
neural networks, in the expanded abstracts of seg international exposition and annual meeting, pp.3477â€“3481.
waheed, u. b., haghighat, e., alkhalifah, t., song, c. & hao, q. 2021. pinneik: eikonal solution using physics-informed neural networks, comput.
geosci., 155, 104833.
wang, h. & alkhalifah, t., 2021. direct microseismic event location and
characterization from passive seismic data using convolutional neural
networks, geophysics, 86(6), ks109â€“ks121.
wang, s., wang, h. & perdikaris, p., 2021. on the eigenvector bias of
fourier feature networks: from regression to solving multi-scale pdes
with physics-informed neural networks, comput. meth. appl. mech. eng.,
384, 113938.
wang, y., 2011. seismic, waveform modelling and tomography, in encyclopedia of solid earth geophysics, springer verlag, pp.1290â€“1301.
wang, y., 2016. seismic inversion: theory and applications, john wiley &
sons.
wang, y. & rao, y., 2009. reflection seismic waveform tomography, j.
geophys. res., 114(b3), b03304.
weiss, k., khoshgoftaar, t. m. & wang, d., 2016. a survey of transfer
learning, j. big. data, 3(1), 1â€“40.
wu, x., liang, l., shi, y. & fomel, s., 2019. faultseg3d: using synthetic
data sets to train an end-to-end convolutional neural network for 3d
seismic fault segmentation, geophysics, 84(3), im35â€“im45.
xu, z.j., zhang, y. & xiao, y., 2019. training behavior of deep neural
network in frequency domain, in international conference on neural
information processing, pp.264â€“274.
yang, q. & malcolm, a., 2021. frequency domain full-waveform inversion in a fluid-saturated poroelastic medium, geophys. j. int., 225,
68â€“84.
zhu, w. & beroza, g.c., 2019. phasenet: a deep-neural-network-based
seismic arrival-time picking method, geophys. j. int., 216, 261â€“273.

downloaded from https://academic.oup.com/gji/article/232/3/1503/6758508 by universidad eafit user on 24 august 2024

li, y., alkhalifah, t. & zhang, z., 2021. deep-learning assisted regularized
elastic full waveform inversion using the velocity distribution information
from wells, geophys. j. int., 226, 1322â€“1335.
liu, d. c. & nocedal, j., 1989. on the limited memory bfgs method for
large scale optimization, math. program., 45, 503â€“528.
mildenhall, b., srinivasan, p.p., tancik, m., barron, j.t., ramamoorthi, r. &
ng, r., 2020. nerf: representing scenes as neural radiance fields for view
synthesis, in european conference on computer vision, pp. 405â€“421.
moseley, b., markham, a. & nissen-meyer, t., 2020. solving the wave
equation with physics-informed deep learning, arxiv:2006.11894.
nemeth, t., wu, c. & schuster, g. t., 1999. least-squares migration of
incomplete reflection data, geophysics, 64, 208â€“221.
paffenholz, j., stefani, j., mclain, b. & bishop, k., 2002. sigsbee 2a
synthetic subsalt dataset-image quality as function of migration algorithm
and velocity model error, in proceedings of the 64th eage conference
& exhibition, cp-5-00108.
pan, s. j. & yang, q. 2009. a survey on transfer learning, ieee trans.
knowl. data eng., 22, 1345â€“1359.
rahaman, n., baratin, a., arpit, d., draxler, f., lin, m., hamprecht, f. &
courville, a., 2019. on the spectral bias of neural networks, proceedings
of machine learning research, 97, 5301â€“5310.
rao, y. & wang, y., 2017. seismic waveform tomography with shotencoding using a restarted l-bfgs algorithm, sci. rep., 7, 8494.
rao, y., wang, y. & han, d., 2019. seismic waveform tomography with
simplified restarting scheme, ieee geosci. remote sens. lett., 16, 135â€“
139.
rasht-behesht, m., huber, c., shukla, k. & karniadakis, g.e., 2022.
physics-informed neural networks (pinns) for wave propagation and
full waveform inversions, j. geophys. res., 127(5), e2021jb023120.
schwarzenberg-czerny, a., 1995. on matrix factorization and efficient least
squares solution, astron. astrophys. suppl. ser., 110, 405.
shi, y., wu, x. & fomel, s., 2019. saltseg: automatic 3d salt segmentation
using a deep convolutional neural network, interpretation, 7(3), se113.
smith, j.d., azizzadenesheli, k. & ross, z.e., 2020. eikonet: solving the
eikonal equation with deep neural networks, ieee trans. geosci. remote
sens., 59, 10 685â€“10 696.
smith, j.d., ross, z.e., azizzadenesheli, k. & muir, j.b., 2022. hyposvi:
hypocentre inversion with stein variational inference and physics informed neural networks, geophys. j. int., 228, 698â€“710.
song, c. & alkhalifah, t., 2022. wavefield reconstruction inversion via
physics-informed neural networks, ieee trans. geosci. remote sens.,
60, 5908012.
song, c., alkhalifah, t. & waheed, u.b., 2021. solving the frequencydomain acoustic vti wave equation using physics-informed neural networks, geophys. j. int., 225, 846â€“859.
song, c., alkhalifah, t. & waheed, u.b., 2022. a versatile framework to
solve the helmholtz equation using physics-informed neural networks,
geophys. j. int., 228, 1750â€“1762.
tancik, m., srinivasan, p., mildenhall, b., fridovich-keil, s., raghavan, n.,
singhal, u., ramamoorthi, r., barron, j. & ng, r., 2020. fourier features



ieee transactions on geoscience and remote sensing, vol. 61, 2023

5904313

autoencoded elastic wave-equation traveltime
inversion: toward reliable near-surface tomogram
chang zhang, jing li , member, ieee, han yu, and bin liu

abstractâ€” due to unexpected environmental variations and
poor consistencies in land data acquisition, complex near-surface
seismograms are usually polluted unreasonably with a low signalto-noise ratio (snr). these complicated circumstances bring
more challenges in identifying accurate first arrivals for the
following wave-equation traveltime (wt) inversion. recently, the
autoencoder (ae) is a typical unsupervised learning network,
whose basic principle is to compress the input seismic data for
their intrinsic features in the latent space with an encoder and,
thereafter, to decipher these features for seismic profiles as the
output with a decoder. this process is fully automatic with high
stability and is not very sensitive to data quality. in this article,
we propose an elastic wt inversion algorithm based on the ae
method (aewt) to invert the p-velocity model. compared with
the standard wt, the aewt method automatically extracts the
intrinsic features of the refractions with ae as reference data
for the misfit functionals. feature images in the latent space
show similar but intensified sensitivity to the traveltimes with
respect to velocity perturbations. we present one synthetic and
two field data tests for comparing the proposed aewt and the
standard wt tomograms to investigate the locations of a buried
fault and the depth of a buried sinkhole. all these experiments
demonstrate that the proposed elastic aewt method can reduce
errors caused by low snr and obtain a more reliable and stable
p-velocity tomogram.
index termsâ€” autoencoder (ae), elastic and acoustic waves,
velocity tomogram, wave-equation traveltime (wt) inversion.

i. i ntroduction

s

eismic first-arrival traveltime inversion is one of the
most critical methods to reconstruct the subsurface
p-velocity model. ray-tracing traveltime tomography (rt)
is a classical method with two significant advantages: good
stability and high computational efficiency [1], [2], [3]. sava
and fomel [4] proposed the huygens wavefront tracing (hwt)
method by combining the minimum traveltime algorithm and

manuscript received 15 june 2022; revised 16 october 2022 and 4 january
2023; accepted 4 february 2023. date of publication 6 february 2023; date
of current version 1 march 2023. this work was supported in part by the
national key research and development program of china under grant
2022yfc3080200 and in part by the natural science foundation of china
under grant 42174065 and grant 42222407. (corresponding author: jing li.)
chang zhang and jing li are with the college of geo-exploration science and technology and the key laboratory of applied geophysics, jilin
university, changchun 130026, china (e-mail: ljwy1209@163.com).
han yu is with the school of computer science, and the jiangsu key
laboratory of big data security and intelligent processing, nanjing university of posts and telecommunications, nanjing 210023, china (e-mail:
han.yu@njupt.edu.cn).
bin liu is with the school of qilu transportation and the geotechnical
and structural engineering techniques research center, shandong university,
jinan 250100, china (e-mail: liubin0635@163.com).
digital object identifier 10.1109/tgrs.2023.3243140

traditional rt methods to perform inversion by tracking rays
elaborately. leung and qian [5] introduced a fast sweeping
method to solve the adjoint-state equation. however, the rt
method is limited by the high-frequency hypothesis in forward
modeling techniques, using only part of the information in the
wavefield. it produces a low-resolved, macroscopic velocity
model, and the inversion fails when the wavelength of the
intrinsic velocity of the subsurface model is the same as the
source wavelet [6].
full-waveform inversion (fwi) is a popular strategy to
improve the velocity tomogram accuracy [7]. pratt and worthington [8] developed the frequency-domain fwi method,
which has been widely used in offshore oil exploration fields.
however, the fwi method is highly nonlinear concerning
parametric perturbations of multiphysical fields. several problems may lead to these difficulties in conducting fwi, such
as cycle skips under inaccurate amplitude modeling, elastic
effects, random noise, and an incorrect initial velocity model.
consequently, significant challenges emerge in dealing with
complex near-surface land data [9], [10].
to avoid the high-frequency assumption of rt and mitigate
the local minima problem of fwi, luo and schuster [11]
proposed the wave-equation traveltime (wt) inversion method
to reconstruct the p-velocity model. the wt method can
provide a more accurate initial velocity model for fwi than
the rt method because it is based on the wave-equation
imaging kernel. it needs more storage memory and computational time because the numerical wavefield should be
estimated and saved for every source. various modified wt
strategies have been implemented recently in a wide range
of applications. for example, the wave-equation reflection
traveltime inversion (werti) utilizes the reflection information [11]. luo et al. [12] proposed full traveltime inversion
(fti), where velocity perturbations only cause traveltime
changes, and the inversion only relies on traveltime information. zhang et al. [13] combined the traveltime, envelope, and
waveform information to adjust the information weights at
different inversion stages to improve the inversion accuracy.
wang et al. [14] used a single frequency component in the
frequency domain to achieve wt inversion without wavefield
reconstruction, which reduces memory requirements and computational costs.
for the standard wt method, picking traveltime manually
is very time-consuming. this procedure tends to cause a
significant uncertainty error due to the low signal-to-noise ratio
(snr) of the recorded traces. yu et al. [15], [16] proposed

1558-0644 Â© 2023 ieee. personal use is permitted, but republication/redistribution requires ieee permission.
see https://www.ieee.org/publications/rights/index.html for more information.

5904313

ieee transactions on geoscience and remote sensing, vol. 61, 2023

the acoustic wt inversion with the autoencoding technique
(aewt), which extracts intrinsic features from the refraction
waveforms by the ae network and inverts them for the
subsurface velocity model. now, the ae method has been
widely used in seismic signal denoising and inversion [17].
it recovers the input data while keeping the feature information
in the latent space and reducing the noise of the output data
in the process of data reconstruction.
following the previous acoustic aewt method [15],
we present the elastic aewt method to invert the p-wave
velocity distributions from land seismic data. the elastic
aewt tomograms are superior to acoustic aewt tomograms when significant s-waves are seen in the recorded
seismograms. it is more robust and reliable for applications
in field data. this article uses two synthetic and two field-data
examples to demonstrate the advantages of the proposed
elastic aewt over the acoustic aewt and the standard wt
methods. in section ii, we explain the basic theory of an
ae neural network and how aewt is implemented with
a proposed workflow. in section iii, the synthetic and field
data examples are presented to evaluate the feasibility of the
proposed aewt, acoustic aewt, and standard wt methods
in p-velocity tomogram. finally, in sections iv and v, the
implications of elastic aewt inversion capabilities and limitations are discussed.
ii. t heory of a ewt m ethod
a. autoencoder (ae) theory
the ae method was proposed in [18] and has since been
developed in several improved forms, such as convolutional
ae (cae), variational aes (vae), and ae with different
normalization terms [19], [20], [21].
the ae method is equivalent to a lossy signal compression
method but can be constantly trained to minimize the residuals
of the input and the output data [22]. data at the input
and the output ends are matched iteratively by the trained
ae network. fig. 1 is a seven-layered ae structure that is
designed to extract intrinsic features of the input data. the
batch normalization (bn) layer can effectively avoid the largescale oscillations in the latent space without affecting the
spatial distribution of the training data. it prevents the training
from falling into a local optimum and better captures the
intrinsic features of the training dataset. here, the activation
function is set as relu [23]. these settings reduce not only
the nonlinearity of the neural network training process but also
the training time. the activation function of the last layer is
set as a sigmoid function, which can normalize the data of
the output layer to the range of [0, 1], in accordance with the
range of the input data.

fig. 1.

schematic of the ae method.

where âˆ†Ï„ (x g , xs ) = Ï„ pre (x g , xs ) âˆ’ Ï„ obs (x g , xs ) indicates the
traveltime differences between the predicted data (Ï„ pre ) and
the observed data(Ï„ obs ). the subscripts g and s are used to
index the geophones and sources, respectively. the objective
function of aewt is similar to the standard wt method in
that the seismic traveltime information is replaced with feature
information in ae latent layer [15], [16]
2
1 xx
âˆ†l x g , xs
(2)
Ïµ = min
v(x) 2
s
g
where âˆ†l(x g , xs ) = l pre (x g , xs ) âˆ’ l obs (x g , xs ) indicates the
difference in feature information between the predicted and
observed data after being encoded from the same training
network. the p-wave velocity distribution v p is updated with
âˆ†v p = 2v p Ïâˆ†Î».

(3)

Î» is one of the lamÃ© parameter, which is used to update
velocity, and the gradient Î³ (x) is

x x âˆ‚âˆ†l x g , xs

âˆ‚Ïµ
Î³ (x) = âˆ’
=âˆ’
âˆ†l x g , xs . (4)
âˆ‚Î»(x)
âˆ‚Î»(x)
s
g
a connection function f is used to relate the seismic data
to the model parameter and obtain an analytic expression for
the gradient equation (4), which is defined as follows [25]:

z obs


el âˆ— +l x g , t; xs pre
 el âˆ— x g , t; xs dt
(5)
f x g , t; xs =
aobs x g , t; xs

b. inversion theory of elastic aewt

where l âˆ— and l âˆ— + l indicate the feature information encoded
from the predicted and observed seismic data, respectively.
pre
elobs
âˆ— +l (x g , t; x s ) and el âˆ— (x g , t; x s ) indicate the envelopes of the
observed and the predicted data corresponding to the feature
information.
aobs (x g , t; xs ) represents the max value of the observed data
for normalization. if the observed and the calculated data agree
with each other, the gradient of connection function is

âˆ‚ f x g , t; xs
fË™âˆ†l =
âˆ‚l
l=âˆ†l

z obs

eÌ‡l âˆ— +1l x g , t; xs pre
 el âˆ— x g , t; xs dt = 0
=
(6)
obs
a
x g , t; xs

the standard wt method uses the following objective
function to minimize the traveltime residual [24]
2
1 xx
âˆ†Ï„ x g , xs
(1)
Ïµ = min
v(x) 2
s
g

where fË™ and eÌ‡ denote the first-order previously in wt, and the
dot means the time derivative; now, it represents the derivative
with respect to the latent variable(s) of f and e, respectively;
when l âˆ— in (5) is constant, the value of l represents the
actual difference between the two channels of data in the

zhang et al.: autoencoded elastic wt inversion

5904313

latent layer. when l gradually approaches âˆ†l, the refraction
waveforms correspond to the true and initial models are closer
to each other. this process is similar to the traveltime getting
progressively matched in the wt method. if the derivative of
the connection function is zero, then the connection function
achieves its maximum value (l = âˆ†l). the minimization of
seismic waveform differences in the connection function are
represented by reducing their feature difference term âˆ†l.
using the implicit function theorem to calculate
(âˆ‚âˆ†l(x g , xs )/âˆ‚Î»(x)), then (4) can be extended as follows:
âˆ‚âˆ†l(x)
=
âˆ‚Î»(x)

âˆ‚ fË™
âˆ‚Î»(x)
âˆ‚ fË™
âˆ‚âˆ†l(x)

where the denominator is a scaler
z
 pre

âˆ‚ fË™
= eÌˆlobs
x g , t; xs dt = k .
âˆ— +âˆ†l x g , t; x s el âˆ—
âˆ‚âˆ†l(x)

(7)

(8)

then, the adjoint-state method [26] is used to derive the
gradient Î³ for aewt in appendix a [27]. it can be written
as follows:
x x 1 âˆ‚ fË™

âˆ†l xg , xs
Î³ (x) =
k âˆ‚Î»(x)
s
g


z
x

âˆ‚w âˆ‚u
=
+
ÏƒÌ‚ x x + ÏƒÌ‚ zz dt
(9)
âˆ‚z
âˆ‚x
s
where (ÏƒÌ‚ x x , ÏƒÌ‚ zz ) are the adjoint-state variables of (Ïƒ x x Ïƒzz )
derived in appendix a. w and u are the horizontal and vertical
velocities, respectively.
the proposed elastic aewt method is a straightforward
extension of the acoustic aewt method presented in [15]
and [16]. we need to replace the acoustic finite-difference
modeling with the elastic finite-difference modeling and to
calculate the gradients using the perturbation solution of the
elastic wave equation. in addition, an adaptive window is
chosen based on the approximate velocity range of the first
arrivals. we open a certain window after the first arrivals,
which is used to mute the surface waves and other scattered
waves before applying elastic aewt.
c. training ae
the training process of the ae consists of the following
steps.
1) select an adaptive time window to mute the surface
and other scatter waves. then, the envelope waveforms are
calculated as input after normalization on the common shot
gather (csg), and this process reduces the data complexity
for the next training.
2) tracewise normalization can balance the far-offset and
near-offset distance influence on the network during the training process. on the one hand, the difference in magnitude
between the far offset and the near-offset data is too large,
making it difficult to fit and converge the neural network.
on the other hand, it increases the complexity of the neural
network training process. taking the trace envelope is also a
simplification of the data, which facilitates the fitting between

fig. 2.

aewt inversion flowchart.

the input and the output ends of the neural network and reduces
the training difficulty.
a neural network (nn) with three layers of both the encoder
and the decoder can match the waveforms between the input
and output. if there are small perturbations in the input data,
the output data can be reconstructed when the data residuals
are lower than 1% of the original loss. at this point, the neural
network can be used for aewt inversion. to speed up the
training process, the adam optimizer is applied with some
important parameter sets as in [28].
the workflow of aewt mainly includes the following five
steps (fig. 2).
1) generate training data, using the observed or predicted
data with a typical model.
2) select an adaptive time window for the observed training
samples to mute the surface wave, and some noise disturbances
retain the first arrival waveform.
3) normalizing the data obtained in 2), taking the envelope
waveform and other data processing methods, and reshaping
the data into a training set.
4) the data obtained in 3) are fed into an ae for training
and recognized by the trained neural network.
5) iteratively update the inverse velocity model with the
trained neural network until it converges with the conjugate
gradient method [29]. here, the line-search method is used to
estimate the step length [30].
network structure: after testing, the ae has seven fully
connected layers, with the number of neurons in each layer
being input-32-2-1-2-32-output. the choice of such a network
structure reduces a lot of neurons in the first layer. the
structure improves the stability and reduces the oscillations
in the residual curves during the training process. the input
and output sections of the network are of the same size as
the length of a seismic trace. a bn layer is added between
the encoder and the latent layer to enhance the ability of the
neural network to learn the features and to reduce the training
time. first, the 2-d velocity model is converted into a set of
1-d seismic traces, and the data are fed into the ae in batch.
the data obtained are in the same format as the initial arrival
of the wt method and finally combined into a 2-d graph and
fed into the inversion program to get the inversion results.
iii. n umerical r esults
this section presents the tests with the proposed elastic
aewt, acoustic aewt, and standard wt methods on two
synthetic models and two near-surface field datasets from the

5904313

fig. 3.

ieee transactions on geoscience and remote sensing, vol. 61, 2023

(a) true p-wave velocity and (b) initial p-wave velocity models.

fig. 4. (a) first-arrival traveltime matrix of wt and (b) encoding result
matrix of aewt.

aqaba fault in saudi arabia and a sinkhole site at park city,
utah.
a. synthetic velocity model test
the synthetic p-wave velocity model is a two-layer model
with an irregular interface, in which the model size is
26 m Ã— 120 m [fig. 3(a)]. the s-wave velocity model is
estimated from the gardner formula vp = 1.732 Ã— vs [31].
a finite-difference algorithm for the 2-d elastic-wave equation
is used to compute 20 shot gathers with 60 geophones evenly
deployed on the free surface with the shot interval of 6 m
and the receiver interval of 2 m in space, respectively. the
source wavelet is a ricker wavelet with a peak frequency
of 30 hz. this synthetic test compares the feasibility of the
present elastic aewt and standard wt methods in p-velocity
tomograms. the initial p-velocity model is a linearly gradient
velocity model, as shown in fig. 3(b).
fig. 4(a) and (b) shows the picked first-arrival matrix and
the encoded feature matrix with aewt, respectively. the
traveltime matrix in fig. 4(a) reveals the characteristics of the
irregular interface structure. although the feature encoded by
ae has no explicit physical meaning, it apparently provides a
good feature of the interface structure of the velocity model
[fig. 4(b)]. both aewt and standard wt methods can invert
for high-accuracy velocity tomograms [fig. 5(a) and (b)]. the
model test demonstrates that the aewt method can obtain
decent velocity inversion results compared with the standard
wt method using the same inversion parameters.
1) noise sensitivity: to test the sensitivity of the proposed
method to the noise, a group of random time shifts and
âˆ’20-db noise (assuming the power of observed data is 0 db)
are, respectively, added to the raw csgs [fig. 6(a) and (d)].
the corresponding aewt and wt velocity tomograms with

fig. 5. p-wave velocity tomogram with (a) wt and (b) aewt (the white
line is the true position of the irregular interface).

random time shifts are shown in fig. 6(b), (c), (e), and (f).
the aewt result has higher accuracy than the standard wt
tomogram, where the white line indicates the position of the
interface of the true model.
for the random noisy data, it is challenging to pick accurate
first breaks to be used for the standard wt method. the
corresponding wt tomograms with noisy data are shown in
fig. 6(e). due to significant uncertainties and errors in the
picking data, the imaging quality of the standard wt method

zhang et al.: autoencoded elastic wt inversion

5904313

fig. 6. (a) csg data with a group of random shifts, (b) and (c) standard wt and the aewt tomograms with random shift, (d) âˆ’20 db random noise, and
(e) and (f) wt and the aewt tomograms with âˆ’20 db noise data.

fig. 7.

(a) and (d) csg with different time windows, (b) and (e) standard wt p-velocity tomogram, and (c) and (f) aewt p-velocity tomogram.

is very low. on the contrary, the aewt tomogram with noisy
data has good inversion accuracy [fig. 6(f)]. this sensitivity
test suggests that the aewt is more robust to the noisy input
data than the standard wt method.
2) window muting test: the adaptive window selection is
another critical parameter affecting the standard wt method.
if the window is too large, it will bring redundant information,
resulting in nonconvergence inversion. if the window is too
small, the first arrival information will be lost, resulting in

low accuracy inversion results. in contrast, aewt extracts
the intrinsic features of all refraction information in the time
window range (almost one wavelets), not just the first arrival
time. so, the aewt is not affected by the adaptive window.
the following test compares the effect of the adaptive
time window for aewt and the standard wt method.
fig. 7(a) and (d) shows one-shot gather with a different timewindow muting, and fig. 7(b)â€“(f) shows the corresponding
standard wt and proposed aewt p-velocity tomograms.

5904313

ieee transactions on geoscience and remote sensing, vol. 61, 2023

fig. 8. complex low-velocity layer model test [(a) true model, (b) starting model, (c) acoustic aewt p-velocity tomogram with elastic data, and (d) elastic
aewt tomogram with elastic data]. where the black line indicates the interface between the high velocity and low velocity layers.

the results demonstrate that the adaptive window significantly
affects the standard wt method and has little influence on the
aewt method.
3) elastic aewt and acoustic aewt: a complex velocity
model is used to test the feasibility of the proposed elastic aewt and previous acoustic aewt. fig. 8(a) shows
this multilayer p-velocity model with the near-surface lowvelocity zone (lvz), where the s-wave velocity model is also
estimated from the empirical relationship vp = 1.732 Ã— vs.
the linearly gradient velocity model is used as the initial
model. fig. 8(c) is the p-velocity tomogram with acoustic
aewt. the recorded data have converted s-wave energy,
so the acoustic aewt tomogram is challenging to reveal
the lvz layer interface. however, the elastic aewt method
provides a more accurate inverted velocity model [fig. 8(d)].
the lvz layer in the elastic aewt tomogram is almost
consistent with the interface in the true model, which provides
a reliable initial model for the elastic fwi in the near-surface
application.
b. first field example: gulf of aqaba data, saudi arabia
two land datasets are selected for experimental comparisons
of methodologies between the elastic aewt and the standard
wt method for the p-wave velocity tomogram.
the first seismic experiment was carried out near the gulf
of aqaba [fig. 9(a)]. the survey line is 300 m long with
120 geophones and shots, each with a spacing of 2.5 m. the
sampling interval of seismic records is 1 ms, and the recording
time of each trace is 0.8 s. the survey line crosses the shallow
surface fault structure in the aqaba area, and a clear fault zone
is visible on the surface [fig. 9(b)].
csgs 1, 30, 60, and 90 after data preprocessing are shown
in fig. 10. the air, rayleigh wave, reflection, and refraction

fig. 9. (a) location of the seismic survey line and (b) photograph shows an
earthquake scarp on the surface [32].

arrivals are indicated in seismic shot gathers. due to topography and geophone coupling, there is some time shift in
the starting point of different shots, which can be found in
the zero-offset profile [fig. 11(a)]. the direct wave energy is
not flat at different horizontal positions. we select reference
points to correct shot gathers. the reference points are the
max energy point in traces. the rectified zero offset profile
has good consistency with different shot gathers [fig. 11(b)].
fig. 11(c) shows the common offset gathers (cogs) with the
offset of 40 m, in which the events are no longer continuous
everywhere, especially at x = 145 m. the time delays shot
gathers in the distorted area implies that the velocity is smaller
and suggests the location of a possible fault. the processed
csgs are used as training datasets for the aewt method.
fig. 12 shows the encoded aqaba csg feature information
matrix for aewt inversion.
the initial p-velocity model is a gradient velocity model
discretized into 50 Ã— 598 grid points with a grid size of
0.5 m. the aewt and wt tomograms after 50 iterations
are shown in fig. 13(a) and (b). the aewt and wt

zhang et al.: autoencoded elastic wt inversion

fig. 10.

5904313

aqaba seismic data csg (a) no.1, (b) no.30, (c) no.60, and (d) no.90.

fig. 12.

fig. 11.
aqaba seismic data zero-offset section (a) before correction,
(b) after correction cogs, and (c) offset = 40-m cog.

tomograms suggest that the hidden fault location is at 135 m <
x < 145 m, agreeing very well with the geological map in
fig. 9(b). however, compared with the standard wt tomogram
[fig. 13(a)], the aewt tomogram [fig. 13(b)] reveals more
details in the bedrock interface. processed by the aewt
neural network, the ultimate decoded waveform is smoother
than the input data. therefore, if there is some intense noise,

encoded aqaba csg matrix for aewt inversion.

aewt utilizes the complete information of the data, making
it more stable compared with the standard wt method. the
inverted waveform by aewt is very close to the raw observed
data [fig. 14(a)]. the red curves represent the observed data,
and the black curves are the predicted data. the difference
between the observed data and the inversion result of aewt
is less than that of the wt result [fig. 14(b)].
for a sanity test, comparing the predicted and observed
cogs is a reliable way to evaluate the accuracy of the velocity
tomogram. different offset cogs profiles indicate the lateral
velocity variations at different depths. fig. 15 shows the cog
comparisons of the aewt inverted data, the standard wt
inverted data, and the raw data at a different offset. all these
cog profiles are able to capture the trend of the observed
events. however, in the cog profile with offset = 37.5 m, the

5904313

ieee transactions on geoscience and remote sensing, vol. 61, 2023

table i
e rror p ercentage of aewt and s tandard wt i nverted data
cog p rofile at d ifferent o ffsets

table ii
parameters of the s eismic s urvey of the
s inkhole at park c ity, u tah

fig. 13.
aqaba elastic p-wave velocity tomograms with (a) wt and
(b) aewt methods.

p
rwt =
((|awt âˆ’ aobs |)/aobs ) represented the errors in
percentage between the predicted data cogs and the observed
data cogs. aaewt , awt , and aobs represent the maximum
amplitude curves of cogs, respectively. table i shows that
the errors of aewt at different offset distances are smaller
than the standard wt method, which verifies that the aewt
velocity tomogram can be more convincible than that with the
standard wt method.
fig. 14. waveform comparison between raw and inverted data for (a) aewt
and (b) standard wt.

c. second field example: park city data, utah

red arrow indicates a high-velocity anomaly [fig. 15(a)â€“(c)].
the aewt [fig. 15(b)] method recovers the anomaly, while
the wt method [fig. 15(c)] misses it. the red arrow indicates a low-velocity anomaly in the cog profile with offset = 75 m [fig. 15(d)â€“(f)]. the aewt recovers the cog
downward trend, while wt produces an upward trend. the
red arrow indicates a high-velocity anomaly in the cog
profile with offset = 112.5 m [fig. 15(g)â€“(i)]. the aewt
[fig. 15(h)] method recovers the anomaly better than the wt
method [fig. 15(i)]. this comparison suggests that the aewt
tomogram provides a more accurate velocity model than the
standard wt tomogram.
we also pick the arrival times with maximum amplitudes
from different offset cogs of the events. fig. 16 compares the
aewt, the standard wt, and the raw data cogs traveltime
with blue, green, and red lines. the aewt cogs traveltime
is in good agreement with the raw cogs traveltime, and the
standard wt traveltimes have some errors. this quantitative
traveltime comparisons describe the differences between the
inverted and the raw data in a more accurate sense. it verifies
that the proposed elastic aewt can obtain more stable and
higher velocity accuracy than the standard wt method for
near-surface seismic field data.
the traveltime errors between the inverted and the raw
data are also compared at thepsame offset distance, as shown
in table i. raewt =
((|aaewt âˆ’ aobs |)/aobs ) and

the second field dataset is recorded to search for the
extension of an existing sinkhole at park city, utah (fig. 17).
the pond and sink hole are parallel to three seismic lines
and perpendicular to the rest of one seismic line. in this work,
it is shown that the position of the sinkhole can be accurately
located with our proposed inversion method. a sledgehammer
hitting a metallic plate is used to excite seismic p-waves,
where each shot location will be fired multiple times for
stacking to increase the snr of the recorded data. the
associated parameters are shown in table ii, including the
number of shots, geophones, and so on. the survey lines were
used to reveal the sinkholes distributed throughout the work
area. the blue part shows the known sinkholes, whose velocity
is relatively low.
the four survey lines were excited with a sledgehammer
hitting the metal plate, except for the first line, which was
excited five times, and the rest were excited 15 times per shot,
and then, the geophone data were stacked to increase the snr.
the proposed aewt method is used to invert for the p-wave
velocity tomogram.
fig. 18(a)â€“(d) shows the aewt p-velocity tomograms
from lines 1 to 4. in line 1, there are no local anomalies.
however, the interface between the bedrock and the soils can
be seen at 6â€“8 m under the ground. in line 2, two distinct lvz
represent cavernous rocks at x = 30 m, 80 m in the horizontal
distance and z = 15 m in depth, respectively [fig. 18(b)].
at the top of this zone lies one large lvz in the shallow

zhang et al.: autoencoded elastic wt inversion

5904313

fig. 15. (a), (d), and (g) comparisons of the observed cogs, (b), (e), and (h) aewt inverted cogs, and (c), (f), and (i) standard wt inverted cogs. the
red arrows depict the event structures.

fig. 17.

fig. 16. arrival times associated with maximum amplitudes for different
offset cogs of aewt, standard wt, and raw data, respectively.

subsurface at 5 m under the ground. this lvz corresponds to
a highly saturated soil/rock, indicating the pondâ€™s distribution.
in line 3, there is one lvz with a depth of 12âˆ’16 m and

aerial map shows the locations of seismic lines.

offsets between 40 and 60 m. inside lvz, a lower velocity
value region is within the range of 50 m horizontally and 15 m
deep. these two zones are interpreted to be partly cavernous.
in line 4, lvz is shown between offsets 14âˆ’40 m and depths
10âˆ’20 m, consistent with the lvz in line 2.
fig. 19 presents the comparisons of four survey linesâ€™
1-d vertical velocity profiles at different coincidence positions. it can be seen that the inverted velocity distributions
of different survey lines at the coincidence position are in

5904313

fig. 18.

ieee transactions on geoscience and remote sensing, vol. 61, 2023

four survey lines aewt p-velocity tomograms. (a) line 1, (b) line 2, (c) line 3, and (d) line 4.

fig. 19. 1-d vertical velocity comparison of aewt p-velocity tomograms.
(a) lines 3 and 4, (b) lines 2 and 3, and (c) lines 1 and 4.

good agreement. following the above sanity test, the cog
comparisons of the raw and the inverted data are shown in
fig. 20. cogs with the aewt method are consistent with
the raw data cogs. fig. 21 shows the 3-d spatial distribution
of the four p-velocity tomograms and the actual line location.
the subsurface low-velocity sinkhole anomaly can be verified
and revealed from the p-wave tomograms associated with
the four survey lines. according to the geological survey, the
results show significant agreement with the existing sinkhole.
for example, the velocity distributions of the western edge of
line 2 and the central part of line 3 overlapped. lines 3 and
4 are connected at the 40th access hole, and the velocity
models between the two survey lines can obviously match
each other. two holes (blue area in the map) are shown in
the map, which indicate low-velocity anomalies along survey
lines 2, 3, and 4, and the low-velocity location matches the
hole location. line 1 crosses the sewage pipe, and there may be
high-resistance objects surrounding the sewage pipe, causing
the high resistance anomaly on line 1.
iv. d iscussion
compared to the standard wt method, the ae strategy does
not need to pick the traveltime information, and the entire
inversion and training processes are completely automatic.
in addition, the ae method is much less sensitive to the first
arrival information than the traditional method. nevertheless,
the ae inversion requires additional neural network training,
which costs about 10â€“30 min with one rtx 3080 gpu.

in this study, the same neural network with identical trained
parameters is used for the two model tests and field data
examples. the comparisons of the results between the elastic and the acoustic aewt methods demonstrate that both
algorithms can produce a reliable p-wave velocity structure
with weak s-wave energy. elastic aewt inversion is superior
to its acoustic peer when significant s-waves exist in the
seismograms.
the disadvantage of elastic aewt inversion is five times
more computationally expensive than the acoustic method. the
aewt strategy shows better inversion stability and accuracy
for complex velocity structures, especially with low-velocity
layers. also, tests on the synthetic and real datasets imply
that the aewt velocity tomograms can be used as a reliable
initial model for fwi. in future work, the elastic aewt and
waveform inversion method will be tested to invert both the
p-wave and s-wave velocity distributions simultaneously.
v. c onclusion
we propose an elastic wt inversion based on the ae
method (aewt). the ae network extracts the intrinsic features from low snr data more efficiently and accurately than
the standard traveltime procedure. moreover, the feature information obtained by the encoderâ€™s compression dramatically
improves the efficiency of manually picking up the first arrival
information. numerical experiments on the synthetic models
and two near-surface seismic land datasets verify that the
proposed aewt method can reduce the uncertainty errors
caused by the inaccurate extraction of first arrival waveform
information and effectively improve the velocity inversion
accuracy. besides, incorporating the ae methodology into the
wt inversion significantly improves the resulting p-velocity
tomogram and becomes more geologically interpretable. the
ae technique provides a new direction of seismic wave
equation traveltime inversion. moreover, the near-surface pwave velocity models obtained from aewt can be used as
input models for the elastic fwi. in future work, we will
continue to optimize the parameter selections of networks of
ae types (i.e., the transformer) and therefore extending such
inversion applications to more fields with other geophysical
signals.

zhang et al.: autoencoded elastic wt inversion

5904313

fig. 20. cog profile comparisons of the observed and the predicted data corresponding to lines 2 and 4 (a) raw data cog (offset=26 m). (b) raw data
cog (offset=40.5 m). (c) aewt inverted data cog (oggset=26 m). (d) aewt inverted data cog (oggset=40.5 m).

the particle velocity components and stresses. equation (a1)
can be written as matrix
a
}|

zï£«

fig. 21. four survey line distributions of the aewt p-velocity tomograms
in 3-d space.

a ppendix a
in our analysis, we assume that the wave propagation honors
the 2-d isotropic elastic equations


âˆ‚
âˆ‚
âˆ‚
Ï uâˆ’
Ïƒx x + Ïƒx z = 0
âˆ‚t
âˆ‚x
âˆ‚z


âˆ‚
âˆ‚
âˆ‚
Ï wâˆ’
Ïƒzz +
Ïƒx z = 0
âˆ‚t
âˆ‚z
âˆ‚x
âˆ‚
âˆ‚
âˆ‚
Ïƒx x = (Î» + 2Âµ) u + Î» w + sx x
âˆ‚t
âˆ‚x
âˆ‚z
âˆ‚
âˆ‚
âˆ‚
Ïƒzz = (Î» + 2Âµ) w + Î» u + szz
âˆ‚t
âˆ‚z
âˆ‚

 x
âˆ‚
âˆ‚
âˆ‚
Ïƒx z = Âµ
w+ u
(a1)
âˆ‚t
âˆ‚x
âˆ‚z
where Î» and Âµ represent the lamÃ© parameter, respectively. sx x
and szz denote the source function. (u, wÏƒx x , Ïƒzz , Ïƒx z ) denote

âˆ‚
ï£¬ Ï âˆ‚t
ï£¬
ï£¬
ï£¬ 0
ï£¬
ï£¬
ï£¬ âˆ’Ï€ âˆ‚
ï£¬
âˆ‚x
ï£¬
âˆ‚
ï£¬
ï£¬ âˆ’Î»
ï£¬
âˆ‚x
ï£­
âˆ‚
âˆ’Âµ
âˆ‚z

0
âˆ‚
Ï
âˆ‚t
âˆ‚
âˆ’Î»
âˆ‚z
âˆ‚
âˆ’Ï€
âˆ‚z
âˆ‚
âˆ’Âµ
âˆ‚x

âˆ‚
âˆ’
âˆ‚x
0
âˆ‚
âˆ‚t
0
0

0
âˆ‚
âˆ’
âˆ‚z
0
âˆ‚
âˆ‚t
0

ï£¶{
âˆ‚
âˆ’ ï£·
âˆ‚z ï£·
âˆ‚ ï£·
âˆ’
ï£·
âˆ‚x ï£·
ï£·
0 ï£·
ï£·
ï£·
ï£·
0 ï£·
ï£·
âˆ‚ ï£¸
âˆ‚t

w

f

zï£« }| ï£¶{ ï£«
z }| ï£¶{
u
fx x
ï£¬ w ï£· ï£¬ f zz ï£·
ï£¬
ï£· ï£¬
ï£·
ï£¬ Ïƒx x ï£· = ï£¬ 0 ï£· .
ï£¬
ï£· ï£¬
ï£·
ï£­ Ïƒzz ï£¸ ï£­ 0 ï£¸
Ïƒx z
0

(a2)

according to the adjoint states method, the perturbation of
the state variable w with respect to the model variable m is
given by (âˆ‚w(m)/âˆ‚m). model variable m includes the lamÃ©
parameter Î» and Âµ, and (âˆ‚w(m)/âˆ‚Î») and (âˆ‚w(m)/âˆ‚Âµ) can be
obtained from
a(m)w(m) = f
âˆ‚a(m)
âˆ‚w(m)
w(m) + a(m)
=0
â‡’
âˆ‚m i
âˆ‚m i
âˆ‚w(m)
âˆ‚a(m)
â‡’
= âˆ’aâˆ’1 (m)
w(m) (a3)
âˆ‚m i
âˆ‚m i

5904313

ieee transactions on geoscience and remote sensing, vol. 61, 2023

where a is the elastic modeling operator in (a2). the connective function in (5) can also be written as follows:
d

obs e
fË™âˆ†l = ar wl xg , t; xs , wÌ‡lâˆ’âˆ†l xg , t; xs
(a4)
where âŸ¨u, vâŸ© represents the inner product between the vectors u
and v and ar is a sampling operator that samples the wavefield
at the receiver locations, while the adjoint of it, arâˆ— , sprays
the recorded data from the receiver coordinates to the model
coordinates.
the misfit function for aewt is
2
1 xx
âˆ†l x g , xs
(a5)
Ïµ = min
2 s g
and p-wave velocity v p update with
âˆ†v p = 2v p Ïâˆ†Î».

(a6)

the gradient can be written as follows:
âˆ‚Ïµ
âˆ‚Î»
âˆ‚ fË™
x x âˆ‚Î»(x)

=
âˆ†l
x
,
x
g
s
Ë™
âˆ‚f
s

g

âˆ‚âˆ†l

x x 1 âˆ‚ fË™

=
âˆ†l xg , xs
k âˆ‚Î»(x)
s
g
xx 1 âˆ‚ d
obs e
ar wl (x, t; xs ), wÌ‡lâˆ’âˆ†l xg , t; xs
=
k âˆ‚Î»
s
g

Ã—âˆ†l xg , xs

xx âˆ‚wl (x, t; xs )
obs
1
, wÌ‡lâˆ’âˆ†l xg , t; xs
âˆ†l xg , xs
=
ar
âˆ‚Î»
k
s g


x âˆ‚a
(a7)
wl (x, t; xs ), wâˆ— (x, t; xs )
=âˆ’
âˆ‚Î»
s
and wâˆ— = (uÌ‚, wÌ‚, ÏƒÌ‚ x x , ÏƒÌ‚ zz , ÏƒÌ‚ x z ) is the adjoint-state variable of w = (u, w, Ïƒx x , Ïƒzz , Ïƒx z ) which is calculated by a
finite-difference solution for the adjoint-state equations of the
elastic wave equation. wl denotes the predicted event for
velocity model recorded at the receiver location xg due to a
obs
source excited at time t = 0 and at location xs while wÌ‡lâˆ’âˆ†l
denotes the same event in the observed data.
the adjoint of the elastic equations can be written as
follows:
ï£«
ï£¶
âˆ‚
âˆ‚
âˆ‚
âˆ‚
Ï
0
âˆ’Ï€
âˆ’Î»
âˆ’Âµ
ï£¬ âˆ‚t
âˆ‚x
âˆ‚x
âˆ‚z ï£·
ï£¬
ï£«
ï£¶
âˆ‚
âˆ‚
âˆ‚
âˆ‚ ï£·
ï£· uÌ‚
ï£¬
Ï
âˆ’Î»
âˆ’Ï€
âˆ’Âµ
ï£¬ 0
ï£·
ï£¬
âˆ‚t
âˆ‚z
âˆ‚z
âˆ‚ x ï£·ï£¬ wÌ‚ ï£·
ï£·ï£¬
ï£·
ï£¬ âˆ‚
âˆ‚
ï£¬âˆ’
ï£¬ ÏƒÌ‚ x x ï£·
0
0
0 ï£·
ï£¬ âˆ‚x
ï£·
ï£¬
ï£·
âˆ‚t
ï£¬
ï£·ï£­ ÏƒÌ‚ zz ï£¸
âˆ‚
âˆ‚
ï£¬
ï£·
ï£¬ 0
âˆ’
0
0 ï£· ÏƒÌ‚ x z
ï£¬
ï£·
âˆ‚z
âˆ‚t
ï£­ âˆ‚
âˆ‚
âˆ‚ ï£¸
âˆ’
âˆ’
0
0
âˆ‚z
âˆ‚x
âˆ‚t ï£«
ï£¶
0
ï£¬ fË†w ï£·
ï£¬
ï£·
ï£·
=ï£¬
ï£¬ 0 ï£· (a8)
ï£­0 ï£¸
0

where fË†w is the residual trace that is backpropagated at every
iteration and is given by the following equation:
syn

1
xg , t; xs
âˆ†l xg , xs .
fË†w =
(a9)
kw
using (a2), we can get
ï£«
0
ï£¬ 0
ï£¬
âˆ‚
âˆ‚a ï£¬
ï£¬
= ï£¬ âˆ’âˆ‚x
ï£¬ âˆ‚
âˆ‚Î»
ï£¬âˆ’
ï£­
âˆ‚x
0

0
0
âˆ‚
âˆ’
âˆ‚z
âˆ‚
âˆ’
âˆ‚z
0

0
0

0
0

0

0

0

0

0

0

ï£¶
0
0ï£·
ï£·
ï£·
0ï£·
ï£·
ï£·
0ï£·
ï£¸
0

(a10)

and using (a7) and (a10), we can write the gradient as
follows:
âˆ‚Ïµ
âˆ‚Î»

x âˆ‚a
=âˆ’
wl (x, t; xs ), wâˆ— (x, t; xs )
âˆ‚Î»
s
ï£¶
ï£«
0
0
0 0 0 ï£«
ï£¶ ï£«
ï£¶
uÌ‚
u
ï£¬
0
0 0 0ï£·
ï£·ï£¬
*ï£¬ 0
ï£¬
ï£·+
ï£·ï£¬ w ï£·
âˆ‚
âˆ‚
x ï£¬
ï£· ï£¬ wÌ‚ ï£·
ï£·ï£¬
ï£¬âˆ’
âˆ’
0
0
0
ï£¬
ï£·
=âˆ’
ï£·ï£¬ Ïƒx x ï£·
ï£¬ âˆ‚x
âˆ‚z
ï£·, ï£¬ ÏƒÌ‚ x x ï£·
ï£·ï£­
ï£¬ âˆ‚
ï£­
ï£¸
âˆ‚
s
Ïƒzz
ÏƒÌ‚ zz ï£¸
ï£¬âˆ’
âˆ’
0 0 0ï£·
ï£¸ Ïƒ
ï£­
ÏƒÌ‚ x z
âˆ‚x
âˆ‚z
xz
0
0
0 0 0
x z  âˆ‚w âˆ‚u 

=
+
ÏƒÌ‚ x x + ÏƒÌ‚ zz dt.
(a11)
âˆ‚z
âˆ‚x
s
r eferences
[1] v. vinje, e. iversen, and h. gjÃ¸ystdal, â€œtraveltime and amplitude
estimation using wavefront construction,â€ geophysics, vol. 58, no. 8,
pp. 1157â€“1166, aug. 1993.
[2] w. a. schneider, k. a. ranzinger, a. h. balch, and c. kruse,
â€œa dynamic programming approach to first arrival traveltime computation in media with arbitrarily distributed velocities,â€ geophysics, vol. 57,
no. 1, pp. 39â€“50, jan. 1992.
[3] f. qin, y. luo, k. b. olsen, w. cai, and g. t. schuster, â€œfinitedifference solution of the eikonal equation along expanding wavefronts,â€
geophysics, vol. 57, no. 3, pp. 478â€“487, mar. 1992.
[4] p. c. sava and s. fomel, â€œangle-domain common-image gathers
by wavefield continuation methods,â€ geophysics, vol. 68, no. 3,
pp. 1065â€“1074, 2003.
[5] s. leung and j. qian, â€œan adjoint state method for three-dimensional
transmission traveltime tomography using first-arrivals,â€ commun. math.
sci., vol. 4, no. 1, pp. 249â€“266, 2006.
[6] y. luo and g. t. schuster, â€œwave-equation traveltime inversion,â€ geophysics, vol. 56, no. 5, pp. 645â€“653, may 1991.
[7] a. tarantola, â€œinversion of seismic reflection data in the acoustic
approximation,â€ geophysics, vol. 49, no. 8, pp. 1259â€“1266, aug. 1984.
[8] r. g. pratt and m. h. worthington, â€œinverse theory applied to multisource cross-hole tomography,â€ geophys. prospecting, vol. 38, no. 3,
pp. 287â€“310, mar., 1990.
[9] g. t. schuster, j. li, k. lu, a. metwally, a. altheyab, and s. hanafy,
â€œopportunities and pitfalls in surface-wave interpretation,â€ interpretation, vol. 5, no. 1, pp. t131â€“t141, feb. 2017.
[10] Ã¶. yilmaz et al., â€œa reality check on full-wave inversion applied to land
seismic data for near-surface modeling,â€ lead. edge, vol. 41, no. 1,
pp. 40â€“46, jan. 2022.
[11] y. ma and d. hale, â€œwave-equation reflection traveltime inversion with
dynamic warping and full-waveform inversion,â€ geophysics, vol. 78,
no. 6, pp. r223â€“r233, nov. 2013.
[12] y. luo, y. ma, y. wu, h. liu, and l. cao, â€œfull-traveltime inversion,â€
geophysics, vol. 81, no. 5, pp. r261â€“r274, sep. 2016.

zhang et al.: autoencoded elastic wt inversion

[13] j. m. zhang, l. g. dong, and j. h. wang, â€œfirst-arrival multiinformation joint inversion method based on wave-equation,â€ chin.
j. geophys.-chin. ed., vol. 64, no. 7, pp. 2447â€“2460, jul., 2021.
[14] j. wang, j. yang, l. dong, and y. liu, â€œfrequency-domain waveequation traveltime inversion with a monofrequency component,â€ geophysics, vol. 86, no. 6, pp. r913â€“r926, nov. 2021.
[15] h. yu, y. chen, s. m. hanafy, and g. t. schuster, â€œskeletonized waveequation refraction inversion with autoencoded waveforms,â€ ieee trans.
geosci. remote sens., vol. 59, no. 10, pp. 8210â€“8227, oct. 2021.
[16] y. chen and g. t. schuster, â€œseismic inversion by newtonian machine
learning,â€ geophysics, vol. 85, no. 4, pp. wa185â€“wa200, jul. 2020.
[17] a. dhara and m. k. sen, â€œphysics-guided deep autoencoder to overcome
the need for a starting model in full-waveform inversion,â€ lead. edge,
vol. 41, no. 6, pp. 375â€“381, jun. 2022.
[18] d. e. rumelhart, g. e. hinton, and r. j. williams, â€œlearning representations by back-propagating errors,â€ nature, vol. 323, no. 6088,
pp. 533â€“536, oct. 1986.
[19] d. p. kingma and m. welling, â€œauto-encoding variational bayes,â€ 2013,
arxiv:1312.6114.
[20] w. luo, j. yang, w. xu, and t. fu, â€œlocality-constrained sparse autoencoder for image classification,â€ ieee signal process. lett., vol. 22,
no. 8, pp. 1070â€“1073, aug. 2015.
[21] w. liu, t. ma, q. xie, d. tao, and j. cheng, â€œlmae: a large margin
auto-encoders for classification,â€ signal process., vol. 141, pp. 137â€“143,
dec. 2017.
[22] v. nair and g. hinton, â€œrectified linear units improve restricted
boltzmann machines,â€ in proc. 27th int. conf. mach. learn., 2010,
pp. 807â€“814.
[23] y. chen and e. saygin, â€œseismic inversion by hybrid machine learning,â€ j. geophys. res., solid earth, vol. 126, no. 9, sep. 2021,
art. no. e2020jb021589, doi: 10.1029/2020jb021589.
[24] k. lu, j. li, b. guo, l. fu, and g. schuster, â€œtutorial for waveequation inversion of skeletonized data,â€ interpretation, vol. 5, no. 3,
pp. so1â€“so10, aug. 2017.
[25] j. li, g. dutta, and g. schuster, â€œwave-equation qs inversion of skeletonized surface waves,â€ geophys. j. int., vol. 209, no. 2, pp. 979â€“991,
feb. 2017.
[26] j. li, z. feng, and g. schuster, â€œwave-equation dispersion inversion,â€
geophys. j. int., vol. 208, no. 3, pp. 1567â€“1578, mar. 2017.
[27] r.-e. plessix, â€œa review of the adjoint-state method for computing the
gradient of a functional with geophysical applications,â€ geophys. j. int.,
vol. 167, no. 2, pp. 495â€“503, nov. 2006.
[28] d. p. kingma and j. ba, â€œadam: a method for stochastic optimization,â€
2014, arxiv:1412.6980.
[29] j. schuster, â€œtwenty five years of least squares migration: current
developments and its future,â€ in proc. seg tech. program expanded
abstr., 2018, p. 5443.
[30] j. nocedal and s. j. wright, numerical optimization. jul. 2006.
[31] g. h. f. gardner, l. w. gardner, and a. r. gregory, â€œformation velocity
and densityâ€”the diagnostic basics for stratigraphic traps,â€ geophysics,
vol. 39, no. 6, pp. 770â€“780, jun. 1974.
[32] s. m. hanafy, a. altheyab, and g. t. schuster, â€œcontrolled noise
seismology,â€ in proc. seg tech. program expanded abstr., 2015,
pp. 5102â€“5106.

chang zhang received the b.e. degree in geoexploration from jilin university, changchun, china,
in 2020, where she is currently pursuing the masterâ€™s
degree.
her research interests include near surface geophysics method and application and the seismic
waveform inversion.

5904313

jing li (member, ieee) received the masterâ€™s
and ph.d. degrees in applied geophysics from jilin
university, changchun, china, in 2011 and 2014,
respectively.
he was a visiting researcher with delaware state
university, dover, de, usa, from 2012 to 2014.
he is a post-doctoral fellow with csim group,
kaust, thuwal, saudi arabia, where he focuses
on seismic surface wave inversion and data processing. he is a professor with the college of
geo-exploration science and technology, jilin university. his research focused on the advance math method in geophysics application. his research interests include seismic interferometry for subsurface
monitoring and ground-penetrating radar data (gpr) simulation and inversion
for shallow subsurface and through-wall radar and lunar radar detection.

han yu received the ph.d. degree in mathematics from the university of wyoming, laramie,
wy, usa, in 2011, under the supervision of
craig c. douglas.
he was a post-doctoral fellow with the division
of physical science and engineering, king abdullah
university of science and technology (kaust),
thuwal, saudi arabia, from 2012 to 2014. he was
a semiprofessional swimmer in high school. he is
also an associate professor with the school of
computer science, nanjing university of posts and
telecommunications, nanjing, china, and an algorithm consultant with
suzhou keda technology company ltd., suzhou, china. his research interests include waveform inversion, numerical analysis, machine learning, and
information security.
dr. yu is a member of the society of exploration geophysicists (seg), the
ieee computer society (ieee cs), the china computer federation (ccf),
and the china society for industrial and applied mathematics (csiam).

bin liu received the b.s. and ph.d. degrees in civil
engineering from shandong university, jinan, china,
in 2005 and 2010, respectively.
he then joined the geotechnical and structural
engineering research center, shandong university,
where he is a professor with the school of qilu
transportation. his research area is engineering geophysical prospecting techniques and especially their
applications in tunnels.
dr. liu is a member of the society of exploration
geophysicists (seg) and the international society
for rock mechanics and rock engineering (isrm). he serves as a council
member for the chinese geophysical society.



ore open research exeter

title
surrogate regression modelling for fast seismogram generation and detection of microseismic events in
heterogeneous velocity models
authors
das, s; chen, x; hobson, mp; et al.
journal
geophysical journal international
deposited in ore
27 july 2018

this version available at
http://hdl.handle.net/10871/33552

copyright and reuse
open research exeter makes this work available in accordance with publisher policies.

a note on versions
the version presented here may diï¬€er from the published version. if citing, you are advised to consult the published version for pagination, volume/issue and date of
publication

geophysical journal international

surrogate regression modelling for fast seismogram generation and
detection of microseismic events in heterogeneous velocity models
saptarshi das1, xi chen1, michael p. hobson1, suhas phadke2, bertwim van beest2, jeroen
goudswaard2, and detlef hohl3
1) cavendish astrophysics group, department of physics, university of cambridge, cambridge cb3
0he, united kingdom. (e-mail: {sd731, xc253, mph}@mrao.cam.ac.uk)
2) shell india markets pvt ltd, bangalore 562149, india. (email: {suhas.phadke, bertwim.vanbeest,
jeroen.goudswaard}@shell.com).
3) shell global solutions international bv, grasweg 31, 1031 hw amsterdam, the netherlands.
(email: detlef.hohl@shell.com)
summary
given a 3d heterogeneous velocity model with a few million voxels, fast generation of accurate seismic
responses at specified receiver positions from known microseismic event locations is a well-known
challenge in geophysics, since it typically involves numerical solution of the computationally expensive
elastic wave equation. thousands of such forward simulations are often a routine requirement for
parameter estimation of microseimsic events via a suitable source inversion process. parameter
estimation based on forward modelling is often advantageous over a direct regression-based inversion
approach when there are unknown number of parameters to be estimated and the seismic data has
complicated noise characteristics which may not always allow a stable and unique solution in a direct
inversion process. in this paper, starting from graphics processing unit (gpu) based synthetic
simulations of a few thousand forward seismic shots due to microseismic events via pseudo-spectral
solution of elastic wave equation, we develop a step-by-step process to generate a surrogate regression
modelling framework, using machine learning techniques that can produce accurate seismograms at
specified receiver locations. the trained surrogate models can then be used as a high-speed metamodel/emulator or proxy for the original full elastic wave propagator to generate seismic responses for
other microseismic event locations also. the accuracies of the surrogate models have been evaluated
1

geophysical journal international
using two independent sets of training and testing latin hypercube (lh) quasi-random samples, drawn
from a heterogeneous marine velocity model. the predicted seismograms have been used thereafter to
calculate batch likelihood functions, with specified noise characteristics. finally, the trained models on
23 receivers placed at the sea-bed in a marine velocity model are used to determine the maximum
likelihood estimate (mle) of the event locations which can in future be used in a bayesian analysis for
microseismic event detection.
keywords: synthetic seismogram generation, time domain compression, surrogate meta-model,
microseismic event detection, gaussian process regression
1. introduction
microseismic event detection has emerged as a significant field of research in computational
geosciences with an aim of studying the changing geological characteristics of a subsurface reservoir
during and after hydrocarbon production. these microseismic events are characterized by low
amplitude ground movements and are often indistinguishable from environmental seismic noise (leet
1949). a frequency band based quantification approach has been adopted in (groos & ritter 2009) to
grossly classify such events as microtremor (>1 hz), transitional (0.6-1 hz) and microseismic (<0.6
hz), although different sources and background noise in a marine environment (e.g. due to water waves,
storms, shipping and anthropogenic activities like drilling) share overlapping frequency bands, making
such a detection task quite challenging, using the real field datasets from marine seismic surveys. in
order to reliably detect these microseismic events, recent attempts have been made to simulate
approximate template seismic waves with known time-frequency domain characteristics using geomechanical modelling from first principles. amongst the available approaches for the forward
geophysical modelling given a heterogeneous velocity model, raytracing, acoustic wave and elastic
wave propagation modelling are widely used (chapman 2004). the elastic wave equation-based
modelling is the most detailed and accurate geophysical approach for microseismic events in marine
velocity models with a rock-water interface, whereas the raytracing method mostly relies on the high
frequency wave propagation approximation using separate compressional (p) and shear (s) wave
velocity models. in spite of the capabilities of accurate geophysical modeling, with mode conversion
2

geophysical journal international
between p-wave and s-waves in the boundaries between the rock layers, bulk scale simulation of the
3d elastic wave equation is often not a favorable solution as it suffers from extremely high
computational requirements to generate accurate synthetic seismograms from a given velocity model
(including density and p/s-wave velocity) with a few million grid points for detailed description of
subsurface heterogeneity. however, a gpu based generic elastic wave propagator like the k-wave
solver (treeby et al. 2014; treeby & cox 2010) can reduce the computational burden of bulk simulation
significantly and has been used in large scale geophysical wave propagation modelling before e.g. (guo
et al. 2016)(das et al. 2017). but gpu based forward simulation is still not fast enough to evaluate
thousands of batches of single shot elastic wave propagation simulations needed for fast computation
of the likelihood values at speculative locations of microseismic events, given recorded noisy
seismograms.
the outputs of the governing partial differential equation (pde) for elastic waves are more numerous
(i.e. multi-receiver seismogram time series) than the inputs to the pde solver (specified microseismic
source positions as 3d co-ordinates). therefore, such a high dimensional mapping from the
microseismic event locations to the full set of observables, i.e. the seismic waves recorded on multiple
receivers, make such a statistical regression modelling problem quite challenging. this becomes even
more challenging since the resulting wave-fields in response to unit strength microseismic events at
random locations are sparse in nature, with time localized information embedded in the time-series data
as multiple spikes. a direct regression modelling using a few thousands of sparse seismic wave-fields
would smear away the predicted seismograms, as the data samples can differ by a few order of
magnitude (depending on the distance from the receivers) and most of the information lies in the form
of localized spikes in time. a robust compression method is thus needed for predicting the simulated
seismic waves, before applying a regression framework as a â€œproxyâ€ for the elastic wave propagator.
the compression can be applied in many different ways (time, frequency or time-frequency domains
e.g. using fourier or wavelet bases). the mapping in the compressed domain needs to be smooth to
capture the short duration variable amplitude oscillations in the seismic waves. due to the timefrequency domain duality criteria, small errors incurred in the frequency domain may lead to wider and
sustained spurious oscillations in time domain, which suggests the use of time domain robust
3

geophysical journal international
compression methods over other frequency domain methods (wood 1974). this approach slightly
sacrifices the achievable compression performance and hence the number of observables in the
regression model and consequently the size of the trained surrogate model to predict accurate synthetic
seismograms.
with the aim of proxy construction, this paper first simulates synthetic elastic wave propagation using
gpus, from a few thousands of unit amplitude explosive microseismic events at random positions in
the subsurface and records the resulting seismograms at specified receiver locations at the seabed. these
synthetic data are then used to approximate, or â€˜statistically learnâ€™, the underlying physics of elastic
wave propagation, therefore generating a reduced physics model (wilson & durlofsky 2013; wilson et
al. 2012), for any random event location within the heterogeneous velocity model. here we also
compare the computational time of the full-scale forward model simulation vs. the trained surrogate
meta-models to obtain an estimate of the run-time saving. this can enable an iterative microseismic
source inversion process within a realistic time using standard computational resources. such a
statistical learning or approximation of physics in the form of pde solverâ€™s outputs has been widely
used in various surrogate meta-model assisted optimization methods before e.g. in (forrester et al. 2008;
forrester & keane 2009; forrester et al. 2007).
surrogate meta-models or proxy methods were traditionally developed for various optimization
problems e.g. constrained single or multi-objective optimization problems, missing data problems etc.
(forrester et al. 2008; forrester & keane 2009; forrester et al. 2007). similar machine learning
approaches have been adopted to approximate complicated likelihood functions within a bayesian
analysis framework in the blind accelerated multimodal bayesian inference (bambi) algorithm (graff
et al. 2013; graff et al. 2012)(hobson et al. 2014). surrogate meta-models are also used to learn
weighted multiple objective functions within single-objective (pan & das 2015), multi-objective (pan
et al. 2014b) and robust optimization frameworks (babaei, pan, et al. 2015)(babaei, alkhatib, et al.
2015), containing expensive function calls for the forward physics simulation. the trained surrogate
meta-models can be viewed as a â€˜proxyâ€™ for the expensive forward simulations, while it also acts as a
smooth interpolator in the parameter space of the forward model (i.e. microseismic event locations)
which can be verified using an independent testing dataset. such reduced physics or approximate
4

geophysical journal international
physics based proxy or surrogate models have been widely used in various other geophysical and
geological problems like shale gas production optimization (wilson & durlofsky 2013; kalantaridahaghi et al. 2015; wilson et al. 2012), geological co2 storage (babaei, pan, et al. 2015)(pan et al.
2014a; pan et al. 2014b), water injection in oil reservoirs (babaei & pan 2016; babaei, alkhatib, et al.
2015), and history matching (goodwin 2015; mohaghegh 2006; rodriguez et al. 2006; slotte &
smorgrav 2008; zubarev 2009), in the context of optimization or uncertainty quantification using
various monte carlo methods. to the best of our knowledge there isnâ€™t any study on surrogate metamodel or proxy development for microseismic response modelling via elastic wave propagation, using
the sparse spike time series which is difficult to learn unlike in many traditional areas of computational
geosciences, except few variable frequency decomposition methods for fixed receiver and source
position (modesto & de la puente 2016) and some not well-explored concepts of seismic inversion
(weglein et al. 2009).
seismic data driven geophysical parameter estimation and inverse problems often need a few
thousands of such likelihood or objective function calls where the forward geophysics simulation
produces a template seismic data to match with the noisy real recordings (aster et al. 2011)(mosegaard
& tarantola 2002; tarantola 2005; tarantola & valette 1982; mosegaard & tarantola 1995). previous
surrogate-based optimization and bayesian inference methods trained a proxy for the single valued
likelihood functions since the likelihood function is dependent on the data. in applications where the
data change frequently, such an approach needs retraining of the surrogate meta-model using the newly
recorded data, which may be a computationally wasteful approach. thus, we take a different approach
here of directly learning the raw observables obtained from the geophysical simulation model. this
poses mainly two challenges â€“ firstly, the observables (seismograms) recorded on multiple receivers
will produce too many parameters for a multivariate regression and secondly, the generated seismic
data are sparse which makes it difficult to predict via a standard regression framework. in other words,
for elastic wave propagation modelling, the simulated datasets are sparse in nature and also, they are
dense multivariate time series, the size of which massively increases with the number of receivers and
the sampling frequency. therefore, the contribution of this paper is to statistically learn the sparse
physical response of unit size microseismic activity, as a function of input parameter-set in the pde
5

geophysical journal international
(i.e. random event locations in this case) which is rather a harder problem than learning scalar valued
likelihood functions within an inference problem as shown in (graff et al. 2013; graff et al. 2012).
amongst previous approaches to supervised learning of physics based models i.e. fewer model
parameter to many observable mapping, the cosmonet algorithm (auld et al. 2007; auld et al. 2008)
employing multilayer perceptron neural networks is worth mentioning. the present paper extends this
idea for predicting sparse data using a robust compression technique. this paper also compares the
performance of various smooth interpolation methods available from a pool of supervised learning
techniques â€“ starting from robust polynomial regression to kernelized shrinkage regression, support
vector machine (svm), decision tree and ensemble regression, feedforward and cascaded forward
neural networks (nn) and gaussian process (gp) regression with various kernels and basis functions.
however the difference between the observable mapping, shown in cosmonet algorithm (auld et
al. 2007; auld et al. 2008) and our approach is that we here learn each compressed domain prediction
separately, rendering multiple partitioned regression models, without leveraging the underlying
correlation structure amongst the observations in the compressed domain. also, in surface seismic data
based microseismic activity monitoring, the gross geological characteristics given by the voxelized 3d
velocity model are not expected to change within a short span of time and can be considered constants,
hence leading to a deterministic mapping of the microsiseismic event parameters on to the observed
seismic profiles at various receivers. this motivates us to conceptually follow a similar route proposed
in physical measurement domain observable learning as shown in the cosmonet algorithm (auld et
al. 2007; auld et al. 2008), rather than the specific historical fixed dataset based likelihood learning as
reported in the bambi algorithm (graff et al. 2013; graff et al. 2012).
therefore, the goal of this paper is to develop a robust method to act as a proxy or surrogate metamodel or fast interpolator for mapping the input parameters in a sufficiently complex pde model with
material heterogeneity onto the sensor or measurement space to be used later in the likelihood calls for
fast parameter estimation and probabilistic inference problems. in other words, the broad objective here
is to teach the machine learning algorithms to rapidly predict the numerical solution of the elastic wave
propagation and then use these predictions to estimate the microseismic event locations in a simple
maximum likelihood or even more involved full posterior distribution estimation. amongst previous
6

geophysical journal international
efforts on such characterization of microseismic source activity in the subsurface from recorded
seismograms using spectral, spectrogram domain methods (eaton et al. 2014) and phase space domain
using polarization diagrams in (levy et al. 2011) are notable. also, (groos & ritter 2009) proposed a
scheme for classifying the sources in microtremor, transitional and microseismic events from the
observed seismograms using realistic field data.
the paper is divided in the following objectives to achieve this broader goal and presented in the
subsequent sections:
i)

fast gpu based synthetic seismogram simulation for training the supervised learning methods

ii)

a robust compression of the sparse seismic signals

iii)

learning a smooth mapping from event location on to the compressed domain seismograms

using various machine learning techniques
iv)

comparing accuracy, storage size, training time trade-offs for these supervised learning-based

surrogate meta-models and
v)

using the proxy-based fast predictions for calculating maximum likelihood estimates of

possible event locations
2. synthetic seismic trace generation for training machine learning algorithms
the aim of this work is to train machine learning algorithms to rapidly generate accurate seismograms
within each likelihood call. it needs to be trained using some example datasets to help statistically learn
the elastic wave propagation mechanism without numerically solving the expensive governing pdes.
to generate the synthetic seismograms, we have used the elastic wave equation solver k-wave, in a
specified 3d geometry using the pseudo-spectral method (treeby et al. 2014). the receivers and
microseismic source positions can be modelled using the given 3d voxelised heterogeneous velocity
model which can be run using general purpose gpus with a single precision (32-bit) number
representation (treeby & cox 2010; treeby et al. 2012). for synthetic trace generation, the medium
can be modelled as simple acoustic (with only p-waves), or elastic (having both p-wave and s-wave
sound velocities) or even as viscoelastic with frequency dependent absorption, which is considered as
zero in the present geophysical wave propagation modelling. in the simulation process, the stress/strain

7

geophysical journal international
tensors are iteratively updated using the specified 3d heterogeneous velocity model. during the
simulation, the 3-component particle velocity and the acoustic pressure are calculated from the
propagating waves at specified receiver locations. in most realistic 3d geophysical simulations, the
sound velocity and density model are used for solving the forward wave propagation in acoustic mode
(phadke et al. 2000) or elastic mode (igel et al. 1995) using a heterogeneous medium where the material
properties or the velocity model with ï»c p , c s , ï² ï½ being specified as 3d matrices with specified voxel
values. microseismic response simulation on gpus using the pseudo-spectral method has been explored
previously in (das et al. 2017). however the seismic data generation process using other numerical
schemes of pde discretizing methods are not the main focus here and a similar seismic wave
propagation method involving either finite difference, finite element, spectral element or finite volume
method can also be employed instead of the pseudo-spectral method (igel 2016). in our simulations, in
order to impose an absorbing boundary condition via the perfectly matched layer (pml), 10 grid points
were reserved along each direction before and after the regular grids of the velocity model. the elastic
wave propagation due to explosive microseismic sources were run on a 3d domain of 81Ã—81Ã—301 =
1.975Ã—106 grid points where the grid spacing in the three directions are given by
ï„ x ï€½ 12.5, ï„ y ï€½ 12.5, ï„ z ï€½ 10 m, therefore representing a geological model of dimension 1 kmÃ—1 kmÃ—3

km along the three directions as shown in figure 1. the elastic wave equation is solved with a sampling
time of 0.8 ms to guarantee numerical stability for this heterogeneous model over a total time interval
of 2 sec and then the recorded seismograms are down-sampled to ts = 4 ms. the strength of the sources
are considered as 1 mpa as many recent literature suggest that the typical range for microseismic
sources is around 1-10 mpa in sedimentary rocks and >20mpa in crystalline rocks (rutledge et al.
1998; collettini & barchi 2002) whereas for earthquakes it ranges between 5-100 mpa (dieterich et al.
2015).

8

geophysical journal international

figure 1: heterogeneous velocity model of 1 kmÃ—1 kmÃ—3 km, comprising of the density (kg/m3), compressional and shear
velocity (m/sec) at each grid-point. heterogeneity is higher in depth compared to the lateral directions.

figure 2: latin hypercube samples for event locations for training and testing dataset in the supervised learning. both training
and testing samples are selected almost uniformly from the whole volume.

in total 4000 random latin hypercube (lh) samples for speculative source positions were used here
for the forward simulations to generate the training and testing seismic data for different machine
learning algorithms. we have randomly split 2000 source positions for training the surrogate meta9

geophysical journal international
models and then use the remaining 2000 sources for testing the performance of the trained meta-model.
both these data-sets are uniformly distributed throughout the volume of the velocity model as shown in
figure 2. the forward simulations were run on the wilkes gpu cluster at the university of cambridge,
uk with non-interactive batch mode as separate matlab scripts. each batch contained 40 random event
locations and 10 such batches (400 sources) were run simultaneously. the generated seismic waves of
2 sec length were recorded at the surface receivers placed at the interface between the rock layers and
the water column in the velocity model in figure 1. the synthetic data after down-sampling to ts = 4
ms, becomes 182 gb for 4000 lh source locations. previous 3d elastic wave modelling attempts on
gpus e.g. (mu et al. 2013a; mu et al. 2013b) used 0.03Ã—106 and 0.3Ã—106 voxels respectively, whereas
our present model is significantly larger (65.8 times and 6.5 times respectively) than the results reported
there. on the 1 square kilometer surface at the sea-bed the 23 receivers are placed with the arrangement
shown in figure 3. this paper initially develops the proxy meta-model for the central receiver (r-12)
and then extends it to all the 23 receivers. we also show the effect of choosing different sub-sets of
these receivers on the final maximum likelihood detection performance of the events. amongst the 4000
forward simulations, 10 representative cases are shown in the supplementary material in map view of
the propagating acoustic pressure wave-field at a fixed time instant t = 1.4 s, where the respective
microseismic source positions in the volume are mentioned in the title of the subplots. the seismic
traces recorded at the 23 receiver locations can be seen in figure 4 where the corresponding map views
of the acoustic pressure wave-fields are shown in the supplementary material. on the seismogram
wiggle plots in figure 4, the appearance of multiple arrivals are actually an effect of strong p-wave,
followed by weak delayed s-waves and also the receiver arrangement where source to receiver distance
does not uniformly vary in different trace numbers, since the receivers are not placed along a line but
distributed all over the surface.
in the next section, we aim to learn a statistical mapping between the event locations and the resulting
seismograms at these 23 receivers without running the expensive forward simulations, for trained or
new test event positions. in order to achieve this goal, the recorded seismic data needs to be compressed
first in order to reduce the number of outputs of the surrogate regression meta-model i.e. 23 receiver Ã—
501 time samples = 11,523 data points per microseismic event location. this 3 to 11,523 dimensional
10

geophysical journal international
mapping is inherently a difficult learning problem because the output has complex correlation structures
and moreover are sparse in nature with time localized spikes. as discussed earlier, fewer observables
in the non-sparse and smooth cases can be statistically learned using multiple-input multiple-output
(mimo) regression frameworks e.g. using various neural network architectures (auld et al. 2007; auld
et al. 2008)(pandey et al. 2016). however most generic regression model involving nonlinear kernels
like svms, decision tree, polynomials and gaussian processes can mostly accommodate a many-toone mapping thus leading to a multiple input single output (miso) regression problem. in general,
neural networks, as universal function approximators, can accommodate both mimo and miso
regression framework, e.g. a comparison has been reported in (pandey et al. 2016), but in general nns
are sensitive to outliers, noise, and may not adequately learn sparse datasets, as it requires several
heuristics for choosing the right combination of hidden nodes, number of layers, activation functions
and optimizers. on the other hand the kernelized gaussian process models have been widely used in
geostatistical modelling and kriging that can naturally accommodate noisy data for regression and
outperformed many other family of algorithms especially on regression problems as shown in (mackay
1997)(sitharam et al. 2008; samui & sitharam 2010). in order to provide a fair comparison here we
have tested 9 different classes of regression models which can learn several many-to-one (miso)
mapping under the same framework i.e. given 3 event location parameters (x, y, z) the prediction of 100
compressed domain seismograms on the 23 receivers, instead of learning a many-to-many (mimo)
mapping that may capture the correlations between the data in the compressed domain components and
also between different receivers. learning this collection of many-to-one statistical mapping for the
compressed seismograms gives a smooth and robust method for predicting the seismic waves due to
microseismic sources, as explored in the next sections.

11

geophysical journal international

figure 3: receiver placement geometry in the sea-bed. receivers are placed at fixed depth of z = 244 in a 2.44 km deep
velocity model of rock layers.

figure 4: seismograms for the 23 receivers recording of the acoustic pressure in the forward simulation with fixed velocity
model. sample numbers and source locations are mentioned in the subplot titles. 500 samples represent 2 sec of seismic data
with Î´t = 0.004.

3. time domain compression of seismic traces and surrogate regression meta-modelling
3.1. compressed representation of seismograms for regression meta-modelling
this section first describes the robust time domain compression method for the time localised seismic
datasets shown in figure 4. each seismic trace at a specified receiver location (in the horizontal x, y
12

geophysical journal international
plane) has been first sorted in decreasing order of absolute amplitude and only the strongest 100 samples
(either capturing positive or negative pressure) are retained while the rest of the entries in the 501 sample
long time-series (equivalent to 2 sec of data) are set to zero. this helps in identifying the dominant time
instants within the sparse array of seismic traces, due to a smoothed delta-function like microseismic
event Î´(t) at different locations (x, y, z) in the heterogeneous volume. smoothing of the source level
spatial delta function is required and can be implemented using blackman window which may otherwise
create temporal oscillations, where more details on this can be found in (treeby et al. 2012)(das et al.
2017). this transforms the original long but sparse time series in two different components â€“ dominant
amplitude (si) and the corresponding index terms (idx) for these non-zero temporal instants, which are
extracted for all the event locations. such a simple time-domain compression technique is thus able to
reduce the number of observables to be predicted, to a lower value (from 501 to 200 i.e. for both si and
idx, only 100 values). upon reconstruction using the signal amplitude si and the index terms idx, the
2d correlation coefficient between the original and compressed images is r2d>0.99, indicating almost
lossless compression while also maintaining a smooth mapping of the observables in the event
parameter space. using frequency or time-frequency domain compression techniques involving fourier
or wavelet transforms can achieve a better compression ratio but often learning the idx term for such
representation need to be very accurate, otherwise the compressed signals upon reconstruction may get
shifted to different locations which needs further investigation. each time domain compressed seismic
dataset has been sorted in ascending order of idx, thus producing a smooth pattern in the location of the
dominant parts of the seismic traces. the first 500 realizations of the sorted data are shown in figure 5.
here the compression is done on the seismic response for a single unit amplitude microseismic event.
for 2 sec of data with 501 time samples, retaining only the strongest 100 samples gives us 99%
reconstructed accuracy for a single microseismic response with strong p-wave and then trailing swaves. it is worth noting here that the purpose of the compression here is to reduce the number of
regression outputs for noiseless template seismic responses for unit events and not noisy seismic traces
with multiple events. for other types of datasets like different source mechanisms or different size of
the velocity model, the length of the template noise free seismic trace may vary and under such a

13

geophysical journal international
scenario, the compression ratio might need to be retuned, but a similar method needs to be adopted to
reconstruct seismograms with an accuracy of r>0.99.

figure 5: sorted index terms (idx) and corresponding signal amplitudes (si) on the central receiverâ€™s seismic data for the first
100 dominant time instants of 500 random microseismic events.

the time-domain robust compression method can be described using the following three steps:
step 1: out of the 501 samples in each 2 sec of seismogram, sort and isolate first 100 strongest positive
or negative (absolute) amplitudes (si)
step 2: sort the strongest signal values with increasing idx (to get a monotonically increasing occurrence
of these dominant time instants)
step 3: sort the strongest signal values si, according to the respective time instants of idx
the aim is now to map each sample of these compressed domain sorted seismic data (both si and
idx) as a function of the event locations (x, y, z). through such a regression modelling, the seismic
traces can be accurately and smoothly interpolated within the heterogeneous medium without running
the computationally expensive pde solver for other event locations which have not been used while
training the surrogate meta-model. we found that apart from the three co-ordinates of the event
locations (x, y, z), in addition its distance from a fixed receiver location can also be an useful predictor
for modelling seismic data recorded on that particular receiver. here the distance (d) refers to the
14

geophysical journal international
euclidean norm of the differential co-ordinates between the source and the receiver. in the next
subsection, we explain with representative examples why compressing the seismic data in each receiver,
prior to statistical learning or the regression modelling is a necessity.
3.2. need for compressed domain representation of the seismic data within regression
the aim to predict each value in a 2d seismic snapshot at a fixed time slice is a regression problem
on a sparse image with important information embedded as time localised spikes as shown in figure 4.
therefore, a direct pixel by pixel regression approach fails to provide sufficient accuracy due to the
presence of too many zeros in the training dataset, because the informative spiky signals get smeared
away, under such direct regression framework. apparently it might seem that there is a clear linear
horizontal pattern for the seismic data amplitudes in figure 6 as a function of x ï€½ ï» x, y, z, d ï½ at the four
fixed time slices (t = 0.25, 0.5, 0.75, 1 sec), but actually the useful information lies only in the few
outliers above and below the baseline, containing mostly low amplitude fluctuations close to zero.
therefore, in order to predict the amplitude and temporal location of such spiky seismic signals, a
straight forward regression analysis cannot be applied, as any flexible machine learning algorithm will
pick up most of the frequently occurring zeros and not the few time-localised spikes or outliers in a
sparse seismic data. although the maximum amplitude shows a smoother variation with respect to z
and d (in the bottom row, last two entries of figure 6 as function of z and d), it can occur anywhere in
the long time trace of the seismic trace and cannot be used to recreate the full seismic wave. there is
another disadvantage of such direct prediction of the sparse seismograms as a function of event
parameters, apart from the computational burden of having more regression models i.e. 501 samples in
the case of a single seismic trace at a single receiver for 2 sec of data. our sparse prediction approach
essentially identifies the informative region in the seismic trace and predicts only the dominant values
at the respective temporal points (i.e. the two components â€“ si and idx), while considering the rest of
signal as sparse with zero values, whereas the voxel by voxel prediction generates small noise-like
fluctuations even at locations where there is not actually any significant information.

15

geophysical journal international

figure 6: variation in raw seismic signal amplitude as a function of microseismic source locations and its distance from the
central receiver {x, y, z, d}. the few outliers contain the most useful information of the seismograms as high/low amplitudes.

in order to remove any bias in the regression process, which may be introduced due to the different
ranges of input predictors or output observables ( x ) in the compressed domains, all the predictor and
compressed observables are standardized to zero mean and unit variance using (1) and the respective
standardization constants ( ï­ x , ï³ x ) are also stored apart from the trained proxy meta-models for
rescaling the new predictions to the actual physical scale:

ï€¨

x standard ï€½ x ï€­ ï­ x

ï€©ï³ .

(1)

x

compared to the uncompressed signal representation shown in figure 6, a smoother variation is
observed in the compressed domain, and can be seen from the 50th dominant time instant of the sorted
seismic data in the form of its two compressed components {si, idx} as a function of {x, y, z, d} in
figure 7. the patterns in the compressed domain are prominent and not sparse and hidden in the form
of outliers as in the previous case, as a function of these four covariates. in addition, the difference in
the signal amplitudes depending on the depth of the source are also an important factor as shown in
figure 5, since the response of the deep source may get smeared away as numerical noise under a
standard regression without any compression and normalization at each dominant time instant. from
figure 7, it is apparent that the two covariates {z, d} give rise to more correlated but complicated
16

geophysical journal international
patterns with few islanded regions which might be an effect of heterogeneous nature of the velocity
model and complex structure of the elastic waves with both p and s-waves in the two parts of
compressed domain.

figure 7: variation in compressed dominant amplitudes and their index terms as a function of source location and distance
from receiver {x, y, z, d}. depth and distance show prominent structure for predicting the two compressed components.

17

geophysical journal international
figure 8: schematic diagram of time domain compression, multiple regression learning in compressed domain and
decompression for predicted seismic trace generation. the unit event seismograms are compressed and learnt with parameter
selection of different regression models to achieve the best predictive accuracy.

next, we apply the same compression method on all the microseismic source locations as shown in
the lh samples in figure 2. therefore, using a few thousand microseismic event locations, each
containing 501 time-samples for a single receiver position, the time domain compression yields 100
dominant sorted time index (idx) and the corresponding signal amplitudes (si), i.e. 200 data points per
compressed seismogram. now, we aim to learn several regression meta-models between {x, y, z, d} as
predictors and the amplitudes of 100 si and the corresponding sorted time-index idx values as the
observables, in order to statistically learn the patterns represented in figure 7. after the compressed
representation of the seismograms, we choose a structure from a pool of regression models and
independently learn the compressed data at 100 dominant time-instants. the standardizing constants
computed before the regression are stored and then used to rescale the predictions to physical units. the
predicted si and idx values can be easily combined in the decompression step to generate the predicted
seismogram, as soon as a new input location for an event comes in. each regression model adjusts its
parameters by minimizing a mean squared error criterion between the ground truth vs. the predictions
in the two compressed domains in the normalized scale. upon reconstruction and rescaling the
predictions, the predictive accuracy of the two components along with the reconstructed signals are
calculated between the ground truth ( gij ) seismograms and the corresponding predicted ( pij ) versions
by different machine learning algorithms using the 2d pearson correlation coefficient in (2), for
choosing the best model parameters or suggesting a new model structure:

ïƒ¥ïƒ¥ ï€¨ g ï€­ g ï€©ï€¨ p ï€­ p ï€©
ij

r2 d ï€½

i

ij

j

2 ïƒ¶ïƒ¦
2ïƒ¶
ïƒ¦
ïƒ§ ïƒ¥ïƒ¥ gij ï€­ g ïƒ·ïƒ§ ïƒ¥ïƒ¥ pij ï€­ p ïƒ·
ïƒ¨ i j
ïƒ¸ïƒ¨ i j
ïƒ¸
1
1
gï€½
gij , p ï€½
ïƒ¥ïƒ¥
ïƒ¥ïƒ¥ pij ,
ni n j i j
ni n j i j

ï€¨

ï€©

ï€¨

ï€©

,

ï» ï½

(2)

where, g, p denote the 2d mean of the ground truth and predicted signals in either
compressed/reconstructed domains.
18

geophysical journal international
although the split regression models only see a smaller sub-problem with a goal of minimizing the
mean squared error (mse) between the grounds truth vs. the predictions, the combination of 200 such
predictions generate the full seismic traces for all the event locations in the training dataset. in order to
judge whether a structure is good enough from the pool of regression models or which parameters of
the models should be fine-tuned, a fixed threshold on 2d correlation coefficient (2) as r2d>0.9 has been
used for the initial screening. if the model structure is found to be less flexible to accommodate the
patterns in the compressed domain seismic data or a wrong control parameter is selected, a different
model and/or control parameter(s) are suggested. the proxy or surrogate meta-model training workflow
is schematically shown in figure 8, starting from the event locations, then finding the compressed
domain representation of seismic signals and then learning 100 split models for the dominant time
indices idx and the corresponding signal si, from the pool of regression models, along with fine tuning
of the associated control parameters. the next section briefly introduces the model structures in the pool
of regression models and discusses the control parameters to learn the patterns in the seismic waves in
the two-component compressed form as shown in figure 7.
4. machine learning techniques for multivariate compressed domain regression metamodelling
4.1. splitting high dimensional mimo regression as multiple miso regression problems
we here explore the predictive performance, computing requirements for training and also the
required storage for the trained surrogate models, using the following classes of regression techniques:
i)

robust polynomial regression

ii) gaussian process (gp) regression
iii) support vector machine (svm) regression
iv) decision tree regression
v) ensemble regression using tree method
vi) kernelized shrinkage regression using ïŒ1 ïŒ2 norm
vii) generalized linear model (glm) regression
viii) kernelized shrinkage based glm regression
19

geophysical journal international
ix) multilayer perceptron (mlp) neural network regression

there are also few hyper-parameters in each of the regression models that make a particular technique
more flexible for learning complex patterns in the data over other classes of models. a sufficiently
accurate surrogate regression meta-model can then be used in the inference or for optimization purposes,
hence a comparison of storage requirements for such models and the training time are also important
factors in such proxy design.
the regression models were trained in the matlab programming platform on a 64 bit 12-core linux
cpu with 32 gb memory and intel xeon e5, 2.5 ghz processor, while each of the 100 regression
problems for idx and si in figure 8 were parallelized independently over 12 cores using the parallel for
(parfor) loops in matlab. in the simplest implementation, the 100 dominant compressed time instants
are learned independently using a separate model without considering a correlation structure between
them. this could have been otherwise learned as a 3 to 200 parameter regression problem but only mlp
neural networks would be usable, with the possibility of accommodating a full mimo regression
instead of a combination of multiple miso regression problems (pandey et al. 2016). however, such
an approach here has not yielded a good prediction accuracy due to the large number of predictors (100
or 200) compared to the covariates (only 4) using moderate size nns and hence we here focus only on
splitting the mimo regression as a collection of multiple miso regression problems, since here the
main purpose is to get a good predictive accuracy without solving the full computationally expensive
elastic pde solver when called for fast likelihood calculation.
4.2. predicting compressed domain seismic data at a fixed receiver location as a function of
event location parameters
initially we explore the performance of various machine learning (regression) algorithms for
predicting the seismic traces, recorded at the central receiver (at nx/2, ny/2) for the sake of simplicity.
here we use all the four predictors i.e. position and distance {x, y, z, d}, as this has been found to yield
a better fitting performance over other subsets of covariates. the regression models have been assumed
to be different on the si and idx, since their patterns are found to be quite different in figure 7. in each

20

geophysical journal international
of the predictors we fit e.g. a higher order kernel with flexibility to adjust the order of the polynomial
using an exhaustive search that maximizes the 2d correlation coefficient between the compressed vs.
the regression model predicted image for both si and idx, hereafter called as rsi and ridx respectively.
the regression uses the robust option to reject outliers and fit a smoother response in its predictions.
also, the maximum order of polynomial kernel has been kept up to 4 in each predictor {x, y, z, d}, as
more complex models are prone to overfit inconsistent patterns, and higher order models with more
degrees of freedom have a larger number of parameters to estimate, hence producing more uncertainty
in the estimates and increased variance on the predictions. the highest accuracy achieved through
simple polynomial regression was insufficient because of the complexity of the data as shown in figure
7, a representative case for sorted 50th time instant, which may not be fitted well with simple quadratic,
cubic or quartic polynomial functions of the predictors. the polynomial case is used here as an example
from the pool of 9 classes of regression models in figure 8 and the different cases the polynomial order
replaces the respective free hyper-parameters of the algorithm to fine tune. also, some of the predictions
for idx may be negative or exceed the maximum time instant, yielding an unfeasible region (as the
signals cannot lie in negative time) or increased time series upon reconstruction. therefore, after the
predictions by the regression models, the idx<0 and idx>501 are thresholded at the lower and upper
bounds respectively and the corresponding signal amplitudes are set to zero. in both the predictions of
si, idx and the reconstructed seismic traces, the 2d correlation coefficient is used as reported in the
tables as {rsi, ridx, rrecon} using (2). it is to be noted that in all the cases, the machine learning algorithms
have been trained on the two compressed components of the signal to select the model with maximum
{rsi, ridx,}. the reconstructed accuracies on the seismograms are calculated outside the training process
to calculate rrecon.
4.3. robust polynomial regression
the robust regression method gives better estimates in the presence of outliers and noise, compared
to the commonly used ordinary least square (ols) method incorporating the moore-penrose pseudoinverse. let us consider the regression problem in (3), x i ï€½ ï› xi

21

yi

zi

di ï is the predictors and
t

geophysical journal international

yi ï€½ ï› idxi

sii ï is the observables with model weight ï¢ and the prediction error ( ï¥i ) being independent
t

and identically distributed (iid) with a scale factor ï³ for the modelling error:
(3)

yi ï€½ x it ï¢ ï€« ï³ï¥ i .

ï‚µ can be calculated from a given estimate of scaling factor ï³ï‚µ , considering
the estimate of the weight ï¢
the data and the weighted function (ï¹ ) of error to be uncorrelated i.e.

ï€¨1 n ï€© ïƒ¥ x iï¹ ï€¨ ï€¨yi ï€­ x it ï¢ï‚µ ï€© ï³ï‚µ ï€© ï€½ 0.
n

(4)

i ï€½1

ï» ï½

in the robust regression, starting from an initial estimate ï¢ï‚µ ,ï³ï‚µ , residuals of the estimates are calculated

ï€¨

as ri ï€½ yi ï€­ x it ï¢ï‚µ

ï€© ï³ï‚µ . the weights are defined as w ï€½ ï¹ ï€¨ r ï€© r and the estimates are updated with a
i

i

i

least square estimate with weight wi . the iterative update continues unless the algorithm converges
(street et al. 1988). there can be different choice of weight functions (holland & welsch 1977) for
robust regression as in (5):

andrews: w ï€½ ï€¨ r ï€¼ ï° ï€© sin ï€¨ r ï€© r , bisquare: w ï€½ ï€¨ r ï€¼ 1ï€© ï€¨1 ï€­ r 2 ï€© , cauchy: w ï€½ 1 ï€¨1 ï€« r 2 ï€© ,
2

ï€¨

ï€©

fair: w ï€½ 1 ï€¨1 ï€« r ï€© , huber: w ï€½ 1 1 ï€« max ï€¨1, r ï€© , logistic: w ï€½ tanh ï€¨ r ï€© r ,

(5)

welsch: w ï€½ exp ï€¨ ï€­r 2 ï€© , talwar: w ï€½ r ï€¼ 1.

ï€¨

ï€©

here the value of r is calculated as r ï€½ res consttune ï³ï‚µ 1 ï€­ h , with ï»r e s , h ï½ being the residual from the
previous iteration and leverage value from ols fit respectively. the standard deviation of error is
calculated as ï³ï‚µ ï€½ mad 0.6745 , using the median absolute deviation (mad) of the residuals,
considering it to be normally distributed.
in the present problem with robust polynomial regression, we first transform the input parameter
space i.e. microseismic event locations and distance from the central receiver ï» x , y , z , d ï½ using a
polynomial kernel function xnfx (ieong 2012) with a chosen order of 2 to 4 in order to form a design
matrix e.g. {1, x, y, z, xy, xz, yz, x2, y2, z2, â€¦} in the case of quadratic kernel, as an example. this high
dimensional transformed feature matrix is then used in the robust linear regression framework through
22

geophysical journal international
the weight functions in (5). the maximum order of the kernel has been chosen as 4 keeping in mind the
lower number of predictors (also 4) and to lower the possibility of overfitting. the results of robust
polynomial regression are reported in table 1.
4.4. gaussian process (gp) regression
starting from a linear model ( y ï€½ x t ï¢ ï€« ï¥ , ï¥ ï€º ï ï€¨ 0, ï³ 2 ï€© ) the gp explains the prediction using the
latent variables f ï€¨ x i ï€© , i ï€½ 1, 2 , ïŒ , n (for modelling the smoothness of the output) and the explicit basis
h (for projecting predictors in high dimensional space). if f ï€¨ x ï€© , x ïƒ ï‚¡ d be a gp having mean m ï€¨ x ï€©
and covariance k ï€¨ x i , x j ï€© , then given n observations ï» x 1 , x 2 , ïŒ , x n ï½ the joint distributions of the latent
variables ï» f ï€¨ x1 ï€© , f ï€¨ x 2 ï€© ,ïŒ, f ï€¨ x n ï€©ï½ are also gaussian. now let us consider the model as (6), with
h ï€¨ x ï€© , h : ï‚¡ d ï‚® ï‚¡ p being the basis and coefficients of the basis are ï¢ ïƒ ï‚¡

y ï€½ h ï€¨ x ï€© ï¢ ï€« f ï€¨ x ï€© , f ï€¨ x ï€© ï€º ï‡ï ï€¨ 0, k ( x i , x j ) ï€© .
t

p ï‚´1

:
(6)

the probabilistic predictions of gp regression is given by (7):

ï€¨

ï€©

p ï€¨yi f ï€¨ x i ï€© , x i ï€© ï€º ï yi h ï€¨ x i ï€© ï¢ ï€« f ï€¨ x i ï€© , ï³ 2 .
t

(7)

the gp regression utilizes the fact that two closely lying predictor values ï» x i , x j ï½ will have similar

ï»

ï½

response f ï€¨ x i ï€© , f ï€¨ x j ï€© and the similarity is represented by the kernel or covariance function

ï€¨

ï€©

k x i , x j ï± with the hyper-parameter vector ï± . the kernels vary mainly due to two parameters i.e. the

signal standard deviation ( ï³ f ) and characteristic length scale ( ï³ l ) which control how fast the
correlation between two points change. given a set of input-output data the gp algorithm estimates the
basis coefficients ï¢ , noise variance and the kernel hyper-parameters ï± . we used three different basis
functions where the model is extended by different basis matrix (h) by multiplying with the vector of
basis coefficients (Î²) i.e. the extended model becomes hÃ—Î². for the constant, linear and quadratic cases,
the basis matrix can be represented as (8)

23

geophysical journal international
constant ïƒ h ï€½ 1; linear ïƒ h ï€½ ï›1, x ï ;
ïƒ© x 112
ïƒª 2
x
ï‚¢ï‚¢
ï‚¢ï‚¢
quadratic ïƒ h ï€½ ï›1, x , x ï , x ï€½ ïƒª 21
ïƒª ï
ïƒª 2
ïƒªïƒ« x n1

x 122
x 222
ï
x n22

ïŒ x 12d ïƒ¹
ïƒº
ïŒ x 22d ïƒº .
ï ï ïƒº
ïƒº
ïŒ x nd2 ïƒºïƒ»

(8)

along with variation in the basis function we also explored six different kernel functions â€“ squared
exponential, matern 3/2, matern 5/2 and also their automatic relevance discovery (ard) versions (neal
1996; rasmussen & williams 2006):

ï€¨

ï€©

ï€¨

ï€©

ï€¨

ï€©

t
k squared ï€­ exp x i , x j ï± ï€½ ï³ 2f exp ïƒ©ïƒª ï€­ ï€¨ x i ï€­ x j ï€© ï€¨ x i ï€­ x j ï€© 2ï³ l2 ïƒ¹ïƒº
ïƒ«
ïƒ»
ïƒ¦
ïƒ¦
3r ïƒ¶
3r ïƒ¶
k matern ï€­3/2 x i , x j ï± ï€½ ï³ 2f ïƒ§ïƒ§ 1 ï€«
ïƒ·ïƒ· exp ïƒ§ïƒ§ ï€­
ïƒ·ïƒ·
ï³l ïƒ¸
ïƒ¨
ïƒ¨ ï³l ïƒ¸
ïƒ¦
ïƒ¦
5 r 5r 2 ïƒ¶
5r ïƒ¶
k matern ï€­5/ 2 x i , x j ï± ï€½ ï³ 2f ïƒ§ïƒ§1 ï€«
ï€« 2 ïƒ·ïƒ· exp ïƒ§ïƒ§ ï€­
ïƒ·ïƒ· ,
ï³ l 3ï³ l ïƒ¸
ïƒ¨
ïƒ¨ ï³l ïƒ¸

where, r ï€½

(9)

ï€¨ x ï€­ x ï€© ï€¨ x ï€­ x ï€© is the euclidean distance between the points ï» x , x ï½ .
t

i

j

i

j

i

j

considering separate length scale ( ï³ m ) for each of the predictors ( m ï€½ 1, 2,ïŒ , d ), the covariance
(kernel) function implementing the ard (neal 1996) takes the form in (10):
ïƒ© 1 d ï€¨ x im ï€­ x jm ï€© ïƒ¹
ïƒº
k ard ï€­ squared ï€­ exp x i , x j ï± ï€½ ï³ 2f exp ïƒª ï€­ ïƒ¥
ï³ m2
ïƒªïƒ« 2 m ï€½1
ïƒºïƒ»
k
x , x ï± ï€½ ï³ 2 1 ï€« 3 rï€¥ exp ï€­ 3 rï€¥

ï€¨

ard ï€­ matern ï€­ 3/ 2

k ard ï€­ matern ï€­5/ 2

where, rï€¥ ï€½

ï€©

ï€¨
ï€© ï€¨
ï€© ï€¨ ï€©
ï€¨ x , x ï± ï€© ï€½ ï³ ïƒ¦ïƒ§ïƒ¨1 ï€« 5 rï€¥ ï€« 53 rï€¥ ïƒ¶ïƒ·ïƒ¸ exp ï€¨ ï€­ 5 rï€¥ ï€©
i

j

f

i

j

2
f

(10)

2

ïƒ¥ ï€¨ï€¨ x ï€­ x ï€© ï³ ï€© .
d

m ï€½1

2

im

jm

2
m

the training of ard kernels in (10) e.g. ard materns are computationally more expensive than
their basic versions in (9) e.g. the simple matern kernels. the ard function automatically finds out
moving how far along a particular predictor will make the predictions uncorrelated. the inverse of the
length scale determines how relevant a predictor is, as also discussed in (rasmussen & williams 2006)
in a detailed manner. in all the regression models, the compact representation was adopted to store the

24

geophysical journal international
models as compact matlab objects that stores only the necessary information, instead of the full model
with information about training data etc. for a reduced storage requirement. also, while training the gp
models with constant basis, sometimes the cholesky decomposition of the covariance can be illconditioned, resulting in convergence failure of the gp algorithms. in such a case, a different initial
value of the kernel parameters, initial value and increased lower bound of the gp noise standard
deviation may improve the solution. for all the gp kernels, the computation for the log-likelihood and
gradient, the standard qr factorization and quasi-newton optimizer have been used for parameter
estimation. gaussian processes with certain kernels are known to have equivalent representation of
neural networks with infinite hidden nodes and are found to outperform many benchmark supervised
learning methods, especially complex regression problems due to its non-parametric bayesian nature
(mackay 1997)(sitharam et al. 2008; samui & sitharam 2010), albeit being computationally expensive
during the training process, compared to the nn learning. the gp based predictions in the compressed
as well as reconstructed domain signals along with the training time and storage requirements are shown
in table 2. here, the accuracies of the gp regression models are worth noticing in comparison with the
other classes of regression structures in figure 8.
4.5. support vector machine (svm) regression
in svm regression similar to the simple linear regression problem y ï€½ ï¢ t x ï€« b , the inputs (

ï€¨

ï€©

x i , i ï€½ 1,ïŒ , n ) can be mapped to a high dimensional space using a kernel ï† x i , x j . we used three

popular kernels viz. linear, polynomial and radial basis function (rbf) kernel in (11), with p being the
tuning parameters for the polynomial kernel (friedman et al. 2001)(rogers & girolami 2015):

ï† ï€¨ x i , x j ï€© ï€½ x it x j

: linear

ï€½ ï€¨1 ï€« x it x j ï€© , p ï€½ ï»2,3,4ï½ : polynomial .
p

ï€¨

ï€½ exp ï€­ x i ï€­ x j

2

ï€©

(11)

: rbf

the svm regression algorithm terminates using either of the three convergence criteria â€“ feasibility
gap (Î´), gradient difference ( ïƒ‘l ), or largest karush-kuhn-tucker (kkt) violation. the kkt criteria
act as constraints to the optimization problem which can be solved using the sequential minimal
25

geophysical journal international
optimization (smo) algorithm which is faster than the traditional quadratic programming approach
(huang et al. 2006). the comparative results of the svm regression models using the above three
kernels viz. linear, polynomial kernel of order 2-4 and the rbf have been reported in the supplementary
material as they do not yield high enough accuracy.
4.6. decision tree regression
in the decision tree regression, deep trees are grown first and then the optimal sequence of subtrees
are determined by pruning. firstly, the predictor space is divided into non-overlapping regions

rj , j ï€½ 1,2,ïŒ, j and then for every observation falling in a particular region, the prediction becomes
mean of the response values in rj . the regions are found out by minimizing the sum of squared error
(sse) in (12) between the real ( yi ) and the mean response of training samples within a particular box (

yï‚µ r j ) (james et al. 2013):
j

ï€¨

sse ï€½ ïƒ¥ïƒ¥ yi ï€­ yï‚µ r j
j ï€½1 iïƒr j

ï€©.
2

(12)

as the controlling parameters, the effects of varying the minimum number of leaf nodes (nleaf) and
minimum number of parent/branch nodes (nparent) are explored here and the accuracy vs. data storage
size trade-offs are also shown in the supplementary material. tree methods allow complicated nonlinear
and partitioned boundaries as non-overlapping regions, especially naturally modelling corners in the
input parameter space which are difficult to model with other regression models, thus often providing
good predictive accuracy, although generalization and storage size for large trees are inherent
challenges (james et al. 2013). also, the surrogate split option is used which is known to improve
predictive accuracy by randomly splitting the data at most 10 times in each leaf node. pruning of
decision trees is another option that has been used to produce a smaller tree with fewer splits. the results
of the tree methods with different leaf and parent size (3 to 20) have been reported in the supplementary
material.
4.7. ensemble regression with tree method

26

geophysical journal international
in ensemble regression, normally two algorithms are commonly used viz. least square boosting
(lsboost) and bootstrap aggregation (bag) (barutÃ§uouglu & alpaydin 2003). bagging grows multiple
weak learner trees on many resampled (bootstrap) replicas of the dataset and the predicted response is
the average prediction from all these trees. minimal leaf size of bagged regression tree is kept fixed at
5 and as the controlling parameter the number of learners (nlearn) are varied from 100 to 1000. in
lsboost every step fits a new learner using the difference between the observed and the aggregated
prediction of all the learners trained so far while minimizing the mse. while using the bagging method,
the size of the surrogate meta-models become huge (>1 gb) with just 100 learners and for just one
receiver location. therefore, bulk scale simulation using this approach is not recommended due to
unmanageable storage size of the trained proxy meta-models. the performance results of the ensemble
regression with several independently grown tree learners (100 to 1000) with bagging and boosting
method have been compared in the supplementary material.
4.8. shrinkage regression with polynomial kernel
when dealing with redundant or few less important predictors, the shrinkage methods give improved
performance over traditional regression methods that gives more priority on significant predictors over
the insignificant ones. in our four predictor ï» x , y , z , d ï½ based regression problem, the covariates are first
projected on to a higher dimensional space using a polynomial kernel of order 2-4 via the kernel function
xnfx (ieong 2012), since in many cases the kernel order for the design matrix exceeding the dimension
of the original inputs yield spurious results. as an example, a 3d event location ï» x , y , z ï½ under such a
3rd order polynomial kernel mapping would yield series of predictors like {1, x, y, z, xy, xz, yz, x2, y2, z2,
xyz, x3, y3, z3} etc. amongst these combinations, the shrinkage methods are expected to pick up the most
useful predictors from these new kernelized predictors, while pushing rest of the insignificant ones to
zero. the three variants of shrinkage regression i.e. least absolute shrinkage and selection operator
(lasso), elastic net and ridge regression solve the following minimization problem in (13) as weighted
sum of prediction error and penalty term on the coefficients (zou & hastie 2005):

27

geophysical journal international
2
ïƒ© 1 n
ïƒ¹
min ïƒª
yi ï€­ ï¢0 ï€­ x it ï¢ ï€© ï€« ï¬ pï¡ ï€¨ ï¢ ï€© ïƒº
ï€¨
ïƒ¥
ï¢0 , ï¢ 2 n
i ï€½1
ïƒ«
ïƒ»

(13)

where, pï¡ ï€¨ ï¢ ï€© ï€½ ï¡ ï¢ 1 ï€« ï€¨ ï€¨1 ï€­ ï¡ ï€© 2ï€© ï¢ 2 is the penalty term of elastic net interpolating between the
mixture of ïŒ1 ïŒ2 norm of the model coefficients and n is the number of samples for training. the above
elastic net problem approaches the lasso at ï¡ ï€½ 1, thus making lasso penalize the ïŒ1 norm only,
whereas the problem approaches ridge regression when ï¡ ï‚® 0 thus giving full penalty on the ïŒ2 norm
only (zou & hastie 2005; friedman et al. 2001). therefore, to implement the three shrinkage regression
methods ï¡ ï€½ ï»1, 0.5,10 ï€­6 ï½ have been considered using the four kernelized predictors ï» x , y , z , d ï½ with an
increasing polynomial order of 2-4. the regularization parameter (Î») in (13) controls the penalty
between the prediction error and a chosen norm ( ïŒ1 ïŒ2 ) of the model coefficients. a 10-fold cross
validation has been adopted to automatically choose the optimum Î» with minimum average error across
the folds of the training data and hence the best model is automatically chosen with optimum Î» for each
of the 100 multiple-regression problems. apart from the 10-fold cross-validation based optimum model
selection or kernel hyper-parameter tuning during the training phase, the best models on the training set
are also tested with a separate hold out dataset which is explored in the subsequent sections.
4.9. generalised linear model (glm) regression
glm is a special class of nonlinear models that still use linear methods for prediction. a linear model
( ï­ ï€½ x ï¢ ) based predictions can be interpreted as a normal distribution with mean ï­ where coefficients
ï¢ map each input on to the predictions linearly. in glm (14) the response can have a wide variety of

distributions f ï€¨ ïƒ— ï€© , known as the link function with mean ï­ :
f ï€¨ï­ ï€© ï€½ x ï¢

.

(14)

for normal distribution the link function becomes the mean i.e. f ï€¨ ï­ ï€© ï€½ ï­ , but for other complex
distributions, the canonical link functions and the mean inverse functions can be chosen in different
ways. for real valued outputs, choosing a normal distribution in glm is recommended which suits our

28

geophysical journal international
standardized outputs, whereas for positive/strictly integer values, other distributions like gamma,
inverse gamma, poisson or binomial can also be used.
4.10.

shrinkage based glm regression with polynomial kernel

these regression techniques have the advantages of both the shrinkage and glm methods, as
described in the earlier subsections. similar to the standard elastic net, norm based penalties are chosen

ï»

ï½

as ï¡ ï€½ 1, 0.5,10ï€­6 to implement lasso, elastic net and ridge regression respectively. in addition, a 10fold cross validation on the training data is also implemented to automatically select the regularization
parameter Î» in each of the multiple regression sub-problems. a normal distribution on the outputs and
an identity link function is considered in lasso-glm and other variants (friedman et al. 2010). for the
case of normal distribution as the link function, the predictions closely approach the base versions
without the glm, whereas glm enhancements are more popular in classification problems over the
regression problems. the kernelized shrinkage, glm and kernelized shrinkage glm based prediction
results have been compared in the supplementary material, where none of them attain a good predictive
accuracy.
4.11.

multi-layer perceptron (mlp) neural network (nn) regression

neural networks are widely used as universal function approximators and thus a popular choice in
many regression problems using a multiple inputs and multiple outputs (mimo) architecture. with an
aim of a fair comparison with other regression methods, we here employ a collection of multiple input
single output (miso) implementation of mlp neural networks with moderate size hidden nodes. often
neural networks are prone to pick up inconsistent patterns or outliers in the data, thus we used a
regularization constant of Î³ = 0.5 in the cost function (jreg), to keep an equal balance on both the penalties
due to the mse and the mean squared weight (msw) during the training process:
j reg ï€½ ï§ ï‚´ msw ï€« ï€¨1 ï€­ ï§ ï€© ï‚´ mse

.
ï€½ ï§ ï‚´ ï€¨1 m ï€© ïƒ¥ w2j ï€« ï€¨1 ï€­ ï§ ï€© ï‚´ ï€¨1 n ï€© ïƒ¥ yi ï€­ yï‚µ i
m

n

j ï€½1

i ï€½1

ï€¨

ï€©

29

(15)

geophysical journal international
although there have been recent developments on optimizers for fast training of large and deep
networks in classification problems, the traditional levenberg-marquardt (lm) backpropagation
algorithm has been shown to outperform on a wide variety of regression problems as it produces low
mse and high speed for training small to medium size networks with <1000 weights and bias terms
(plumb et al. 2005). as the size of the network grows, there are even efficient optimizers like scaled
conjugate gradient (scg) compared to traditional training algorithms like levenberg-marquardt. in the
present scenario, the whole dataset here during the nn training has been randomly divided in training
(70%), testing (15%) and validation (15%) set for each of the miso regression problems. the
hyperbolic tangent sigmoid (tansig) activation function in (16) is employed in the hidden layers and a
pure linear (purelin) activation function in the output layer which is commonly used for regression
modelling:
a ï€½ tansig ï€¨ n ï€© ï€½

2
1 ï€­ e ï€­2 n
ï€­
1
ï€½
, a ï€½ purelin ï€¨ n ï€© ï€½ n .
1 ï€« e ï€­2 n
1 ï€« e ï€­2 n

(16)

we also explored two different nn architectures â€“ feedforward and cascaded-forward networks
with single and double layer while the number of nodes is varied from 10 to 100 in each hidden layer
to keep the storage and training time comparable with other methods. the cascaded forward network
has similar architecture like feedforward networks except that it has an extra connection to the input
directly in each hidden layer, apart from the inputs from previous layer. the comparative performance
of these two nn architectures are shown in the supplementary material.
5. results and discussion
from the velocity model in figure 1, it is evident that along the y-direction, there is relatively small
variation in the rock properties, compared to the variation along the x-direction. also, the density and
p/s-wave velocities have rapid variation along the z-direction. therefore, the effect of the heterogeneity
will be different on different receivers placed at the sea-bed and thus finally affecting the likelihood
calculation in different ways. we here explore 6 different receiver arrangements for calculating the
likelihood by taking a subset of the 23 receivers, as shown in figure 3 (all 23, along principle diagonal,
anti-diagonal, central one, lower and upper triangular parts). in this section, the best regression model
30

geophysical journal international
from the previous section has been selected and the parameters are fine-tuned based on the seismic
responses recorded at the central receiver from the microseismic sources anywhere in the subsurface as
shown in figure 2.

5.1. prediction of a single seismogram at the central receiver
table 1: accuracy, computation time and size of the robust polynomial regression learning surrogates
polynomial

robustness

order

criteria

rsi

ridx

rrecon

andrews

0.7720

0.9626

0.3406

bisquare

0.7723

0.9626

0.3402

cauchy

0.7864

0.9661

0.3388

fair

0.7986

0.9679

0.3291

huber

0.7932

0.9672

0.3283

logistic

0.7946

0.9674

0.3322

ols

0.7913

0.9695

0.0951

talwar

0.7720

0.9629

0.3324

welsch

0.7756

0.9635

0.3410

andrews

0.8451

0.9665

0.7822

bisquare

0.8453

0.9665

0.7798

cauchy

0.8531

0.9700

0.5995

fair

0.8616

0.9720

0.6258

huber

0.8587

0.9713

0.6312

logistic

0.8593

0.9715

0.6254

2

3

31

training

model size for single

time (s)

receiver (kb)

3.70

16

3.52

16

3.45

16

3.44

16

3.43

16

3.85

16

3.13

15

3.30

16

3.54

16

4.32

21

4.62

21

4.00

21

3.83

21

3.88

21

4.08

21

geophysical journal international

4

ols

0.8706

0.9736

0.3289

talwar

0.8454

0.9667

0.7814

welsch

0.8465

0.9671

0.6816

andrews

0.8648

0.9725

0.7119

bisquare

0.8642

0.9725

0.7081

cauchy

0.8743

0.9759

0.7144

fair

0.8825

0.9776

0.7611

huber

0.8799

0.9770

0.7374

logistic

0.8803

0.9771

0.7414

ols

0.8827

0.9791

0.1963

talwar

0.8670

0.9729

0.6999

welsch

0.8660

0.9732

0.7091

3.31

21

3.58

21

4.22

21

4.68

25

4.65

25

4.44

25

4.14

25

4.26

25

4.25

25

3.54

25

4.06

25

4.54

25

table 2: accuracy, computation time and size of the gaussian process learning surrogates

basis

quadratic

linear

time in

compact model

kernel

rsi

ridx

rrecon

hour

size (in mb)

squared exponential

0.9575

0.9911

0.7851

1.96

5.017

matern 3/2

0.9728

0.9939

0.8729

2.53

5.024

matern 5/2

0.9647

0.9925

0.7136

2.42

5.023

ard squared exponential

0.9631

0.9945

0.9377

29.68

5.032

ard matern 3/2

0.9707

0.9963

0.9377

9.29

5.031

ard matern 5/2

0.9668

0.9956

0.9427

9.79

5.03

squared exponential

0.9522

0.9908

0.8125

2.04

5.022

matern 3/2

0.9535

0.9938

0.8434

3.25

5.02

matern 5/2

0.9624

0.9922

0.6873

3.16

5.02

32

geophysical journal international

constant

ard squared exponential

0.9625

0.9943

0.9186

4.65

5.032

ard matern 3/2

0.9696

0.9962

0.9467

8.12

5.027

ard matern 5/2

0.9664

0.9954

0.9431

9.64

5.027

squared exponential

0.9453

0.9902

0.8617

1.78

5.036

matern 3/2

0.9692

0.9916

0.8312

4.60

5.036

matern 5/2

0.9587

0.9919

0.6559

3.29

5.051

ard squared exponential

0.9621

0.9937

0.8812

36.19

4.995

ard matern 3/2

0.9703

0.9957

0.9428

7.08

4.941

ard matern 5/2

0.9409

0.9857

0.2389

1.53

4.991

here the central receiver (r-12 in figure 3) is considered to be seated at a fixed location (nx/2,
ny/2) at z = 244th grid point, whereas the sources can roam around anywhere in the rock volume
underneath. we first aim to predict the seismic traces at the central receiver using a regression metamodel, fitted using the 2000-unit amplitude microseismic events at different lh sample locations as
shown in figure 2. under an exhaustive search for the best polynomial kernel combining the right
polynomial order in the 4 different predictors, it is revealed from table 1 that using the random source
positions, a 3rd order polynomial with andrews robustness criterion yields the best prediction accuracy
upon reconstruction, with a 2d correlation coefficient of rsi = 0.8451, ridx = 0.9665 and rrecon = 0.7822
with respect to the original nsourceÃ—nt = 2000Ã—501 samples of seismic dataset. the other combinations
like 3rd and 4th order polynomials and different robustness criteria work fairly similarly, except the
ordinary least square as this is prone to outliers and non-normal datasets. the training time and the
storage of the robust regression coefficients are minimal amongst all the proxy meta-models, explored
in this section. other complex models can push the predictive accuracy to a higher value which are
explored next although they need more computational time for training. in this section, we show
comparison of different classes of regression models in terms of training accuracy (r2d) for the two
compressed parts having 100 dominant time instants, accuracy of the reconstructed seismograms,
training time and proxy storage size, utilizing the 2000 training samples, with 10-fold cross validation
to select hyper-parameters of different family of regression models. the best classes of models found

33

geophysical journal international
with this exploration as an initial screening has been further tested with 2000 independently held out
testing samples and reported in the following sections.

figure 9: ground truth vs. predicted variation in si and idx along depth (z-direction) using gaussian process surrogates.

figure 10: ground truth vs. predicted variation in si and idx as a function of distance d using gaussian process surrogates.

most variants of gaussian process surrogates with quadratic and linear basis are found to have a
high predictive accuracy particularly with ard covariance structures, although it is more
34

geophysical journal international
computationally expensive as compared in table 2. the squared exponential kernel produces inferior
results compared to the matern 3/2 and matern 5/2 versions with both ard and the basic kernels. the
best accuracy has been obtained using the ard matern 3/2 kernel using linear basis on the training
dataset in the initial screening. validation of the prediction performance on the 2000 testing datasets
and moreover on a sample by sample seismogram prediction using 1d pearson correlation coefficient,
instead of average 2d correlation coefficient has been shown in the subsequent sections, for the final
choice of best gp model structure.
the svm with polynomial and rbf kernel, glm and/or kernelized shrinkage regressions like
lasso, ridge, elastic nets and moderate size neural networks have produced a poor predictive
performance, particularly most of them fail to partition between the positive and negative pressures in
the scatter diagrams as a function of the predictors. the increased computation time in lasso and elastic
net is due to the cross-validation based automatic selection of hyper-parameter Î» by finding a balance
between penalizing prediction error and the model coefficients.
as described before, the decision tree method can produce high accuracy particularly with lesser
value of nparent thus producing large size of the tree and hence larger model size. in general, within the
ensemble methods, boosting trees produced better results than the bagging tree methods with the same
number of weak learners. with 1000 weak tree learners, the prediction accuracy reaches around r ï‚» 0.6
with the bagging method, while the sizes of the learned models become greater than few gbs and hence
not investigated further with higher number of ensemble learners. in each case of the tabulated results
using various family of regression models which can be found the supplementary material, the best
predictive accuracy and the associated tuned parameters has been highlighted as bold italics entries for
reconstructed accuracy rrecon.
from these exhaustive comparisons, it is found that the gp with linear basis function and ard
matern 3/2 kernel outperform all the rest of the model families to predict an accurate seismogram on
the central receiver and hence has been chosen for further analysis. however, although the accuracy of
ard family of kernels produce best results, they can be computationally expensive during the training
process and also depending on the number of data-points in the training set for the covariance
estimation. apart from the gp models, the decision tree with lower number of parents and in the family
35

geophysical journal international
of ensemble learning methods, bagging with higher number of weak learners also perform well but
these models have a larger storage requirement. therefore, as a compromise between the achievable
accuracy, training time, and storage requirement, the gp model with ard matern kernel is found to be
the best choice for this regression problem in seismology. however, it is important to note that the
regression models map each location in two compressed domains which are indeed smooth, as can be
seen from figure 9-figure 10. however, using the predicted samples in the compressed domain each
seismogram is reconstructed using the decompression steps that may not finally make the whole event
co-ordinate to seismic trace mapping to be smooth enough. a slight decrease in the index term may not
also yield smooth reconstructed seismograms which makes the final reconstructed correlation
coefficient (rrecon) to have a lower value for most of the learning algorithms compared to the gaussian
process regression models.
also, from the results with increasing number of layers and number of hidden nodes in both the
feedforward and cascaded neural networks, the compressed domain accuracies are found to be fair
(provided in the supplementary material). however, they take more time during the training process.
training of very large networks (>100 nodes in 3 layers) has not been attempted since they need more
time during the training process and since the alternative models are already giving better accuracy
within similar training time-frame. also, according to the â€œno free lunchâ€ theorem, for different
statistical learning scenario, there is no consensus that one single class of models whether neural
network or gaussian process would consistently outperform other family of learners and the best
recommendations are to try a pool of models amongst which a class of models wins for a specific
application. this fact is even more prominent in the context of difficult regression problems, as
discussed in (lattimore & hutter 2013; wolpert 2002; goutte 1997; domingos 2012), to achieve high
enough accuracy compared to the well-researched classification problems where nns are shown to
outperform in contemporary research.
previously in figure 7, the variation of the two compressed components of the original seismic
signal â€“ si and idx have been shown. here, the predictions of the machine learning algorithms have also
been shown on these two components as a function of 4 covariates {x, y, z, d} in figure 9 and figure
36

geophysical journal international
10 respectively showing variation with depth (z) and distance (d) and the two lateral directions in the
supplementary material. the ground truth of compressed domain data is presented as the circles and the
corresponding predictions are shown as square boxes. it is evident that the gaussian process metamodel is capable of learning the split predictions for the positive and negative pressure values and the
corresponding complicated shape for idx as found with respect to {z, d} in figure 9 and figure 10
respectively.

(a)

37

geophysical journal international
(b)
figure 11: (a) original, compressed and predicted reconstructed seismograms using gaussian process surrogates. (b)
zoomed seismic traces with predicted samples. the amplitude is in pascal and 500 samples represent 2 sec of seismic data.

it is understandable from the schematic diagram in figure 8 that the regression models are trained
to predict only the two components in the compressed domain and the respective accuracies have been
reported as rsi and ridx in the tables. the predictions in compressed domain are then used to decompress
and obtain the sparse predicted seismograms as shown in figure 11(b), corresponding to the
reconstructed accuracies rrecon in the tables. figure 11(a) compares the original (elastic pde
simulations), compressed and predicted compressed seismograms which show minimal loss of
information with the compressed representation (r = 0.999) and predicted compressed (r = 0.947) data.
in particular, the arrival times of the seismograms are accurately predicted which carry most of the
useful information in a source location inversion process (tarantola 2005), as also evident from the 10
representative samples from the 2000 data-points along with a zoomed version of three seismograms in
figure 11(b). the predictions of the surrogate regression models are most commonly visualized in the
form of cross plots as the deviation around the optimal least square line which are shown in the two
compressed as well as the reconstructed domains in figure 12, along with the achieved predictive
accuracies mentioned in the titles of the subplots.

figure 12: cross-plots between the ground truth and the best gp predictions in compressed and reconstructed domains.

38

geophysical journal international

figure 13: 2-d visualization of ground truth and the gp predictions in compressed and reconstructed domains.

the predictions can also be visualized in the form of a 2d image where the rows indicate different
trace numbers, sorted against the distance from the central receiver and the columns denote the
dominant time instant in the compressed domain and time samples in the reconstructed seismograms as
shown in figure 13, using the best found regression model i.e. gaussian process with linear basis and
ard matern 3/2 kernel. on the reconstructed image of the seismograms in the last column of figure
13, the power law behaviour in the dominant amplitude as a function of increasing distance from the
central receiver is evident. the first column of figure 13 clearly shows an oscillatory i.e. first increase
in pressure, followed by a pressure drop. the corresponding dominant time instants in figure 13 (also
incorporating the first arrival time) has more complexity due the heterogeneity, complex ray paths and
p/s-wave mode conversion of the seismic waves.
5.2. learning curve analysis and computation speed up using the gp regression models
the best found trained model structures reported in the previous subsection are now evaluated for
their predictive accuracy on both the 2000 training and 2000 testing lh samples shown in figure 2 with
a random choice of the subset of samples and multiple shuffles, commonly known as the learning curve
analysis. the learning curve analysis shows the accuracy vs. robustness trade-off for a trained model
39

geophysical journal international
and helps in selecting the minimum number of training samples required to get a fair predictive
accuracy. here the number of samples are gradually increased as shown in figure 14 while the average
prediction accuracy of 100 monte carlo shuffles are carried out to select a subset of samples from each
of the 2000 training and 2000 testing lh samples. both the training and testing datasets seem to
converge after 1000 samples. a smaller gap between training and testing dataset is reflected in the 2d
correlation coefficient of the data and indicates an improved performance over the other methods.
figure 14 also shows that the ard matern 3/2 kernel produces slightly better accuracy with the linear
basis over the quadratic basis on both training and testing dataset. however, the results seem to converge
closely using the ard matern 5/2 kernel. depending on the heterogeneity of the velocity model, the
learning curves on the training and testing datasets may vary, in other studies.

figure 14: learning curves of the gp models using the training and testing data using 100 monte carlo shuffles of the datasets.

in figure 15, we show the run time distributions of the 2000 synthetic seismogram simulations using
various trained gp proxy meta-models. it is evident from figure 15 that the time required to generate a
single seismogram is less than a sec using the surrogate model as compared to the gpu based full elastic
wave equation solving, as shown in earlier sections. however, there is an intermediate computationally
expensive step to train the surrogate meta-models as shown in table 2, which gradually increases with

40

geophysical journal international
the number of samples for gaussian process regression and particularly with ard family of kernels,
although they provide more accurate results than other methods. this has been investigated in the next
subsection.

figure 15: histogram of a single seismogram generation time at the central receiver using the trained surrogate model and
4000 data-points.

5.3. effect of training sample size on the regression modelling

41

geophysical journal international
figure 16: training time requirements and predictive accuracy with increased training samples size using ard matern 3/2
kernel with quadratic and linear basis function. saving in training is clear using 12 parallel cores (over 4 and 8 parallel cores)
and with linear basis function (over the quadratic one). predictive accuracies are comparable between linear and quadratic
basis with increased sample size.

here we explore the training time and predictive performances with increasing training sample size
for the best gp models which are found to outperform the other family of regression models. the gp
ard matern 3/2 model has been trained in parallel using 4, 8 and 12 core cpu via the parallel for
(parfor) loops in matlab on the 100 dominant time instants to learn the compressed seismograms. in
order to show the scalability of the training process, the number of samples have been increased from
20 to 500 in steps of 20 samples and the required training times along with the corresponding predictive
accuracies have also been shown in figure 16, as a function of the sample number (nsample). it is evident
that there is a steep increase in training time for over nsample>300 and even more with the quadratic basis
in the gp ard matern 3/2 kernel while the predictive accuracy fluctuates around 2d r2 = 0.93-0.95.
also the small fluctuations on the training accuracy can be observed in figure 16, as the random samples
come from different positions of the heterogeneous velocity model and thus introducing certain set of
samples may slightly reduce the overall accuracy but varies within a small range and finally settles
down. this is more evident in a finer resolution in the learning curve analysis in figure 14 on the trained
model using 2000 training/testing data with 100 monte carlo shuffles of increasing subset of samples,
as presented in the previous subsection.
5.4. prediction enhancement by using smoothing filter
since the gp models predict the dominant 100 time-instants and the corresponding signal values
independently, as a function of event spatial locations, without explicitly considering the temporal
correlation of the seismogram time series, sometimes the predicted signals may not be smooth in time.
especially in some cases, rapid positive and negative pressure fluctuation may be encountered i.e. with
reverse polarity with a small movement of the event locations as shown in figure 9 and figure 10.
therefore, a moving average (ma) smoothing filter is applied on the gp predicted seismic data while
varying its span size from 1-10 in order to select the best filter settings for ensuring the smoothness of
42

geophysical journal international
the seismogram time series. since the dominant signal values are predicted separately in the learning
process, without considering the temporal information between two consecutive time samples, the
predicted seismic signals may not vary smoothly in few cases. here, the purpose of the smoothing filter
is thus to introduce some amount of inertia against rapid fluctuation of the signals against changing
polarity within a short span of time. it is apparent that a larger span of the smoothing filter introduces a
delay in the seismograms and hence the performance degrades gradually, as evident from figure 17
showing a sweep over ma smoothing filter window size from 1-10 consecutive time samples. in the
smoothed versions of the seismograms, both the ard matern 3/2 and ard matern 5/2 kernels with
either linear or quadratic basis win over the other combinations, particularly on the test-set. figure 17
also suggests that these gp settings with a ma smoothing filter of span size of 3 samples are capable
of producing accurate predictions both in the training and testing set with a 2d r>0.91. representative
examples of the predicted reconstructed and smoothed seismograms are shown in figure 18 and figure
19 from the training and testing set respectively.

figure 17: performance of smoothed seismogram predictions using gp quadratic basis and six kernels. both ard matern
3/2 and 5/2 kernels for both linear and quadratic basis give good prediction performance on both the training and testing set.

43

geophysical journal international

figure 18: original simulated, gp predicted and smoothed reconstructed seismograms in the training dataset.

figure 19: original simulated, gp predicted and smoothed reconstructed seismograms in the testing dataset.

it is observed from figure 17 that the curves attain their maxima at ma filter span = 3 samples for
all the cases and also, the ard matern 3/2 and ard matern 5/2 kernels with both linear and quadratic
basis functions give comparable average accuracy on the training and testing dataset, in terms of 2d
correlation coefficient. individual accuracies of each seismogram may be different using these two
44

geophysical journal international
kernels. hence the 1d pearson correlation coefficient based on the individual smoothed seismograms
have also been calculated after the initial model screening, on both the training and testing dataset using
the ard matern 3/2 and 5/2 kernels with both the linear and quadratic basis functions. although the
overall predictive accuracy (in terms of 2d correlation) on the training and testing dataset are similar
for the ard matern 3/2 and 5/2 kernels with both linear and quadratic basis, as shown in figure 14,
the number of relatively poor predictions or outliers present in the predictions are actually different.
therefore, from the first stage screening from the pool of machine learning algorithms using 2d
correlation coefficient on all time instants and samples, following the schematic in figure 8, we carry
out a further second stage selection of the best algorithm that yields minimum number of outliers in its
predictions. we define a predicted data-point as outlier if the 1d pearson correlation coefficient between
a particular simulated and the corresponding predicted smoothed seismogram becomes negative i.e.
r1d<0. the goal here is to minimise such extreme predictions, although most of the predicted
seismograms show fairly high accuracy.

figure 20: histograms of 1d correlation coefficients between ground-truth vs. smoothed predicted seismograms for the
central receiver using the training and testing dataset. the percentage outliers in different models are mentioned in the titles.

45

geophysical journal international

figure 20 shows that in all the cases for both training and testing dataset, most of the samples give a
good predictive accuracy, as revealed from the peaks near r â‰ˆ 1. in all the cases there is a small decaying
left tail, indicating a drop in the predictive accuracy. counting these outliers as a fraction of the total
sample size below a fixed threshold r1d<0 can identify the best surrogate meta-model structure with
minimum number of outliers. it is apparent from figure 20 that the ard matern 3/2 kernel with both
linear and quadratic basis functions produce the same lowest number of outliers (0.7%), on the training
dataset. the same kernel with linear basis produces 0.05% less outliers having negative correlation,
over that with the quadratic basis and hence chosen in remainder of the paper for further analysis. the
presence of few predicted outliers can also be viewed from the cross-plots in figure 21, after applying
the tuned smoothing filter on the predicted seismograms. in general the ard matern 3/2 kernel with
linear basis gives a trade-off between high average predictive accuracy (as revealed from the cross-plots
on training/testing dataset in figure 21) and minimum number of outliers (represented by the left tail of
the histograms in figure 20).

figure 21: cross-plots of the training and testing dataset with and without smoothing using the gp quadratic basis matern
3/2 kernel. data is reshaped in 1d array to calculate the optimum least square line and correlation coefficient r.

6. prediction performance on all the 23 receivers
6.1. selection of the best regression model for all the 23 receivers

46

geophysical journal international

figure 22: histograms of 1d correlation coefficients of each predicted seismograms for all the 23 receivers using the training
and testing dataset. the percentage outliers in different models are mentioned in the titles of subplots.

the exploration reported in the previous section shows fine-tuning of the proxy models on the
central receiver when the event can roam around within the subsurface volume underneath. this
mapping for the event location to receiverâ€™s response is not necessarily similar for different receivers
(in figure 3), due to the heterogeneity of the velocity model. we now verify the performance of the
proxy or surrogate meta-model on all 23 receivers using the best set of models that produced good
predictive accuracy on the central receiver i.e. gaussian process regression with linear and quadratic
basis having ard matern 3/2 and ard matern 5/2 kernels. amongst these four class of models, the
smoothing filter-tuning and outlier detection has been carried out in the same way for the multiple
receiversâ€™ case, as shown in the earlier sections. the other choices of smoothing filter window size apart
from 3 samples are found to be worse, as also shown before for the central receiver. here, individual
receiver responses are analysed separately instead of comparing aggregated predictions on the 23
receivers. with the ma smoothing filter having a window size of 3 samples, the 1d pearsonâ€™s
correlation coefficient (r1d) between the predicted vs. original seismogram on training and testing data
for all 23 receivers are shown in figure 22, using the best four gp proxy meta-models. it is also evident
47

geophysical journal international
from figure 22 that although the ard matern 3/2 kernel with quadratic basis in gp produces 0.51%
fewer outliers with r1d<0 on the training dataset, on the testing dataset the same kernel with linear basis
produces 0.29% fewer outliers and is hence chosen for the rest of the analysis and the likelihood
calculation.

figure 23: box-plots of the receiver-wise correlation coefficient between the original vs. predicted seismograms on the
training and testing dataset using ard matern 3/2 kernel with linear (top panel) and quadratic basis (bottom panel). the red
crosses indicate outliers in the prediction on individual receivers.

48

geophysical journal international
figure 24: predicted seismogram wiggle plots using the ard matern 3/2 kernel on the training dataset (top) ground truth,
(middle) with linear basis, (bottom) with quadratic basis. corresponding r2d on 23 receivers are reported in the subplot titles.

receiver-wise prediction performances are shown in figure 23 on the 2000 training and 2000
testing datasets using the top 2 surrogate meta-models using ard matern 3/2 kernel with linear and
quadratic basis functions. it is apparent from figure 23 that the central receiver (r-12) produces the
best prediction accuracy amongst all the 23 receivers. also, amongst these 23 receivers in figure 3, r5, r-8, r-15, r-19 are found to have the next best responses whereas r-3, r-10, r-17, r-21 contain
relatively more outliers. the presence of outliers in certain channels does not necessarily represent
unusable predictions, as the 1d correlation coefficient r1d essentially compares the full morphology of
the spiky seismograms. in most cases, the arrival times and the polarity of first arrival of the seismic
waves are predicted accurately, containing most of the useful information (tarantola 2005). we have
also shown 5 representative examples of true vs. predicted seismograms on all the 23 receivers from
both the training and testing datasets in figure 24 and figure 25 respectively using the ard matern
3/2 kernel with linear/quadratic basis (in the two bottom rows), where the arrival times and morphology
of the seismic response using the proxy meta-models are identified almost accurately with the original
solutions of the expensive elastic pde solver (represented in the top row).

figure 25: predicted seismogram wiggle plots using the ard matern 3/2 kernel on the testing dataset (top) ground truth,
(middle) with linear basis, (bottom) with quadratic basis. corresponding r2d on 23 receivers are reported in the subplot titles.

49

geophysical journal international
6.2. comparison of the computation time for surrogate proxy meta-models vs. solving the full
elastic wave equation
in this subsection, we compare the run time saving due to the use of proxy or surrogate meta-models
instead of the full elastic pde solver for obtaining the seismic response at the 23 receivers at specified
locations due to explosive microseismic events at random locations in the subsurface. as discussed in
the introduction section, the purpose of surrogate meta-modelling is to reduce the computational time
for fast generation of approximate template seismic events and hence facilitate a likelihood-based
inversion approach where such fast noiseless template data generation is required in batches of
thousands of speculative event locations.
figure 26 shows that using the gp linear basis with both ard matern 3/2 and 5/2 kernel, the peak
of the run time distribution is around 2 sec and for the quadratic kernels the peak run time is around 2.8
sec on a standard 4-core 64-bit windows desktop pc with 16 gb memory and intel i5, 3.3 ghz
processor. whereas for a single shot seismic simulation, the original elastic wave propagation on a 12core linux pc with k20 gpu card with 5.5 gb memory and 1.1 ghz processor, the peak run time is
1063 sec â‰ˆ 17.7 min. therefore, to simultaneously compute the seismic response at the 23 receivers,
the surrogate regression meta-models produce a 531-fold acceleration using the linear kernel and 380fold acceleration using the quadratic kernel. this speed up for the forward simulation when called from
the likelihood function comes at the cost of initial simulation for training data generation and required
training efforts of gp regression meta-models, but this is needed only once for a fixed velocity model.
there is also a small inaccuracy incurred due to the compressed domain regression modelling in
comparison with the original elastic wave simulation. in many real microseismic monitoring
applications such approximate templates are sufficient for probabilistic event parameter estimation
problems, since the measurements are often buried under significant amounts of noise, thus making the
effect of such small modelling uncertainties due to the proxy negligible.

50

geophysical journal international

figure 26: comparison of run time distributions between elastic wave propagation on k20 gpu card and surrogate proxy
meta-model predictions on cpu for all the 23 receivers.

7. fast computation of the likelihood function using the trained surrogate meta-models
7.1. formulation of the likelihood function for detecting microseismic events
in this section, we use the best surrogate meta-model i.e. the gp with linear basis and ard matern
3/2 kernel, followed by a ma smoothing filter with a span-size of 3 samples, for fast computation of
template seismic response in the likelihood computation. in many geophysical inverse problems, fast
calculation of the likelihood is necessary in order to get the maximum likelihood (ml) or maximum aposteriori (map) estimates or calculation of the evidence to enable model comparison. representing
the true noiseless template seismic response due to a microseismic event as yï‚µ and a measured noisy
response as y , the gaussian likelihood function can be calculated as (17):

lï€½

ï€¨

ï€© ï€¨

ï€©

t
ïƒ© 1
ïƒ¹
exp ïƒªï€­ y ï€­ yï‚µ cï€­1 y ï€­ yï‚µ ïƒº .
n
ïƒ»
ï€¨ 2ï° ï€© c ïƒ« 2

1

(17)

here, c is the covariance matrix of the noise on the measured data y, and n is the number of observed
data points. often the likelihood is represented in log-scale for convenience in bayesian analysis and is
given by (18) considering either a full or only diagonal covariance matrix:

51

geophysical journal international

ï€¨

ï€©

ï€¨

ï€©

t
log c 1
n
log ï€¨ 2ï° ï€© ï€­
ï€­ y ï€­ yï‚µ c ï€­1 y ï€­ yï‚µ ,
2
2
2
2
ïƒ¬ n log ï€¨ï³ ï€©
for diagonal covariance .
ïƒ¯
log c ï€½ ïƒ­
cholesky
ï€© for full covariance
ïƒ¯ 2 ï‚´ ïƒ¥ log diag ï€¨ ci
i
ïƒ®

log l ï€½ ï€­

ï€¨

(18)

ï€©

in the log-likelihood calculation involving the full covariance matrix, the cholesky decomposition is
commonly used for numerical stability and increased speed, whereas for diagonal covariance the log
determinant of covariance (log|c|) can be easily computed using the common variance (Ïƒ2) as in (18).
for calculating the log-likelihood in (18), given some speculative microseismic event locations {x,
y, z} the noiseless predicted seismic data can be obtained using the trained proxy meta-model in (19)
following the steps shown in the schematic diagram figure 8:

y ï€½ fproxy ï€¨ x ï€© ï€½ fproxy ï€¨ x, y, z, d ï€© .

(19)

the covariance matrix c required in the likelihood (17) can be calculated from the measured noisy data
(y) using (20), considering a diagonal covariance or uncorrelated noise for the sake of simplicity:

ï€¨

ï€©ï€¨

ï€©

t
c ï€½ ï…ïƒ© y ï€­y
y ï€­ y ïƒ¹ , y ï€½ ï… ï›y ï ,
ïƒ«ïƒª
ïƒ»ïƒº
y ï€½ yï‚µ ï€« ï ï€¨ 0, c ï€© , c ï€½ ï³ 2 i .

(20)

here, Ïƒ2 is the common variance of the data, reshaped as 1d vector y in the multi-receiver case, with
the assumption of no correlation amongst them and y represent the mean of the measured data, while

ï… being the mathematical expectation operator. here in (20), the noise has been considered to have a
gaussian distribution with zero mean and a specified variance Ïƒ2, however any expert choice of the
noise covariance can also be incorporated in the likelihood function (17).
next we calculate and visualize the likelihood as an inverse problem for the microseismic event
locations (tarantola 2005)(aster et al. 2011), in different cases viz. using only the central receiverâ€™s
data, seismograms along the principal and anti-diagonals, in the upper/lower triangular parts or using
all the 23 receivers. for computation of the likelihood, template seismic responses corresponding to
single microseismic events at random positions are calculated first using the fast proxy/surrogate metamodels and independent white gaussian noise (wgn) of two different standard deviations Ïƒ = {100,
52

geophysical journal international
250} which are added on the noiseless seismic data to generate some realistic corrupted dataset. the
noise free data is assumed to be generated due to a microseismic event at the grid point (31, 25, 158),
as a representative example for the log-likelihood calculation. the signal to noise ratio (snr) has been
calculated on the single/multiple receivers using the ratio of average energy calculated through the sum
of squared signal amplitudes and represented in the decibel scale as in (21):

ïƒ¦
ïƒ¶
2
2
snr ï€½10log10 ïƒ§ ïƒ¥asignal
,i ïƒ¥anoise,i ïƒ· db .
i
ïƒ¨ i
ïƒ¸

(21)

it is understandable that in a relatively less noisy or high signal to noise ratio (snr) case, the
likelihood function will be manifested as a narrow delta function in the event parameter space which
may be harder to detect. in the case of higher noise or in other words low snr levels, the likelihood
function gets softened which may help navigating towards the maximum likelihood regions by standard
optimization or sampling algorithms like markov chain monte carlo (mcmc) etc. here we focus on
obtaining the ml estimates of the event parameters for a single microseismic source, with a specified
noise variance by gradually increasing the number of receivers.
the most likely event positions are visualized using the scatter diagrams for different receiver
combinations. for this purpose, here we use the top 90 percentile of all the log-likelihood values out of
the 4000 uniformly distributed lh samples. out of these 4000 samples one of them is the ground truth
voxel which is expected to have the highest likelihood value. in order to verify this, we calculate the
maximum likelihood estimate of the microseismic event location and the norm difference of the event
positions from the ground truth location (31, 25, 158), with two different snr levels using various
combinations of receiver positions which can be found in the supplementary material. in the next subsection, the joint distribution of the event parameters in the 2d scatter plots are shown using the top 90
percentile of the likelihood values where the higher likelihood values are represented by bubbles with
a darker shade. in all the cases the legends show the log-likelihood values.

53

geophysical journal international

(a)

(b)

(c)

(d)

figure 27: receiver subset selection for the likelihood calculation (a) principal diagonal, (b) anti-diagonal, (c) uppertriangular region, (d) lower triangular region.

7.2. different receiver subset selection and its effect on the likelihood function
here we explore 6 different cases of the receiver subset out of the 23, in order to calculate the
likelihood using the lh samples. the positions of the central receiver (r-12) and all the 23 receivers
have been shown in figure 3. out of the 23 receivers, we now select a subset of receivers along the
principal-diagonal (total 7) and anti-diagonals (total 5) as shown in the top row of figure 27 (a and b).
in order to show the effect of the heterogeneity in the velocity model of figure 1, here the 23 receivers
are divided in the upper and lower triangular parts as shown in the bottom row of figure 27 (c and d).
54

geophysical journal international
the corresponding noiseless and noisy seismograms in these different geometries are shown in figure
28. to better understand the spiky or narrow spatially localised nature of the likelihood function for
multiple receivers, the scatter diagrams of the randomly sampled likelihoods are provided in figure 29.
moreover, there are small errors incurred between the true pde simulated vs. the proxy predicted
seismic data in all the receivers which get combined within the likelihood calculation. the propagation
of these small modelling uncertainties due to the use of proxies or surrogates for different receivers on
the final parameter estimates in the inversion process may be explored more systematically in a future
work. as per the previous reports of geophysical inversion e.g. in (tarantola 2005), incorporating more
receiversâ€™ data should make the estimates more accurate and the non-vanishing high likely regions
should ideally shrink towards a smaller region within the volume under scanning which is also observed
here. however, addition of higher noise level decreases the snr and consequently softens the
likelihood in all the cases. the gross natures of the likelihood are not drastically altered for the same
receiver subsets but different noise levels.

figure 28: seismograms used for calculating the likelihoods and maximum likelihood estimate of event position (top) noiseless
(middle) wgn with Ïƒ = 100, (bottom) wgn with Ïƒ = 250.

it has been found that some of the regions have more high likelihood values where the data was
originally generated from whereas in certain cases some other regions have more non-vanishing
55

geophysical journal international
samples with high likelihood values. this is essentially a problem of resolution vs. smoothness tradeoff of the likelihood, whereas an accurate detection should locate towards the highest likelihood voxel
and its neighbouring regions. this may be an effect of the heterogeneity of the velocity model that
neighbouring samples not always yield a smooth variation of the likelihood values. alternatively, the
likelihood values could have been calculated using some derived features of the seismic traces like the
arrival times as shown in (tarantola 2005) or some other feature like the polarity of first arrival etc.
which may be pursued in a future research. the choice of the feature in such cases is crucial to yield a
smooth variation of the likelihood values in the neighbouring voxels whereas here we focus on the raw
seismic data based likelihood calculation only.
the non-vanishing most likely regions can also be summarized in terms of maximum likelihood
point estimates, by bulk likelihood calculation using the lh samples. a systematic exploration would
need a bayesian sampling of the posterior distribution using a chosen likelihood function involving the
raw seismic data itself or using some derived features (like arrival time or polarity) where the samples
will gather more towards the mode of the posterior probability distribution which may be pursued in a
future study. here we explore the maximum likelihood values for convenience, corresponding to the
bulk likelihood calculation at random locations. the detection error norm ( e ) for the event positions
has been calculated as the euclidean distance between the ground truth ( x o , y o , z o ) and estimated (

xï‚µo , ï‚µ
yo , zï‚µo ) locations via maximum likelihood using (22):
e ï€½

ï€¨ x ï€­ xï‚µ ï€© ï€« ï€¨ y ï€­ ï‚µy ï€© ï€« ï€¨ z ï€­ zï‚µ ï€© .
2

o

o

2

o

o

2

o

(22)

o

the noise levels, the corresponding maximum likelihood estimate based detected voxels and
detection error norm are reported in the supplementary material. it is evident that all the different cases
can essentially capable of identifying the ground truth voxel with highest likelihood value. depending
on the noise level and receiver arrangements, some other voxels may also spuriously show high
likelihood values which is explored next.

56

geophysical journal international

(a)

(b)
figure 29: scatter-diagram of top 10 percentile of likelihood values for two different noise levels and receiver combinations
(a) central, (b) all 23 receivers. similar plots with other receiver combinations are shown in the supplementary material.

as discussed before, the top 90 percentile of the likelihood values can also be visualised as scatter
diagrams between the event location parameters as shown in figure 29 for two different receiver
57

geophysical journal international
combinations. it is evident that using just the central receiver (figure 29a), although the ground truth
voxel is revealed in darker shade, there are other spurious voxels producing similar but slightly less
likelihood values. the number of these spurious voxels reduces in the case of 5 and 7 receivers along
the diagonals as shown in the supplementary material. the ground truth voxel becomes more prominent
when more receivers â€“ 15 (upper or lower triangular as in the supplementary material) or 23 (all of them
in figure 29b) are used in the inversion. use of more receivers shows the presence of many lower
likelihood values at a different depth instead of the ground truth which is expected to shrink with a
higher threshold on the likelihood values. in all the scatter diagrams in figure 29, the colour/shade of
the data points are proportional to its log-likelihood values shown in the legend.
the ml estimates of the event location parameters have been reported in the supplementary
material, using voxel by voxel batch likelihood evaluation with all the 4000 lh samples. here, the

yo , zï‚µo ) are obtained as the voxel returning the maximum logestimated location parameters ( xï‚µo , ï‚µ
likelihood value is found to be accurate in all the receiver combinations. a more systematic way could
be to maximize the likelihood function using an optimizer or using bayesian sampling methods with
accelerated likelihood calculation using the trained surrogate or proxy meta-model. the purpose of the
present work is to make the likelihood calculation faster and independent of the data under
consideration, as here the proxy directly predicts the raw observables i.e. template seismic patterns and
is different from the likelihood training approach in the bambi algorithm in (graff et al. 2013; graff
et al. 2012), that needs retraining the surrogate meta-model when the dataset and consequently the
nature of the likelihood changes.
a closer look at the scatter plots of the 2d joint distributions reveal that the high likelihood values
change rapidly with small variation in the event location, particularly with less number of receivers e.g.
only 1 receiver (figure 29a). incorporating more receivers reduces such variations as shown in the
supplementary material using 15 for the upper and lower triangular parts to all 23 receivers (figure
29b). even though in a binned histogram, it may show more number of non-vanishing higher likelihood
areas, the peak of the likelihood may lie in a different location i.e. the ground truth voxel for generating
the data.
58

geophysical journal international
it is also important to note that in all of the above likelihood scatter diagrams, many islanded regions
can be identified rendering such an event detection essentially a multi-modal inference problem. the
adopted lh samples drawn throughout the 3d volume of the velocity model smoothly interpolates the
true noiseless seismic responses in the forward problem but may be insufficient to accurately localise
the events using bulk-scale likelihood calculation at these prospective locations while using the noisy
seismic data in the likelihood calculation. hence the bayesian analysis techniques via mcmc or nested
sampling family of algorithms may be useful here with a suitable choice of likelihood function by
utilising the proposed method for fast data independent proxy meta-model to predict the observables,
in order to get the localised event posterior distributions along with calculation of the marginal
likelihood or evidence for comparing different models or carrying out hypothesis testing.
7.3. maximum likelihood estimate using various receiver geometries in the event detection
in the previous sub-section, the top 90 percentile likelihood values have been shown in the scatter
diagrams where some lower likelihood samples are gathered at a different depth compared to the ground
truth. now in this subsection, we show the maximum likelihood based most likely event location
estimation, using a much higher threshold of top 99 percentile of all the likelihood values amongst the
4000 lh samples. this helps in graphically understanding the accurate localisation of the microseismic
source using increasing number of receivers and different snr levels, corresponding to the location
estimates for the various receiver geometries. this also allows traditional â€˜dots in the boxâ€™ type
visualization of the most-likely microseismic event locations (kendall et al. 2011; eisner et al. 2010),
using the 6 different receiver sub-sets as explored in the earlier subsection.
here, figure 30 shows the event locations using the top 99 percentile of highest log-likelihood
values using a noise standard deviation of Ïƒ = 250, whereas the less noisy case with Ïƒ = 100 is shown
in the supplementary material. both figure 30 and its less noisy version in the supplementary material
(Ïƒ = 100) compares the top 99 percentile of likelihood values between the true likelihood (from the
elastic wave propagator) vs. the gp regression surrogate meta-model or proxy generated likelihoods. a
closer look at these figures will reveal that the introduction of the external noise manifests many
possible event locations with high likelihood values, particularly when the inversion is attempted using
59

geophysical journal international
just the central receiver or multiple receivers across the principal diagonal (7 receivers) and antidiagonals (5 receivers). with increasing number of receivers, the variation in the likelihood values for
other possible locations gradually decreases and the true event location becomes quite prominent, as
manifested in the form of a larger and darker bubble in the last 3 columns of figure 30, using 15 and
23 receivers respectively. it is also evident that with more number of receivers, the true vs. proxy
generated likelihood peaks are located at the same position, compared to that using less number of
receivers. this shows employing 15 or 23 receivers, the maximum likelihood detection is invariant
between the choice of expensive true likelihood vs. the cheap gp proxy-based likelihood.
during the proxy training it might seem that the near surface shallow sources introduce a bias due
to their higher amplitude compared to the deep sources. in order to show that the proxy-based
likelihoods are indeed unbiased, we have compared the true likelihood values vs. the proxy-based
likelihood values and their difference in figure 31. in the joint plane of depth vs. distance, the likelihood
differences are found to be low compared to the original likelihood values and are almost uniform with
variation in depth or distance from the central receiver. this indicates the efficacy of the proxy in
generating fast likelihood values close to the original ones. also, the likelihood surface is not smooth
owing to the fact that the medium is heterogeneous, and the seismograms containing complex structures
of both the p-wave and s-waves. the likelihood values from the full-physics simulation at the sampled
4000 locations vs. the likelihood obtained from the surrogate meta-model are compared in figure 31,
along with their differences. our simulations show that there is very small difference between these two
cases, due to the fact that the surrogate meta-model has learnt the data generation mechanism by the
elastic wave propagator rather than the likelihood surface itself. also, introduction of the surrogate does
not increase the complexity of the likelihood structure and indeed retains its shape intact.

60

geophysical journal international

(a)

(b)
figure 30: most likely event locations using various receiver arrangement with added noise std Ïƒ = 250. log-likelihood values
are shown in the color-bars and the size of the data-points are proportional to the likelihood values: (a) true likelihood, (b)
proxy-based likelihood.

61

geophysical journal international

figure 31: comparison of the true likelihood vs. the proxy-based likelihood and their difference for noise std Ïƒ = 250. the
legends represent the likelihood values or the difference.

8. discussions
it is important to note here that the likelihood calculation has become relatively cheaper using the
trained proxy meta-model, compared to the full elastic wave propagation solution, and therefore bulk
calculation of voxel by voxel likelihood or a subset of lh or uniformly sampled voxel values may also
help identifying the high likelihood regions for possible microseismic events. on larger velocity models
or in higher dimensions, this approach of bulk likelihood calculation may be wasteful. because using a
suitable sampler may easily identify the highly likely event locations within fewer likelihood calls.
using the raw seismic data makes a relatively wilder variation of the likelihood values between
neighbouring voxels for the microseismic source. several derived features like arrival times, polarity of
first arrival etc. can be used to calculate the likelihood instead as shown in (tarantola 2005), which may
produce a smoother likelihood function in the form of almost concentric circles (in 2d) or spheres (in
3d), for a single event. the trained proxy model can easily be used to derive any complex features out
of the raw predicted signals and use them in the likelihood function which may be explored in a future
research.

62

geophysical journal international
moreover, in our likelihood formulation the ground truth signal (y) has been generated from the
elastic wave equation solver and then corrupted with specified noise level (Ïƒ) whereas the template
seismic data ( yï‚µ ) has been generated from the surrogate or proxy meta-model. therefore, the likelihood
contains the misfit due to both the measurement noise on the receivers as well as the inaccuracy due to
the approximate seismic wave modelling with the proxy. we have shown through the above simulation
results that even with both these two components of possible inaccuracy, a single microseismic event
with known ground truth position can be reliably identified as the maximum likelihood point amongst
4000 randomly drawn source positions where the bulk likelihood calculation at possible source
locations can be made extremely fast compared to the full elastic wave solution. it is rational that the
variance of this likelihood analysis may be increased with the use of the surrogate/proxy model for fast
template data generation apart from the specified measurement noise that goes in to the likelihood
function. but the mode or the maximum likelihood point is unaffected by such an approximation due
to the accurate surrogate model which is shown by the zero detection-error for the test cases using all
the receiver combinations. a more elegant and accurate but massively computationally expensive
solution is to calculate both the ground truth data (y) and template data ( yï‚µ ) in each likelihood
evaluation by directly using the elastic pde solution which is explored here in brief and as a proof of
concept, on the sampled 4000 prospective locations which were used for training/testing of the surrogate
meta-model.
in previous literature on microseismic monitoring, there are abundant use of physically simplified
models instead of full-physics simulation with velocity model heterogeneity and elastic wave
propagation. this is one of the viable solutions to reduce the computational cost compared to the proxy
or surrogate meta-model based approach for fast likelihood calculation. however, for microseismic
simulation, the mode conversion between p-wave and s-wave are predominant at the layer edges, even
for explosive sources as described in this paper. therefore, approximate methods like ray-tracing etc.
that depends on separately calculating the p-wave and s-wave responses and then superimposing them
may miss these aspects of the geophysical modelling. rather we here took an alternative approach using
the full elastic solution of the wave equation and then using the surrogate regression meta-models. here,
63

geophysical journal international
the proxy meta-models are trained to produce close approximations of the full elastic solution which is
preferable than solving a reduced physics models for fast likelihood calculation.
this paper develops a methodology for learning wave propagation through heterogeneous medium.
given sufficient samples in the training process and from the convergence characteristics of the learning
curve in figure 14, it is apparent that the surrogate meta-model captures a close enough approximation
of the true seismic wave, obtained through the numerical solution of elastic wave propagation. the
velocity model we consider here is heterogenous compared to the layered ones and represented by
voxelized grids with different values of density and p/s-wave velocity in each voxel. however, the
vertical variations of the rock properties are larger than that in the lateral direction in our model. the
heterogeneity along different directions of similar models can be seen in (das et al. 2017). it is also
worth noting that the surrogate meta-model is trained on a fixed velocity model. for other complex
models, the same machine learning framework can be applied in principle but needs retraining using
thousands of independently simulated seismograms which are dependent on the structure of the velocity
model. in a more heterogenous case, the training process is likely to take more samples for the
convergence of the learning curve, as shown in figure 14. however, this paper aims at first developing
the generic methodology and testing on different velocity models may be addressed in a future research.

9. conclusion
starting from a heterogeneous velocity model, we propose a technique to teach machine learning
based surrogate regression meta-models to approximate elastic wave propagation solutions due to
microseismic events which is computationally expensive even using state of the art gpu computing
facilities. this allows calculation of thousands of batch evaluations of proxy-generated approximate
template seismic responses with reduced physics modelling for rapid calculation of likelihood
functions, for comparing with noisy dataset in a microseismic source inversion algorithm. the paper
first develops a robust time domain compression method to reduce the number of observables in a sparse
pressure wave-field generated by unit amplitude seismic events using a fixed heterogeneous velocity
model. then it compares 9 different families of surrogate regression models along with the details of
64

geophysical journal international
their parameter tuning to obtain sufficient predictive accuracy on the learned seismogram patterns on
multiple receivers. the machine learning algorithms essentially learn the mapping between the
compressed domain sparse and spiky time series of the seismic waves as a function of event location
parameters which can be decompressed next to get the full seismic waves with great saving of the
computational cost compared to solving the full elastic pdes with new event locations.
this paper also determines the achievable accuracy vs. the training time and storage requirement
trade-offs using different flexible regression structures for synthetic template seismogram generation.
the best results are achieved using the gaussian process regression by fine tuning of the kernels and
basis-functions, as it naturally incorporates a bayesian regression framework instead of yielding only
point estimates and hence provide superior performance as a smooth interpolator. seismic data
generation on 23 receivers using this proxy meta-model are found to be â“530 times faster than the gpu
simulations for full elastic wave equation, at the cost of negligible reduction in quality of the signals,
as revealed by the correlation analysis of the ground truth vs. predicted seismograms. however, the gp
proxy meta-models in spite of its high predictive accuracy on smooth regression problems need more
computational effort with growing sample size and number of receivers during the training period. for
fast likelihood calculation, it is not intended to online train the proxy models but to train it only once as
an offline process on a multi-core cpu, assuming the fact that in the real-fields the velocity model do
not change over shorter span of time. however, with uncertain velocity model the seismic patterns,
especially the arrival times, may be different, thus leading to inaccurate likelihood values which needs
further investigation in future. a combined approach of incorporating seismic measurement noise and
velocity model uncertainty together in the proxy models and hence in the likelihood function may also
be investigated in future.
as discussed before, a similar proxy based fast multi-modal bayesian inference technique has been
previously proposed in the bambi algorithm (graff et al. 2013; graff et al. 2012) by directly learning
complex likelihood functions which changes and need retraining for inference on different datasets.
however, the present paper extends this concept by learning the raw observables instead i.e. the multireceiver seismograms which does not need to be retrained if the data and consequently the likelihood
65

geophysical journal international
values had changed. in addition, mapping of the useful information in sparse observables buried under
few millions of data-points in the output i.e. the multi-receiver spiky time-series needed a robust
compression method which this paper develops first, to frame it as a non-sparse regression problem.
here we also show the predictive accuracy vs. training time and storage requirements using 9 different
family of regression models out of which gaussian process families with ard kernels outperform the
rest. future works may include extending the methodology for unknown number of microseismic events
in the presence of background noise of different spatio-temporal characteristics and comparing different
models using a bayesian analysis with evidence calculation for hypothesis testing. it may also be worth
exploring other compression methods e.g. wavelet compression, instead of the adopted time domain
method, considering the full seismic wavefield rather than individual seismograms and test for the best
regression model for this application. also, modelling stress tensor components along with the event
locations for non-explosive microseismic source mechanism is a challenging research topic and even
more in geological models with higher complexity and uncertainty. research in these directions are in
progress and will be reported in our future works.
acknowledgement
this work has been supported by the shell projects and technology. the wilkes high performance
gpu computing service at the university of cambridge has been used in this work.

appendix
additional analysis and high-resolution images for the simulation results are provided in the
supplementary material.

references
aster, r.c., borchers, b. & thurber, c.h., 2011. parameter estimation and inverse problems,
academic press.

66

geophysical journal international
auld, t. et al., 2007. fast cosmological parameter estimation using neural networks. monthly notices
of the royal astronomical society: letters, 376(1), pp.l11â€“l15.
auld, t., bridges, m. & hobson, m., 2008. cosmonet: fast cosmological parameter estimation in
non-flat models using neural networks. monthly notices of the royal astronomical society, 387(4),
pp.1575â€“1582.
babaei, m., alkhatib, a. & pan, i., 2015. robust optimization of subsurface flow using polynomial
chaos and response surface surrogates. computational geosciences, 19(5), pp.979â€“998.
babaei, m. & pan, i., 2016. performance comparison of several response surface surrogate models
and ensemble methods for water injection optimization under uncertainty. computers &
geosciences, 91, pp.19â€“32.
babaei, m., pan, i. & alkhatib, a., 2015. robust optimization of well location to enhance hysteretical
trapping of co2: assessment of various uncertainty quantification methods and utilization of
mixed response surface surrogates. water resources research, 51(12), pp.9402â€“9424.
barutÃ§uouglu, z. & alpaydin, e., 2003. a comparison of model aggregation methods for regression.
in artificial neural networks and neural information processingâ€”icann/iconip 2003.
springer, pp. 76â€“83.
chapman, c., 2004. fundamentals of seismic wave propagation, cambridge university press.
collettini, c. & barchi, m.r., 2002. a low-angle normal fault in the umbria region (central italy): a
mechanical model for the related microseismicity. tectonophysics, 359(1), pp.97â€“115.
das, s., chen, x. & hobson, m.p., 2017. fast gpu-based seismogram simulation from
microseismic events in marine environments using heterogeneous velocity models. ieee
transactions on computational imaging, 3(2), pp.316â€“329.
dieterich, j.h., richards-dinger, k.b. & kroll, k.a., 2015. modeling injection-induced seismicity
with the physics-based earthquake simulator rsqsim. seismological research letters, 86(4),
pp.1102â€“1109.
domingos, p., 2012. a few useful things to know about machine learning. communications of the
acm, 55(10), pp.78â€“87.

67

geophysical journal international
eaton, d.w. et al., 2014. scaling relations and spectral characteristics of tensile microseisms:
evidence for opening/closing cracks during hydraulic fracturing. geophysical journal
international, p.ggt498.
eisner, l. et al., 2010. beyond the dots in the box: microseismicity-constrained fracture models for
reservoir simulation. the leading edge, 29(3), pp.326â€“333.
forrester, a., sobester, a. & keane, a., 2008. engineering design via surrogate modelling: a
practical guide, john wiley & sons.
forrester, a.i. & keane, a.j., 2009. recent advances in surrogate-based optimization. progress in
aerospace sciences, 45(1), pp.50â€“79.
forrester, a.i., sÃ³bester, a. & keane, a.j., 2007. multi-fidelity optimization via surrogate modelling.
proceedings of the royal society of london a: mathematical, physical and engineering sciences,
463(2088), pp.3251â€“3269.
friedman, j., hastie, t. & tibshirani, r., 2010. regularization paths for generalized linear models via
coordinate descent. journal of statistical software, 33(1), p.1.
friedman, j., hastie, t. & tibshirani, r., 2001. the elements of statistical learning, springer, berlin.
goodwin, n., 2015. bridging the gap between deterministic and probabilistic uncertainty
quantification using advanced proxy based methods. in spe reservoir simulation symposium.
goutte, c., 1997. note on free lunches and cross-validation. neural computation, 9(6), pp.1245â€“
1249.
graff, p. et al., 2012. bambi: blind accelerated multimodal bayesian inference. monthly notices of
the royal astronomical society, 421(1), pp.169â€“180.
graff, p. et al., 2013. neural networks for astronomical data analysis and bayesian inference. in 2013
ieee 13th international conference on data mining workshops. pp. 16â€“23.
groos, j. & ritter, j., 2009. time domain classification and quantification of seismic noise in an
urban environment. geophysical journal international, 179(2), pp.1213â€“1231.
guo, p., mcmechan, g.a. & guan, h., 2016. comparison of two viscoacoustic propagators for qcompensated reverse time migration. geophysics, 81(5), pp.s281â€“s297.

68

geophysical journal international
hobson, m. et al., 2014. machine-learning in astronomy. proceedings of the international
astronomical union, 10(s306), pp.279â€“287.
holland, p.w. & welsch, r.e., 1977. robust regression using iteratively reweighted least-squares.
communications in statistics-theory and methods, 6(9), pp.813â€“827.
huang, t.-m., kecman, v. & kopriva, i., 2006. kernel based algorithms for mining huge data sets,
springer.
ieong, 2012. xnfx - high order predictor matrix for regression analysis. available at:
https://uk.mathworks.com/matlabcentral/fileexchange/39144-xnfx-high-order-predictor-matrix-forregression-analysis.
igel, h., 2016. computational seismology: a practical introduction, oxford university press.
igel, h., mora, p. & riollet, b., 1995. anisotropic wave propagation through finite-difference grids.
geophysics, 60(4), pp.1203â€“1216.
james, g. et al., 2013. an introduction to statistical learning, springer.
kalantari-dahaghi, a., mohaghegh, s. & esmaili, s., 2015. coupling numerical simulation and
machine learning to model shale gas production at different time resolutions. journal of natural
gas science and engineering, 25, pp.380â€“392.
kendall, m. et al., 2011. microseismicity: beyond dots in a boxâ€”introduction. geophysics, 76(6),
pp.wc1â€“wc3.
lattimore, t. & hutter, m., 2013. no free lunch versus occamâ€™s razor in supervised learning. in
algorithmic probability and friends. bayesian prediction and artificial intelligence. springer, pp.
223â€“235.
leet, l.d., 1949. microseisms. scientific american, 180, pp.42â€“45.
levy, c., jongmans, d. & baillet, l., 2011. analysis of seismic signals recorded on a prone-to-fall
rock column (vercors massif, french alps). geophysical journal international, 186(1), pp.296â€“
310.
mackay, d.j., 1997. gaussian processes-a replacement for supervised neural networks? lecture notes
for a tutorial at nips 1997.

69

geophysical journal international
modesto, d. & de la puente, j., 2016. exploring a priori reduced order models for fast seismic
simulations. in 78th eage conference and exhibition 2016.
mohaghegh, s.d., 2006. quantifying uncertainties associated with reservoir simulation studies using
a surrogate reservoir model. in spe annual technical conference and exhibition.
mosegaard, k. & tarantola, a., 2002. 16 probabilistic approach to inverse problems. international
geophysics, 81, pp.237â€“265.
mosegaard, k. & tarantola, a., 1995. monte carlo sampling of solutions to inverse problems.
journal of geophysical research: solid earth, 100(b7), pp.12431â€“12447.
mu, d., chen, p. & wang, l., 2013a. accelerating the discontinuous galerkin method for seismic
wave propagation simulations using multiple gpus with cuda and mpi. earthquake science,
26(6), pp.377â€“393.
mu, d., chen, p. & wang, l., 2013b. accelerating the discontinuous galerkin method for seismic
wave propagation simulations using the graphic processing unit (gpu)â€”single-gpu
implementation. computers & geosciences, 51, pp.282â€“292.
neal, r.m., 1996. bayesian learning for neural networks, springer science & business media.
pan, i. et al., 2014a. a multi-period injection strategy based optimisation approach using kriging
meta-models for co 2 storage technologies. energy procedia, 63, pp.3492â€“3499.
pan, i. et al., 2014b. artificial neural network based surrogate modelling for multi-objective
optimisation of geological co 2 storage operations. energy procedia, 63, pp.3483â€“3491.
pan, i. & das, s., 2015. kriging based surrogate modeling for fractional order control of microgrids.
ieee transactions on smart grid, 6(1), pp.36â€“44.
pandey, d.s. et al., 2016. artificial neural network based modelling approach for municipal solid
waste gasification in a fluidized bed reactor. waste management, 58, pp.202â€“213.
phadke, s., bhardwaj, d. & dey, s., 2000. an explicit predictor-corrector solver with application to
seismic wave modelling. computers & geosciences, 26(9), pp.1053â€“1058.
plumb, a.p. et al., 2005. optimisation of the predictive ability of artificial neural network (ann)
models: a comparison of three ann programs and four classes of training algorithm. european
journal of pharmaceutical sciences, 25(4), pp.395â€“405.
70

geophysical journal international
rasmussen, c.e. & williams, c.k., 2006. gaussian processes for machine learning, mit press.
rodriguez, a. et al., 2006. a multiscale and metamodel simulation-based method for history
matching. in ecmor x-10th european conference on the mathematics of oil recovery.
rogers, s. & girolami, m., 2015. a first course in machine learning, crc press.
rutledge, j.t., phillips, w.s. & schuessler, b.k., 1998. reservoir characterization using oilproduction-induced microseismicity, clinton county, kentucky. tectonophysics, 289(1), pp.129â€“
152.
samui, p. & sitharam, t., 2010. site characterization model using artificial neural network and
kriging. international journal of geomechanics, 10(5), pp.171â€“180.
sitharam, t., samui, p. & anbazhagan, p., 2008. spatial variability of rock depth in bangalore using
geostatistical, neural network and support vector machine models. geotechnical and geological
engineering, 26(5), pp.503â€“517.
slotte, p.a. & smorgrav, e., 2008. response surface methodology approach for history matching and
uncertainty assessment of reservoir simulation models. in europec/eage conference and
exhibition.
street, j.o., carroll, r.j. & ruppert, d., 1988. a note on computing robust regression estimates via
iteratively reweighted least squares. the american statistician, 42(2), pp.152â€“154.
tarantola, a., 2005. inverse problem theory and methods for model parameter estimation, siam.
tarantola, a. & valette, b., 1982. inverse problems= quest for information. journal of geophysics,
50(3), pp.150â€“170.
treeby, b., cox, b. & jaros, j., 2012. k-wave a matlab toolbox for the time domain simulation of
acoustic wave fields user manual.
treeby, b.e. et al., 2014. modelling elastic wave propagation using the k-wave matlab toolbox. in
ultrasonics symposium (ius), 2014 ieee international. pp. 146â€“149.
treeby, b.e. & cox, b.t., 2010. k-wave: matlab toolbox for the simulation and reconstruction of
photoacoustic wave fields. journal of biomedical optics, 15(2), pp.021314â€“021314.
weglein, a.b. et al., 2009. clarifying the underlying and fundamental meaning of the approximate
linear inversion of seismic data. geophysics, 74(6), pp.wcd1â€“wcd13.
71

geophysical journal international
wilson, k., durlofsky, l.j. & others, 2012. computational optimization of shale resource
development using reduced-physics surrogate models. in spe western regional meeting.
wilson, k.c. & durlofsky, l.j., 2013. optimization of shale gas field development using direct
search techniques and reduced-physics models. journal of petroleum science and engineering,
108, pp.304â€“315.
wolpert, d.h., 2002. the supervised learning no-free-lunch theorems. in soft computing and
industry. springer, pp. 25â€“42.
wood, l.c., 1974. seismic data compression methods. geophysics, 39(4), pp.499â€“525.
zou, h. & hastie, t., 2005. regularization and variable selection via the elastic net. journal of the
royal statistical society: series b (statistical methodology), 67(2), pp.301â€“320.
zubarev, d.i., 2009. pros and cons of applying proxy-models as a substitute for full reservoir
simulations. in spe annual technical conference and exhibition.

72



seismic wave propagation and inversion with neural
operators
yan yang*â€ 1 , angela f. gaoâ€ 2 , jorge c. castellanosâ€ 1 , zachary e. ross1 , kamyar azizzadenesheli3 , and
robert w. clayton1

abstract
seismic wave propagation forms the basis for most aspects of seismological research, yet
solving the wave equation is a major computational burden that inhibits the progress of
research. this is exacerbated by the fact that new simulations must be performed whenever the velocity structure or source location is perturbed. here, we explore a prototype
framework for learning general solutions using a recently developed machine learning
paradigm called neural operator. a trained neural operator can compute a solution in
negligible time for any velocity structure or source location. we develop a scheme to train
neural operators on an ensemble of simulations performed with random velocity models
and source locations. as neural operators are grid free, it is possible to evaluate solutions
on higher resolution velocity models than trained on, providing additional computational
efficiency. we illustrate the method with the 2d acoustic wave equation and demonstrate
the methodâ€™s applicability to seismic tomography, using reverse-mode automatic differentiation to compute gradients of the wavefield with respect to the velocity structure. the
developed procedure is nearly an order of magnitude faster than using conventional
numerical methods for full waveform inversion.

cite this article as yang, y., gao, a. f.,
castellanos, j. c., ross, z. e.,
azizzadenesheli, k., and clayton, r. w.
(2021). seismic wave propagation and
inversion with neural operators, the seismic
record. 1(3), 126â€“134, doi: 10.1785/
0320210026.

supplemental material

introduction
the simulation of seismic wave propagation through earthâ€™s
interior underlies most aspects of seismological research, from
the simulation of strong ground shaking due to large earthquakes (graves and pitarka, 2016; rodgers et al., 2019), to
imaging the subsurface velocity structure (fichtner et al., 2009;
tape et al., 2009; virieux and operto, 2009; lee et al., 2014;
gebraad et al., 2020), to derivation of earthquake source properties (duputel et al., 2015; ye et al., 2016; wang and zhan,
2020). the compute costs associated with these wavefield simulations are substantial; and, for reasons of computational efficiency, 1d models are often used, even when better 3d velocity
models are available. as a result, seismic wave simulations are
often the limiting factor in the pace of geophysical research.
recently, deep learning approaches have been explored with
the goal of solving various geophysical partial differential
equations (moseley, markham, and nissen-meyer, 2020;
moseley, nissen-meyer, and markham, 2020; smith et al.,

2020; moseley et al., 2021). beyond the goal of accelerating
compute capabilities, such physics-informed neural networks
may offer other advantages such as grid independence, lowmemory overhead, differentiability, and on-demand solutions.
these properties facilitate deep learning being used to solve
geophysical inverse problems (zhu et al., 2020; smith et al.,
2021; xiao et al., 2021; zhang and gao, 2021), as a wider

1. seismological laboratory, california institute of technology, pasadena, california, u.s.a.,
https://orcid.org/0000-0002-6105-2918 (yy);
https://orcid.org/0000-0002-0103-6430
(jcc);
https://orcid.org/0000-0002-6343-8400 (zer);
https://orcid.org/0000-0003-33233508 (rwc); 2. department of computing and mathematical sciences, california institute of
technology, pasadena, california, u.s.a.,
https://orcid.org/0000-0001-8574-8728 (afg);
3. department of computer science, purdue university, west lafayette, indiana, u.s.a., https://
orcid.org/0000-0001-8507-1868 (ka)
*corresponding author: yanyang@caltech.edu
â€ these authors contributed equally to this work.
Â© 2021. the authors. this is an open access article distributed under the terms of the cc-by
license, which permits unrestricted use, distribution, and reproduction in any medium, provided the
original work is properly cited.

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

126

selection of algorithms and frameworks then are available for
use, such as approximate bayesian inference techniques like
variational inference.
one of the major challenges associated with wave propagation is that a new simulation must be performed whenever the
properties of the source or velocity structure are perturbed in
some way. this alone substantially increases the necessary compute costs, making some problems prohibitively expensive even
if they are mathematically or physically tractable. for the most
part, these limitations have been accepted as an inevitable part of
seismology, but now physics-informed machine learning
approaches have started to offer some pathways for moving
beyond this issue. for example, smith et al. (2020) use a deep
neural network to solve the eikonal equation for any sourceâ€“
receiver pair by taking these locations as input. then, this
can be exploited for hypocenter inversion by allowing for gradients of the travel time field to be computed with respect to the
source location (smith et al., 2021). however, these models are
relatively inefficient to train and even then are only able to learn
approximate solution operators to the differential equations.
the aforementioned limitations may seem surprising but
result from a basic attribute of neural networks that in fact makes
them ill-suited for solving differential equations. specifically, neural networks are designed for learning maps between two finitedimensional spaces, whereas learning a general solution operator
for a differential equation requires the ability to map between two
infinite dimensional spaces (i.e., function spaces). a paradigm for
learning maps between function spaces was recently developed
(li et al., 2020a,b, 2021) and has been termed as neural operator.
the general idea behind these models is that they have shared
parameters over all possible functions describing the initial conditions, which allows them to operate on functions, even when
the inputs are a numerically discretized representation of them.
here, we explore the potential of neural operators in
improving seismic wave propagation and inversion. we
develop a prototype framework for training neural operators
on the 2d acoustic wave equation and show that this approach
provides a suite of tantalizing new advantages over conventional numerical methods for seismic wave propagation.
this study provides a proof of concept of this technology
and its application to seismology.

preliminaries
for a given function a and a greenâ€™s function g, let u denotes
the solution to a linear partial differential equation (pde), that
is, the solution operator,

z
eq-target;temp:intralink-;df1;314;708

ux  lax 

in which l is the corresponding linear operator. for example,
suppose that the pde to be solved is the acoustic wave equation; and then a could describe the velocity structure as well as
the initial conditions. neural operator generalizes this formulation to the nonlinear setting in which a set of linear operators
are sequentially applied to construct a general nonlinear solution operator. in its basic form, an â„“-layered neural operator
is constructed as follows:
ux  lâ„“ Ïƒlâ„“âˆ’1 â€¦Ïƒl1 vâ€¦x;

2

in which li is such that for any function v, we have,
z
li vx  w i x  k i x; yvydy:

3

eq-target;temp:intralink-;df2;314;569

eq-target;temp:intralink-;df3;314;513

under this framework, w i x and k i x; y constitute the learnable components of the neural operator and allow for a solution to be produced for any prescribed function a. in a limited
sense, neural operators can be viewed as generalized greenâ€™s
functions.

methods
we designed a framework that applies neural operators to the
2d acoustic wave equation. the basic idea for this procedure is
outlined schematically in figure 1. a specific type of neural
operator called a fourier neural operator (fno; li et al.,
2021) receives a velocity model specified on an arbitrary, possibly irregular mesh, along with the coordinates of a point
source. one of the main features of fno is that the major calculations are performed in the frequency domain that allows
the convolutions in equation (3) to be rapidly computed. the
output of the fno is the complete wavefield solution, which
can be queried anywhere within the medium, regardless of
whether the points lie on the input mesh.
the most basic component of the fno is a fourier block
(fig. 1), which first transforms an input function (v) to the
fourier domain. in the first layer of the network, v is equal
to the initial conditions a. then, a kernel (k i ) is computed
specifically for this function and is truncated at low order,
before performing the integration via multiplication. finally,
the result is transformed back and a nonlinear activation function is applied, which concludes the fourier block. for this
study, the architecture of the fno contains four sequential
fourier blocks and applies a rectified linear unit (relu)

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

1

gx; yaydy;

the seismic record

127

(

)

activation to the output of each (fig. 1). we note that the truncation of the fourier modes is performed on the function values after lifting them to a higher dimensional space, rather than
the raw input function, so that this does not lead to compression. we refer the interested readers to li et al. (2021) for more
mathematical details about the fno.
we constructed a training dataset of simulations to learn
from by first generating random velocity models. we set up
a 64 Ã— 64 grid with 0.16 km spacing to define the velocity
model. then, we created 5000 random velocity models by sampling random fields having a von karman covariance function
with the following parameters: hurst exponent Îº  0:5, correlation length a  ax ; ay   1:6 km; 1:6 km, and Îµ  0:1,
Î¼  3 km=s, and Ïƒ  0:15 km=s. then, for each of these
velocity models, we apply a ricker wavelet source at a random
point and solve the acoustic wave equation using a spectral

figure 1. our approach applying fourier neural operator (fno) to the 2d
acoustic wave equation. the inputs to the fno model are the velocity
model with dimensions d Ã— d Ã— 1 and the source location, indicated by the
white star. the input velocity model is lifted to a higher dimensional space
with size d Ã— d Ã— w using a neural network. then, we apply four fourier
operator layers and finally project back to the target wavefield dimensions of d Ã— d Ã— n using a neural network. the bottom panel shows
details of the fourier layer architecture; we define v to be the input. on
top: we apply a fourier transform f to v and then apply a linear transformation r to the lower fourier modes, filtering out higher modes. then,
we apply an inverse fourier transform f âˆ’1 . on the bottom, we apply a
local linear transform w to v.

element method (sem; afanasiev et al., 2019). it should be
noted that there is a source grid used, because this is a requirement of the sem. because gaussian random fields (grfs) can
represent all continuous functions, the purpose of these steps is

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

128

to create a suite of simulations that span a range of possible
conditions. we show later that they can even well approximate
strongly discontinuous velocity models. an example velocity
model and simulation is shown in figure 1. applying the aforementioned procedure results in a training dataset of 5000 data
samples, each of which is a different simulation.
given the simulation dataset, we can proceed to train an
fno model in a supervised capacity, in which the goal is to
learn a model that can reliably output a solution to the wave
equation for arbitrary input conditions. the training is performed using batch gradient descent, for which the parameters
of the fno are updated to minimize the error against the
â€œtrueâ€ spectral element solutions. a mean-square error loss
function is used. we use a batch size of 30 simulations and
train the model for a maximum of 300 epochs. we use all
but 200 of the simulations for training and set aside the
remainder for cross validation of the model. the time required
to train the model from scratch is approximately 18 hr using
six nvidia tesla v100 graphical processing units (gpus).

experiments
initial wavefield demonstration
figure 2 shows two example wavefields corresponding to two
different velocity models, each with a different source. the spectral element solution is shown alongside the wavefield predicted
by the fno for the eight different receivers (blue triangles). for
these examples, the input velocity model is 64 Ã— 64. the relative
â„“2 loss of the fno wavefields are 0.180 and 0.363. these examples are representative of the entire validation dataset, which has
a loss of 0.273 relative to the spectral element solutions.
the number of simulations needed for training
once fully trained, the fno can evaluate a new solution in a
fraction of a second, and thus the time to train the fno will be
the vast majority of the needed compute time. a primary concern about the computational demands of the fno approach
is, therefore, the number of simulations needed for training.
here, we examine how the number of training simulations
influences the accuracy of the solution. we create a series
of tests in which the number of training simulations is varied
from 1200 at the fewest to 4800 at the most. the results are
shown in figure 3, in which we show the fno wavefield predictions for each dataset. even with 1200 training samples,
there is no indication that there is overfitting, because the
training waveform error is similar across different models
(fig. 3a,b). training using just 1200 simulations is able to

predict the major arrival. increasing number of training samples provides a better fit of the reflections (e.g., 3.2 s in fig. 3c).

generalization to arbitrary velocity models
the fno was trained only on velocity models drawn from
gaussian random fields; and, although this family of functions
is broad, it does not include some types of functions that exist
in the earth, such as discontinuous functions. this raises the
question of whether the fno can still generalize well to these
cases. figure 4aâ€“c shows an example of a predicted wavefield
for a velocity model containing a constant velocity square
embedded within a homogeneous medium. although the
velocity model itself is rather simple, it is actually very far
removed from the characteristics of the random fields that
the fno was trained on and represents a challenging example.
we can see that the predicted wavefield does a very good job of
approximating the wavefield compared to the ground truth.
we believe that the small residual errors can be reduced with
better hyperparameter selection.
generalization to higher resolution grids
fno can be viewed in some sense as a method for learning generalized greenâ€™s functions valid for arbitrary boundary conditions. because it is intrinsically learning a mapping between
function spaces, the fno is theoretically independent of the resolution at which the functions are discretized (this is only a
requirement for evaluation on a computer). one important
advantage of this is that the fno can be trained on velocity
models with a certain grid spacing and then be evaluated on
velocity models with a different grid spacing at inference time.
here, we are not simply talking about interpolating the wavefield
after solving the pde; but, rather, the solutions to the pde can
actually be evaluated on a higher resolution velocity model with
negligible extra compute cost. to demonstrate this, figure 4dâ€“f
shows the fno prediction for a random velocity field with 2Ã—
higher resolution (128 Ã— 128) than the models used during training, alongside the spectral element solution. the fno solution
closely approximates the spectral element solution. the velocity
models with different meshes have the same roughness as the
training data set. resolving more rough structure with denser
spacing can be achieved by training with many more grfs with
varying correlation length scales and variance.
full waveform inversion with neural operators
one of the most useful applications of wavefield simulations is
in inversion, to image the earthâ€™s interior. the adjoint-state

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

129

(a)

(b)

(d)

(e)

(c)

(f)

method is an approach to efficiently compute the gradients of
an objective function with respect to parameters of interest and
can be used for seismic tomography (e.g., tape et al., 2009;
gebraad et al., 2020). neural operators are differentiable by
design, which enables gradient computation with reversemode automatic differentiation. automatic differentiation
has been shown to be mathematically equivalent to the
adjoint-state method (zhu et al., 2021). this allows for the gradients of the wavefield to be determined with respect to the
inputs (velocity model and source location).
figure 5 demonstrates our full waveform inversion (fwi)
performance. for each case, we compute synthetic observations

figure 2. examples of two validation wavefield simulations from the
trained fno model. (a) the sourceâ€“receiver locations with receivers in
blue and source in red, (b) the velocity structure, (c) waveforms simulated
with spectral element method (sem; black) and fno (red), and (dâ€“f) same
as (aâ€“c), but for a different velocity model. the relative â„“2 loss of the two
examples are 0.180 and 0.363, respectively, which are representative of
the entire validation data set with an average â„“2 loss of 0.273. we
demonstrate that the fno simulation results are able to capture both the
major arrivals as well as some reflections.

using the source distribution as shown (red circles), taking every
point in the 64 Ã— 64 grid as a receiver. the observations are
noise-free for this experiment. then, we perform tomography

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

130

(a)

(b)

by starting with a homogeneous initial velocity model and forward propagating a wavefield with the fno for each source. we
p p
calculate the loss l  i j uobs xi ; xj  âˆ’ upred xi ; xj 2 and
compute âˆ‡l with automatic differentiation. the velocity model
is then iteratively updated with gradient descent for 1000 iterations using the adam optimizer (kingma and ba, 2014) and a
learning rate of 0.01. for comparison, figure 5a,b shows the
imaging result using sem and adjoint-state method, with a
relative â„“2 misfit between the inverted and true velocity model
of 0.0289. figure 5c,d shows the result for the same velocity
structure using fno and automatic differentiation, with a misfit
of 0.0319. figure 5e,f is designed to demonstrate sharp discontinuous changes with a short wavelength. the results demonstrate the remarkable capabilities of fno to learn a general
solution operator.
we note that our fwi approach neither requires an adjoint
wavefield to be computed nor a cross correlation; the gradients
can be rapidly computed with gpus using automatic differentiation. the rapid simulation makes it substantially more efficient than adjoint methods. for these experiment, 20 sources
take âˆ¼1 s for one tomographic iteration, including the costs of
computing the forward model, whereas the spectral element
method with adjoint methods takes âˆ¼100 s for one tomographic iteration. these time measurements are from using
only a single nvidia tesla v100 gpu.

discussion
this study presents a prototype framework for applying neural
operators to the 2d acoustic wave equation. we anticipate that
the general framework would also be suitable for the 3d

(c)

figure 3. model performance as a function of the number of training
samples. (a) training and validation loss curves as a function of different
numbers of training samples. (b) example waveform fitting of a single
training example from models trained with varying number of training
examples. (c) example waveform fitting of a single validation example
from models trained with varying number of training examples. the
numbers to the right of each waveform shows the relative â„“2 misfit. this
shows that the model trained on 4800 samples is able to capture the
reflections, whereas the model trained on smaller number of samples
does not generalize to reflections in the validation example.

elastic-wave equation with relatively little modification.
indeed, the fno method was applied successfully to the
navierâ€“stokes equations (li et al., 2021), which can be more
challenging to solve than the elastic-wave equation. in our
tests, we found that only a few thousand simulations were
needed to train an fno model and, from there, required negligible time to compute a new solution. because fno can be
trained on lower resolution simulations and then generalize to
higher resolution solutions once trained, this results in substantially faster computations than using traditional numerical
methods at the full resolution.
one of the limitations of the approach is that the solutions
are approximate, as seen in several of the figures. however,
because this is a learning-based approach, the performance
can be improved in the future using a better model architecture, thorough tuning of hyperparameters, improving the size
of the training dataset, using a more appropriate objective
function, and various other factors. in addition, as new developments within machine learning emerge in this area, they
would be able to be incorporated. thus, these performance

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

131

(a)

(b)

(d)

(e)

(c)

(f)

metrics should only be viewed as a starting point. for some
applications, the error may be enough of an issue, and traditional numerical methods may be preferable; however, for
many other situations in geophysics, a reasonably accurate solution may be acceptable.
among the most exciting benefits of our approach is that by
training the fno on random velocity models, the fno is able to
produce solutions for arbitrary velocity models. this is because
fno learns a general solution operator to the pde and not specifically the velocity model. this means that the model does not
need to be retrained for each region. thus, the approach offers
the potential for a single fno model to be used by the entire

figure 4. model generalization experiments. (a) the sourceâ€“receiver
locations with receivers in blue and source in red, (b) a velocity model with
a homogeneous background of 3 km/s and a 5% square anomaly,
(c) waveform simulated with sem (black) and fno (red), and (dâ€“f) same as
(aâ€“c), but for an input velocity model with 2Ã— finer resolution than trained
on. these experiments show that the model is not just memorizing the
solutions, but is able to generalize to entirely new conditions.

seismology community for any region of a similar size. although
the initial cost of training an fno and performing the training
simulations may be expensive, it only needs to be done a single
time for the community as a whole.

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

132

(a)

(b)

(c)

(d)

(e)

(f)

figure 5. example of a full waveform inversion using fno. (a,c,e) true
velocity models with source locations indicated by red circles and receivers
placed at every node of the 64 Ã— 64 grid (10 km Ã— 10 km region).
(b) reconstruction using sem and adjoint tomography. (d,f) reconstruction
using fno as the forward model and automatic differentiation to compute
gradients. no regularization is used for these experiments.

data and resources
all the data presented in this study are synthetic and available
upon request. the supplemental material for this article demonstrates that the misfit between the simulations using spectral
element method (sem) and the fourier neural operator (fno)
is minimal.

declaration of competing interests
the authors declare that there are no competing interests.

acknowledgments
the authors thank jack muir for helpful comments on an
early version of the article.

references
afanasiev, m., c. boehm, m. van driel, l. krischer, m. rietmann, d.
a. may, m. g. knepley, and a. fichtner (2019). modular and

flexible spectral-element waveform modelling in two and three
dimensions, geophys. j. int. 216, no. 3, 1675â€“1692, issn 0956540x, doi: 10.1093/gji/ggy469.
duputel, z., j. jiang, r. jolivet, m. simons, l. rivera, j.-p. ampuero, b.
riel, s. e. owen, a. w. moore, s. v. samsonov, et al. (2015). the
iquique earthquake sequence of april 2014: bayesian modeling
accounting for prediction uncertainty, geophys. res. lett. 42,
no. 19, 7949â€“7957, issn 1944-8007, doi: 10.1002/2015gl065402.
fichtner, a., b. l. n. kennett, h. igel, and h.-p. bunge (2009). full seismic
waveform tomography for upper-mantle structure in the australasian
region using adjoint methods, geophys. j. int. 179, no. 3, 1703â€“1725,
issn 0956-540x, doi: 10.1111/j.1365-246x.2009.04368.x.
gebraad, l., c. boehm, and a. fichtner (2020). bayesian elastic fullwaveform inversion using hamiltonian monte carlo, j. geophys.
res. 125, no. 3, e2019jb018428, issn 2169-9356, doi: 10.1029/
2019jb018428.
graves, r., and a. pitarka (2016). kinematic ground-motion simulations on rough faults including effects of 3d stochastic velocity
perturbations, bull. seismol. soc. am. 106, no. 5, 2136â€“2153,
geoscienceworld, issn 0037-1106, doi: 10.1785/0120160088.
kingma, d. p., and j. ba (2014). adam: a method for stochastic otimization, available at http://arxiv.org/abs/1412.6980 (last accessed
august 2021).
lee, e.-j., p. chen, t. h. jordan, p. b. maechling, m. a. m. denolle,
and g. c. beroza (2014). full-3-d tomography for crustal structure in southern california based on the scattering-integral and
the adjoint-wavefield methods, j. geophys. res. 119, no. 8,
6421â€“6451, issn 2169-9356, doi: 10.1002/2014jb011346.
li, z., n. kovachki, k. azizzadenesheli, b. liu, k. bhattacharya, a.
stuart, and a. anandkumar (2020a). multipole graph neural operator for parametric partial differential equations, available at
http://arxiv.org/abs/2006.09535 (last accessed august 2021).
li, z., n. kovachki, k. azizzadenesheli, b. liu, k. bhattacharya, a.
stuart, and a. anandkumar (2020b). neural operator: graph kernel network for partial differential equations, available at http://
arxiv.org/abs/2003.03485 (last accessed august 2021).
li, z., n. kovachki, k. azizzadenesheli, b. liu, k. bhattacharya, a.
stuart, and a. anandkumar (2021). fourier neural operator for
parametric partial differential equations, available at http://arxiv
.org/abs/2010.08895 (last accessed august 2021).
moseley, b., a. markham, and t. nissen-meyer (2020). solving the
wave equation with physics-informed deep learning, available at
http://arxiv.org/abs/2006.11894 (last accessed august 2021).
moseley, b., a. markham, and t. nissen-meyer (2021). finite basis
physics-informed neural networks (fbpinns): a scalable domain
decomposition approach for solving differential equations, available
at http://arxiv.org/abs/2107.07871 (last accessed august 2021).
moseley, b., t. nissen-meyer, and a. markham (2020). deep learning
for fast simulation of seismic waves in complex media, solid earth
11, no. 4, 1527â€“1549, issn 1869-9510, copernicus gmbh, doi:
10.5194/se-11-1527-2020.
rodgers, a. j., n. anders petersson, a. pitarka, d. b. mccallen, b.
sjogreen, and n. abrahamson (2019). broadband (0â€“5 hz)

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

133

fully deterministic 3d ground-motion simulations of a magnitude
7.0 hayward fault earthquake: comparison with empirical
ground-motion models and 3d path and site effects from source
normalized intensities, seismol. res. lett. 90, no. 3, 1268â€“1284,
geoscienceworld, issn 0895-0695, doi: 10.1785/0220180261.
smith, j. d., k. azizzadenesheli, and z. e. ross (2020). eikonet:
solving the eikonal equation with deep neural networks, ieee
trans. geosci. remote sens. 1â€“12, issn 1558-0644, doi:
10.1109/tgrs.2020.3039165.
smith, j. d., z. e. ross, k. azizzadenesheli, and j. b. muir (2021).
hyposvi: hypocenter inversion with stein variational inference
and physics informed neural networks, available at http://arxiv
.org/abs/2101.03271 (last accessed august 2021).
tape, c., q. liu, a. maggi, and j. tromp (2009). adjoint tomography
of the southern california crust, science 325, no. 5943, 988â€“992,
issn 0036-8075, 1095-9203, doi: 10.1126/science.1175298.
virieux, j., and s. operto (2009). an overview of full-waveform inversion in exploration geophysics, geophysics 74, no. 6, wcc1â€“
wcc26, society of exploration geophysicists, issn 0016-8033,
doi: 10.1190/1.3238367.
wang, x., and z. zhan (2020). moving from 1-d to 3-d velocity
model: automated waveform-based earthquake moment tensor
inversion in the los angeles region, geophys. j. int. 220, no. 1,
218â€“234, issn 0956-540x, doi: 10.1093/gji/ggz435.

xiao, c., y. deng, and g. wang (2021). deep-learning-based adjoint
state method: methodology and preliminary application to inverse
modeling, water resour. res. 57, no. 2, e2020wr027400, issn
1944-7973, doi: 10.1029/2020wr027400.
ye, l., t. lay, h. kanamori, and l. rivera (2016). rupture
characteristics of major and great mw â‰¥ 7.0 megathrust earthquakes from 1990 to 2015: 2. depth dependence, j. geophys.
res. 121, no. 2, 2015jb012427, issn 2169-9356, doi: 10.1002/
2015jb012427.
zhang, w., and j. gao (2021). deep-learning full-waveform inversion
using seismic migration images, ieee trans. geosci. remote sens.
1â€“18, issn 1558-0644, doi: 10.1109/tgrs.2021.3062688.
zhu, w., k. xu, e. darve, and g. c. beroza (2021). a general
approach to seismic inversion with automatic differentiation,
comput. geosci. 151, 104,751.
zhu, w, k. xu, e. darve, b. biondi, and g. c. beroza (2020).
integrating deep neural networks with full-waveform inversion:
reparametrization, regularization, and uncertainty quantification,
available at http://arxiv.org/abs/2012.11149 (last accessed august
2021).

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

manuscript received 17 august 2021
published online 2 november 2021

the seismic record

134



downloaded 08/27/24 to 200.12.181.232. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/image2022-3745050.1

elastic-adjointnet : a physics guided deep autoencoder to overcome cross talk effects in
multiparameter full waveform inversion
arnab dhara*, mrinal sen, university of texas at austin

summary
full waveform inversion (fwi) is the most popular
technique to obtain high resolution estimates of earth model
parameters using all information present in seismic. elastic
fwi inverts multicomponent data for p and s-wave
velocities and densities. we propose an alternative approach
for fwi using a combination of machine learning and the
physics of wave propagation. unlike a conventional
supervised machine learning, we do not require known
answers to train our network. the multicomponent shot
gathers are input to a convolutional neural network (cnns)
based auto encoder whose outputs are used as p-wave, swave and density models that are used to compute synthetic
seismograms using the stress-velocity formulation of the
elastic wave equation. the synthetic data are compared
against observed input data and the misfit is estimated. the
gradient of the misfit with respect to the velocity model
parameters is calculated using the adjoint state method. the
adjoint state gradient is then used to update the network
weights using the automatic differentiation technique. once
the misfit term converges, the neural network can generate
subsurface models consistent with the observed data. we
observe that the neural network can capture spatial
correlations at different scales and thus can introduce
regularization in our inverse problem. the regularization is
enough to mitigate the cross-talk problem in elastic fwi and
also produce good results in areas with low illumination.

convergence. they are created because different elastic
parameters may have same radiation pattern (pan et al.,
2019; keating and innanen, 2020; kamath and tsvankin,
2016).
recent works have used deep learning-based models to learn
the mapping from seismic gathers to velocity models (kazei
et al., 2021; yang and ma, 2019). a deep learning approach
is a purely supervised technique, the goal of which is to learn
from a large amount of paired seismic data and
corresponding true velocity maps. this approach is agnostic
to the physics of wave propagation. because of the limited
availability of labelled datasets, this approach has been
limited to simpler velocity models. moreover, the large
memory requirements of deep learning libraries have limited
the application of such approaches to only acoustic imaging.
in this work we introduce a physics constrained machine
learning framework elastic-adjointnet, which allows users
to embed their elastic wave physics codes in deep learning
libraries and develop neural network workflows. previous
approaches to embed multiparameter full-waveform
inversion (fwis) in deep learning framework required
reimplementation of pdes as recurrent neural networks
(rnns) (wang et al., 2021; zhang et al., 2021). this reimplementation of pdes may not be an attractive option for
geoscientists that are working with wave propagation codes
that have been developed for decades with sophisticated
discretization techniques.

introduction
since early development of the theory (tarantola, 1984),
successful application of fwi has been reported in
exploration seismology, earthquake seismology, deep
crustal imaging, and medical imaging. using a partial
equation differential equation (pde) solver to generate
synthetic seismograms and a local optimization method,
fwi iteratively updates the subsurface model parameters by
reducing the misfit between recorded data/observed data and
the estimated data. however, utilizing fwi only to estimate
p-wave velocity (acoustic imaging) limits its benefits when
applying to problems like reservoir characterization and near
surface geophysics.
current elastic fwi algorithms suffers from a critical issue.
elastic fwi suffers from the issue of interparameter tradeoff or crosstalk (operto et al., 2013). cross-talk occurs when
data residuals caused by an error in the estimate of one
physical property are attributed to another, impeding

in this work, we propose a new network the input to which
is the multicomponent shot gathers. the output of the network, which is a velocity model is fed to a physics based
pde solver. the physics based pde solver generates
synthetic seismic data. the synthetic seismic data is
compared with the observed data and the gradient of the
misfit between observed and synthetic data is used to update
the network weights. the schematic of the proposed
approach is shown in figure 1.
the above idea has its origins from the works of calderÃ³nmacÃ­as et al. (2000) and biswas et al. (2019). in this work,
we use the adjoint state method to calculate the gradient of
the misfit and then backpropagate it throughout the network
to update its weights. ulyanov et al. (2018) applied untrained
convnets to solution of problems like image restoration,
interpolation and demonstrated capability of neural
networks to capture great deal of priors from images.
nnfwi (zhu et al., 2021) which was based on a similar

second international meeting for applied geoscience & energy
10.1190/image2022-3745050.1
page 882
Â© 2022 society of exploration geophysicists and the american association of petroleum geologists

downloaded 08/27/24 to 200.12.181.232. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/image2022-3745050.1

fwi using physics guided autoencoder

concept showed improved acoustic fwi result in presence
of gaussian data noise. we demonstrate that the reparameterization of the model parameters using neural
networks and optimization using adam (kingma
and ba, 2014) algorithm introduces regularization , thus
overcoming local minima and crosstalk issues in elastic
fwi. moreover, the regularization helps to produce robust
results in area of low data coverage.
method
we use the time domain formulation of elastic wave
equation to invert for p-wave velocity (vp), s-wave velocity
(vs) and density (Ï). the objective function is formulated as
1
ğ¸ = 2 âˆ‘ğ‘  âˆ‘ğ‘Ÿ âˆ«[ğ‘‘ğ‘œğ‘ğ‘  âˆ’ ğ‘‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ ]2 ğ‘‘ğ‘¡,
where ğ‘‘ğ‘œğ‘ğ‘  is observed wavefield and ğ‘‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ is the
calculated wavefield at sources s and receivers r. we use the
eighth-order staggered grid finite difference scheme in space
and second order in time of the velocity-stress formulation
of the elastic wave equation (kÃ¶hn et al., 2012).
we used an encoder-decoder type of neural network. the
multicomponent data is given as input to two convolutional
layers at the starting of the network. the number of input
channels is equal to the number of receivers and the number
of output channel is equal to one. a third convolutional layer
with two input channels takes the two inputs, combines
them, and gives an output which is given to rest of the
network. the rest of the network has an encoder network
then generates low-dimensional feature maps (z) through a
sequence of convolution blocks and maxpooling layers. the
same low dimensional feature map (z) is now fed into 3
separate decoder networks, output from the decoders
represent three different elastic model parameters.
our decoder transforms the feature maps into
velocity/density models (y) using a sequence of convolution
blocks and upsampling layers. the architecture of our
network is shown in figure 2. each convolution block
consists of two convolution layers and two relu units. each
convolution block outputs twice the input channels. the
maxpooling layers reduce the size of the feature maps by a
factor of two. the output of the encoder is connected to a
linear layer which generates latent representation. the final
output of the encoder is a 1d latent vector. this sequence of
convolutional blocks and maxpooling layers thus transforms
the high dimensional inputs into a low dimensional latent
space representation.
the low dimensional latent space is given as input to the decoder. the decoder consists of convolutional blocks interleaved with upsampling layers. the components of the convolutional blocks are the same as those of the encoder. each
convolution block outputs half the input channels. the
upsampling layers increase the lateral feature dimensions by

a factor of 2. the output of the decoder is fed into a
convolutional layer which reduces the channel dimension to
1. the intermediate output is added to a starting model and
given as input to our pde. to train our network we use
adam optimization using a learning rate of 0.005.
experiments
to test the effectiveness of our approach, we generated an
elastic test model and named it as the sth (square-trianglehourglass) model. the model is inspired from the cts
model, demonstrated in dokter et al. (2017). the model
consists of an elastic layer over a half-space. except for a
free surface boundary condition, on all the other sides
absorbing boundary conditions are applied. present inside
the elastic layer, are bodies of three different geometrical
shapes: 4 squares on the vp model, 4 triangles on the vs
model and 4 hourglass on the Ï model. all the bodies are
placed by taking care to ensure bodies of different shapes
donâ€™t occupy the same (x,y) position on the 2d grid. this is
not a representation of the true earth, but a good model to
demonstrate the inherent crosstalk in multiparameter
full waveform inversion. the grid size of the velocity model
is 150x294 with a grid interval of 10 m. a ricker wavelet
with a peak frequency of 10 hz is used as the source. we
simulate 28 shots with 223 receivers. the sources are placed
at equal distance. each shot record has 5000 samples with a
sampling interval of 1 ms. the true model along with the
source (in white) and receiver (in pink) position is shown in
figure 3a.
we choose to invert for velocities and densities since model
parameterization using velocities show weakest cross-talk
effects when compared to elastic inversion using lamÃ©
parameterization (kÃ¶hn et al., 2012). the effect of crosstalk
can further be damped by performing the inversion in a
multiscale way, starting from a band having a low value of
maximum frequency and going up to a band having the
maximum frequency content present in the data. the starting
model for inversion contained correct elastic material
parameters for the layer and the half space but without the
geometrical structures. to compare our proposed approach,
we show the results of a conventional multiscale inversion
in figure 3. a sequential inversion of the frequency bands
up to 2, 5, 10 and 20 hz using l-bfgs algorithm was carried
out in case of conventional fwi. even with a multiscale
strategy the crosstalk artifacts can be clearly seen in the
results in figure 3b. the crosstalk artifacts are highlighted
by green arrows. moreover, in areas of low data coverage
(highlighted by red arrow), the inversion shows significant
artifacts. inversion using our proposed elastic-adjointnet
shows significantly no such artifacts even in areas of low
data coverage (figure 3c). notice from the convergence
curve (figure 4), we donâ€™t use a multiscale approach when
using our neural network for inversion.

second international meeting for applied geoscience & energy
10.1190/image2022-3745050.1
page 883
Â© 2022 society of exploration geophysicists and the american association of petroleum geologists

downloaded 08/27/24 to 200.12.181.232. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/image2022-3745050.1

fwi using physics guided autoencoder

figure 1: workflow for deep learning based elastic full waveform inversion

figure 2: autoencoder architecture

figure 3: inversion results: a) true model b) conventional inversion c) proposal approach

second international meeting for applied geoscience & energy
10.1190/image2022-3745050.1
page 884
Â© 2022 society of exploration geophysicists and the american association of petroleum geologists

downloaded 08/27/24 to 200.12.181.232. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/image2022-3745050.1

fwi using physics guided autoencoder

figure 4 : convergence curve
the network architecture is particularly suitable for avoiding
the issue of local minima and cross talk effects in elastic
fwi. we interpret that the encoder carries out a regularized
inverse operation resulting in sparse latent representation of
the velocity model. the decoder then transforms this to a
higher dimension using a nonlinear operation. since three
separate decoders with different network weights are used to
upscale the same latent space, the crosstalk issue can be
mitigated using the proposed approach.
in our proposed approach, the gradient calculation is implemented in c language using the mpi framework and the network update is done in the pytorch (paszke et al., 2019)
framework. a python api connects the two; run time for one
iteration was 60 seconds.
next, we apply the proposed network for elastic inversion of
the geologically realistic marmousi-ii model. the gridsize
for the velocity model is 100x300 with grid spacing of 0.02
km. a ricker wavelet of peak frequency 10 hz is used as the
source. we simulate 35 shots over 276 receivers. the true
model, the initial model and the final inversion results are
shown in figure 5.
conclusion
we combined a deep convolutional autoencoder with a
physics based forward modeling code for elastic fwi. the
proposed approach overcomes local minima and cross talk
issues. here, we used a deterministic autoencoder to solve
our inverse problem. in future, we plan to test a variational
autoencoder which shall aid in quantifying uncertainty in
multiparameter full waveform inversion.
acknowledgements
this project is partially supported by edger forum at
jackson school of geosciences, ut-austin

figure 5: marmousi inversion models a) true b)
initial c) inverted

second international meeting for applied geoscience & energy
10.1190/image2022-3745050.1
page 885
Â© 2022 society of exploration geophysicists and the american association of petroleum geologists

downloaded 08/27/24 to 200.12.181.232. redistribution subject to seg license or copyright; see terms of use at http://library.seg.org/page/policies/terms
doi:10.1190/image2022-3745050.1

references
biswas, r., m. k. sen, v. das, and t. mukerji, 2019, prestack and poststack inversion using a physics- guided convolutional neural network:
interpretation, 7, se161â€“se174, doi: https://doi.org/10.1190/int-2018-0236.1.
calderÃ³n-macÃ­as, c., m. k. sen, and p. l. stoffa, 2000, artificial neural networks for parameter estimation in geophysics: geophysical prospecting,
48, 21â€“47, doi: https://doi.org/10.1046/j.1365-2478.2000.00171.x.
dokter, e., d. kÃ¶hn, d. wilken, d. de nil, and w. rabbel, 2017, full waveform inversion of sh-and love-wave data in near-surface prospecting:
geophysical prospecting, 65, 216â€“236, doi: https://doi.org/10.1111/1365-2478.12549.
kamath, n., and i. tsvankin, 2016, elastic full-waveform inversion for vti media: methodology and sensitivity analysis: geophysics, 81, c53â€“c68,
doi: https://doi.org/10.1190/geo2014-0586.1.
kazei, v., o. ovcharenko, p. plotnitskii, d. peter, x. zhang, and t. alkhalifah, 2021, mapping full seismic waveforms to vertical velocity profiles by
deep learning: geophysics, 86, r711â€“r721, doi: https://doi.org/10.1190/geo2019-0473.1.
keating, s., and k. a. innanen, 2020, parameter crosstalk and leakage between spatially separated unknowns in viscoelastic full-waveform inversion
crosstalk analysis in viscoelastic fwi: geophysics, 85, r397â€“r408, doi: https://doi.org/10.1190/geo2019-0370.1.
kingma, d. p., and j. ba, 2014, adam: a method for stochastic optimization: arxiv preprint arxiv:1412.6980.
kÃ¶hn, d., d. de nil, a. kurzmann, a. przebindowska, and t. bohlen, 2012, on the influence of model parametrization in elastic full waveform
tomography: geophysical journal international, 191, 325â€“345, doi: https://doi.org/10.1111/j.1365-246x.2012.05633.x.
operto, s., y. gholami, v. prieux, a. ribodetti, r. brossier, l. metivier, and j. virieux, 2013, a guided tour of multiparameter full-waveform
inversion with multicomponent data: from theory to practice: the leading edge, 32, 1040â€“1054, doi: https://doi.org/10.1190/tle32091040.1.
pan, w., k. a. innanen, y. geng, and j. li, 2019, interparameter trade-off quantification for isotropic-elastic full- waveform inversion with various
model parameterizations: geophysics, 84, r185â€“r206, doi: https://doi.org/10.1190/geo2017-0832.1.
paszke, a., s. gross, f. massa, a. lerer, j. bradbury, g. chanan, t. killeen, z. lin, n. gimelshein, and l. antiga, 2019, pytorch: an imperative style,
high-performance deep learning library: advances in neural information processing systems, 32, doi: https://doi.org/10.48550/arxiv.1912.01703.
tarantola, a., 1984, inversion of seismic reflection data in the acoustic approximation: geophysics, 49, 1259â€“1266, doi: https://doi.org/10.1190/1
.1441754.
ulyanov, d., a. vedaldi, and v. lempitsky, 2018, deep image prior: proceedings of the ieee conference on computer vision and pattern
recognition, 9446â€“9454.
wang, w., g. a. mcmechan, and j. ma, 2021, elastic isotropic and anisotropic full-waveform inversions using automatic differentiation for gradient
calculations in a framework of recurrent neural networks: geophysics, 86, r795â€“r810, doi: https://doi.org/10.1190/geo2020-0542.1.
yang, f., and j. ma, 2019, deep-learning inversion: a next-generation seismic velocity model building method: geophysics, 84, r583â€“r599,
doi: https://doi.org/10.1190/geo2018-0249.1.
zhang, t., j. sun, k. a. innanen, and d. trad, 2021, numerical analysis of a deep learning formulation of elastic full waveform inversion with high
order total variation regularization in different parameterization: arxiv preprint arxiv:2101.08924.
zhu, w., k. xu, e. darve, b. biondi, and g. c. beroza, 2021, integrating deep neural networks with full-waveform inversion: reparametrization,
regularization, and uncertainty quantification: geophysics, 87, 1â€“103, https://doi.org/doi:10.1190/geo2020-0933.1

second international meeting for applied geoscience & energy
10.1190/image2022-3745050.1
page 886
Â© 2022 society of exploration geophysicists and the american association of petroleum geologists



7

deep learning-based groundwater storage estimation from
seismic data
t. lÃ¤hivaara1, a. malehmir2, a. pasanen3, l. kÃ¤rkkÃ¤inen4, j.m.j. huttunen5, j.s. hesthaven6
1

university of eastern finland; 2 uppsala university; 3 geological survey of finland; 4 aalto
university; 5 nokia bell labs; 6 ecole polytechnique fÃ©dÃ©rale de lausanne

summary
in this work, an inverse problem of estimating the water table level and actual stored water in an aquifer from
seismic data is studied. both the elastic and poroviscoelastic wave equations are used to model wave propagation
in the ground. the equations are spatially discretized using a nodal discontinuous galerkin method, while
temporally using the explicit low-storage runge-kutta method. the actual inverse problem is solved using a deep
learning approach based the convolutional neural networks. our simulation results suggest that the neural
network-based method can be used to accurately estimate the water table level and stored water from noisy data.

second eage conference on seismic inversion
7-9 february 2022, porto | online event

introduction
the developed computational techniques can be used to tackle current critical barriers towards an
accurate characterization of groundwater resources. detailed knowledge of the underground water
storage (aquifer) properties and subsurface parameters is crucial in aquifer management, for example,
to planning aquifer protection and prevent water level drawdown.
seismic imaging provides a potential framework to characterize and monitor aquifers, consisting of
layers of porous substrates. seismic surveys are often done by placing a set of receivers, i.e., geophones
or accelerometers, on the ground surface, after which a seismic source is generated at one or more
locations. vibrators or man-made impacts (impulsive sources) are typically used as such sources. the
generated seismic wavefield interacts with the medium and hence the poroelastic signature of the
aquifer can be captured in data. therefore, recorded seismic signals can be used, with efficient
numerical techniques, to increase the knowledge of the groundwater reservoir state.
in this simulation study, we use a high-order accurate discontinuous galerkin method (hesthaven and
warburton 2007) to simulate seismic wave propagation in coupled elastic-poroviscoelastic media while
a convolutional neural network (cnn) (buduma 2017) is applied for solving the actual inverse problem.
in the inverse problem, the network is trained to recover either the water table level or the actual stored
water. we build a training and validation database for the cnn algorithm and test its performance by
applying the cnn to a separate testing dataset.
model configuration
as a test case, we consider a 48 metres long model domain, which consists of three subdomains, see
figure 1. right below the top surface, we assume an air-saturated porous layer, followed by a watersaturated porous layer, and finally an elastic (low porosity) bedrock layer. the interface between airand water-saturated layers is the water table. a total of 38 receivers are placed on the ground surface,
and they are used to capture the vertical velocity data. the data is generated using 10 force-type point
sources, which are placed at a depth of 0.5 m from the ground surface. the force is parallel to the
negative vertical axis. to illustrate wave propagation in the studied case, figure 1 also shows an
example of the total velocity field.

figure 1 an example realization of the problem geometry. figure also includes an example snapshot
of the total velocity field. the selected sample contains a sharp discontinuity in the basement profile.
as the physical model for the wave propagation model, we use the coupled poroviscoelastic-elastic
model studied in dudley ward et al. 2017. the aquifer is modeled as a fully saturated porous material,
based on the biot theory, while the bedrock layer is modeled as an elastic layer. as a boundary condition
(bc) configuration, we set outflow condition on the left, bottom, and right boundaries while the top
surface is modelled with free bc.

second eage conference on seismic inversion

in this study, both the problem geometry (water table level and basement) and physical parameters are
unknown. simulated training and validation datasets for neural networks are generated by randomizing
the problem geometry and physical parameters for different subdomains. the unknown material
parameters are generated using an isotropic ornstein-uhlenbeck process while the basement geometry
is a realization of the matern field, for detailed discussion we refer to lÃ¤hivaara et al. 2019 and
references therein. the water table level, i.e., the line parallel to the horizontal axis, can vary between
the ground surface and basement. the physical parameters for air and water are assumed be known
accurately. to illustrate the realization process, two examples of the porosity fields are shown in figure
2 (a). visualized cases are selected so that the amount of stored groundwater is low (left) or high (right).
figure 2 (b) shows the noiseless shot records corresponding to the geometries visualized on top. in this
study, the collected data is assumed to be the vertical solid velocity component. from the shot records,
we can easily see direct arrival and reflection from the basement boundary. in addition, the collected
responses show clearly multiple reflections from different interfaces.

figure 2 two example porosity fields (a) and the corresponding simulated vertical velocity data (b).
the porosity values are given in percentages. selected results correspond to samples where the amount
of stored water is low (left) or high (right). on top, the source location (red x symbol) and water table
level (horizontal blue line) are also visualized .

figure 3 graphical illustration of the neural network architecture used in this work.

deep convolutional neural networks
the neural network architecture used in our study comprises two convolutional layers with max-pooling
and two fully connected layers. the details of the architecture are presented in figure 3. the architecture

second eage conference on seismic inversion

was found after carrying out experiments with different network architectures and the selected network
provided good performance with a reasonable number of unknowns.
training, validation, and test datasets
for the training of the cnn, we generated a dataset comprising 15,000 samples. computational grids
were generated based on the randomized geometry (see above) and had ~3.5 elements per wavelength.
to monitor the network's generalization capabilities during training and for model selection, we
generated an additional validation dataset of 3,000 samples. to further control the numerical accuracy
of the discontinuous galerkin-based wave solver, the order of the polynomial basis approximation in
each triangle element of the computational grid can vary (lÃ¤hivaara and huttunen 2011). figure 4
shows an example of the computational grid and the corresponding basis order distribution.

figure 4 an example grid used in the computations. colorbar shows the order of the basis functions.
to train the model to tolerate the presence of measurement noise, the samples were corrupted with
gaussian noise. each of the simulated samples were multiplied five times and corrupted with variable
levels of noise as
ğ‘‹ğ‘–noised = ğ‘‹ğ‘– + ğ´Î±Ïµa + ğµ|ğ‘‹ğ‘– |Ïµb ,
where ğ‘‹ğ‘– is the data of ith training sample, Î± is the maximum absolute value of the training dataset, and
Ïµa/b are independent zero-mean gaussian random variables. the first added noise component
represents additive white noise, and the second noise component represents noise with magnitude
relative to the velocity signal amplitude. the coefficients a and b for each sample were randomly
chosen. the standard deviations (stds) of the first term varies logarithmically between (0.03-5)% Î±,
and the stds of the second term is between (0-5)% |ğ‘‹ğ‘– |. total number of samples in the training dataset
is 5 x 15,000 = 75,000.
the performance was evaluated using two test sets. the first set consisting of 3,000 samples was
generated with the same approach as in the generation of the training set. in addition, to study the
networkâ€™s extrapolation capabilities, the second test set was generated to consist of 200 samples that
contain two jumps in the basement profile. for the samples in the test datasets, the computational grids
were set to have ~4 elements per wavelength.

results
figure 5 shows estimates for the test data, contaminated with the noise generated with parameters a =
2.3% and b = 5.5%. estimates are shown for both stored water (left) and water table level (middle). to
test the networks extrapolation capabilities, we also show the results of the 200 additional test samples
in which the basement profile contains two discontinuities. note that, in the training data, the basement
can contain only one discontinuity. the estimation accuracy is very good for all samples and for
estimated parameters. the figures also include relative prediction error histograms (bottom) and two
example noisy and noiseless signals (right).

second eage conference on seismic inversion

figure 5 estimates for the amounts of stored water (top left), water table levels (top middle), and
example velocity signals for noisy (black) and noiseless (green) data for the test data (right). bottom
row (left) and (middle) shows the histograms of the relative prediction error (i.e., difference between
the estimated and true values). on top, red diamond symbols show the additional test samples that have
two jumps in the basement profile. here the noise parameters a = 2.3% and b = 5.5%.
conclusions
results of this study support that the neural networks can be used to estimate the aquifer quantities of
interest, i.e., the water table level and actual stored water, from seismic data. the error histograms for
both estimated quantities show promising accuracy in terms of relative prediction error and bias. in
addition to the promising estimation accuracy, the estimates are obtained in a very fast manner once the
time-consuming database generation and network training phase are done.
acknowledgements
this work has been supported by the academy of finland (finnish centre of excellence of inverse
modelling and imaging) and project 321761. the authors also wish to acknowledge csc-it center for
science, finland, for computational resources.
references
buduma, n. [2017]. fundamentals of deep learning, designing next- generation machine
intelligence algorithms. oâ€™reilly media, newton, ma.
dudley ward, n.f., lÃ¤hivaara, t. and eveson, s. [2017]. a discontinuous galerkin method for
poroelastic wave propagation: two-dimensional case. journal of computational physics 350, 690-727.
hesthaven, j.s. and warburton, t. [2007]. nodal discontinuous galerkin methods: algorithms,
analysis, and applications, springer.
lÃ¤hivaara, t. and huttunen, t. [2011]. a non-uniform basis order for the discontinuous galerkin
method of the acoustic and elastic wave equations. applied numerical mathematics 61, 473-486.
lÃ¤hivaara, t., malehmir, a., pasanen, a., kÃ¤rkkÃ¤inen, l., huttunen, j.m.j. and hesthaven, j.s. [2019].
estimation of groundwater storage from seismic data using deep learning. geophysical prospecting 67,
2115-2126.

second eage conference on seismic inversion



seismic wave propagation and inversion with neural
operators
yan yang*â€ 1 , angela f. gaoâ€ 2 , jorge c. castellanosâ€ 1 , zachary e. ross1 , kamyar azizzadenesheli3 , and
robert w. clayton1

abstract
seismic wave propagation forms the basis for most aspects of seismological research, yet
solving the wave equation is a major computational burden that inhibits the progress of
research. this is exacerbated by the fact that new simulations must be performed whenever the velocity structure or source location is perturbed. here, we explore a prototype
framework for learning general solutions using a recently developed machine learning
paradigm called neural operator. a trained neural operator can compute a solution in
negligible time for any velocity structure or source location. we develop a scheme to train
neural operators on an ensemble of simulations performed with random velocity models
and source locations. as neural operators are grid free, it is possible to evaluate solutions
on higher resolution velocity models than trained on, providing additional computational
efficiency. we illustrate the method with the 2d acoustic wave equation and demonstrate
the methodâ€™s applicability to seismic tomography, using reverse-mode automatic differentiation to compute gradients of the wavefield with respect to the velocity structure. the
developed procedure is nearly an order of magnitude faster than using conventional
numerical methods for full waveform inversion.

cite this article as yang, y., gao, a. f.,
castellanos, j. c., ross, z. e.,
azizzadenesheli, k., and clayton, r. w.
(2021). seismic wave propagation and
inversion with neural operators, the seismic
record. 1(3), 126â€“134, doi: 10.1785/
0320210026.

supplemental material

introduction
the simulation of seismic wave propagation through earthâ€™s
interior underlies most aspects of seismological research, from
the simulation of strong ground shaking due to large earthquakes (graves and pitarka, 2016; rodgers et al., 2019), to
imaging the subsurface velocity structure (fichtner et al., 2009;
tape et al., 2009; virieux and operto, 2009; lee et al., 2014;
gebraad et al., 2020), to derivation of earthquake source properties (duputel et al., 2015; ye et al., 2016; wang and zhan,
2020). the compute costs associated with these wavefield simulations are substantial; and, for reasons of computational efficiency, 1d models are often used, even when better 3d velocity
models are available. as a result, seismic wave simulations are
often the limiting factor in the pace of geophysical research.
recently, deep learning approaches have been explored with
the goal of solving various geophysical partial differential
equations (moseley, markham, and nissen-meyer, 2020;
moseley, nissen-meyer, and markham, 2020; smith et al.,

2020; moseley et al., 2021). beyond the goal of accelerating
compute capabilities, such physics-informed neural networks
may offer other advantages such as grid independence, lowmemory overhead, differentiability, and on-demand solutions.
these properties facilitate deep learning being used to solve
geophysical inverse problems (zhu et al., 2020; smith et al.,
2021; xiao et al., 2021; zhang and gao, 2021), as a wider

1. seismological laboratory, california institute of technology, pasadena, california, u.s.a.,
https://orcid.org/0000-0002-6105-2918 (yy);
https://orcid.org/0000-0002-0103-6430
(jcc);
https://orcid.org/0000-0002-6343-8400 (zer);
https://orcid.org/0000-0003-33233508 (rwc); 2. department of computing and mathematical sciences, california institute of
technology, pasadena, california, u.s.a.,
https://orcid.org/0000-0001-8574-8728 (afg);
3. department of computer science, purdue university, west lafayette, indiana, u.s.a., https://
orcid.org/0000-0001-8507-1868 (ka)
*corresponding author: yanyang@caltech.edu
â€ these authors contributed equally to this work.
Â© 2021. the authors. this is an open access article distributed under the terms of the cc-by
license, which permits unrestricted use, distribution, and reproduction in any medium, provided the
original work is properly cited.

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

126

selection of algorithms and frameworks then are available for
use, such as approximate bayesian inference techniques like
variational inference.
one of the major challenges associated with wave propagation is that a new simulation must be performed whenever the
properties of the source or velocity structure are perturbed in
some way. this alone substantially increases the necessary compute costs, making some problems prohibitively expensive even
if they are mathematically or physically tractable. for the most
part, these limitations have been accepted as an inevitable part of
seismology, but now physics-informed machine learning
approaches have started to offer some pathways for moving
beyond this issue. for example, smith et al. (2020) use a deep
neural network to solve the eikonal equation for any sourceâ€“
receiver pair by taking these locations as input. then, this
can be exploited for hypocenter inversion by allowing for gradients of the travel time field to be computed with respect to the
source location (smith et al., 2021). however, these models are
relatively inefficient to train and even then are only able to learn
approximate solution operators to the differential equations.
the aforementioned limitations may seem surprising but
result from a basic attribute of neural networks that in fact makes
them ill-suited for solving differential equations. specifically, neural networks are designed for learning maps between two finitedimensional spaces, whereas learning a general solution operator
for a differential equation requires the ability to map between two
infinite dimensional spaces (i.e., function spaces). a paradigm for
learning maps between function spaces was recently developed
(li et al., 2020a,b, 2021) and has been termed as neural operator.
the general idea behind these models is that they have shared
parameters over all possible functions describing the initial conditions, which allows them to operate on functions, even when
the inputs are a numerically discretized representation of them.
here, we explore the potential of neural operators in
improving seismic wave propagation and inversion. we
develop a prototype framework for training neural operators
on the 2d acoustic wave equation and show that this approach
provides a suite of tantalizing new advantages over conventional numerical methods for seismic wave propagation.
this study provides a proof of concept of this technology
and its application to seismology.

preliminaries
for a given function a and a greenâ€™s function g, let u denotes
the solution to a linear partial differential equation (pde), that
is, the solution operator,

z
eq-target;temp:intralink-;df1;314;708

ux  lax 

in which l is the corresponding linear operator. for example,
suppose that the pde to be solved is the acoustic wave equation; and then a could describe the velocity structure as well as
the initial conditions. neural operator generalizes this formulation to the nonlinear setting in which a set of linear operators
are sequentially applied to construct a general nonlinear solution operator. in its basic form, an â„“-layered neural operator
is constructed as follows:
ux  lâ„“ Ïƒlâ„“âˆ’1 â€¦Ïƒl1 vâ€¦x;

2

in which li is such that for any function v, we have,
z
li vx  w i x  k i x; yvydy:

3

eq-target;temp:intralink-;df2;314;569

eq-target;temp:intralink-;df3;314;513

under this framework, w i x and k i x; y constitute the learnable components of the neural operator and allow for a solution to be produced for any prescribed function a. in a limited
sense, neural operators can be viewed as generalized greenâ€™s
functions.

methods
we designed a framework that applies neural operators to the
2d acoustic wave equation. the basic idea for this procedure is
outlined schematically in figure 1. a specific type of neural
operator called a fourier neural operator (fno; li et al.,
2021) receives a velocity model specified on an arbitrary, possibly irregular mesh, along with the coordinates of a point
source. one of the main features of fno is that the major calculations are performed in the frequency domain that allows
the convolutions in equation (3) to be rapidly computed. the
output of the fno is the complete wavefield solution, which
can be queried anywhere within the medium, regardless of
whether the points lie on the input mesh.
the most basic component of the fno is a fourier block
(fig. 1), which first transforms an input function (v) to the
fourier domain. in the first layer of the network, v is equal
to the initial conditions a. then, a kernel (k i ) is computed
specifically for this function and is truncated at low order,
before performing the integration via multiplication. finally,
the result is transformed back and a nonlinear activation function is applied, which concludes the fourier block. for this
study, the architecture of the fno contains four sequential
fourier blocks and applies a rectified linear unit (relu)

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

1

gx; yaydy;

the seismic record

127

(

)

activation to the output of each (fig. 1). we note that the truncation of the fourier modes is performed on the function values after lifting them to a higher dimensional space, rather than
the raw input function, so that this does not lead to compression. we refer the interested readers to li et al. (2021) for more
mathematical details about the fno.
we constructed a training dataset of simulations to learn
from by first generating random velocity models. we set up
a 64 Ã— 64 grid with 0.16 km spacing to define the velocity
model. then, we created 5000 random velocity models by sampling random fields having a von karman covariance function
with the following parameters: hurst exponent Îº  0:5, correlation length a  ax ; ay   1:6 km; 1:6 km, and Îµ  0:1,
Î¼  3 km=s, and Ïƒ  0:15 km=s. then, for each of these
velocity models, we apply a ricker wavelet source at a random
point and solve the acoustic wave equation using a spectral

figure 1. our approach applying fourier neural operator (fno) to the 2d
acoustic wave equation. the inputs to the fno model are the velocity
model with dimensions d Ã— d Ã— 1 and the source location, indicated by the
white star. the input velocity model is lifted to a higher dimensional space
with size d Ã— d Ã— w using a neural network. then, we apply four fourier
operator layers and finally project back to the target wavefield dimensions of d Ã— d Ã— n using a neural network. the bottom panel shows
details of the fourier layer architecture; we define v to be the input. on
top: we apply a fourier transform f to v and then apply a linear transformation r to the lower fourier modes, filtering out higher modes. then,
we apply an inverse fourier transform f âˆ’1 . on the bottom, we apply a
local linear transform w to v.

element method (sem; afanasiev et al., 2019). it should be
noted that there is a source grid used, because this is a requirement of the sem. because gaussian random fields (grfs) can
represent all continuous functions, the purpose of these steps is

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

128

to create a suite of simulations that span a range of possible
conditions. we show later that they can even well approximate
strongly discontinuous velocity models. an example velocity
model and simulation is shown in figure 1. applying the aforementioned procedure results in a training dataset of 5000 data
samples, each of which is a different simulation.
given the simulation dataset, we can proceed to train an
fno model in a supervised capacity, in which the goal is to
learn a model that can reliably output a solution to the wave
equation for arbitrary input conditions. the training is performed using batch gradient descent, for which the parameters
of the fno are updated to minimize the error against the
â€œtrueâ€ spectral element solutions. a mean-square error loss
function is used. we use a batch size of 30 simulations and
train the model for a maximum of 300 epochs. we use all
but 200 of the simulations for training and set aside the
remainder for cross validation of the model. the time required
to train the model from scratch is approximately 18 hr using
six nvidia tesla v100 graphical processing units (gpus).

experiments
initial wavefield demonstration
figure 2 shows two example wavefields corresponding to two
different velocity models, each with a different source. the spectral element solution is shown alongside the wavefield predicted
by the fno for the eight different receivers (blue triangles). for
these examples, the input velocity model is 64 Ã— 64. the relative
â„“2 loss of the fno wavefields are 0.180 and 0.363. these examples are representative of the entire validation dataset, which has
a loss of 0.273 relative to the spectral element solutions.
the number of simulations needed for training
once fully trained, the fno can evaluate a new solution in a
fraction of a second, and thus the time to train the fno will be
the vast majority of the needed compute time. a primary concern about the computational demands of the fno approach
is, therefore, the number of simulations needed for training.
here, we examine how the number of training simulations
influences the accuracy of the solution. we create a series
of tests in which the number of training simulations is varied
from 1200 at the fewest to 4800 at the most. the results are
shown in figure 3, in which we show the fno wavefield predictions for each dataset. even with 1200 training samples,
there is no indication that there is overfitting, because the
training waveform error is similar across different models
(fig. 3a,b). training using just 1200 simulations is able to

predict the major arrival. increasing number of training samples provides a better fit of the reflections (e.g., 3.2 s in fig. 3c).

generalization to arbitrary velocity models
the fno was trained only on velocity models drawn from
gaussian random fields; and, although this family of functions
is broad, it does not include some types of functions that exist
in the earth, such as discontinuous functions. this raises the
question of whether the fno can still generalize well to these
cases. figure 4aâ€“c shows an example of a predicted wavefield
for a velocity model containing a constant velocity square
embedded within a homogeneous medium. although the
velocity model itself is rather simple, it is actually very far
removed from the characteristics of the random fields that
the fno was trained on and represents a challenging example.
we can see that the predicted wavefield does a very good job of
approximating the wavefield compared to the ground truth.
we believe that the small residual errors can be reduced with
better hyperparameter selection.
generalization to higher resolution grids
fno can be viewed in some sense as a method for learning generalized greenâ€™s functions valid for arbitrary boundary conditions. because it is intrinsically learning a mapping between
function spaces, the fno is theoretically independent of the resolution at which the functions are discretized (this is only a
requirement for evaluation on a computer). one important
advantage of this is that the fno can be trained on velocity
models with a certain grid spacing and then be evaluated on
velocity models with a different grid spacing at inference time.
here, we are not simply talking about interpolating the wavefield
after solving the pde; but, rather, the solutions to the pde can
actually be evaluated on a higher resolution velocity model with
negligible extra compute cost. to demonstrate this, figure 4dâ€“f
shows the fno prediction for a random velocity field with 2Ã—
higher resolution (128 Ã— 128) than the models used during training, alongside the spectral element solution. the fno solution
closely approximates the spectral element solution. the velocity
models with different meshes have the same roughness as the
training data set. resolving more rough structure with denser
spacing can be achieved by training with many more grfs with
varying correlation length scales and variance.
full waveform inversion with neural operators
one of the most useful applications of wavefield simulations is
in inversion, to image the earthâ€™s interior. the adjoint-state

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

129

(a)

(b)

(d)

(e)

(c)

(f)

method is an approach to efficiently compute the gradients of
an objective function with respect to parameters of interest and
can be used for seismic tomography (e.g., tape et al., 2009;
gebraad et al., 2020). neural operators are differentiable by
design, which enables gradient computation with reversemode automatic differentiation. automatic differentiation
has been shown to be mathematically equivalent to the
adjoint-state method (zhu et al., 2021). this allows for the gradients of the wavefield to be determined with respect to the
inputs (velocity model and source location).
figure 5 demonstrates our full waveform inversion (fwi)
performance. for each case, we compute synthetic observations

figure 2. examples of two validation wavefield simulations from the
trained fno model. (a) the sourceâ€“receiver locations with receivers in
blue and source in red, (b) the velocity structure, (c) waveforms simulated
with spectral element method (sem; black) and fno (red), and (dâ€“f) same
as (aâ€“c), but for a different velocity model. the relative â„“2 loss of the two
examples are 0.180 and 0.363, respectively, which are representative of
the entire validation data set with an average â„“2 loss of 0.273. we
demonstrate that the fno simulation results are able to capture both the
major arrivals as well as some reflections.

using the source distribution as shown (red circles), taking every
point in the 64 Ã— 64 grid as a receiver. the observations are
noise-free for this experiment. then, we perform tomography

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

130

(a)

(b)

by starting with a homogeneous initial velocity model and forward propagating a wavefield with the fno for each source. we
p p
calculate the loss l  i j uobs xi ; xj  âˆ’ upred xi ; xj 2 and
compute âˆ‡l with automatic differentiation. the velocity model
is then iteratively updated with gradient descent for 1000 iterations using the adam optimizer (kingma and ba, 2014) and a
learning rate of 0.01. for comparison, figure 5a,b shows the
imaging result using sem and adjoint-state method, with a
relative â„“2 misfit between the inverted and true velocity model
of 0.0289. figure 5c,d shows the result for the same velocity
structure using fno and automatic differentiation, with a misfit
of 0.0319. figure 5e,f is designed to demonstrate sharp discontinuous changes with a short wavelength. the results demonstrate the remarkable capabilities of fno to learn a general
solution operator.
we note that our fwi approach neither requires an adjoint
wavefield to be computed nor a cross correlation; the gradients
can be rapidly computed with gpus using automatic differentiation. the rapid simulation makes it substantially more efficient than adjoint methods. for these experiment, 20 sources
take âˆ¼1 s for one tomographic iteration, including the costs of
computing the forward model, whereas the spectral element
method with adjoint methods takes âˆ¼100 s for one tomographic iteration. these time measurements are from using
only a single nvidia tesla v100 gpu.

discussion
this study presents a prototype framework for applying neural
operators to the 2d acoustic wave equation. we anticipate that
the general framework would also be suitable for the 3d

(c)

figure 3. model performance as a function of the number of training
samples. (a) training and validation loss curves as a function of different
numbers of training samples. (b) example waveform fitting of a single
training example from models trained with varying number of training
examples. (c) example waveform fitting of a single validation example
from models trained with varying number of training examples. the
numbers to the right of each waveform shows the relative â„“2 misfit. this
shows that the model trained on 4800 samples is able to capture the
reflections, whereas the model trained on smaller number of samples
does not generalize to reflections in the validation example.

elastic-wave equation with relatively little modification.
indeed, the fno method was applied successfully to the
navierâ€“stokes equations (li et al., 2021), which can be more
challenging to solve than the elastic-wave equation. in our
tests, we found that only a few thousand simulations were
needed to train an fno model and, from there, required negligible time to compute a new solution. because fno can be
trained on lower resolution simulations and then generalize to
higher resolution solutions once trained, this results in substantially faster computations than using traditional numerical
methods at the full resolution.
one of the limitations of the approach is that the solutions
are approximate, as seen in several of the figures. however,
because this is a learning-based approach, the performance
can be improved in the future using a better model architecture, thorough tuning of hyperparameters, improving the size
of the training dataset, using a more appropriate objective
function, and various other factors. in addition, as new developments within machine learning emerge in this area, they
would be able to be incorporated. thus, these performance

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

131

(a)

(b)

(d)

(e)

(c)

(f)

metrics should only be viewed as a starting point. for some
applications, the error may be enough of an issue, and traditional numerical methods may be preferable; however, for
many other situations in geophysics, a reasonably accurate solution may be acceptable.
among the most exciting benefits of our approach is that by
training the fno on random velocity models, the fno is able to
produce solutions for arbitrary velocity models. this is because
fno learns a general solution operator to the pde and not specifically the velocity model. this means that the model does not
need to be retrained for each region. thus, the approach offers
the potential for a single fno model to be used by the entire

figure 4. model generalization experiments. (a) the sourceâ€“receiver
locations with receivers in blue and source in red, (b) a velocity model with
a homogeneous background of 3 km/s and a 5% square anomaly,
(c) waveform simulated with sem (black) and fno (red), and (dâ€“f) same as
(aâ€“c), but for an input velocity model with 2Ã— finer resolution than trained
on. these experiments show that the model is not just memorizing the
solutions, but is able to generalize to entirely new conditions.

seismology community for any region of a similar size. although
the initial cost of training an fno and performing the training
simulations may be expensive, it only needs to be done a single
time for the community as a whole.

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

132

(a)

(b)

(c)

(d)

(e)

(f)

figure 5. example of a full waveform inversion using fno. (a,c,e) true
velocity models with source locations indicated by red circles and receivers
placed at every node of the 64 Ã— 64 grid (10 km Ã— 10 km region).
(b) reconstruction using sem and adjoint tomography. (d,f) reconstruction
using fno as the forward model and automatic differentiation to compute
gradients. no regularization is used for these experiments.

data and resources
all the data presented in this study are synthetic and available
upon request. the supplemental material for this article demonstrates that the misfit between the simulations using spectral
element method (sem) and the fourier neural operator (fno)
is minimal.

declaration of competing interests
the authors declare that there are no competing interests.

acknowledgments
the authors thank jack muir for helpful comments on an
early version of the article.

references
afanasiev, m., c. boehm, m. van driel, l. krischer, m. rietmann, d.
a. may, m. g. knepley, and a. fichtner (2019). modular and

flexible spectral-element waveform modelling in two and three
dimensions, geophys. j. int. 216, no. 3, 1675â€“1692, issn 0956540x, doi: 10.1093/gji/ggy469.
duputel, z., j. jiang, r. jolivet, m. simons, l. rivera, j.-p. ampuero, b.
riel, s. e. owen, a. w. moore, s. v. samsonov, et al. (2015). the
iquique earthquake sequence of april 2014: bayesian modeling
accounting for prediction uncertainty, geophys. res. lett. 42,
no. 19, 7949â€“7957, issn 1944-8007, doi: 10.1002/2015gl065402.
fichtner, a., b. l. n. kennett, h. igel, and h.-p. bunge (2009). full seismic
waveform tomography for upper-mantle structure in the australasian
region using adjoint methods, geophys. j. int. 179, no. 3, 1703â€“1725,
issn 0956-540x, doi: 10.1111/j.1365-246x.2009.04368.x.
gebraad, l., c. boehm, and a. fichtner (2020). bayesian elastic fullwaveform inversion using hamiltonian monte carlo, j. geophys.
res. 125, no. 3, e2019jb018428, issn 2169-9356, doi: 10.1029/
2019jb018428.
graves, r., and a. pitarka (2016). kinematic ground-motion simulations on rough faults including effects of 3d stochastic velocity
perturbations, bull. seismol. soc. am. 106, no. 5, 2136â€“2153,
geoscienceworld, issn 0037-1106, doi: 10.1785/0120160088.
kingma, d. p., and j. ba (2014). adam: a method for stochastic otimization, available at http://arxiv.org/abs/1412.6980 (last accessed
august 2021).
lee, e.-j., p. chen, t. h. jordan, p. b. maechling, m. a. m. denolle,
and g. c. beroza (2014). full-3-d tomography for crustal structure in southern california based on the scattering-integral and
the adjoint-wavefield methods, j. geophys. res. 119, no. 8,
6421â€“6451, issn 2169-9356, doi: 10.1002/2014jb011346.
li, z., n. kovachki, k. azizzadenesheli, b. liu, k. bhattacharya, a.
stuart, and a. anandkumar (2020a). multipole graph neural operator for parametric partial differential equations, available at
http://arxiv.org/abs/2006.09535 (last accessed august 2021).
li, z., n. kovachki, k. azizzadenesheli, b. liu, k. bhattacharya, a.
stuart, and a. anandkumar (2020b). neural operator: graph kernel network for partial differential equations, available at http://
arxiv.org/abs/2003.03485 (last accessed august 2021).
li, z., n. kovachki, k. azizzadenesheli, b. liu, k. bhattacharya, a.
stuart, and a. anandkumar (2021). fourier neural operator for
parametric partial differential equations, available at http://arxiv
.org/abs/2010.08895 (last accessed august 2021).
moseley, b., a. markham, and t. nissen-meyer (2020). solving the
wave equation with physics-informed deep learning, available at
http://arxiv.org/abs/2006.11894 (last accessed august 2021).
moseley, b., a. markham, and t. nissen-meyer (2021). finite basis
physics-informed neural networks (fbpinns): a scalable domain
decomposition approach for solving differential equations, available
at http://arxiv.org/abs/2107.07871 (last accessed august 2021).
moseley, b., t. nissen-meyer, and a. markham (2020). deep learning
for fast simulation of seismic waves in complex media, solid earth
11, no. 4, 1527â€“1549, issn 1869-9510, copernicus gmbh, doi:
10.5194/se-11-1527-2020.
rodgers, a. j., n. anders petersson, a. pitarka, d. b. mccallen, b.
sjogreen, and n. abrahamson (2019). broadband (0â€“5 hz)

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

the seismic record

133

fully deterministic 3d ground-motion simulations of a magnitude
7.0 hayward fault earthquake: comparison with empirical
ground-motion models and 3d path and site effects from source
normalized intensities, seismol. res. lett. 90, no. 3, 1268â€“1284,
geoscienceworld, issn 0895-0695, doi: 10.1785/0220180261.
smith, j. d., k. azizzadenesheli, and z. e. ross (2020). eikonet:
solving the eikonal equation with deep neural networks, ieee
trans. geosci. remote sens. 1â€“12, issn 1558-0644, doi:
10.1109/tgrs.2020.3039165.
smith, j. d., z. e. ross, k. azizzadenesheli, and j. b. muir (2021).
hyposvi: hypocenter inversion with stein variational inference
and physics informed neural networks, available at http://arxiv
.org/abs/2101.03271 (last accessed august 2021).
tape, c., q. liu, a. maggi, and j. tromp (2009). adjoint tomography
of the southern california crust, science 325, no. 5943, 988â€“992,
issn 0036-8075, 1095-9203, doi: 10.1126/science.1175298.
virieux, j., and s. operto (2009). an overview of full-waveform inversion in exploration geophysics, geophysics 74, no. 6, wcc1â€“
wcc26, society of exploration geophysicists, issn 0016-8033,
doi: 10.1190/1.3238367.
wang, x., and z. zhan (2020). moving from 1-d to 3-d velocity
model: automated waveform-based earthquake moment tensor
inversion in the los angeles region, geophys. j. int. 220, no. 1,
218â€“234, issn 0956-540x, doi: 10.1093/gji/ggz435.

xiao, c., y. deng, and g. wang (2021). deep-learning-based adjoint
state method: methodology and preliminary application to inverse
modeling, water resour. res. 57, no. 2, e2020wr027400, issn
1944-7973, doi: 10.1029/2020wr027400.
ye, l., t. lay, h. kanamori, and l. rivera (2016). rupture
characteristics of major and great mw â‰¥ 7.0 megathrust earthquakes from 1990 to 2015: 2. depth dependence, j. geophys.
res. 121, no. 2, 2015jb012427, issn 2169-9356, doi: 10.1002/
2015jb012427.
zhang, w., and j. gao (2021). deep-learning full-waveform inversion
using seismic migration images, ieee trans. geosci. remote sens.
1â€“18, issn 1558-0644, doi: 10.1109/tgrs.2021.3062688.
zhu, w., k. xu, e. darve, and g. c. beroza (2021). a general
approach to seismic inversion with automatic differentiation,
comput. geosci. 151, 104,751.
zhu, w, k. xu, e. darve, b. biondi, and g. c. beroza (2020).
integrating deep neural networks with full-waveform inversion:
reparametrization, regularization, and uncertainty quantification,
available at http://arxiv.org/abs/2012.11149 (last accessed august
2021).

https://www.seismosoc.org/publications/the-seismic-record/ â€¢ doi: 10.1785/0320210026
downloaded from http://pubs.geoscienceworld.org/ssa/tsr/article-pdf/1/3/126/5455773/tsr-2021026.1.pdf
by universidad eafit user

manuscript received 17 august 2021
published online 2 november 2021

the seismic record

134



an intriguing property of geophysics inversion

yinan feng 1 yinpeng chen 2 shihang feng 1 peng jin 3 1 zicheng liu 2 youzuo lin 1

in this paper, we show that such a mapping can be
well modeled by a very shallow (but not wide) network with only five layers. this is achieved based
on our new finding of an intriguing property: a
near-linear relationship between the input and
output, after applying integral transform in high
dimensional space. in particular, when dealing
with the inversion from seismic data to subsurface
velocity governed by a wave equation, the integral
results of velocity with gaussian kernels are linearly correlated to the integral of seismic data with
sine kernels. furthermore, this property can be
easily turned into a light-weight encoder-decoder
network for inversion. the encoder contains the
integration of seismic data and the linear transformation without need for fine-tuning. the decoder
only consists of a single transformer block to reverse the integral of velocity.
experiments show that this interesting property
holds for two geophysics inversion problems over
four different datasets. compared to much deeper
inversionnet (wu & lin, 2019), our method
achieves comparable accuracy, but consumes significantly fewer parameters.
1
earth and environmental sciences division, los alamos national laboratory,usa 2 microsoft research, usa 3 college of
information sciences and technology, the pennsylvania state university, usa. correspondence to: youzuo lin <ylin@lanl.gov>.

proceedings of the 39 th international conference on machine
learning, baltimore, maryland, usa, pmlr 162, 2022. copyright 2022 by the author(s).

geophysical
measurements

ğ‘¢ ğ‘¥, ğ‘¡

ğ‘¼
geophysical
properties

ğ’€

x

ğ‘¦ ğ‘¥, ğ‘§

x

ğ’€ â‰ˆ ğ´ğ‘¼
t

â€¦

inversion techniques are widely used to reconstruct subsurface physical properties (e.g., velocity, conductivity) from surface-based geophysical
measurements (e.g., seismic, electric/magnetic
(em) data). the problems are governed by partial differential equations (pdes) like the wave
or maxwellâ€™s equations. solving geophysical inversion problems is challenging due to the illposedness and high computational cost. to alleviate those issues, recent studies leverage deep
neural networks to learn the inversion mappings
from measurements to the property directly.

1. introduction

z

â€¦

arxiv:2204.13731v2 [cs.lg] 16 jun 2022

abstract

ğ‘ˆğ‘› = âˆ¬ ğ‘¢ ğ‘¥, ğ‘¡ Ï†ğ‘› ğ‘¥, ğ‘¡ ğ‘‘ğ‘¡ğ‘‘ğ‘¥

ğ‘Œğ‘š = âˆ¬ ğ‘¦ ğ‘¥, ğ‘§ Ïˆğ‘š ğ‘¥, ğ‘§ ğ‘‘ğ‘¥ğ‘‘ğ‘§

figure 1. illustration of the near-linear relation property between
geophysical measurements and properties after applying integral
transform. {Ï†n } and {Ïˆm } are two families of kernels for integral
transforms (e.g., sine and gaussian). here, the full waveform
inversion from seismic data to velocity map is used as an example.

geophysics inversion techniques are commonly used to characterize site geology, stratigraphy, and rock quality. these
techniques uncover subsurface layering and rock geomechanical properties (such as velocity, conductivity), which
are crucial in subsurface applications such as subsurface energy exploration, carbon capture and sequestration, groundwater contamination and remediation, and earthquake early
warning systems. technically, these subsurface geophysical properties can be inferred from geophysical measurements (such as seismic, electromagnetic (em)) acquired
on the surface. some underlying partial differential equations (pdes) between measurements and geophysical property exist, where inversion gets its name. for example,
velocity is reconstructed from seismic data based on full
waveform inversion (fwi) of a wave equation, while conductivity is recovered from em measurements based on em
inversion of maxwellâ€™s equations.
however, these inversion problems can be rather challenging to solve, as they are ill-posed. recent works study them
from two perspectives: physics-driven and data-driven. the
former approaches search for the optimal geophysical property (e.g., velocity) from an initial guess, such that the generated geophysical simulations based on the forward modeling
of the governing equation are closed to the real measurements (virieux & operto, 2009; feng & schuster, 2019;
feng et al., 2021). these methods are computationally expensive as they require iterative optimization per sample.

published as a conference paper at icml 2022

the latter methods (i.e., data-driven approaches) (wu & lin,
2019), inspired by the image-to-image translation task, employ encoder-decoder convolution neural networks (cnn)
to learn the mapping between physical measurements and
geophysical properties. deep network architecture that involves multiple convolution blocks is employed as both
encoder and decode, which also results in heavy reliance on
data and very high computational cost in training.

better) performance on two geophysics inversion problems
(seismic full waveform inversion and electric/magnetic inversion) over four datasets (kimberlina leakage (jordan &
wagoner, 2017), marmousi (feng et al., 2021), salt (yang
& ma, 2019), and kimberlina-reservoir (alumbaugh et al.,
2021)), but uses significantly less parameters than prior
works. for instance, on marmousi, our model only needs
1/20 parameters, compared to previous inversionnet.

in this paper, we found an intriguing property of geophysics
inversion that can significantly simplify data-driven methods
as:

2. background

geophysical measurements (e.g., seismic data) and
geophysical property (e.g., velocity map) have nearlinear relationship in high dimensional space after
integral transform.

âˆ‡2 p(r, t) âˆ’

let u(x, t) denote a spatio-temporal geophysical measurement along horizontal x and time t dimensions, and y(x, z)
denote a 2d geophysical property along horizontal x and
depth z. since, in practice, geophysical measurement is
mostly collected at the surface, and people want to invert
the subsurface geophysical property, measurement u only
contains spatial variable x, while property y includes (x, z).
as illustrated in figure 1, the proposed property can be
mathematically represented as follows:
zz
u = [u1 , . . . , un ]t , un =
u(x, t)Ï†n (x, t)dxdt,
zz
y = [y1 , . . . , ym ]t , ym =
y(x, z)Ïˆm (x, z)dxdz,
y â‰ˆ a u,

the governing equation of the seismic full waveform inversion is acoustic wave equation (schuster, 2017),

(1)

where Ï†n and Ïˆm are kernels for integral transforms. after
applying integral transforms, both geophysical measurement
u(x, t) and property y(x, z) are projected into high dimensional space (denoted as u and y ), and they will have a
near-linear relationship (y â‰ˆ a u ). note that the kernels
({Ï†n }, {Ïˆm }) are not learnable, but well-known analytical
kernels like sine, fourier, or gaussian.
interestingly, this intriguing property can significantly simplify the encoder-decoder architecture in data-driven methods. the encoder only contains the integral with kernel
{Ï†n } followed by a linear layer with weight matrix a in
eq. 1. the decoder just uses a single transformer (vaswani
et al., 2017) block followed by a linear projection to reverse
the integral with kernels {Ïˆm }. this results in a much shallower architecture. in addition, the encoder and decoder are
learnt separately. the matrix a in encoder can be directly
solved by pseudo inverse and is frozen afterward. only the
transformer block and following linear layer in the decoder
are learnt via sgd based optimizer.
our method, named invlint (inversion via linear relationship between integrals), achieves comparable (or even

1 âˆ‚2
p(r, t) = s(r, t),
c2 (r) âˆ‚t2

(2)

where r = (x, z) represents the spatial location in cartesian
coordinates (x is the horizontal direction and z is the depth),
t denotes time, c(r) is the velocity map, p(r, t) represents
the pressure wavefield, âˆ‡2 is the laplacian operator, and
s(r, t) is the source term that specifies the location and time
history of the source.
for the em forward modeling, the governing equation is the
maxwellâ€™s equations (commer & newman, 2008),
Ïƒe âˆ’ âˆ‡ Ã— h

= âˆ’j,

âˆ‡ Ã— e + iÏ‰Âµ0 h

= âˆ’m,

(3)

where e and h are the electric and magnetic fields. j and
m are the electric and magnetic sources. Ïƒ is the electrical
conductivity and Âµ0 = 4Ï€ Ã— 10âˆ’7 Ï‰ Â· s/m is the magnetic
permeability of free space.

3. methodology
in this section, we use seismic full waveform inversion (from
seismic data to velocity) as an example to illustrate our
derivation of the linear property after integral transforms.
we will also show the encoder-decoder architecture based
on this linear property. empirically, our solution is also
applicable to em inversion (from em data to conductivity).
3.1. near-linear relationship between integral
transformations
in the following part, we will show the seismic data and
velocity maps have the near-linear relation after integral
transformation like the format of equation 1. the seismic
data p and velocity map c are governed by the wave equation (equation 2). note that seismic data p and velocity map
c in wave equation corresponds to the input u and output y
in equation 1, respectively.
rewriting wave equation in fourier series: similar to
constant coefficients pdes, we assume spatial variable

published as a conference paper at icml 2022

r = (x, z) and temporal variable t are separable, i.e.,
p(x, z, t) = p1 (x, z)p2 (t), and s(x, z, t) = s1 (x, z)s2 (t).
thus, equation 2 is rewritten as
c2 (x, z)(âˆ‡2 p1 (x, z)p2 (t) âˆ’ s1 (x, z)s2 (t))
=

âˆ‚2
(p1 (x, z)p2 (t)).
âˆ‚t2

1
4Ï€ 2 n2

z

c2 (x, z) âˆ‡2 p1 (x, z)bn âˆ’ s1 (x, z)gn dx
z=0
zz
2
â‰ˆ
c (x, z)fn (x, z)dxdz,
(8)

(4)
where fn (x, z) is the kernel function.

next the temporal parts p2 (t) and s2 (t) are represented
pn
j2Ï€nt
as fourier series: p2 (t) =
and s2 (t) =
n=1 bn e
pn
j2Ï€nt
g
e
.
this
turns
equation
4
as:
n=1 n
n
x

c2 (x, z)(âˆ‡2 p1 (x, z)bn âˆ’ s1 (x, z)gn )ej2Ï€nt

n=1

=

n
x

4Ï€ 2 n2 p1 (x, z)bn ej2Ï€nt .

(5)

n=1

to make sure both p
sides have the same coefficient for each
n
n, the aggregation n=1 and ej2Ï€nt can be removed from
equation 5 as:
c2 (x, z)(âˆ‡2 p1 (x, z)bn âˆ’ s1 (x, z)gn )
= 4Ï€ 2 n2 p1 (x, z)bn .

(6)

by further integrating over x, we have
z
1
c2 (x, z) âˆ‡2 p1 (x, z)bn âˆ’ s1 (x, z)gn dx,
4Ï€ 2 n2
z
= p1 (x, z) |bn | dx,
ï£¦
ï£¦
ï£¦z z
ï£¦
ï£¦
ï£¦
âˆ’j2Ï€nt
ï£¦
=ï£¦
p1 (x, z)p2 (t) e
dtdxï£¦
(7)
ï£¦,
{z
} | {z }
|
ï£¦
ï£¦
seismic data f ourier kernel

where |rÂ· | is the modulus operator of complex numbers and
bn = p2 (t)eâˆ’j2Ï€nt dt are the fourier coefficients. note
that since bn and gn are complex numbers, we take module
on both sides. here, taking the real or imaginary part, rather
than modulo, does not affect our conclusions. now, the
right hand of equation 7 is in the same format with un in
equation 1. the kernel function Ï†n (x, t) = eâˆ’j2Ï€nt 1(x),
where 1(x) = 1 for all x.
approximation by integral over z: in reality the seismic
data p(x, z, t) is mostly collected at the surface (z = 0).
thus, the right-hand side of equation 7 is computable at
z = 0. however, the left-hand side is hard to calculate, since
âˆ‡2 p1 (x, z) and s1 (x, z) are unknown. here, we hypothesize that the left-hand side at z = 0 can be approximated by
leveraging velocity map at multiple depth positions as:

this hypothesisrr(eq. 7â€“8) bridges integral transforms of the
seismicrrdata ( p(t, x, z)eâˆ’j2Ï€nt dtdx|z=0 ) and velocity
maps (r c2 (x, z)fn (x, z)dxdz) via an auxiliary function
1
c2 (x, z) âˆ‡2 p1 (x, z)bn âˆ’ s1 (x, z)gn dx|z=0 .
4Ï€ 2 n2
it has two parts: (a) the double integral of velocity
maps equals the auxiliary function, and (b) the 2d kernel
fn (x, z) can be estimated by a set of basis functions, so
we can further calculate the inverse problem we want to
solve. the existence of fn (x, z) to achieve the former
equality can be validated by a special case fn (x, z) =
1
2
4Ï€ 2 n2 âˆ‡ p1 (x, z)bn âˆ’ s1 (x, z)gn Î´(z) where Î´(z) is an
impulse function. the latter may weaken the former assertion of equality, but the misfit is likely small, as velocity
map is continuous at most (x, z) positions and seismic data
p1 (x, z) and source s1 (x, z) in the auxiliary function has
strong correlation along x and z. our experimental results
over three datasets empirically validate this hypothesis.
further simplification by a single kernel family: as discussed above, we simplify fn (x, z) as a weighted sum of a
series of basis functions:
fn (x, z) =

m
x

dn,m Ïˆm (x, z),

(9)

m=1

where dn,m is the weight and Ïˆm (x, z) is the basis function.
by further plugging equations 8 and 9 into equation 7, we
get
m
x

zz
dn,m

c2 (x, z)Ïˆm (x, z)dxdz

m=1

zz
â‰ˆ

p(x, z, t)eâˆ’j2Ï€nt dtdx

.

(10)

z=0

relation to equation 1: equation 10 is special case of
equation 1 we can, therefore, express equation 10 in the
form of equation 1 by letting:
y(x, z) = c2 (x, z),
a = dâ€  ,

u(x, t) = p(x, t),
Ï†n (x, t) = eâˆ’j2Ï€nt 1(x),

where a is pseudo inverse of matrix d and d =
[dn,m ]n Ã—m is the matrix format.
in particular, u = [u1 , . . . , um ]t and y = [y1 , . . . , yn ]t
are the high dimensional embeddings of the measurement

published as a conference paper at icml 2022

input:
ğ‘¢ ğ‘¥, ğ‘¡

output:
ğ‘¦ ğ‘¥, ğ‘§
ğ‘¼ = âˆ¬ ğ‘¢Ï†ğ‘› ğ‘‘ğ‘¡ğ‘‘ğ‘¥ 1:ğ‘

ğ’€ = ğ´ğ‘¼

sine kernels
linear layer
{Ï†ğ‘› ğ‘¥, ğ‘¡ }
ğ´
encoder

transformer Ã— 1
âˆ’1 ğ‘¥, ğ‘§ }
{Ïˆğ‘š
decoder

figure 2. schematic illustration of our proposed method, using
seismic fwi as an example. the linear regression for two transformed embeddings is solved by pseudo inverse and is frozen
afterward. the decoder is trained via sgd-based optimizer.

and geophysical property. {Ï†n } is chosen as cosine/sine or
fourier transform; while, based on the experiments, gaussian kernel becomes our choice of the {Ïˆm } to embed the
spatial information in the geophysics property. it is true that
the hypothesis may seem strong, however, its validity can
be supported via our extensive experimental results using
multiple datasets and various pdes.
3.2. simplified encoder-decoder architecture
based on the proposed mathematical property as shown in
equation 1, we can easily design a simple network architecture, accordingly. the encoder plays exactly the same
role as the right-hand side of equation 1, while the decoder,
with a neural network, approximates the inverse mapping of
the integral transformation (Ïˆâˆ’1
m (x, z)). the structure (figure 2) of our invlint is described below.
encoder: as illustrated in figure 2, we design the encoder
exactly the same to equation 1, where an integral transformation with kernel {Ï†n }, n âˆˆ [1, n ] is first implemented
and followed by a linear layer represented by a. with such
a simple linear relation, one can easily map the input measurement to the embedding of the output.
decoder: there are many kernel functions (like gaussian
kernel), which does not have a close form inverse transformations. instead, we use a shallow decoder network to
approximate such a pseudo-inverse. to achieve this, we first
use a linear layer l1 to map y to a more compact embedding and tile it a grid with the shape rhÃ—wÃ—k . here, h and
w are the size of the velocity map with 32 times downsampling, and k is the number of channels. after that, l1 (y ) is
input into a 1-layer transformer, with patch size of 1 Ã— 1 Ã— k.
this shallow transformer is the only nonlinear part of our
decoder.
the last parts of the model are a linear layer lr . it upsamples each 1 Ã— 1 Ã— k patch to a (32 + d) Ã— (32 + d) block1 ,
where d is a small integer. the final predicted velocity map cÌ‚
can be construed by stitching all h Ã— w blocks together. the
1

since the size of output may not be divided exactly by 32, the
recovered shape will be slightly different for different datasets.

purpose of this is to recover the output to the original shape
with overlaps among blocks to remove the block effect.
3.3. training
because of the near-linear relation, we can easily solve
the linear layer in the encoder, l1 , with the least squares
method. specifically, we first compute the embedding of
both encoder and decoder by integral transformations, calculate the solution of matrix a, and freeze it while training the
decoder. the decoder is trained by an sgd-based optimizer.
the loss function of our invlint is a pixel-wise mae loss
given as
l(cÌ‚, c) = `1 (cÌ‚, c).

(11)

peng et al. (jin et al., 2022) find that combining mae,
mse, and perceptual loss together is helpful to improve the
performance. however, to make a fair comparison with the
previous work, we only use mae as our loss function.

4. experiment
in this section, we present experimental results of our proposed invlint evaluated on four datasets and compare
our method with the previous works, inversionnet (wu &
lin, 2019) and velocitygan zhang et al. (2019). we also
discuss different factors that affect the performance of our
method.
4.1. implementation details
4.1.1. datasets
in experiments, we verify our method on four datasets, of
which three of them are used for seismic fwi, and one of
which is for an em inversion.
kimberlina-leakage: the geophysical properties were
developed under doeâ€™s national risk assessment program (nrap). it contains 991 co2 leakage scenarios, each
simulated over a duration of 200 years, with 20 leakage
velocity maps provided (i.e., at every ten years) for each scenario (jordan & wagoner, 2017). excluding some missing
velocity maps, the data are split as 807/166 scenarios for
training and testing, respectively. the size of the velocity
maps is 401 Ã— 141 grid points, and the grid size is 10 meters in both directions. to synthesize the seismic data, nine
sources are evenly distributed along the top of the model,
with depths of 5 m. the seismic traces are recorded by
101 receivers positioned at each grid with an interval of 15
m. the source frequency is 10 hz. each receiver collects
1251-timestep data for 1 second.
marmousi: we apply the generating method in jin et al.
(2022), which follows feng et al. (2021) and adopts the marmousi velocity map as the style image to construct this low-

published as a conference paper at icml 2022

dataset
kimberlina
leakage
marmousi

salt
kimberlinareservoir

model
inversionnet (wu & lin, 2019)
velocitygan zhang et al. (2019)
invlint (ours)
inversionnet (wu & lin, 2019)
velocitygan zhang et al. (2019)
invlint (ours)
inversionnet (wu & lin, 2019)
velocitygan zhang et al. (2019)
invlint (ours)
inversionnet (wu & lin, 2019)
velocitygan zhang et al. (2019)
invlint (ours)

maeâ†“
9.43
9.73
8.13
149.67
124.62
136.67
25.98
332.62
24.60
0.01330
0.01313
0.00703

mseâ†“
1086.99
1026.27
1534.60
45936.23
30644.31
36003.43
8669.98
145669.11
8840.79
0.000855
0.000688
0.000537

ssimâ†‘
0.9868
0.9863
0.9812
0.7889
0.8642
0.7972
0.9764
0.7760
0.9742
0.9175
0.8611
0.9370

#parameters
15.81m
16.99m
1.49m (9.4%)
24.41m
25.59m
1.45m (5.9%)
13.74m
14.92m
1.62m (11.8%)
0.30m
1.48m
0.16m (53.3%)

flops
563.52m
1.31g
44.30m (7.9%)
189.58m
259.49m
9.31m (4.9%)
32.37m
65.98m
5.98m (18.5%)
1.20g
3.95g
96.10m (8.0%)

table 1. quantitative results evaluated on four datasets in terms of mae, mse and ssim, the number of parameters and flops. the
percentages indicate the ratio of #parameters (flops) required by invlint to that required by inversionnet. our invlint achieves
comparable (or even better) inversion accuracy comparing to the inversionnet and velocitygan with a much smaller number of
parameters and flops.

resolution dataset. this dataset contains 30k with paired
seismic data and velocity map. 24k samples are set as the
training set, 3k samples are used as the validation set, and
the rest are the testing set. the size of the velocity map is
70 Ã— 70, with the 10-meter grid size in both directions. the
velocity ranges from 1, 500m/s to 4, 700m/s. there are
s = 5 sources placed evenly with a spacing of 170 m. the
source frequency is 20 hz. the seismic data are recorded by
70 receivers with a receiver interval of 10 m. each receiver
collects 1,000-timestep data for 1 second.
salt: the dataset contains 140 velocity maps (yang & ma,
2019). we downsample it to 40 Ã— 60 with a grid size of
10 m, and the splitting strategy 120/10/10 is applied. the
velocity ranges from 1, 500m/s to 4, 500m/s. there are
also s = 5 sources used, with 12-hz source frequency
and a spacing of 150 m. the seismic data are recorded by
60 receivers with an interval of 10 m, too. each receiver
collects 600-timestep data for 1 second.
kimberlina-reservoir: the geophysical properties were
also developed under doeâ€™s nrap. it is based on a potential
co2 storage site in the southern san joaquin basin of
california (alumbaugh et al., 2021). we use this dataset to
test our method in the em inversion problem. in this data,
there are 780 em data as the geophysical measurement, and
corresponding conductivity as the geophysical property. we
use 750/30 as training and testing. em data are simulated
by finite-difference method (commer & newman, 2008)
with two sources location at x = 2.5 km, z = 3.025 km
and x = 4.5 km, z = 2.5 km. there are s = 8 source
frequencies from 0.1 to 8.0 hz and recorded with its real
and imaginary part. the conductivity is with the size of
351Ã—601 (h Ã—w ), where the grid is 10 m in all dimensions.

4.1.2. t raining d etails
the input seismic data and em data are normalized to the
range [-1, 1]. in practice, to supply more information, it always uses multiple sources to measure, where s âˆˆ [1, Â· Â· Â· s]
is the index of different sources. after integration, all
sources vectors will be concatenated. for the seismic data,
we use Ï†n (x, t) = sin(nÏ€t)1(x)/(xmax âˆ’ xmin ) as the
kernel function. however, for the em data, since the raw
data are already in the frequency domain and the input size
is small, we skip the integral transformation step.
the gaussian kernel can be represented as Ïˆm (x, z) =
âˆ’k(x,z)âˆ’Âµm k22
exp
. we let Âµm distribute evenly over the
2Ïƒ 2
output shape. then, the Ïƒ is set equal to the distance of adjacent Âµ. when applying ridge regression to solve the linear
layer in the encoder, and set the regularization parameter
Î± = 1.
we employ adamw (loshchilov & hutter, 2018) optimizer
with momentum parameters Î²1 = 0.5, Î²2 = 0.999 and a
weight decay of 1 Ã— 10âˆ’4 to update decoder parameters of
the network. the initial learning rate is set to be 1 Ã— 10âˆ’3 ,
and we decay the learning rate with a cosine annealing
(loshchilov & hutter, 2016), where t0 = 5, tmult = 2 and
the minimum learning rate is set to be 1 Ã— 10âˆ’3 . the size of
every mini-batch is set to be 128. we implement our models
in pytorch and train them on 1 nvidia tesla v100 gpu.
4.1.3. e valuation m etrics
we apply three metrics to evaluate the geophysical properties generated by our method: mae, mse and structural
similarity (ssim). in the existing literature (wu & lin,
2019; zhang & lin, 2020), both mae and mse have been
employed to measure the pixel-wise error. ssim is also con-

published as a conference paper at icml 2022

sidered to measure the perceptual similarity (jin et al., 2022),
since both velocity maps and conductivity have highly structured information, and degradation or distortion can be easily perceived by a human. note that when calculating mae
and mse, we denormalize geophysical properties to their
original scale while we keep them in the normalized scale
[âˆ’1, 1] for ssim according to the algorithm.
moreover, we also employ two common metrics to measure the complexity and computational cost of the model:
the number of parameters (#parameters) and floating-point
operations per second (flops).
4.2. main results
table 1 shows the comparison results of our method with
inversionnet on different datasets. overall, our method
achieves comparable or even better performance with a
smaller amount of parameters and lower flops. below, we
will provide in detail the comparison of all four datasets. it
may be worthwhile mentioning that fwi is a quantitative
inversion technique, meaning that it will yield both the shape
and the quantitative values of the subsurface property.
results on kimerlina-leakage: compared to inversionnet and velocitygan, our method outperforms in mae,
slightly worse in mse and ssim. however, our invlint
only needs less than 1/10 parameters and flops. this
demonstrates the power of our model, and further validates
the properties we found. the velocity maps inverted by
ours and inversionnet are shown in the first two rows of
figure 3. in the second example, despite of some noise
produced by our method in the background, the co2 leakage plume (most important region as boxed out in green)
has been very well imaged. compared to ground truth,
our method yields even better quantitative values than that
obtained by inversionnet.
results on marmousi: marmousi is a more challenging
dataset due to its more complex structure. compared to inversionnet, our method outperforms in all three metrics with
significantly less computational and memory cost (about
1/20 parameters and flops). this result again demonstrates not only the power of our model but also the validity
of the near-linear relationship that we found. however, in
such a large and complex dataset, velocitygan outperform
others, where the gan structure helps generating better results. the velocity maps inverted by ours and inversionnet
are illustrated in the third and fourth rows of figure 3. our
invlint and inversionnet perform comparably in both the
shallow and deep regions compared to the ground truth.
results on salt: compared to inversionnet, our method
outperforms in mae, and is slightly worse in mse and
ssim with a very small gap. moreover, our method uses
1/8 parameters and 1/5 flops to those of inversionnet.

note that, in this challenging dataset, which only has a small
number of samples, velocitygan cannot converge well and
yields bad results. this is a side effect of its complex structure. the velocity maps inverted by ours and inversionnet
are illustrated in the fifth and sixth rows of figure 3. consistent with quantitative results, both methods generate similar
results. in the shallow region, our method output a slightly
clear structure; but in a deeper region (e.g., the red region in
the first example), the output of inversionnet is a little close
to the ground truth. however, the overall difference can
be hard to distinguish. our method achieves comparable
results with much less complexity.
results on kimberlina-reservoir: compared to inversionnet and velocitygan, our method outperforms in all
three metrics, with 1/2 parameters and 1/12 flops to those
of inversionnet. because of the compact input, all model
utilize the much smaller number of parameters. however,
due to the simple architecture, invlint yields significantly
fewer flops but achieves better inversion accuracy. the
conductivity results inverted by different models are shown
in the last two rows of figure 3. contrary to previous results
on the kimberlina-leakage dataset, our model yields clearer
results. in the first example, we can see that the outputs of
our model are less noisy; and in the second case, invlint
inverts the deep region more precisely, as highlighted by
the red squares. this is also consistent with the quantitative
results.
at the same time, we find that the number of parameters of
our model varies less for the same inverse problem. the
number of model parameters is relatively independent of
data size. in contrast, the previous methods are greatly
affected by the input and output sizes. moreover, our model
not only requires fewer parameters, but also enables more
efficient training and inference. when training on marmousi
dataset using 1 gpu (nvidia quadro rtx 8000), our
model is 9 times faster than inversionnet/velocitygan (1
hour vs. 9 hours). we also tested inference runtime with
batch size 1 on a single thread of an intel(r) xeon(r) cpu
gold 6248 v3 (2.5ghz). our model is 16 times faster than
inversionnet/velocitygan (5 ms vs. 80ms). the small
model size is suitable for memory-limited mobile devices.
more visualization results are provided in the appendix for
readers who might be interested.
4.3. ablation tests
in this part, we will test how different kernel functions and
network architectures will influence the performance of our
method. we put our default setting at the first row of each
table. for ease of illustration, we only provide results on
marmousi. results on kimberlina leakage are given in the
appendix.
different encoder kernels

published as a conference paper at icml 2022
ground truth

inversionnet

invlint (ours)

kimberlina leakage

first, we conduct experiments by replacing the 1d sine
kernel in the encoder to different 2d sine kernels. the quantitative results are shown in the table 2. by comparing the
results in marmousi and the results in kinberlina-leakage
(shown in the appendix), we can see that the optimal strategy to integrate over x axis is distinct for different datasets.
in marmousi, using kernel sin(nÏ€t) cos(nÏ€x) can improve
the performance a lot. this kernel, however, does not perform well on other datasets (e.g., kimberlina-leakage).
dataset

marmousi

encoder kernel
sin(nÏ€t)1(x)/(xmax âˆ’ xmin )
sin(nÏ€t) sin(nÏ€x)
sin(nÏ€t) cos(nÏ€x)
marmousi
cos(nÏ€t) sin(nÏ€x)
sin(nÏ€(x + t))
sin(nÏ€t) + sin(nÏ€x)

maeâ†“
136.67
138.76
128.33
140.14
141.58
142.12

mseâ†“
36003.43
37648.80
32451.22
38417.23
38383.58
38261.56

ssimâ†‘
0.7972
0.8042
0.8115
0.8031
0.7892
0.7884

table 2. quantitative results for different encoder kernel.

different decoder kernels

salt

then, we test different kernels for geophysical properties. in
particular, we evaluate a series of 2d kernels: different 2d
sine kernels, a sinc function kernel (sin(Ï€kr âˆ’ Âµm k2 )/kr âˆ’
Âµm k2 ), and a gaussian kernel with a smaller variance, noted
as gaussianÏƒ. for the sinc function, the choice of Âµm is
the same as the gaussian kernel, while for gaussianÏƒ, we
choose Ïƒ as 1/3 of the original. the quantitative results
are shown in table 3. as we can see, our choice of kernel
outperforms the rest kernels. a smaller variance of gaussian
will yield a slightly worse result, while sinc kernel performs
similarly to the gaussianÏƒ.
dataset

marmousi

kimberlina-reservoir

decoder kernel
gaussian
sinc
gaussianÏƒ
sin(nÏ€x) sin(nÏ€z)
cos(nÏ€x) sin(nÏ€z)
sin(nÏ€x) cos(nÏ€z)
sin(nÏ€(x + z))
sin(nÏ€x) + sin(nÏ€z)

maeâ†“
136.67
138.02
138.19
177.36
165.38
175.92
209.47
216.12

mseâ†“
36003.43
36534.44
36579.46
56102.75
49463.79
55424.26
74167.16
78496.77

ssimâ†‘
0.7972
0.7952
0.7954
0.7455
0.7491
0.7376
0.7057
0.7030

table 3. quantitative results for different decoder kernel.

different number of kernels

figure 3. illustration of results evaluated on four datasets

we also test different numbers of kernels for both sine and
gaussian. we evaluate performance over a 6Ã—6 grid where
the dimensions of u and y vary from 128 to 4096. the
quantitative results are shown in figure 4. results indicate that the current selection of dimensions is appropriate.
obviously, reducing the modelâ€™s size reduces its capacity,
while choices of higher dimension are more prone to overfit. however, choosing a small dimension yields a smaller
number of parameters and flops. one can easily balance
the performance and the cost based on his requirements and
resources, indicating the flexibility of our model.
different decoder architectures

published as a conference paper at icml 2022
dataset
marmousi

architecture
1 layer transformer
multi-linear
2 layers transformer
3 layers transformer

maeâ†“
136.67
138.82
134.24
132.19

mseâ†“
36003.43
36801.89
35111.23
34502.25

ssimâ†‘
0.7972
0.7939
0.8002
0.8037

table 5. quantitative results for a larger decoder.

4.4. singular value analysis
figure 4. performance over dimensions of u and y .

we aim to design an effective and efficient decoder to reverse the integral transform over a velocity map. the shifted
gaussian kernels used in integral transform split the velocity
map into overlapping windows and encode the local structure within each window. to reconstruct the global structure
of the velocity map from these local features, we leverage
the transformerâ€™s power in modeling long-range interaction
in a single layer. options like conv/deconv require more
layers to cover long range.
to better illustrate this, we test the performance of different
decoder architecture. results are provided in table 4. a
transformer layer followed by a linear layer is a more accurate decoder than shallow conv/deconv layers. deeper decoders with more conv/deconv layers achieve more accurate
results, but require a larger model. when using the deconv
decoder of inversionnet in our method, we achieve better
performance, clearly outperforming inversionnet (mae
126.6 vs. 149.7).

another major benefit of our simplified model is the ease
of analysis. since we use only one linear layer in the encoder, we can analyze it by performing singular value decomposition. the results are shown in figure 5. since the
singular value varies greatly in different datasets, we divide
it by its maximum value to normalize it and trunk it at 150
dim. results indicate that for all datasets, the number of
essential dimensions is less than 100. in other words, a
100-dimensional latent space is sufficient to represent the
data. specifically, we can see that a ten-dimensional latent
space is enough for kimberlina-reservoir dataset. that
answers why the required number of parameters of both
our invlint and inversionnet on kimberlina-reservoir
datasets are much smaller than that on other datasets. all
in all, with such a simple architecture, our invlint is able
to not only help in analyzing the problem but also help us
quantify the difficulty of different datasets.

dataset

architecture
maeâ†“ mseâ†“ ssimâ†‘ #params flops
transformerÃ—1 + linear* 136.67 36003.43 0.7972 1.45m 9.3m
conv Ã—2 + linear
140.72 37345.58 0.7903 0.30m 9.2m
marmousi
deconv Ã—1 + linear
167.98 49728.14 0.7520 0.35m 10.8m
(deconv + conv) Ã—5
126.59 33830.73 0.8158 12.71m 94.6m
(up + conv) Ã—5
128.74 34854.78 0.8120 4.01m 56.7m

table 4. comparison among different decoder structures. (*) indicates the default decoder option.

results for a larger decoder
here, we evaluate our method with a larger/deeper decoder.
firstly, we test it using multiple unshared linear layers,
rather than a shared one lr1 , in the last part of our decoder. furthermore, we evaluate our model with a deeper
transformer. the quantitative results are shown in table 5.
the result using unshared linear layers indicates that a single
linear layer is enough and the model does not benefit from
more parameters. on the other hand, a deeper transformer
can improve the performance. similar to the number of
kernels, the balance is based on requirements.

figure 5. normalized singular value decomposition of the linear
layer on different datasets.

4.5. comparison to traditional fwi:
we performed new comparison with a widely used traditional fwi method (i.e., multiscale fwi (virieux & operto, 2009)) on three seismic fwi datasets (marmousi,
kimberlina-leakage, salt). our method is consistently
better on 3 datasets (mae: 11.7 vs. 42.0 in kimberlinaleakage, 140.7 vs. 199.5 in marmousi, 26.1 vs. 176.6
in salt). the traditional fwi requires a good initial guess

published as a conference paper at icml 2022

and optimization per sample, resulting in slow processing
(e.g., 4 hours per sample in kimberlina-leakage). due to
the limited rebuttal duration, we ran the comparison over 5
samples per dataset.

experiments show that this interesting property holds for
two geophysics inversion problems over four different
datasets. compared to much deeper inversionnet, our
method achieves comparable accuracy, but consumes significantly fewer parameters.

5. related works
5.1. data-driven methods for fwi
recently, based on deep learning, a new type of method has
been developed. araya-polo et al. (2018) use a fully connected network to invert velocity maps in fwi. wu & lin
(2019) consider the fwi as an image-to-image translation
problem, and employ encoder-decoder cnn to solve. by
using generative adversarial networks (gans) and transfer
learning, zhang et al. (2019) achieved improved performance. in zeng et al. (2021), authors present an efficient
and scalable encoder-decoder network for 3d fwi. feng
et al. (2021) develop a multi-scale framework with two
convolutional neural networks to reconstruct the low- and
high-frequency components of velocity maps. a thorough
review on deep learning for fwi can be found in adler et al.
(2021).
5.2. physics-informed machine learning
previous pure data-driven methods can be considered as
incorporating physic information in training data. on the
other hand, integrating the physic knowledge into loss function or network architecture is another direction. all of them
are called physics-informed neural networks (pinn). raissi
et al. proposed utilizing nonlinear pdes in the loss function
as a soft constrain (raissi et al., 2019). through a hard
constraint projection, chen et al. proposed a framework
to ensure modelâ€™s predictions strictly conform to physical
mechanisms (chen et al., 2021). based on the universal
approximation theorem of operators, in lu et al. (2021),
authors proposed deeponet to learn continuous operators
or complex systems. sun et al. (2021) proposed a hybrid network design, which involves deterministic, physics-based
modeling and data-driven deep learning. a comprehensive
review of pinn can be found in karniadakis et al. (2021).

6. conclusion
in this paper, we find an intriguing property of geophysics
inversion: a near-linear relationship between the input and
output, after applying integral transform in high dimensional space. furthermore, this property can be easily turned
into a light-weight encoder-decoder network for inversion.
the encoder contains the integration of seismic data and
the linear transformation without fine-tuning. the decoder
consists of a single transformer block to reverse the integral
of velocity with gaussian kernels.

references
adler, a., araya-polo, m., and poggio, t. deep learning
for seismic inverse problems: toward the acceleration of
geophysical analysis workflows. ieee signal processing
magazine, 38(2):89â€“119, 2021.
alumbaugh, d., commer, m., crandall, d., gasperikova,
e., feng, s., harbert, w., li, y., lin, y., manthila samarasinghe, s., and yang, x. development of a multiscale synthetic data set for the testing of subsurface co2
storage monitoring strategies. in american geophysical
union (agu), 2021.
araya-polo, m., jennings, j., adler, a., and dahlke, t.
deep-learning tomography. the leading edge, 37(1):
58â€“66, 2018.
chen, y., huang, d., zhang, d., zeng, j., wang, n., zhang,
h., and yan, j. theory-guided hard constraint projection
(hcp): a knowledge-based data-driven scientific machine
learning method. journal of computational physics, 445:
110624, 2021.
commer, m. and newman, g. a. new advances in
three-dimensional controlled-source electromagnetic inversion. geophysical journal international, 172(2):513â€“
535, 2008.
feng, s. and schuster, g. t. transmission+ reflection
anisotropic wave-equation traveltime and waveform inversion. geophysical prospecting, 67(2):423â€“442, 2019.
feng, s., fu, l., feng, z., and schuster, g. t. multiscale
phase inversion for vertical transverse isotropic media.
geophysical prospecting, 69(8-9):1634â€“1649, 2021.
jin, p., zhang, x., chen, y., huang, s. x., liu, z., and lin, y.
unsupervised learning of full-waveform inversion: connecting cnn and partial differential equation in a loop.
in proceedings of the tenth international conference on
learning representations (iclr), 2022.
jordan, p. and wagoner, j. characterizing construction of
existing wells to a co2 storage target: the kimberlina site,
california. technical report, national energy technology
laboratory (netl), pittsburgh, pa, morgantown, wv ,
2017.
karniadakis, g. e., kevrekidis, i. g., lu, l., perdikaris,
p., wang, s., and yang, l. physics-informed machine
learning. nature reviews physics, 3(6):422â€“440, 2021.

published as a conference paper at icml 2022

loshchilov, i. and hutter, f. sgdr: stochastic gradient descent with warm restarts.
arxiv preprint
arxiv:1608.03983, 2016.
loshchilov, i. and hutter, f. decoupled weight decay regularization. in sixth international conference on learning
representations (iclr), 2018.
lu, l., jin, p., pang, g., zhang, z., and karniadakis, g. e.
learning nonlinear operators via deeponet based on the
universal approximation theorem of operators. nature
machine intelligence, 3(3):218â€“229, 2021.
raissi, m., perdikaris, p., and karniadakis, g. e. physicsinformed neural networks: a deep learning framework for
solving forward and inverse problems involving nonlinear
partial differential equations. journal of computational
physics, 378:686â€“707, 2019.
schuster, g. t. seismic inversion. society of exploration
geophysicists, 2017.
sun, j., innanen, k. a., and huang, c. physics-guided
deep learning for seismic inversion with hybrid training
and uncertainty analysis. geophysics, 86(3):r303â€“r317,
2021.
vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones,
l., gomez, a. n., kaiser, l., and polosukhin, i. attention is all you need. in guyon, i., luxburg, u. v.,
bengio, s., wallach, h., fergus, r., vishwanathan, s.,
and garnett, r. (eds.), advances in neural information
processing systems, volume 30. curran associates, inc.,
2017.
virieux, j. and operto, s. an overview of full-waveform
inversion in exploration geophysics. geophysics, 74(6):
wcc1â€“wcc26, 2009.
wu, y. and lin, y. inversionnet: an efficient and accurate
data-driven full waveform inversion. ieee transactions
on computational imaging, 6:419â€“433, 2019.
yang, f. and ma, j. deep-learning inversion: a nextgeneration seismic velocity model building method.
geophysics, 84(4):r583â€“r599, 2019.
zeng, q., feng, s., wohlberg, b., and lin, y. inversionnet3d: efficient and scalable learning for 3d full waveform inversion. arxiv preprint arxiv:2103.14158, 2021.
zhang, z. and lin, y. data-driven seismic waveform inversion: a study on the robustness and generalization.
ieee transactions on geoscience and remote sensing,
58(10):6900â€“6913, 2020.
zhang, z., wu, y., zhou, z., and lin, y. velocitygan:
subsurface velocity image estimation using conditional

adversarial networks. in 2019 ieee winter conference
on applications of computer vision (wacv), pp. 705â€“
714. ieee, 2019.

published as a conference paper at icml 2022
ground truth

a. appendix

inversionnet

invlint (ours)

a.1. inversion results of different datasets
ground truth

inversionnet

invlint (ours)

figure 7. illustration of results evaluated on marmousi

figure 6. illustration of results evaluated on kimberlina leakage

published as a conference paper at icml 2022
ground truth

inversionnet

invlint (ours)

figure 8. illustration of results evaluated on salt

ground truth

inversionnet

invlint (ours)

figure 9. illustration of results evaluated on kimberlina reservoir

published as a conference paper at icml 2022

a.2. ablation test on kimberlina leakage
the ablation test results on kimberlina leakage are shown
in table 6-10
dataset

encoder kernel
sin(nÏ€t)1(x)/(xmax âˆ’ xmin )
sin(nÏ€t) sin(nÏ€x)
kimberlina sin(nÏ€t) cos(nÏ€x)
leakage
cos(nÏ€t) sin(nÏ€x)
sin(nÏ€(x + t))
sin(nÏ€t) + sin(nÏ€x)

maeâ†“
8.13
11.07
8.88
10.95
8.17
8.10

mseâ†“
1534.60
3227.71
2015.23
3222.21
1751.89
1760.43

ssimâ†‘
0.9812
0.9783
0.9804
0.9782
0.9815
0.9817

table 6. quantitative results for different encoder kernel.

dataset

kimberlina
leakage

decoder kernel
gaussian
sinc
gaussianÏƒ
sin(nÏ€x) sin(nÏ€z)
cos(nÏ€x) sin(nÏ€z)
sin(nÏ€x) cos(nÏ€z)
sin(nÏ€(x + z))
sin(nÏ€x) + sin(nÏ€z)

maeâ†“
8.13
8.90
8.84
15.41
15.40
15.37
12.86
13.21

mseâ†“
1534.60
2051.99
2042.94
7357.48
7349.02
7252.45
4721.48
74719.95

ssimâ†‘
0.9812
0.9789
0.9790
0.9764
0.9764
0.9765
0.9767
0.9764

table 7. quantitative results for different decoder kernel.
dataset
kimberlina
leakage

#kernel
n=2048; m=512
n=1024; m=512
n=4096; m=512
n=2048; m=128
n=2048; m=1024

maeâ†“
8.13
8.63
8.29
8.76
8.59

mseâ†“
1534.60
1946.62
1780.75
2007.14
1898.95

ssimâ†‘
0.9812
0.9811
0.9808
0.9805
0.9808

table 8. quantitative results for different number of kernels.
dataset
kimberlina
leakage

architecture
transformerÃ—1 + linear*
conv Ã—2 + linear
deconv Ã—1 + linear
(deconv + conv) Ã—5
(up + conv) Ã—5

maeâ†“
8.13
13.42
21.32
6.86
6.87

mseâ†“
1534.60
2447.81
4919.03
1462.29
1516.80

ssimâ†‘
0.9812
0.9762
0.9648
0.9841
0.9840

table 9. quantitative results for different decoder architectures.
(*) indicates the default decoder option.
dataset
kimberlina
leakage

architecture
1 layer transformer
multi-linear
2 layers transformer
3 layers transformer

maeâ†“
8.13
8.14
8.24
8.09

mseâ†“
1534.60
1799.70
1781.50
1707.53

ssimâ†‘
0.9812
0.9811
0.9812
0.9813

table 10. quantitative results for a larger decoder.

a.3. regression results for the encoder linear layer
we also show here the regression results of the linear layer in
our encoder on different datasets in table 11. as a reference,
we also show the range and mean of the regression target
value as ymax -ymin , |ymean |. the result demonstrate that
how well the regressions are fitted.

dataset
set
kimberlina training set
leakage
test set
training set
marmousi
test set
training set
salt
test set
kimberlina training set
reservoir
test set

maeâ†“
2.83
4.63
4.26
4.29
0.28
0.48
169.25
212.64

mseâ†“
45.48
245.09
33.38
33.9
0.46
1.98
2607.27
109288.08

ymax âˆ’ ymin
884.4
885.27
107.05
103.01
51.3
49.35
26497.1
26496.956

|ymean |
490.93
492.65
4.92
5.1
12.11
11.97
7197.4873
6849.95

table 11. quantitative results for a larger decoder.



math geosci (2020) 52:53â€“79
https://doi.org/10.1007/s11004-019-09832-6
special issue

stochastic seismic waveform inversion using
generative adversarial networks as a geological prior
lukas mosser1 Â· olivier dubrule1 Â·
martin j. blunt1

received: 23 october 2018 / accepted: 30 september 2019 / published online: 15 november 2019
Â© the author(s) 2019

abstract we present an application of deep generative models in the context of partial
differential equation constrained inverse problems. we combine a generative adversarial network representing an a priori model that generates geological heterogeneities and
their petrophysical properties, with the numerical solution of the partial-differential
equation governing the propagation of acoustic waves within the earthâ€™s interior.
we perform bayesian inversion using an approximate metropolis-adjusted langevin
algorithm to sample from the posterior distribution of earth models given seismic
observations. gradients with respect to the model parameters governing the forward
problem are obtained by solving the adjoint of the acoustic wave equation. gradients of the mismatch with respect to the latent variables are obtained by leveraging
the differentiable nature of the deep neural network used to represent the generative
model. we show that approximate metropolis-adjusted langevin sampling allows an
efficient bayesian inversion of model parameters obtained from a prior represented by
a deep generative model, obtaining a diverse set of realizations that reflect the observed
seismic response.
keywords ill-posed problems Â· bayes Â· deep learning Â· earth modeling
mathematics subject classification 86a22 Â· 62f15 Â· 86a32

b lukas mosser

lukas.mosser15@imperial.ac.uk
olivier dubrule
o.dubrule@imperial.ac.uk
martin j. blunt
m.blunt@imperial.ac.uk

1

imperial college london, london, uk

123

54

math geosci (2020) 52:53â€“79

1 introduction
solving an inverse problem means finding a set of model parameters that best fit
observed data (tarantola 2005). the observed data or measurements are often noisy
and/or sparse, and therefore lead to an ill-posed inverse problem where numerous
realizations of the underlying model parameters may lead to a model response that
matches observed data (kabanikhin 2008). additionally, the model used to describe
how the observed data are generated, the so-called forward model, may be uncertain
(hansen and cordua 2017).
based on natural observations or an understanding of the underlying data generating process we may have a preconception about possible or impossible states of the
model parameters. we may formulate this knowledge as a prior probability distribution function (pdf) of our model parameters and use bayesian inference to obtain a
posterior pdf of the model parameters given the observations (tarantola 2005).
seismic inversion involves modeling the physical process of waves radiating
through the earthâ€™s interior (fig. 1). by comparing the simulated synthetic measurements to actual acoustic recordings of reflected waves, we can modify model
parameters and minimize the misfit between synthetic data and measurements. the
adjoint of the partial differential equation (pde) represents the gradient of the data
mismatch with respect to the parameters, leading to a gradient-based optimization of
the model parameters (plessix 2006). in the most general case, which has been used
in this study, these gradients are obtained by back-propagating the full wavefield in
time, an approach commonly referred to as full-waveform inversion (fwi). the set of
parameters represented by the spatial distribution of the acoustic velocity of the rocks
within the earth can easily exceed 106 values, depending on the resolution of the simulation grid and the observed data. large three-dimensional seismic observations may
require millions of parameters to be inverted for, demanding enormous computational
resources (akcelik et al. 2003).
for direct observations of the earthâ€™s interior, boreholes may have been drilled for
hydrocarbon exploration/development or hydrological measurements. these represent
a quasi-one-dimensional source of information of spatially sparse nature. typical
borehole sizes are on the order of tens of centimeters in diameter, whereas the lateral
resolution of seismic observations is usually on the order of tens of meters.
we can deduce prior knowledge of the earthâ€™s interior from observations of analog
outcrops or subsurface reservoirs. this geological knowledge can be incorporated
into prior distributions of physical properties of rocks, such as the acoustic p-wave
velocity, or into the distribution of geological features such as geological facies and
fault distributions within the earth.
efficient parameterizations (akcelik et al. 2002; kadu et al. 2016) that enable
a dimensionality-reduced representation of the high-dimensional parameter space of
possible models have been shown to reduce computational cost and increase spatial resolution. because of the high computational cost incurred by full-waveform inversion
(modrak and tromp 2015; akcelik et al. 2003), probabilistic ensembles of models that
match observed data are rarely generated, and often only a single model that satisfies
predefined quality criteria is created and used for interpretation and decision-making
processes.

123

math geosci (2020) 52:53â€“79

55

fig. 1 computational domain for the acoustic inversion problem. acoustic recording devices are placed
on the surface (âˆ‡) and record incoming acoustic waves reflected from geological structures emanating
from an artificial source (âˆ—). the computational domain is embedded within a dampened boundary domain
to emulate lateral and vertical dissipation of the wave-source. the generative model g Î¸ (z) creates the
underlying spatially distributed p-wave velocity. additional lower-dimensional constraints (dashed vertical
line representing a well) can be placed on the generative model, by incorporating loss terms. the vertical
axis of the computational domain has been rescaled by a factor of 10 for visualization purposes

we parameterize the earth model by a deep generative model that creates stochastic
realizations of possible model parameters. the probabilistic distribution of model
parameters is parameterized by a lower-dimensional set of multi-gaussian-distributed
latent variables. combined with a generative deep neural network, this represents a
differentiable prior on the possible model parameters. we combine this differentiable
generative model with the numerical solution of the acoustic wave equation to produce
synthetic acoustic observations of the earthâ€™s interior (louboutin et al. 2017). using the
adjoint method (plessix 2006), we compute a gradient of the mismatch between real
and synthetic data with respect to model parameters not only in the high-dimensional
model space, but also in the much smaller set of latent variables. these gradients
are required to perform a metropolis-adjusted langevin (mala) sampling of the
posterior of the model parameters given the observed seismic data. performing mala
sampling allows us to obtain a diverse ensemble of model parameters that match
the observed seismic data. additional constraints on the generative model, such as
information located at existing boreholes, are readily incorporated and included in the
mala sampling procedure.

123

56

math geosci (2020) 52:53â€“79

we summarize our contributions as follows:
(i) we combine a differentiable generative model controlled by a set of latent variables with the solution of a pde-constrained numerical solution of a physical
forward problem.
(ii) we use gradients obtained from the adjoint method and from neural network
back-propagation to perform approximate mala sampling of the posterior in
the lower-dimensional set of latent variables.
(iii) we illustrate the proposed inversion framework using a simple synthetic seismic
inversion problem and evaluate the resulting ensemble of model parameters.
(iv) the framework allows integration of additional information, such as the knowledge of geological facies along one-dimensional vertical boreholes.
(v) the proposed approach may be readily extended to a number of inverse problems
where gradients of the objective function with respect to input parameters can be
calculated.
the code, data and trained weights of the neural networks have been made available
under an open-source license.1

2 related work
tarantola (2005) cast the geophysical seismic inversion problem in a bayesian framework. mosegaard and tarantola (1995) presented a general methodology to perform
probabilistic inversion using monte carlo sampling. they used a metropolis rule
combined with a sampling of the prior to obtain the posterior distribution. in a similar
manner, sen and stoffa (1996) evaluated the use of gibbs sampling to obtain a posteriori model parameters and evaluate parameter uncertainties. mosegaard (1998) showed
that the general bayesian inversion approach of mosegaard and tarantola (1995) also
gives information on the ability to resolve geological features. geostatistical models
enable spatial relationships and dependencies of the petrophysical parameters to be
modeled and incorporated into a stochastic inversion framework (bortoli et al. 1993;
haas and dubrule 1994). bayesian linear inversion has been successfully applied
to infer petrophysical property distributions (grana and della rossa 2010). buland
and omre (2003) developed an approach to perform bayesian inversion for elastic
petrophysical properties in a linearized setting. grana et al. (2017) used a gaussian
mixture model for bayesian linear inversion from seismic and well data. stochastic
sampling of petrophysical properties conditioned to well-log data allows petrophysical
property distributions to be inferred using an appropriate sampling strategy such as
markov chain monte carlo (mcmc) (bosch et al. 2009). a fully integrated stochastic inversion method that allows direct inversion from seismic amplitude-versus-angle
(ava) data creates a direct link between observed seismic data and underlying rock
physics models (azevedo et al. 2018). geological modeling using multi-point statistics (guardiano and srivastava 1993) can be employed for inversion from seismic data
(gonzÃ¡lez et al. 2007) where geological features are represented by a set of representative training images. for a more extensive review of statistical inversion approaches
1 code repository: https://github.com/lukasmosser/stochastic_seismic_waveform_inversion.

123

math geosci (2020) 52:53â€“79

57

we refer to bosch et al. (2010) and the comprehensive overviews of dubrule (2003),
doyen (2007), azevedo and soares (2017).
in the case of nonlinear physics-based inversion schemes such as fwi, computation
of the solution to the forward problem is very expensive. therefore, computationally
efficient approximations to the full solution of the wave equation may allow efficient
solutions to complex geophysical inversion problems. neural networks have been
shown to be universal function approximators (hornik et al. 1989) and as such lend
themselves to use as possible proxy models for solutions to the geophysical forward
and inverse problem (hansen and cordua 2017).
the early work by rÃ¶th and tarantola (1994) presents an application of neural
networks to invert from acoustic time-domain seismic amplitude responses to a depth
profile of acoustic velocity in a supervised setting. they used pairs of synthetic data
and velocity models to train a multi-layer feed-forward neural network with the goal
of predicting acoustic velocities from recorded data only. they showed that neural
networks can produce high-resolution approximations to the solution of the inverse
problem based on representations of the input model parameters and resulting synthetic waveforms alone. in addition, they showed that neural networks can invert for
geophysical parameters in the presence of significant levels of acoustic noise.
representing the geophysical model parameters at each point in space quickly leads
to a large number of model parameters, especially in the case of three-dimensional
problems. berg and nystrÃ¶m (2017) represented the spatially varying coefficients that
govern the solution of a pde by a neural network. the neural network acts as an
approximation to the spatially varying coefficients characterized by the weights of the
neural network. the weights of the individual neurons are modified by leveraging the
adjoint-state equation in the reduced-dimensional space of network parameters rather
than at each spatial location of the computational grid.
hansen and cordua (2017) replaced the solution of the partial differential equation
by a neural network, enabling fast computation of forward models and facilitating a
solution to the inversion problem by monte carlo sampling. araya-polo et al. (2018)
used deep neural networks to perform a mapping between seismic features and the
underlying p-wave velocity domain; they validated their approach based on synthetic
examples. a number of applications of deep generative priors have recently been
presented in the context of computer vision for image reconstruction, linear (chang
et al. 2017) and bilinear (asim et al. 2018) inverse problems, and compressed sensing
(bora et al. 2017). mosser et al. (2017) proposed gans to generate three-dimensional
stochastic realizations of porous media from binary and grayscale computed tomography images (mosser et al. 2018b). these deep generative models can be further
conditioned to honor lower-dimensional features such as cross-sections or borehole
data (dupont et al. 2018; mosser et al. 2018a; chan and elsheikh 2018). for more general subsurface inverse problems, laloy et al. (2017) used a gan to create geological
models for hydrological inversion. inversion was performed using an adapted markov
chain monte carlo (mcmc) (laloy and vrugt 2012) algorithm where the generative
model was used as an unconditional prior to sample hydrological model parameters.
chan and elsheikh (2017) evaluated the applicability of wasserstein-gans to parameterize geological models for uncertainty propagation.

123

58

math geosci (2020) 52:53â€“79

mosser et al. (2018c) used a generative adversarial network with cycle constraints
(cyclegan) (zhu et al. 2017) to perform seismic inversion, formulating the inversion
task as a domain-transfer problem. their work used a cyclegan to map between the
seismic amplitude domain and p-wave velocity models. the cycle constraint ensures
that models obtained by transforming from the amplitude to p-wave velocity representation and back to the amplitude domain are consistent. because the p-wave velocity
models and seismic amplitudes are represented as a function of depth rather than depth
and time, respectively, this approach lends itself to stratigraphic inversion, where a
pre-existing velocity model is used to perform time-depth conversion of the seismic
amplitudes. richardson (2018) showed that a quasi-newtonian method can optimize
model parameters in the latent space of a pre-trained gan for a synthetic salt-body
benchmark dataset.

3 problem definition
3.1 bayesian inversion
in the bayesian framework of inverse problems, we aim to find the posterior of latent
variables z given the observed data dobs (fig. 2). the joint probability of the latent
variables z and observed data dobs is
p(z, dobs ) = p(dobs |z) p(z)

(1)

furthermore, by applying bayes rule, we define the posterior over the latent variables
z given the observed seismic data dobs
p(z|dobs ) =

p(dobs |z) p(z)
âˆ p(dobs |z) p(z)
p(dobs )

(2)

we express the observed data by assuming conditional independence between the
observed seismic data dseis and data observed at the wells dwell
p(dobs |z) = p(dseis |z) p(dwell |z)

Î¸
z

p de

(3)

dseis

m
earth models

dwell

fig. 2 graphical model of the geological inversion problem. the set of possible earth models is represented
by a generative model with parameters Î¸ (the parameters of the generator m âˆ¼ g Î¸ (z)). we obtain model
observations of the acoustic waves dseis via the deterministic pde, as well as partial observation of the
model parameters m from local information, for example, at boreholes dwell

123

math geosci (2020) 52:53â€“79

59

we represent the observed seismic data by
2
i)
dseis = s(m) + Îµ, Îµ âˆ¼ n (0, Ïƒseis

(4)

where s(m) = s(m(x)) = s(g Î¸ (z)), denoting the spatial model coordinates by x,
the seismic forward modeling operator by s, and the generative model by g Î¸ (z) with
parameters Î¸ . we assume a normally distributed noise term Îµ with zero mean and
standard deviation Ïƒseis equal to 25% of the standard deviation of the reference model
seismic amplitude data. the geological facies mfacies , the p-wave velocity m v p , and
the rock density mÏ represent the set of model parameters m. the model parameter
mfacies represents the probability of a geological facies occurring at a spatial location
x.
the aim is to generate samples of the posterior z âˆ¼ p(z|dobs ). we reformulate
the approach using an iterative approximate metropolis-adjusted langevin sampling
rule (mala-approx) with iteration number t as follows (roberts and tweedie 1996;
roberts and rosenthal 1998; nguyen et al. 2016)
zt+1 = zt + Î³t âˆ‡ log p(zt |dobs ) + Î·t

(5a)

zt+1 = zt + Î³t âˆ‡ log[ p(dseis |zt ) p(dwell |zt ) p(zt )] + Î·t
(5b)
zt+1 = zt + Î³t {âˆ‡ log p(dseis |zt ) + âˆ‡ log p(dwell |zt ) + âˆ‡ log p(zt )} + Î·t (5c)
where Î·t âˆ¼ n (0, 2Î³t i) is a sample from a gaussian distribution with variance proportional to the step size Î³t at mala iteration t. assuming a gaussian log-likelihood
of the seismic data given the latent variables log p(dseis |zt ) âˆ âˆ’s(g Î¸ (zt )) âˆ’ dseis 22
leads to the proposal rule of the mala approximation (nguyen et al. 2016) for the
case when only seismic observations dseis are considered
zt+1 = zt âˆ’ Î³t

âˆ‚s(g Î¸ (zt )) âˆ’ dseis 22 âˆ‚g Î¸ (zt )
+ Î³t âˆ‡ log p(zt ) + Î·t
âˆ‚g Î¸ (zt )
âˆ‚zt

(6)

using this sampling approach requires gradients of the data mismatch with respect
to model parameters, which are obtained by the adjoint-state method which will be
Î¸ (zt )
presented in the following section. the gradients of the model parameters âˆ‚gâˆ‚z
t
with respect to the latent variables are obtained by traditional neural network backpropagation. the gradient of the log-probability of the gaussian prior distribution of
latent variables âˆ‡ log p(zt ) can be interpreted as a regularization of the latent variables
against deviation from the gaussian prior assumption (creswell and bharath 2018).
we follow the mala step-proposal algorithm using an initial step size Î³t=0 =
10âˆ’2 for every model inference (xifara et al. 2013). to obtain valid samples of the
posterior, we furthermore anneal the step size from the initial value of Î³t=0 = 10âˆ’2
to Î³t=200 = 10âˆ’5 over 200 iterations.
where lower-dimensional information is available, such as at boreholes, the
geological models should honor both the seismic response and this additional lowerdimensional information. in this study, we additionally find samples of the posterior
facies at a one-dimensional
that reflect observed geological facies indicators dwell = mwell

123

60

math geosci (2020) 52:53â€“79

borehole. when including borehole information, the step-proposal corresponds to
âˆ‚s(g Î¸ (zt )) âˆ’ dseis 22 âˆ‚g Î¸ (zt )
âˆ‚g Î¸ (zt )
âˆ‚zt
facies
âˆ‚ log p(dwell = mwell |zt )
+ Î³t
+ Î³t âˆ‡ log p(zt ) + Î·t
âˆ‚zt

zt+1 = zt âˆ’ Î³t

(7)

where we obtain samples of the posterior given the observed seismic data dseis and
facies .
geological facies at the wells dwell = mwell
facies |z ) in eq. 7 represents the assumption
the additional term log p(dwell = mwell
t
of a bernoulli distribution for the facies as derived from the generator and observed
at the borehole.
3.2 adjoint-state method
we perform numerical solutions of the time-dependent acoustic wave equation given
a set of model parameters
f(u, m v p ) =

1
m v p (x)2

d2 u(x, t)
du(x, t)
âˆ’ q(x, xs , t) = 0 (8)
âˆ’ Î´u(x, t) + Î·
dt 2
dt

where u(x, t) is the unknown wave-field and m v p (x) is the acoustic p-wave velocity.
prevents reflections from domain boundaries and ensures
the dampening term Î· du(x,t)
dt
that waves dissipate laterally. we refer to the evaluation of f(u, m v p ) = 0 (eq. 8) as
the forward problem.
time-dependent source wavelets q(x, xs , t) are introduced at locations xs . we emulate the seismic acquisition process by placing regularly spaced acoustic receivers that
record the incoming wave-field at the top edge of the simulation domain (fig. 1). to
show the impact of adding additional information from the acoustic forward problem
to the posterior pdf of models, we perform bayesian inversion using the proposed
approach in a number of scenarios where we increase the number of acoustic shot data
from 2 to 27 acoustic sources.
to perform sampling according to the mala algorithm presented in eq. 6, we
seek to obtain a gradient of the following functional
j (m v p (x)) =

n sources


pr ed

dseis (m v p (x)), qi ) âˆ’ dseis 22

(9)

i=1
pred

where dseis and dseis are the predicted and observed seismic observations, respectively.
we augment the functional j (m v p (x)) by forming the lagrangian
l (m v p , u, Î») = j (m v p ) âˆ’ Î», f(u, m v p )

123

(10)

math geosci (2020) 52:53â€“79

61

differentiating l (m v p , u, Î») with respect to Î» leads to the state equation 8,
but differentiation with respect to the acoustic wave-field u leads to the adjoint state
equations (plessix 2006)



 t


âˆ‚ f u, m v p
pred
Î» = dseis âˆ’ dseis
âˆ‚u

(11)

showing that we obtain a similar back-propagation equation as that used to derive gradients in neural networks (lecun et al. 1988): the data mismatch is back-propagated
thanks to a linear equation in the adjoint state vector Î». by differentiating the
lagrangian in eq. 10 with respect to m(x) we obtain
âˆ‚s(g Î¸ (zt )) âˆ’ dseis 22
âˆ‚j
âˆ‚ f(u, m v p )
=
Î»,
=
âˆ‚g Î¸ (zt )
âˆ‚m v p
âˆ‚m v p

(12)

which is the gradient required to perform mala sampling of the posterior distribution
of latent variables, eq. 6.
we perform a numerical solution of the acoustic wave equation and the respective
adjoint computation using the domain-specific symbolic language devito (kukreja
et al. 2016; louboutin et al. 2017). the numerical solution is performed using a
fourth-order finite-difference scheme in space and second-order in time.

4 generative model
we use a generative model to sample realizations of spatially varying model parameters
m(x) âˆ¼ g Î¸ (z). these realizations are obtained by sampling a number of latent variable
vectors z. the associated model representations represent the a priori knowledge about
the spatially varying properties of the geological structures in the subsurface.
we model the prior distribution of the spatially varying model parameters m(x)
(sect. 3.1) by a generative adversarial network (gan) (goodfellow et al. 2014). gans
represent a generative model where the underlying probability density function is
implicitly defined by a set of training examples. to train gans, two functions are
required: a generator g Î¸ (z) and a discriminator dÏ‰ (m). the role of the generator
is to create random samples of an implicitly defined probability distribution that are
statistically indistinguishable from a set of training examples. the discriminatorâ€™s role
is to distinguish real samples from those created by the generator. both functions are
trained in a competitive two-player min-max game where the overall loss is defined
by
min max{emâˆ¼ pm [log dÏ‰ (m)]
Î¸

Ï‰

+ezâˆ¼ pz [log (1 âˆ’ dÏ‰ (g Î¸ (z)))]}

(13)

because of the opposing nature of the objective functions, training gans is inherently
unstable, and finding stable training methods remains an open research problem. nev-

123

62

math geosci (2020) 52:53â€“79

ertheless, a number of training methods have been proposed that allow more stable
training of gans. in this work we use a so-called wasserstein-gan (arjovsky et al.
2017; gulrajani et al. 2017; chan and elsheikh 2017), that seeks to minimize the
wasserstein distance between the generated and real probability distribution. we use
a lipschitz penalty term proposed by petzka et al. (2017) to stabilize training of the
wasserstein-gan. for the discriminator, keeping the parameters Î¸ of the generator
fixed, we minimize
min{ezâˆ¼ pz [dÏ‰ (g Î¸ (z))] âˆ’ emâˆ¼ pm [dÏ‰ (m)]
Ï‰

2
+ Î» l p emÌ‚âˆ¼ pmÌ‚ [ max 0, âˆ‡ dÏ‰ (mÌ‚) âˆ’ 1 ]}

(14)

where mÌ‚ is linear combination between a real and generated sample controlled by a
random variable Ï„ (petzka et al. 2017). for the generator, keeping the parameters of
the discriminator Ï‰ constant, we minimize
âˆ’ezâˆ¼ pz [dÏ‰ (g Î¸ (z))]

(15)

in our work we set Î» l p = 200 to train the generative model. we represent both
the generator and discriminator2 function by deep convolutional neural networks (see
appendix table 1). the generator uses a number of convolutional layers followed by
so-called pixel-shuffle transformations to create output models (shi et al. 2016).
the latent vector is parameterized as a multivariate standardized normal distribution
z âˆ¼ n (0, i)50Ã—1Ã—2
g Î¸ : z â†’ r3Ã—64Ã—128

(16a)
(16b)

because of the geological properties represented in our dataset, namely, geological
facies indicators mfacies , acoustic p-wave velocity m v p and density mÏ , the generator
must output three data channels. we represent the geological facies as the probability of
a spatial location belonging to a sandstone facies. to facilitate numerical stability of the
gan training process, we apply a hyperbolic tangent activation function and convert to
a probability mfacies for subsequent computation (eq. 7). we apply a hyperbolic tangent
activation function to model the output distribution of the p-wave model parameters
m v p . for rock density mÏ , a soft-plus activation function is used to ensure positive
values (appendix a.1). in this study, only the facies indicator mfacies and acoustic
p-wave velocity m v p are used in the inversion process.
the generator-discriminator pairing is trained on the set of training images
described in sect. 5. gan training required approximately 8 hours on eight nvidia
k80 graphics processing units. a set of samples obtained from the gan prior are
presented in appendix fig. 9. after training, the generator g Î¸ (z) and the forward
modeling operator s(m) are arranged in a fully differentiable computational graph.
to accommodate the sources and receivers of the acoustic forward modeling process
described in sect. 3 and fig. 1, we pad the output of the generator by a domain of
constant p-wave velocity.
2 in the wasserstein-gan literature, the discriminator is also termed a â€œcriticâ€.

123

math geosci (2020) 52:53â€“79

63

fig. 3 overview of the object-based model realization used as a reference model for evaluating the inversion
procedure. geological facies a distinguish between river channel bodies (light) and shale (dark). b acoustic
p-wave velocity v p and c rock density Ï are constant within river channels and vary by layer within shale

5 dataset
to demonstrate the proposed inversion method, we will use a model of a fluvialdominated system consisting of highly porous sandstones embedded in a fine-grained
shaly material. object-based models are commonly used to model such geological
systems (deutsch and wang 1996). they represent the fluvial environment as a set
of randomly located geometric objects following various size, shape, and property
distributions. we train a set of gans on a dataset of 10,000 realizations of twodimensional cross-sections of fluvial object-based models.
the individual cross-sections are created with an object-based model, where halfcircle sand-bodies follow a uniform width distribution. p-wave velocity and density are
constant within each channel-body, and their values are sampled independently from a
gaussian distribution for each individual channel-body. the locations of the channelbodies are determined by a uniform distribution in spatial location. the fine-grained
material surrounding the river systems comprises layers of single-pixel thickness,
where each layer has a constant value of acoustic p-wave velocity and density which
varies randomly from one layer to another and is sampled from a gaussian distribution.
we use a binary indicator variable to distinguish the two facies regions, river channel
versus shale matrix. the ratio of how much of a given cross-section is filled with river
channels compared with the overall area of the geological domain is a key property in
understanding the geological nature of these structures. this ratio follows a uniform
distribution from 30 to 60% in our dataset, and river channels are placed at random
until a cross-section meets the randomly sampled ratio.
a total of 10,000 training images were created as a training set for the gan. a
further 4000 images were retained as a test set to evaluate the inversion technique.
while training the generative model outlined in sect. 4, we monitor image quality
and output distribution for each of the modeled properties. the reference realization
(fig. 3) used to evaluate the bayesian inversion approach was chosen randomly from
the test set of object-based models. figure 3 shows a comparison of the distribution
of the three modeled properties: geological facies indicator, acoustic p-wave velocity,
and rock density for the reference model.

6 results
we evaluate the proposed method of inversion by sampling a set of latent variables
z determining the output of the generative model g Î¸ (z) (sect. 5, fig. 3). first, we

123

64

math geosci (2020) 52:53â€“79

fig. 4 a pixel-wise mean and b standard deviation of the ensemble of 100 models sampled unconditionally
from the prior (1) represented by the generator function g Î¸ (z). posterior ensemble of geological indicator
variables matched to the seismic representation of the reference model shown in fig. 3 for (2) two sources,
(3) two sources and a single borehole, (4) three sources, (5) nine sources, (6) 27 sources. source locations
are indicated by red diamonds and the borehole location by a blue circle. the reference model is indicated
by red contours

evaluate the generative model as a prior for representing possible earth models and
generating n = 100 unconditional samples (fig. 4-1, appendix fig. 9).
two cases of inversion are considered: inversion for the acoustic p-wave velocity
v p and combined inversion of acoustic velocity and of geological facies along a
borehole. in all the cases presented, we assume that density is a constant. for all
tests, we perform inversion using the approximate mala scheme. for the additional
borehole constraint, we require accuracy of geological facies of above 95% to be
accepted as a valid inverted sample. while lower errors in seismic mismatch and
borehole accuracy can be achieved, evaluating the forward problem and adjoint of
the partial differential equation comes at a high computational cost, and therefore a
cost-effectiveness trade-off was necessary.
for the first case of seismic inversion without borehole constraints, we perform simulations where the number of acoustic sources are increased. fewer acoustic sources
means that less of the domain is properly imaged, leading to high uncertainty in areas
where no incoming waves have been reflected and recorded by the receivers on the
surface. the acoustic sources and 128 receivers are equally spaced across the top edge
of the domain.

123

math geosci (2020) 52:53â€“79

65

fig. 5 comparison of a the seismic waveform based on the reference model acoustic velocity with b
the waveform of an inferred model with three seismic sources. the difference c in amplitude of the two
waveforms. color maps are scaled based on one standard deviation in amplitudes of the reference model
waveform (a)

in fig. 4 we show the pixel-wise mean (fig. 4a) and standard deviation (fig. 4b)
of 100 inferred models for an increasing number of acoustic sources (from 2 to 27
sources). as the total number of acoustic sources increases, we obtain a lower standard deviation for the resulting model ensembles. in the case of two acoustic sources
(fig. 4b-2), we find that close to the sources, there is a small variation among the
inferred models (dark shades), whereas the central area where no acoustic source has
been placed shows a very high degree of variation. this is confirmed by the three-source
case where a central acoustic source has been placed in addition to the sources on the
borders of the domain. lower variability in the inverted ensemble can be observed.
this correlates well with the bayesian interpretation of the inverse problem: where
acoustic sources allow the subsurface to be imaged, we arrive at a low standard deviation in the posterior ensemble of geological models, whereas within regions that are
only sparsely sampled by the acoustic sources, we expect the priorâ€”the unconditional
generative modelâ€”to be more prevalent, leading to a higher variability in geological features. as expected, when we increase the number of sources, we find overall
smaller variability in the resulting ensemble of inverted earth models. we observe only
marginal reduction in variability between the cases with 9 and 27 sources (fig. 4b, 5,
6). for all inversion scenarios considered, we present samples from the posterior in
the appendix (figs. 10, 11, 12, 13, 14).
in the case where lower-dimensional information such as a borehole was included
as an additional objective function constraining the generative model (fig. 4b-3), we
find a lower standard deviation around this borehole. the standard deviation along the
borehole is close to zero due to the per-realization 95% accuracy constraint. furthermore, there is a region of influence where the borehole constrains lateral features such
as channel bodies. this is shown by channel-shaped features of low standard deviation
at the top and bottom of the domain. comparison with the reference model (fig. 3-a)
shows that two channel bodies can be found along the one-dimensional feature.
for each generated realization we have recorded the ratio of the squared error norm
(eq. 6) and the squared norm of the noise in the seismic data (fig. 6) at each mala
sampling iteration. the global minimum of the data mismatch in the presence of
gaussian noise is reached when the objective function value is equal to the squared
norm of the noise in the data, i.e., at a ratio equal to 1 (fig. 6). in practice, we find
that performing 200 mala iterations leads to a sufficient reduction in the mismatch

123

66

math geosci (2020) 52:53â€“79

fig. 6 comparison of the ratio of the squared error norm and the squared norm of the gaussian noise. the
global minimum is reached at values of 1. shaded regions indicate Â± Ïƒ of the squared error ratio. we perform
200 approximate mala iterations to obtain samples of the posterior given seismic observations only, as
well as where borehole information and seismic observations are included. the step size was annealed to
very small values, leading to a stabilization of the squared error norm at the end of the sampling procedure

123

math geosci (2020) 52:53â€“79

67

of the seismic data, and as required by the approximate mala algorithm, the error
stabilizes as the step size is reduced.
because modern fwi methods come at very high computational cost for two- and
possibly three-dimensional inversion, a small number of required iterations is imperative. in further tests, reducing the number of iterations of the mala approximation or
simply optimizing by gradient descent, as performed by richardson (2018), enables
convergence to small errors, but this approach has been shown to lead to reduced
sample diversity (nguyen et al. 2016).

7 discussion
we have shown that it is possible to obtain posterior realizations inferred from the latent
space of a gan generator that honor seismic and well-bore data by using an approximate bayesian sampling method. a number of open questions remain concerning the
generative model and the posterior distribution of models that are obtained.
a common challenge with gans specifically is their so-called mode-collapse
behavior, where the distribution represented by the generative model has collapsed
to one or a few modes of the distribution implicitly represented by the set of training
images. gans do not represent the density explicitly, and therefore it is not possible
to evaluate the ability of a gan to represent the distribution by, for example, evaluating the likelihood of a set of test images given the model parameters p(m|Î¸ ). theis
et al. (2015) have shown that evaluating sample quality and diversity of generative
adversarial networks is difficult. nevertheless, a number of heuristic approaches have
been proposed, such as the inception score (is) (salimans et al. 2016) or the frechet
inception distance (fid) (heusel et al. 2017), and while these methods are popular for evaluating gans trained on natural images, they may not be representative
measures for comparing gans, as shown by barratt and sharma (2018). arora and
zhang (2017) propose a method to empirically evaluate the support of the distribution
represented by a gan.
another common failure case of gans occurs when the generator only memorizes
the images of the training set and does not learn a representation of the entire distribution. in this case, it should only be possible to infer models which are part of the
training set and which match the well and seismic data associated with the reference
model. in the following, we investigate whether the ensemble of models obtained by
solving the inverse problem represent new stochastic realizations of the underlying
distribution implicitly represented by the training images.
we have evaluated the mean-squared-error (mse) and the structural similarity index
(ssim) (wang et al. 2004) between pairs of binary facies models. a perfect agreement
between two models is reached for an mse of zero and an ssim of one. the mse,
while being a common measure for comparing pairs of data, is very sensitive to small
translations of the models that are compared. the structural similarity index attempts
to capture perceptual similarity and is less sensitive to pixel-wise differences in the
two compared models (wang and bovik 2009).
in fig. 7 we show kernel density estimates for the distributions of the two image
similarity measures. first, we compare the distribution of the mse and ssim between

123

68

math geosci (2020) 52:53â€“79

fig. 7 kernel density estimates of the distributions of the (left) mean-squared-error and (right) structural
similarity index (ssim) with respect to the reference model for models sampled from the gan prior, and
inferred models obtained by bayesian inversion

the reference model and the 104 models in the training set (ref.-ti) with that between
the reference model and 105 models sampled from the gan prior (ref.-prior). we
find that the two distributions match closely. this confirms that images drawn from
the gan prior and from the training set are statistically similar and that none of the
images from the training set and prior are likely to be identical to the reference model.
this finding is a good indication that the gan does not seem to have collapsed to
a few modes, but it does not exclude the possibility of our generative model having
memorized the training set, as in this case we would expect the distributions between
ref.-prior and ref.-ti to match.
in a second step we now compare the reference model to the models inferred by our
bayesian inversion approach using the gan as a prior. we find that the distributions
are all consistently shifted to regions of higher similarity to the reference model, i.e.,
lower mse and higher ssim for models inferred when considering the seismic data as
well as seismic and well data. this shows that our inversion, when the number of data
is increased, tends towards models that are increasingly similar to the reference model.
when 9 and 27 acoustic sources are used, we find that inversion leads to models that
on average have a ssim that has very low probability under the ref.-ti. and ref.-prior
distributions showing that our gan is able to create images outside the set of training
images. if the generator had only memorized the training set, we should not be able
to infer models with higher similarity as the number of data increased.
in fig. 8a we show models from the training set, samples from the gan prior,
and models inferred with the highest ssim when compared with the reference case
(fig. 3). in fig. 8b we show models that have an ssim close to the mode of the ssim
distributions and find that the model from the posterior inferred by inversion using 27
acoustic sources is visually more similar to the reference case (fig. 3) than the samples
obtained from the prior and from the training set.
it is important to note that the evaluation of the inferred models with respect to
a reference model is only possible in the case of synthetic data. in subsurface applications, it is not possible to obtain the entire reference model. furthermore, models
that are structurally very different can be valid solutions of the ill-posed inverse problem. these models, which represent possible solutions of the inverse problem, may be
associated with different modes of the prior distribution. in the case of gans, the gen-

123

math geosci (2020) 52:53â€“79

69

fig. 8 overview of models from the training set, gan prior, and inferred models using the mala
approach that a show the highest similarity to the reference model (fig. 3) measured by the ssim metric
and b represent realizations close to the mode of the distribution of the ssim

erator may be able to represent all of these modes or only a subset (mode-collapse). if
mode-collapse has occurred, the posterior ensemble only represents solutions obtained
from the modes represented by the generator. therefore, checking for the occurrence
of mode-collapse is key for practical applications, as mode-collapse may significantly
affect the ensemble of obtained solutions and possibly lead to underestimated uncertainty.
for future work, evaluating other deep generative models based on explicit density
representations (kingma and welling 2013; dinh et al. 2016; van den oord et al. 2016;
kingma and dhariwal 2018), which can calculate the likelihood of a set of test images,
may help to improve the representation of the prior distribution and mitigate the effect
of mode-collapse on inversion.

8 conclusions
inversion of subsurface geological heterogeneities from acoustic reflection seismic
data is a classical method designed to aid the understanding of the earthâ€™s interior. the
inference of model parameters from measured acoustic properties is often performed
in the very high-dimensional space of model properties, leading to very cpu-intensive
optimization (akcelik et al. 2003).
we apply a method that combines a generative model of geological heterogeneities
efficiently parameterized by a lower-dimensional set of latent variables, with a numerical solution of the acoustic inverse problem for seismic inversion using the adjoint
method. leveraging the adjoint of the studied partial differential equation, we deduce
gradients that are subsequently used to sample from the posterior over the latent variables given the mismatch of the observed seismic data by following an approximate
mala scheme (nguyen et al. 2016).

123

70

math geosci (2020) 52:53â€“79

while the proposed application was illustrated on a simple geophysical inversion,
this method may find use in other domains where spatial property models control the
evolution of physical systems, such as in fluid flow in porous media or materials
science. the combination of a deep generative model parameterized by a lowerdimensional set of latent variables and gradients obtained by the adjoint method may
lead to new efficient techniques for solving high-dimensional inverse problems.
acknowledgements o. dubrule would like to thank total s.a. for seconding him as visiting professor at
imperial college london.
open access this article is distributed under the terms of the creative commons attribution 4.0 international license (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,
and reproduction in any medium, provided you give appropriate credit to the original author(s) and the
source, provide a link to the creative commons license, and indicate if changes were made.

a appendix
a.1 generative model network architectures
see table 1 and figs. 9, 10, 11, 12, 13 and 14.
table 1 generator and
discriminator network
architectures used to create
synthetic geological structures.
geological facies are
represented by a binary indicator
variable and p-wave velocities
are represented by a bimodal
gaussian distribution. a
hyperbolic tangent activation
function is used to represent the
two families of properties. rock
density shows a gaussian
distribution. a soft-plus
activation function
( f (x) = Î²1 log(1 + exp(Î² x)),
Î² = 1) is used to ensure positive
values of density. notation for
convolutional layers: layertype
(number of filters),
k = kernel size, s = stride, p =
padding, bn =
batchnorm, ps = pixelshuffle

(a) multi-channel generator
latent variables z âˆˆ r50Ã—1Ã—2
conv2d(512)k3s1p1, bn, relu, psx2
conv2d(256)k3s1p1, bn, relu, psx2
conv2d(128)k3s1p1, bn, relu, psx2
conv2d(64)k3s1p1, bn, relu, psx2
conv2d(64)k3s1p1, bn, relu, psx2
conv2d(64)k3s1p1, bn, relu, psx2
conv2d(3)k3s1p1
tanh (0,1)â€”softplus (2)
(b) discriminator for multi-channel gan
geological properties m âˆˆ r3Ã—64Ã—128
conv2d(64)k5s2p2, relu
conv2d(64)k5s2p1, relu
conv2d(128)k3s2p1, relu
conv2d(256)k3s2p1, relu
conv2d(512)k3s2p1, relu
conv2d(512)k3s2p1, relu
conv2d(1)k3s1p1, relu

123

math geosci (2020) 52:53â€“79

71

fig. 9 samples from the prior distribution of models obtained from the gan with the reference model
(fig. 3) shown in the first row

123

72

math geosci (2020) 52:53â€“79

fig. 10 samples obtained from latent space optimization with two acoustic sources with the reference
model (fig. 3) shown in the first row

123

math geosci (2020) 52:53â€“79

73

fig. 11 samples obtained from latent space optimization with three acoustic sources with the reference
model (fig. 3) shown in the first row

123

74

math geosci (2020) 52:53â€“79

fig. 12 samples obtained from latent space optimization with nine acoustic sources with the reference
model (fig. 3) shown in the first row

123

math geosci (2020) 52:53â€“79

75

fig. 13 samples obtained from latent space optimization with 27 acoustic sources with the reference model
(fig. 3) shown in the first row

123

76

math geosci (2020) 52:53â€“79

fig. 14 samples obtained from latent space optimization with two acoustic sources and one borehole with
the reference model (fig. 3) shown in the first row

references
akcelik v, biros g, ghattas o (2002) parallel multiscale gaussâ€“newtonâ€“krylov methods for inverse wave
propagation. in: acm/ieee 2002 conference on supercomputing. ieee, pp 41â€“41
akcelik v, bielak j, biros g, epanomeritakis i, fernandez a, ghattas o, kim ej, lopez j, oâ€™hallaron
d, tu t (2003) high resolution forward and inverse earthquake modeling on terascale computers. in:
2003 acm/ieee conference on supercomputing. ieee, pp 52â€“52
araya-polo m, jennings j, adler a, dahlke t (2018) deep-learning tomography. lead edge 37(1):58â€“66
arjovsky m, chintala s, bottou l (2017) wasserstein gan. arxiv preprint arxiv:1701.07875
arora s, zhang y (2017) do gans actually learn the distribution? an empirical study. arxiv preprint
arxiv:1706.08224
asim m, shamshad f, ahmed a (2018) blind image deconvolution using deep generative priors. arxiv
preprint arxiv:1802.04073
azevedo l, soares a (2017) geostatistical methods for reservoir geophysics. springer, berlin
azevedo l, grana d, amaro c (2018) geostatistical rock physics ava inversion. geophys j int
216(3):1728â€“1739 issn 0956-540x
barratt s, sharma r (2018) a note on the inception score. arxiv preprint arxiv:1801.01973
berg j, nystrÃ¶mk (2017) neural network augmented inverse problems for pdes. arxiv preprint
arxiv:1712.09685

123

math geosci (2020) 52:53â€“79

77

bora a, jalal a, price e, dimakis ag (2017) compressed sensing using generative models. arxiv preprint
arxiv:1703.03208
bortoli lj, alabert f, haas a, journel a (1993) constraining stochastic images to seismic data. in: soares
a (ed) geostatistics trÃ³iaâ€™92. springer, dordrecht, pp 325â€“337
bosch m, carvajal c, rodrigues j, torres a, aldana m, sierra j (2009) petrophysical seismic inversion
conditioned to well-log data: methods and application to a gas reservoir. geophysics 74(2):1â€“15
bosch m, mukerji t, gonzalez ef (2010) seismic inversion for reservoir properties combining statistical
rock physics and geostatistics: a review. geophysics 75(5):75a165â€“75a176
buland a, omre h (2003) bayesian linearized avo inversion. geophysics 68(1):185â€“198
chan s, elsheikh ah (2017) parametrization and generation of geological models with generative adversarial networks. arxiv preprint arxiv:1708.01810
chan s, elsheikh ah (2018) parametric generation of conditional geological realizations using generative
neural networks. arxiv preprint arxiv:1807.05207
chang jhr, li cl, poczos b, vijaya kumar bvk, sankaranarayanan ac (2017) one network to solve them
allâ€”solving linear inverse problems using deep projection models. arxiv preprint arxiv:1703.09912
creswell a, bharath aa (2018) inverting the generator of a generative adversarial network. ieee trans
neural netw learn syst 30:1967â€“1974
deutsch cv, wang l (1996) hierarchical object-based stochastic modeling of fluvial reservoirs. math geol
28(7):857â€“880
dinh l, sohl-dickstein j, bengio s (2016) density estimation using real nvp. arxiv e-prints
doyen p (2007) seismic reservoir characterization: an earth modelling perspective, vol 2. eage, houten
dubrule o (2003) geostatistics for seismic data integration in earth models. european association of
geoscientists and engineers, houten
dupont e, zhang t, tilke p, liang l, bailey w (2018) generating realistic geology conditioned on physical
measurements with generative adversarial networks. arxiv preprint arxiv:1802.03065
gonzÃ¡lez ef, mukerji t, mavko g (2007) seismic inversion combining rock physics and multiple-point
geostatistics. geophysics 73(1):r11â€“r21
goodfellow i, pouget-abadie j, mirza m, xu b, warde-farley d, ozair s, courville a, bengio y (2014)
generative adversarial nets. in: advances in neural information processing systems, pp 2672â€“2680
grana d, della rossa e (2010) probabilistic petrophysical-properties estimation integrating statistical rock
physics with seismic inversion. geophysics 75(3):o21â€“o37
grana d, fjeldstad t, omre h (2017) bayesian gaussian mixture linear inversion for geophysical inverse
problems. math geosci 49(4):493â€“515
guardiano fb, srivastava rm (1993) multivariate geostatistics: beyond bivariate moments. in: soares a
(ed) geostatistics troiaâ€™92. springer, dordrecht, pp 133â€“144
gulrajani i, ahmed f, arjovsky m, dumoulin v, courville a (2017) improved training of wasserstein
gans. arxiv preprint arxiv:1704.00028
haas a, dubrule o (1994) geostatistical inversionâ€”a sequential method of stochastic reservoir modelling
constrained by seismic data. first break 12(11):561â€“569
hansen tm, cordua ks (2017) efficient monte-carlo sampling of inverse problems using a neural networkbased forward-applied to gpr crosshole traveltime inversion. geophys j int 211(3):1524â€“1533
heusel m, ramsauer h, unterthiner t, nessler b, hochreiter s (2017) gans trained by a two time-scale
update rule converge to a local nash equilibrium. in: advances in neural information processing
systems, pp 6626â€“6637
hornik k, stinchcombe m, white h (1989) multilayer feedforward networks are universal approximators.
neural netw 2(5):359â€“366
kabanikhin si (2008) definitions and examples of inverse and ill-posed problems. j inverse ill posed probl
16(4):317â€“357
kadu a, van leeuwen t, mulder w (2016) a parametric level-set approach for seismic full-waveform
inversion. in: seg technical program expanded abstracts 2016. society of exploration geophysicists,
pp 1146â€“1150
kingma dp, dhariwal p (2018) glow: generative flow with invertible 1 Ã— 1 convolutions. in: advances in
neural information processing systems, pp 10236â€“10245
kingma dp, welling m (2013) auto-encoding variational bayes. arxiv preprint arxiv:1312.6114
kukreja n, louboutin m, vieira f, luporini f, lange m, gorman g (2016) devito: automated fast finite
difference computation. arxiv preprint arxiv:1608.08658

123

78

math geosci (2020) 52:53â€“79

laloy e, vrugt ja (2012) high-dimensional posterior exploration of hydrologic models using multiple-try
dream(zs) and high-performance computing. water resour res 48(1):w01526. https://doi.org/10.
1029/2011wr010608
laloy e, hÃ©rault r, jacques d, linde n (2017) efficient training-image based geostatistical simulation and
inversion using a spatial generative adversarial neural network. arxiv preprint arxiv:1708.04975
lecun y, touresky d, hinton g, sejnowski t (1988) a theoretical framework for back-propagation.
in: proceedings of the 1988 connectionist models summer school, cmu, pittsburgh, pa. morgan
kaufmann, los altos, pp 21â€“28
louboutin m, witte p, lange m, kukreja n, luporini f, gorman g, herrmann fj (2017) full-waveform
inversion, part 1: forward modeling. lead edge 36(12):1033â€“1036
modrak r, tromp j (2015) computational efficiency of full waveform inversion algorithms. society of
exploration geophysicists, pp 4838â€“4842
mosegaard k (1998) resolution analysis of general inverse problems through inverse monte carlo sampling.
inverse probl 14(3):405
mosegaard k, tarantola a (1995) monte carlo sampling of solutions to inverse problems. j geophys res
solid earth 100(b7):12431â€“12447
mosser l, dubrule o, blunt mj (2017) reconstruction of three-dimensional porous media using generative
adversarial neural networks. phys rev e 96(4):043309
mosser l, dubrule o, blunt mj (2018a) conditioning of generative adversarial networks for pore and
reservoir scale models. in: 80th eage conference and exhibition 2018
mosser l, dubrule o, blunt mj (2018b) stochastic reconstruction of an oolitic limestone by generative
adversarial networks. transp porous media 125(1):81â€“103
mosser l, kimman w, dramsch j, purves s, de la fuente a, ganssle g (2018c) rapid seismic domain
transfer: seismic velocity inversion and modeling using deep generative neural networks. arxiv preprint
arxiv:1805.08826
nguyen a, clune j, bengio y, dosovitskiy a, yosinski j (2016) plug and play generative networks:
conditional iterative generation of images in latent space. arxiv preprint arxiv:1612.00005
petzka h, fischer a, lukovnicov d (2017) on the regularization of wasserstein gans. arxiv preprint
arxiv:1709.08894
plessix re (2006) a review of the adjoint-state method for computing the gradient of a functional with
geophysical applications. geophys j int 167(2):495â€“503
richardson a (2018) generative adversarial networks for model order reduction in seismic full-waveform
inversion. arxiv preprint arxiv:1806.00828
roberts go, rosenthal js (1998) optimal scaling of discrete approximations to langevin diffusions. j r
stat soc ser b (stat methodol) 60(1):255â€“268
roberts go, tweedie rl (1996) exponential convergence of langevin distributions and their discrete
approximations. bernoulli 2(4):341â€“363
rÃ¶th g, tarantola a (1994) neural networks and inversion of seismic data. j geophys res solid earth
99(b4):6753â€“6768
salimans t, goodfellow i, zaremba w, cheung v, radford a, chen x (2016) improved techniques for
training gans. in: advances in neural information processing systems, pp 2226â€“2234
sen mk, stoffa pl (1996) bayesian inference, gibbsâ€™ sampler and uncertainty estimation in geophysical
inversion. geophys prospect 44(2):313â€“350
shi w, caballero j, huszÃ¡r f, totz j, aitken ap, bishop r, rueckert d, wang z (2016) real-time single
image and video super-resolution using an efficient sub-pixel convolutional neural network. arxiv
preprint arxiv:1609.05158
tarantola a (2005) inverse problem theory and methods for model parameter estimation, vol 89. siam,
philadelphia
theis l, van den oord a, bethge m (2015) a note on the evaluation of generative models. arxiv preprint
arxiv:1511.01844
van den oord a, kalchbrenner n, espeholt l, vinyals o, graves a, others (2016) conditional image
generation with pixelcnn decoders. in: advances in neural information processing systems, pp 4790â€“
4798
wang z, bovik ac (2009) mean squared error: love it or leave it? a new look at signal fidelity measures.
ieee signal process mag 26(1):98â€“117
wang z, bovik ac, sheikh hr, simoncelli ep et al (2004) image quality assessment: from error visibility
to structural similarity. ieee trans image process 13(4):600â€“612

123

math geosci (2020) 52:53â€“79

79

xifara t, sherlock c, livingstone s, byrne s, girolami m (2013) langevin diffusions and the metropolisadjusted langevin algorithm. arxiv preprint arxiv:1309.2983
zhu jy, park t, isola p, efros aa (2017) unpaired image-to-image translation using cycle-consistent
adversarial networks. arxiv preprint arxiv:1703.10593

123



see discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/327612425

velocity model building with a modiï¬ed fully convolutional network
conference paper Â· august 2018
doi: 10.1190/segam2018-2997566.1

citations

reads

67

1,483

3 authors:
wenlong wang

fangshu yang

harbin institute of technology

harbin institute of technology at weihai

30 publications 919 citations

17 publications 693 citations

see profile

jianwei ma
peking university
200 publications 6,304 citations
see profile

all content following this page was uploaded by fangshu yang on 26 december 2018.
the user has requested enhancement of the downloaded file.

see profile

velocity model building with a modified fully convolutional network
wenlong wang, fangshu yang and jianwei ma, harbin institute of technology
summary
we introduce a machine learning based method to estimate
the p-wave velocity models directly from the prestack seismic traces using a modified fully convolutional network. the
network is tuned to map multi-shot seismic traces to velocity
models. we train the network with pairs of synthetic velocity models and their corresponding multi-shot seismic traces
which are simulated from the velocity models with acoustic
wave equations. multiple shots are used as channels in the network to increase data redundancy. the training process is expensive, but it only occurs once up front. the trained network
is then used to predict velocity models from testing seismic
traces, and it shows satisfactory prediction results. the cost
for predicting velocity models is negligible once the training is
complete.

in this paper, we work on the same problem of velocity model
building from prestack seismic traces, but we use a modified
fully convolutional network (fcn) (long et al., 2015), which
has fewer parameters and thus is more efficient than dnns.
multiple shots can be easily cooperated as channels. the paper is organized as follows, we first introduce the architecture
of the network for velocity model building using seismic data.
tests with synthetic data are then performed to compare the
results of the network trained with only 1 channel (1-shot network) and those trained with multiple channels (10-shot network). we show the improvements of training the network
with multiple shots as channels. to limit the scope of this paper, we focus on 2d isotropic acoustic models with a uniform
and constant density.

methodology
introduction
velocity model building is an essential step in seismic exploration. good velocity models are prerequisites for reverse time
migration (mcmechan, 1983) and other seismic imaging techniques. the estimated velocity models can also be used as
initial models to recursively generate high resolution velocity
models with optimization algorithms (tarantola, 1984). common practices for generating velocity models include tomography and full-waveform inversion (fwi). however, both tomography and fwi are time-consuming, computationally expensive, and rely heavily on human interactions and quality
control.
recent developments of machine learning (ml) technologies
provide the possibility to reduce or completely remove human
intervention from many formerly human-curated activities, for
example, image recognition, voice recognition and etc. geophysicists may also be empowered with those new technologies, and seek possible applications in seismic data processing
routines to minimize human intervenes.
many ml algorithms are built with artificial neural networks
(anns), which have a long history being used in geophysics.
however, most of the applications of anns focus on pattern
recognition in seismic attributes (zeng, 2004; zhao et al., 2015)
and faces classification in well logs (lim, 2005; hall, 2016). a
more challenging and interesting application is to input the network with prestack seismic traces and train the network to directly give geological interpretations of the subsurface. early
pioneers who implement anns for velocity estimation include
roÌˆth and tarantola (1994) and nath et al. (1999). in 2014,
zhang et al. (2014) propose to use neural networks for automatic fault prediction from seismic traces. araya-polo arayapolo et al. (2018) build velocity models from seismic traces
with a deep neural network (dnn) and a feature extraction
step to reduce computational cost.

velocity model building from seismic traces is challenging because it involves data transformation from seismic traces (x t) to the space/model domain (x - z), which poses as an inverse
problem. neural networks are capable of approximating any
continuous function up to a specified accuracy (hornick et al.,
1989), which provides the theoretical basis for this research.
fcn is a state-of-the-art network that performs pixel-wise semantic segmentation for images. it supplement a traditional
convolutional neural network (cnn) by successive upsampling
layers, thus the resolution of output is increased. a fcn usually consists of a series of convolutional units (convolution,
batch normalization and activation functions) and deconvolution layers for upsampling. the fcn architecture can also
be interpreted as a contracting path (encoder) and a expansive
path (decoder). detailed descriptions of its components can be
found in long et al. (2015).
to achieve velocity model building from seismic traces, we
made two major modifications to the fcn architecture (figure 1). first, the original fcn is designed for image segmentation and reads input images in grey or rgb color channels,
while for processing seismic data, we assign different shot
gathers, which are recorded from sources at different locations
in the same model, as channels for the input. so the number
of channels for the input is the same as the number of sources
for each model. thus, the multi-shot seismic data can be fed
into the network together to improve data redundancy. second, different from an original fcn architecture, whose output
and input are in the same (image) domain and share the same
size. we utilize the encoder-decoder architecture of the fcn
to perform domain transformation from seismic data domain
(x - t) to the model domain (x - z). to handle the size discrepancy between inputs and outputs, we truncate the final output
layer to the size of the model before applying the loss function. the proposed network in figure 1 is capable of training
itself during the contracting and expanding processes to map
the seismic data to the model domain.

64 64

128
128 128

256
256

512
512

512

1024

10 x 13

19 x 25

38 x 50

75 x 100

150 x 200

512

1024

512

128 128

64 64
1

256

512
150 x 80

256

256

1024
150 x 200

256

75 x 100

128

38 x 50

64

19 x 25

10

conv 3 x 3, batch normalization and relu
max pool 2 x 2
up-conv 2 x 2
conv 1 x 1 and harvest

figure 1: a modified fcn architecture for velocity model building using multi-shot seismic traces.

the loss function measures the distance between the prediction
and the ground truth (true velocity model). the loss function
that we use in the network is defined as the squared difference
(l2) between the ground truth velocity model v and the predicted velocity model vÌƒ
1x
e=
[v (x) âˆ’ vÌƒ (x)]2 ,
(1)
n x
where n is the number of grid points in the model, and x indicates the pixel positions. the ground truth velocity models v
is given during the training process, but are hidden for testing
after the training. note that the loss function is different from
that in a full wave inversion, in which the loss function measures the squared difference between observed and simulated
seismograms.
a popular variation of the fcn is called unet (ronneberger
et al., 2015). characterized by a u-shaped network with the a
symmetric shape of a contracting path and an expansive path,
unet is also equipped tunnels between the contracting and expansive layers [see figure 1 in ronneberger et al. (2015)] and
and demonstrates improved prediction accuracy with limited
training samples. a unet architecture is built for automatic
salt detection (wang et al., 2018), which is a classification
problem, with a cross-entropy loss function.

synthetic tests
in this section, we test the proposed algorithm with synthetic
data. we generate 1020 velocity models, and assign 1000 of
them as training samples, and the other 20 for testing. each of
the models have 2, 3 or 4 layers as background velocity, and
the velocity of each layer ranges arbitrarily from 2500 to 3500
m/s with smooth interface curvatures and increases with depth.
a salt body with an arbitrary shape and position is embedded
into each of the models, and the salt bodies have a constant
velocity of 4500 m/s. all the models have the same size of x
Ã— z = 150 Ã— 80 grid points with spatial interval h = 10 m. we
use eighth-order in space and second-order in time finite differencing scheme to solve the acoustic wave equation with a
15 hz ricker wavelet. convolutional perfectly matched layer
(cpml) absorbing boundary conditions (komatitsch and martin, 2007) are applied on all four grid edges to reduce unwanted
edge reflections.
for each model, 10 sources are evenly placed from (x, z) =
(0.2, 0.0) km to (1.3, 0.0) km and shot gathers are simulated

one after another. 150 receivers are evenly placed from (0.0,
0.7) km to (1.5, 0.7) km, which are at the bottom of the models, and form a top-bottom geometry. signals recorded by this
geometry are mostly transmitted waves. six sample models
(a) with a representative acquisition geometry and their corresponding seismograms (b) for training are plotted in figure 2.
to validate the benefits of using multiple shots as channels for
input, we train the network with one channel (the fifth shot of
the 10) and 10 channels (10 shots) as input separately. both
networks are trained with the 1000 training samples (pairs of
the shot gathers and velocity models), and the trainings are
terminated after 200 epochs.
the networks are trained on a workstation with one gforce
gtx 1080ti gpu and 64 gb ram. the generated seismic
traces and their corresponding true velocity models are used to
train the network with the stochastic gradient descent implementation of pytorch (www.pytorch.org).
the computational time for training is proportional to the size
of the seismic traces and number of training samples, and it
is also related with the number of channels and the complexity of the network. in this paper, the training of the 1-shot
network took 1.4 hours to complete 200 epochs, and the training of 10-shot network took 3.2 hours. once the trainings are
complete, it only takes a few seconds to apply the networks
to predict velocity models from seismic traces in the following testing process. figure 3 shows the normalized losses, or
the l2 norm between the predicted velocity model and ground
truth decrease during the learning process. the 10-shot network converges faster than the 1-shot network.
to see the evolution of the network, 10 network snapshots are
captured during the training of the 10-shot network. we apply
those partially trained networks to one of the test samples (the
first velocity model in figure 5a). the predicted velocity models using the 10 partially trained networks are shown in figure 4 with their corresponding epoch status labelled at the bottom of each prediction. it shows that the network learns to fit
the low wavenumber components of velocity models at early
stages of the training, and gradually fits the high wavenumber
details in the velocity models as the training proceeds. this
process is similar to a multi-scale fwi approach, but the mechanism is different as no initial model nor wave equations need
to be provided to the artificial neural network.
the trained networks are then tested with the 20 test samples.

(a)

position (km)
0.0

position (km)
1.5

0.0

(b)

position (km)

1.5

0.0

0.0

0.0

0.8

0.8

0.8

2.0

depth (km)

0.0

1.5

0.0
time (s)

4.5
velocity (km/s)

0.0

position (km)
1.5

0.8

0.0

0.0

0.8

0.8

figure 2: (a) six representative models for training, and their corresponding geometry seismograms (10 shots are generated for
each model and only the fifth one is shown). the red dots indicate the source positions, and blue squares are receivers.

1.0

1 shot
10 shots

position (km)

0.8
normalized loss

0.0

position (km)
1.5

0.0

1.5

0.0

0.6
0.4

0.8

0.2

epoch = 10

epoch = 30

epoch = 50

epoch = 70

0.0
0

25

50

75
100 125
num. of epoch

150

175

200

0.8

figure 3: the loss functions of 1-shot and 10-shot networks
decrease as the training proceeds.

to quantitatively analyze the accuracy of predictions, we choose
three horizontal positions x = 200 m, 600 m and 1100 m, respectively, and plot the 1-shot network predicted velocity values (red), 10-shot network predicted velocity values (green)
against the true velocity values (black) in velocity vs. depth
profiles (figure 6). the 10-shot network gives better velocity
prediction than the 1-shot network. considering the fact that
the loss functions of both 1-shot and 10-shot networks reach
the same level by the end of the training, it is reasonable to
assume the the 1-shot network is overfitted, and incorporating
more shots reduces the risk of overfitting.

4.5

depth (km)

six representative velocity models for testing are shown in figure 5(a), and 10 shot gathers are generated for each model with
the same acquisition geometries as in the training set. the test
results of the 1-shot network are shown in figure 5(b), and
the results of 10-shot network are shown in figure 5(c). the
results show good approximations of the velocity models predicted directly from the seismic traces.

0.0

velocity (km/s)

0.0

0.8

2.0

epoch = 90

epoch = 110

epoch = 130

epoch = 150

epoch = 170

epoch = 190

0.0

0.8
0.0

0.8

figure 4: the test results using partially trained network at
different epochs with 10 shots as input.

(a)

position (km)
0.0

(b)

position (km)
1.5

0.0

1.5

position (km)

0.0

(c)

position (km)
1.5

0.0

1.5

position (km)

0.0

position (km)
1.5

0.0

1.5

0.0

0.8
4.5

0.8

2.0

depth (km)

velocity (km/s)

0.0

0.0

0.8

figure 5: (a) six representative velocity models for testing, and the corresponding predictions of 1-shot network (b) and 10-shot
network (c) during the test.

conclusions

velocity (m/s)
2000
4000

velocity (m/s)
2000
4000

velocity (m/s)
2000
4000
gt
1 shot pd
10 shots pd

0
100
200

depth (m)

300
400
500
600

in this paper, we explore a data-driven technique that employs
a modified fully convolutional network to build velocity models directly from recorded seismic traces. multiple shots are
learned as input channels to provide redundant information for
predicting the velocity model, and this scheme helps to reduce
this risk of overfitting. once the network training is complete,
it is extremely fast to build velocity models with the trained
network and input seismic traces, and no human interventions
are involved. because the loss function is measured in the
model domain, and no seismograms are generated when using
the network to predict velocity models, so there is no problem of cycle skipping. network snapshots during the training shows that the low wavenumber component of the velocity
models are learnt first. the test results reach high accuracy
using synthetic data. due to the limited number of training
samples and oversimplicitied synthetic models and data, applications to real seismic data are still difficult, and may require
a large database composed of real seismic traces and corresponding formerly interpreted velocity models for training the
network.

700
800

acknowledgments
(a) x = 200 m

(b) x = 600 m

(c) x = 1100 m

figure 6: the predicted velocity models using 1-shot (red) and
10-shot (green) compared with the ground truth velocity model
(black) in velocity vs. depth profiles at three horizontal positions of the first test sample in figure 6a. â€gtâ€ represents
ground truth and â€pdâ€ is prediction.

the research leading to this paper is supported by the young
talent program (auga5710053217) from the harbin institute
of technology.

references
araya-polo, m., j. jennings, a. adler, and t. dahlke, 2018,
deep-learning tomography: the leading edge, 37, 58â€“66.
hall, b., 2016, facies classification using machine learning:
the leading edge, 35, 906â€“909.
hornick, k., m. stinchcombe, and h. white, 1989, multilayer
feedforward networks are universal approximators: neural
networks, 2, no. 5, 359â€“366.
komatitsch, d., and r. martin, 2007, an unsplit convolutional
perfectly matched layer improved at grazing incidence for
the seismic wave equation: geophysics, 72, no. 5, sm155â€“
sm167.
lim, j. s., 2005, reservoir properties determination using
fuzzy logic and neural networks from well data in offshore
korea: journal of petroleum science and engineering, 49,
182â€“192.
long, j., e. shelhamer, and t. darrell, 2015, fully convolutional models for smantic segmentation: arxiv:1411.4038.
mcmechan, g. a., 1983, migration by extrapolation of timedependent boundary values: geophysical prospecting, 31,
no. 03, 413â€“420.
nath, s. k., s. chakroborty, s. k. singh, and n. ganguly,
1999, velocity inversion in cross-hole seismic tomography by counterpropagation neural network, genetic algorithm and evolutionary programming techniques: geophysical journal international, 138, no. 01, 108â€“124.
ronneberger, o., p. fischer, and t. brox, 2015, u-net: convolutional networks for biomedical image segmentation:
arxiv:1505.0459v1.
roÌˆth, g., and a. tarantola, 1994, neural networks and inversion of seismic data: journal of geophysical research, 99,
no. b4, 6753â€“6769.
tarantola, a., 1984, inversion of seismic reflecton data in the
acoustic approximation: geophysics, 49, 1259â€“1266.
wang, w., f. yang, and j. ma, 2018, automatic salt detection
with machine learning: 80th annual international conference and exhibition, extended abstracts, eage, submitted.
zeng, h., 2004, seismic geomorphology-based classification:
the leading edge, 23, 644â€“688.
zhang, c., c. frogner, m. araya-polo, and d. hohl, 2014,
machine-learning based automated fault detection in seismic traces: 76th annual international conference and exhibition, extended abstracts, eage, 807â€“811.
zhao, t., v. jayaram, and k. j. marfurt, 2015, a comparison
of classification techniques for seismic facies recognition:
interpretation, 3, sae29â€“sae58.

view publication stats



